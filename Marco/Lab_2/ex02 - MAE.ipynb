{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## answers of the questions\n",
    "\n",
    "1) In the tx we have two columns: the first one tx[0] represents only 0-1, it describes if the persons we are talking about is a male (0) or a famale(1); the second column in equal to the height of the persson itesel.\n",
    "<br>\n",
    "<br>\n",
    "2) each row represents one person\n",
    "<br>\n",
    "<br>\n",
    "3) y = 3 x 1 and X = 3 x 2 ;  X[32] would represent the height of the person we are considering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Computing the Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the `compute_loss` function below:\n",
    "<a id='compute_loss'></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    e = y - np.dot(tx,w)\n",
    "    return 1/(2*len(y))*sum(abs(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0, w1 = generate_w(num_intervals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(y, tx, w0, w1):\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\n",
    "    losses = np.zeros((len(w0), len(w1)))\n",
    "    for i in range(len(w0)):\n",
    "        for j in range(len(w1)):\n",
    "            w = np.array([int(grid_w0[i]), int(grid_w1[j])] )\n",
    "            losses[i,j] = compute_loss(y,tx,w)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=3.975552817253795, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.156 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = y - np.dot(tx,w)\n",
    "dh[dh ==0] = random.uniform(-1,1)\n",
    "dh[dh>0] = 1\n",
    "dh[dh<0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+00,  -2.63753464e-15])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/len(y)*np.dot(np.transpose(tx),dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    dh = y - np.dot(tx,w)\n",
    "    dh[dh ==0] = random.uniform(-1,1)\n",
    "    dh[dh>0] = 1\n",
    "    dh[dh<0] = -1\n",
    "    return -1/len(y)*np.dot(np.transpose(tx),dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.00000000e+00,   2.63753464e-15])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_gradient(y, tx, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        G = compute_gradient(y, tx,w)\n",
    "        loss = compute_loss(y, tx,w)\n",
    "        w = w-gamma*G\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/499): loss=13.353038998947419, w0=90.0, w1=10.000000000000027\n",
      "Gradient Descent(1/499): loss=8.35581450879489, w0=80.048, w1=10.040192172320344\n",
      "Gradient Descent(2/499): loss=3.866739842172617, w0=73.37, w1=12.42605328415944\n",
      "Gradient Descent(3/499): loss=2.2661956746785332, w0=73.196, w1=14.343849276510966\n",
      "Gradient Descent(4/499): loss=2.23720036415093, w0=73.212, w1=13.084750162979685\n",
      "Gradient Descent(5/499): loss=2.2209589812828026, w0=73.26, w1=13.87372525280404\n",
      "Gradient Descent(6/499): loss=2.216780964097546, w0=73.218, w1=13.382967495732178\n",
      "Gradient Descent(7/499): loss=2.213239482009118, w0=73.266, w1=13.626079688515658\n",
      "Gradient Descent(8/499): loss=2.2126931771244, w0=73.23, w1=13.463723325741814\n",
      "Gradient Descent(9/499): loss=2.2124936581092505, w0=73.26, w1=13.580705380999392\n",
      "Gradient Descent(10/499): loss=2.212417156427818, w0=73.25, w1=13.505461047316814\n",
      "Gradient Descent(11/499): loss=2.2123412283796107, w0=73.23, w1=13.543879475852359\n",
      "Gradient Descent(12/499): loss=2.2123312388168097, w0=73.242, w1=13.527108493008408\n",
      "Gradient Descent(13/499): loss=2.212315871781176, w0=73.242, w1=13.526620676600471\n",
      "Gradient Descent(14/499): loss=2.212315859882922, w0=73.242, w1=13.526132860192535\n",
      "Gradient Descent(15/499): loss=2.21231584798468, w0=73.242, w1=13.525645043784598\n",
      "Gradient Descent(16/499): loss=2.2123158360864377, w0=73.242, w1=13.525157227376662\n",
      "Gradient Descent(17/499): loss=2.212315824188197, w0=73.242, w1=13.524669410968725\n",
      "Gradient Descent(18/499): loss=2.2123158161635823, w0=73.24000000000001, w1=13.524486706662053\n",
      "Gradient Descent(19/499): loss=2.212315807833648, w0=73.24000000000001, w1=13.523998890254116\n",
      "Gradient Descent(20/499): loss=2.2123157959354014, w0=73.24000000000001, w1=13.52351107384618\n",
      "Gradient Descent(21/499): loss=2.2123157943759524, w0=73.242, w1=13.526695497072813\n",
      "Gradient Descent(22/499): loss=2.21231586170786, w0=73.242, w1=13.526207680664877\n",
      "Gradient Descent(23/499): loss=2.2123158498096083, w0=73.242, w1=13.52571986425694\n",
      "Gradient Descent(24/499): loss=2.212315837911368, w0=73.242, w1=13.525232047849004\n",
      "Gradient Descent(25/499): loss=2.2123158260131253, w0=73.242, w1=13.524744231441067\n",
      "Gradient Descent(26/499): loss=2.2123158168470813, w0=73.24000000000001, w1=13.524561527134395\n",
      "Gradient Descent(27/499): loss=2.212315809658567, w0=73.24000000000001, w1=13.524073710726459\n",
      "Gradient Descent(28/499): loss=2.2123157977603367, w0=73.24000000000001, w1=13.523585894318522\n",
      "Gradient Descent(29/499): loss=2.2123157858620894, w0=73.24000000000001, w1=13.523098077910586\n",
      "Gradient Descent(30/499): loss=2.2123158601336614, w0=73.242, w1=13.526282501137219\n",
      "Gradient Descent(31/499): loss=2.2123158516345445, w0=73.242, w1=13.525794684729282\n",
      "Gradient Descent(32/499): loss=2.2123158397363025, w0=73.242, w1=13.525306868321346\n",
      "Gradient Descent(33/499): loss=2.212315827838055, w0=73.242, w1=13.52481905191341\n",
      "Gradient Descent(34/499): loss=2.212315817530597, w0=73.24000000000001, w1=13.524636347606737\n",
      "Gradient Descent(35/499): loss=2.2123158114835055, w0=73.24000000000001, w1=13.5241485311988\n",
      "Gradient Descent(36/499): loss=2.2123157995852702, w0=73.24000000000001, w1=13.523660714790864\n",
      "Gradient Descent(37/499): loss=2.212315787687027, w0=73.24000000000001, w1=13.523172898382928\n",
      "Gradient Descent(38/499): loss=2.2123158482206593, w0=73.242, w1=13.526357321609561\n",
      "Gradient Descent(39/499): loss=2.212315853459479, w0=73.242, w1=13.525869505201625\n",
      "Gradient Descent(40/499): loss=2.2123158415612494, w0=73.242, w1=13.525381688793688\n",
      "Gradient Descent(41/499): loss=2.2123158296629866, w0=73.242, w1=13.524893872385752\n",
      "Gradient Descent(42/499): loss=2.2123158182140994, w0=73.24000000000001, w1=13.52471116807908\n",
      "Gradient Descent(43/499): loss=2.212315813308444, w0=73.24000000000001, w1=13.524223351671143\n",
      "Gradient Descent(44/499): loss=2.2123158014102082, w0=73.24000000000001, w1=13.523735535263206\n",
      "Gradient Descent(45/499): loss=2.2123157895119587, w0=73.24000000000001, w1=13.52324771885527\n",
      "Gradient Descent(46/499): loss=2.2123158363076536, w0=73.242, w1=13.526432142081903\n",
      "Gradient Descent(47/499): loss=2.2123158552844155, w0=73.242, w1=13.525944325673967\n",
      "Gradient Descent(48/499): loss=2.212315843386172, w0=73.242, w1=13.52545650926603\n",
      "Gradient Descent(49/499): loss=2.2123158314879303, w0=73.242, w1=13.524968692858094\n",
      "Gradient Descent(50/499): loss=2.2123158195896875, w0=73.242, w1=13.524480876450157\n",
      "Gradient Descent(51/499): loss=2.212315814441285, w0=73.24000000000001, w1=13.524298172143485\n",
      "Gradient Descent(52/499): loss=2.2123158032351187, w0=73.24000000000001, w1=13.523810355735549\n",
      "Gradient Descent(53/499): loss=2.2123157913369043, w0=73.24000000000001, w1=13.523322539327612\n",
      "Gradient Descent(54/499): loss=2.2123158243946506, w0=73.242, w1=13.526506962554246\n",
      "Gradient Descent(55/499): loss=2.2123158571093477, w0=73.242, w1=13.526019146146309\n",
      "Gradient Descent(56/499): loss=2.212315845211107, w0=73.242, w1=13.525531329738373\n",
      "Gradient Descent(57/499): loss=2.2123158333128714, w0=73.242, w1=13.525043513330436\n",
      "Gradient Descent(58/499): loss=2.2123158214146064, w0=73.242, w1=13.5245556969225\n",
      "Gradient Descent(59/499): loss=2.212315815124781, w0=73.24000000000001, w1=13.524372992615827\n",
      "Gradient Descent(60/499): loss=2.212315805060073, w0=73.24000000000001, w1=13.52388517620789\n",
      "Gradient Descent(61/499): loss=2.2123157931618294, w0=73.24000000000001, w1=13.523397359799954\n",
      "Gradient Descent(62/499): loss=2.212315812481637, w0=73.242, w1=13.526581783026588\n",
      "Gradient Descent(63/499): loss=2.2123158589342697, w0=73.242, w1=13.526093966618651\n",
      "Gradient Descent(64/499): loss=2.2123158470360376, w0=73.242, w1=13.525606150210715\n",
      "Gradient Descent(65/499): loss=2.212315835137794, w0=73.242, w1=13.525118333802778\n",
      "Gradient Descent(66/499): loss=2.2123158232395546, w0=73.242, w1=13.524630517394842\n",
      "Gradient Descent(67/499): loss=2.2123158158082896, w0=73.24000000000001, w1=13.52444781308817\n",
      "Gradient Descent(68/499): loss=2.2123158068850017, w0=73.24000000000001, w1=13.523959996680233\n",
      "Gradient Descent(69/499): loss=2.212315794986753, w0=73.24000000000001, w1=13.523472180272297\n",
      "Gradient Descent(70/499): loss=2.21231580056865, w0=73.242, w1=13.52665660349893\n",
      "Gradient Descent(71/499): loss=2.212315860759211, w0=73.242, w1=13.526168787090993\n",
      "Gradient Descent(72/499): loss=2.212315848860981, w0=73.242, w1=13.525680970683057\n",
      "Gradient Descent(73/499): loss=2.2123158369627305, w0=73.242, w1=13.52519315427512\n",
      "Gradient Descent(74/499): loss=2.212315825064484, w0=73.242, w1=13.524705337867184\n",
      "Gradient Descent(75/499): loss=2.2123158164917838, w0=73.24000000000001, w1=13.524522633560512\n",
      "Gradient Descent(76/499): loss=2.212315808709939, w0=73.24000000000001, w1=13.524034817152575\n",
      "Gradient Descent(77/499): loss=2.2123157968116804, w0=73.24000000000001, w1=13.523547000744639\n",
      "Gradient Descent(78/499): loss=2.212315788655642, w0=73.242, w1=13.526731423971272\n",
      "Gradient Descent(79/499): loss=2.212315862584135, w0=73.242, w1=13.526243607563336\n",
      "Gradient Descent(80/499): loss=2.2123158506858958, w0=73.242, w1=13.5257557911554\n",
      "Gradient Descent(81/499): loss=2.2123158387876702, w0=73.242, w1=13.525267974747463\n",
      "Gradient Descent(82/499): loss=2.2123158268894136, w0=73.242, w1=13.524780158339526\n",
      "Gradient Descent(83/499): loss=2.2123158171752877, w0=73.24000000000001, w1=13.524597454032854\n",
      "Gradient Descent(84/499): loss=2.21231581053487, w0=73.24000000000001, w1=13.524109637624917\n",
      "Gradient Descent(85/499): loss=2.212315798636632, w0=73.24000000000001, w1=13.523621821216981\n",
      "Gradient Descent(86/499): loss=2.2123157867383876, w0=73.24000000000001, w1=13.523134004809044\n",
      "Gradient Descent(87/499): loss=2.212315854413332, w0=73.242, w1=13.526318428035678\n",
      "Gradient Descent(88/499): loss=2.212315852510836, w0=73.242, w1=13.525830611627741\n",
      "Gradient Descent(89/499): loss=2.212315840612584, w0=73.242, w1=13.525342795219805\n",
      "Gradient Descent(90/499): loss=2.2123158287143476, w0=73.242, w1=13.524854978811868\n",
      "Gradient Descent(91/499): loss=2.212315817858781, w0=73.24000000000001, w1=13.524672274505196\n",
      "Gradient Descent(92/499): loss=2.2123158123597917, w0=73.24000000000001, w1=13.52418445809726\n",
      "Gradient Descent(93/499): loss=2.2123158004615604, w0=73.24000000000001, w1=13.523696641689323\n",
      "Gradient Descent(94/499): loss=2.2123157885633096, w0=73.24000000000001, w1=13.523208825281387\n",
      "Gradient Descent(95/499): loss=2.2123158425003373, w0=73.242, w1=13.52639324850802\n",
      "Gradient Descent(96/499): loss=2.212315854335762, w0=73.242, w1=13.525905432100084\n",
      "Gradient Descent(97/499): loss=2.212315842437526, w0=73.242, w1=13.525417615692147\n",
      "Gradient Descent(98/499): loss=2.212315830539283, w0=73.242, w1=13.52492979928421\n",
      "Gradient Descent(99/499): loss=2.212315818641035, w0=73.242, w1=13.524441982876274\n",
      "Gradient Descent(100/499): loss=2.2123158140859642, w0=73.24000000000001, w1=13.524259278569602\n",
      "Gradient Descent(101/499): loss=2.2123158022864913, w0=73.24000000000001, w1=13.523771462161665\n",
      "Gradient Descent(102/499): loss=2.212315790388257, w0=73.24000000000001, w1=13.523283645753729\n",
      "Gradient Descent(103/499): loss=2.2123158305873267, w0=73.242, w1=13.526468068980362\n",
      "Gradient Descent(104/499): loss=2.212315856160706, w0=73.242, w1=13.525980252572426\n",
      "Gradient Descent(105/499): loss=2.212315844262454, w0=73.242, w1=13.52549243616449\n",
      "Gradient Descent(106/499): loss=2.212315832364214, w0=73.242, w1=13.525004619756553\n",
      "Gradient Descent(107/499): loss=2.212315820465967, w0=73.242, w1=13.524516803348616\n",
      "Gradient Descent(108/499): loss=2.212315814769467, w0=73.24000000000001, w1=13.524334099041944\n",
      "Gradient Descent(109/499): loss=2.212315804111419, w0=73.24000000000001, w1=13.523846282634008\n",
      "Gradient Descent(110/499): loss=2.212315792213179, w0=73.24000000000001, w1=13.523358466226071\n",
      "Gradient Descent(111/499): loss=2.2123158186743246, w0=73.242, w1=13.526542889452704\n",
      "Gradient Descent(112/499): loss=2.2123158579856295, w0=73.242, w1=13.526055073044768\n",
      "Gradient Descent(113/499): loss=2.212315846087383, w0=73.242, w1=13.525567256636831\n",
      "Gradient Descent(114/499): loss=2.21231583418915, w0=73.242, w1=13.525079440228895\n",
      "Gradient Descent(115/499): loss=2.2123158222908885, w0=73.242, w1=13.524591623820958\n",
      "Gradient Descent(116/499): loss=2.2123158154529787, w0=73.24000000000001, w1=13.524408919514286\n",
      "Gradient Descent(117/499): loss=2.2123158059363512, w0=73.24000000000001, w1=13.52392110310635\n",
      "Gradient Descent(118/499): loss=2.2123157940381115, w0=73.24000000000001, w1=13.523433286698413\n",
      "Gradient Descent(119/499): loss=2.2123158067613273, w0=73.242, w1=13.526617709925047\n",
      "Gradient Descent(120/499): loss=2.212315859810557, w0=73.242, w1=13.52612989351711\n",
      "Gradient Descent(121/499): loss=2.2123158479123313, w0=73.242, w1=13.525642077109174\n",
      "Gradient Descent(122/499): loss=2.2123158360140764, w0=73.242, w1=13.525154260701237\n",
      "Gradient Descent(123/499): loss=2.2123158241158296, w0=73.242, w1=13.5246664442933\n",
      "Gradient Descent(124/499): loss=2.2123158161364773, w0=73.24000000000001, w1=13.524483739986628\n",
      "Gradient Descent(125/499): loss=2.2123158077612857, w0=73.24000000000001, w1=13.523995923578692\n",
      "Gradient Descent(126/499): loss=2.2123157958630526, w0=73.24000000000001, w1=13.523508107170755\n",
      "Gradient Descent(127/499): loss=2.212315794848311, w0=73.242, w1=13.526692530397389\n",
      "Gradient Descent(128/499): loss=2.2123158616355063, w0=73.242, w1=13.526204713989452\n",
      "Gradient Descent(129/499): loss=2.212315849737248, w0=73.242, w1=13.525716897581516\n",
      "Gradient Descent(130/499): loss=2.212315837839005, w0=73.242, w1=13.52522908117358\n",
      "Gradient Descent(131/499): loss=2.2123158259407765, w0=73.242, w1=13.524741264765643\n",
      "Gradient Descent(132/499): loss=2.21231581681998, w0=73.24000000000001, w1=13.52455856045897\n",
      "Gradient Descent(133/499): loss=2.21231580958622, w0=73.24000000000001, w1=13.524070744051034\n",
      "Gradient Descent(134/499): loss=2.2123157976879853, w0=73.24000000000001, w1=13.523582927643098\n",
      "Gradient Descent(135/499): loss=2.2123157857897287, w0=73.24000000000001, w1=13.523095111235161\n",
      "Gradient Descent(136/499): loss=2.21231586060601, w0=73.242, w1=13.526279534461795\n",
      "Gradient Descent(137/499): loss=2.2123158515621926, w0=73.242, w1=13.525791718053858\n",
      "Gradient Descent(138/499): loss=2.212315839663939, w0=73.242, w1=13.525303901645922\n",
      "Gradient Descent(139/499): loss=2.2123158277657002, w0=73.242, w1=13.524816085237985\n",
      "Gradient Descent(140/499): loss=2.212315817503485, w0=73.24000000000001, w1=13.524633380931313\n",
      "Gradient Descent(141/499): loss=2.2123158114111625, w0=73.24000000000001, w1=13.524145564523376\n",
      "Gradient Descent(142/499): loss=2.212315799512908, w0=73.24000000000001, w1=13.52365774811544\n",
      "Gradient Descent(143/499): loss=2.212315787614662, w0=73.24000000000001, w1=13.523169931707503\n",
      "Gradient Descent(144/499): loss=2.2123158486930112, w0=73.242, w1=13.526354354934137\n",
      "Gradient Descent(145/499): loss=2.2123158533871075, w0=73.242, w1=13.5258665385262\n",
      "Gradient Descent(146/499): loss=2.212315841488884, w0=73.242, w1=13.525378722118264\n",
      "Gradient Descent(147/499): loss=2.212315829590636, w0=73.242, w1=13.524890905710327\n",
      "Gradient Descent(148/499): loss=2.21231581818699, w0=73.24000000000001, w1=13.524708201403655\n",
      "Gradient Descent(149/499): loss=2.212315813236089, w0=73.24000000000001, w1=13.524220384995719\n",
      "Gradient Descent(150/499): loss=2.2123158013378306, w0=73.24000000000001, w1=13.523732568587782\n",
      "Gradient Descent(151/499): loss=2.2123157894396015, w0=73.24000000000001, w1=13.523244752179846\n",
      "Gradient Descent(152/499): loss=2.212315836779998, w0=73.242, w1=13.526429175406479\n",
      "Gradient Descent(153/499): loss=2.212315855212042, w0=73.242, w1=13.525941358998542\n",
      "Gradient Descent(154/499): loss=2.2123158433138093, w0=73.242, w1=13.525453542590606\n",
      "Gradient Descent(155/499): loss=2.212315831415565, w0=73.242, w1=13.52496572618267\n",
      "Gradient Descent(156/499): loss=2.212315819517331, w0=73.242, w1=13.524477909774733\n",
      "Gradient Descent(157/499): loss=2.212315814414174, w0=73.24000000000001, w1=13.52429520546806\n",
      "Gradient Descent(158/499): loss=2.2123158031627788, w0=73.24000000000001, w1=13.523807389060124\n",
      "Gradient Descent(159/499): loss=2.212315791264547, w0=73.24000000000001, w1=13.523319572652188\n",
      "Gradient Descent(160/499): loss=2.21231582486699, w0=73.242, w1=13.526503995878821\n",
      "Gradient Descent(161/499): loss=2.212315857036974, w0=73.242, w1=13.526016179470885\n",
      "Gradient Descent(162/499): loss=2.212315845138735, w0=73.242, w1=13.525528363062948\n",
      "Gradient Descent(163/499): loss=2.212315833240501, w0=73.242, w1=13.525040546655012\n",
      "Gradient Descent(164/499): loss=2.2123158213422647, w0=73.242, w1=13.524552730247075\n",
      "Gradient Descent(165/499): loss=2.2123158150976816, w0=73.24000000000001, w1=13.524370025940403\n",
      "Gradient Descent(166/499): loss=2.212315804987708, w0=73.24000000000001, w1=13.523882209532466\n",
      "Gradient Descent(167/499): loss=2.2123157930894544, w0=73.24000000000001, w1=13.52339439312453\n",
      "Gradient Descent(168/499): loss=2.212315812954008, w0=73.242, w1=13.526578816351163\n",
      "Gradient Descent(169/499): loss=2.212315858861917, w0=73.242, w1=13.526090999943227\n",
      "Gradient Descent(170/499): loss=2.2123158469636777, w0=73.242, w1=13.52560318353529\n",
      "Gradient Descent(171/499): loss=2.2123158350654384, w0=73.242, w1=13.525115367127354\n",
      "Gradient Descent(172/499): loss=2.2123158231671782, w0=73.242, w1=13.524627550719417\n",
      "Gradient Descent(173/499): loss=2.2123158157811758, w0=73.24000000000001, w1=13.524444846412745\n",
      "Gradient Descent(174/499): loss=2.2123158068126396, w0=73.24000000000001, w1=13.523957030004809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(175/499): loss=2.2123157949143963, w0=73.24000000000001, w1=13.523469213596872\n",
      "Gradient Descent(176/499): loss=2.212315801040993, w0=73.242, w1=13.526653636823506\n",
      "Gradient Descent(177/499): loss=2.2123158606868447, w0=73.242, w1=13.526165820415569\n",
      "Gradient Descent(178/499): loss=2.2123158487886077, w0=73.242, w1=13.525678004007633\n",
      "Gradient Descent(179/499): loss=2.2123158368903724, w0=73.242, w1=13.525190187599696\n",
      "Gradient Descent(180/499): loss=2.2123158249921104, w0=73.242, w1=13.52470237119176\n",
      "Gradient Descent(181/499): loss=2.2123158164646837, w0=73.24000000000001, w1=13.524519666885087\n",
      "Gradient Descent(182/499): loss=2.212315808637577, w0=73.24000000000001, w1=13.52403185047715\n",
      "Gradient Descent(183/499): loss=2.212315796739339, w0=73.24000000000001, w1=13.523544034069214\n",
      "Gradient Descent(184/499): loss=2.2123157891280147, w0=73.242, w1=13.526728457295848\n",
      "Gradient Descent(185/499): loss=2.2123158625117787, w0=73.242, w1=13.526240640887911\n",
      "Gradient Descent(186/499): loss=2.2123158506135483, w0=73.242, w1=13.525752824479975\n",
      "Gradient Descent(187/499): loss=2.2123158387152912, w0=73.242, w1=13.525265008072038\n",
      "Gradient Descent(188/499): loss=2.2123158268170635, w0=73.242, w1=13.524777191664102\n",
      "Gradient Descent(189/499): loss=2.21231581714818, w0=73.24000000000001, w1=13.52459448735743\n",
      "Gradient Descent(190/499): loss=2.2123158104624983, w0=73.24000000000001, w1=13.524106670949493\n",
      "Gradient Descent(191/499): loss=2.2123157985642616, w0=73.24000000000001, w1=13.523618854541557\n",
      "Gradient Descent(192/499): loss=2.2123157866660215, w0=73.24000000000001, w1=13.52313103813362\n",
      "Gradient Descent(193/499): loss=2.2123158548856905, w0=73.242, w1=13.526315461360253\n",
      "Gradient Descent(194/499): loss=2.2123158524384703, w0=73.242, w1=13.525827644952317\n",
      "Gradient Descent(195/499): loss=2.2123158405402394, w0=73.242, w1=13.52533982854438\n",
      "Gradient Descent(196/499): loss=2.2123158286419944, w0=73.242, w1=13.524852012136444\n",
      "Gradient Descent(197/499): loss=2.21231581783168, w0=73.24000000000001, w1=13.524669307829772\n",
      "Gradient Descent(198/499): loss=2.2123158122874376, w0=73.24000000000001, w1=13.524181491421835\n",
      "Gradient Descent(199/499): loss=2.2123158003891876, w0=73.24000000000001, w1=13.523693675013899\n",
      "Gradient Descent(200/499): loss=2.212315788490965, w0=73.24000000000001, w1=13.523205858605962\n",
      "Gradient Descent(201/499): loss=2.212315842972683, w0=73.242, w1=13.526390281832596\n",
      "Gradient Descent(202/499): loss=2.2123158542634, w0=73.242, w1=13.52590246542466\n",
      "Gradient Descent(203/499): loss=2.2123158423651725, w0=73.242, w1=13.525414649016723\n",
      "Gradient Descent(204/499): loss=2.212315830466933, w0=73.242, w1=13.524926832608786\n",
      "Gradient Descent(205/499): loss=2.212315818568671, w0=73.242, w1=13.52443901620085\n",
      "Gradient Descent(206/499): loss=2.212315814058883, w0=73.24000000000001, w1=13.524256311894177\n",
      "Gradient Descent(207/499): loss=2.212315802214132, w0=73.24000000000001, w1=13.523768495486241\n",
      "Gradient Descent(208/499): loss=2.21231579031589, w0=73.24000000000001, w1=13.523280679078304\n",
      "Gradient Descent(209/499): loss=2.2123158310596884, w0=73.242, w1=13.526465102304938\n",
      "Gradient Descent(210/499): loss=2.212315856088337, w0=73.242, w1=13.525977285897001\n",
      "Gradient Descent(211/499): loss=2.212315844190091, w0=73.242, w1=13.525489469489065\n",
      "Gradient Descent(212/499): loss=2.2123158322918557, w0=73.242, w1=13.525001653081128\n",
      "Gradient Descent(213/499): loss=2.21231582039362, w0=73.242, w1=13.524513836673192\n",
      "Gradient Descent(214/499): loss=2.2123158147423787, w0=73.24000000000001, w1=13.52433113236652\n",
      "Gradient Descent(215/499): loss=2.2123158040390494, w0=73.24000000000001, w1=13.523843315958583\n",
      "Gradient Descent(216/499): loss=2.2123157921408234, w0=73.24000000000001, w1=13.523355499550647\n",
      "Gradient Descent(217/499): loss=2.2123158191466765, w0=73.242, w1=13.52653992277728\n",
      "Gradient Descent(218/499): loss=2.2123158579132673, w0=73.242, w1=13.526052106369344\n",
      "Gradient Descent(219/499): loss=2.2123158460150263, w0=73.242, w1=13.525564289961407\n",
      "Gradient Descent(220/499): loss=2.2123158341168003, w0=73.242, w1=13.52507647355347\n",
      "Gradient Descent(221/499): loss=2.2123158222185433, w0=73.242, w1=13.524588657145534\n",
      "Gradient Descent(222/499): loss=2.212315815425874, w0=73.24000000000001, w1=13.524405952838862\n",
      "Gradient Descent(223/499): loss=2.2123158058639985, w0=73.24000000000001, w1=13.523918136430925\n",
      "Gradient Descent(224/499): loss=2.212315793965744, w0=73.24000000000001, w1=13.523430320022989\n",
      "Gradient Descent(225/499): loss=2.212315807233685, w0=73.242, w1=13.526614743249622\n",
      "Gradient Descent(226/499): loss=2.212315859738204, w0=73.242, w1=13.526126926841686\n",
      "Gradient Descent(227/499): loss=2.212315847839969, w0=73.242, w1=13.52563911043375\n",
      "Gradient Descent(228/499): loss=2.2123158359417237, w0=73.242, w1=13.525151294025813\n",
      "Gradient Descent(229/499): loss=2.2123158240434684, w0=73.242, w1=13.524663477617876\n",
      "Gradient Descent(230/499): loss=2.2123158161093803, w0=73.24000000000001, w1=13.524480773311204\n",
      "Gradient Descent(231/499): loss=2.21231580768892, w0=73.24000000000001, w1=13.523992956903268\n",
      "Gradient Descent(232/499): loss=2.2123157957906794, w0=73.24000000000001, w1=13.523505140495331\n",
      "Gradient Descent(233/499): loss=2.2123157953206825, w0=73.242, w1=13.526689563721964\n",
      "Gradient Descent(234/499): loss=2.2123158615631375, w0=73.242, w1=13.526201747314028\n",
      "Gradient Descent(235/499): loss=2.2123158496649094, w0=73.242, w1=13.525713930906091\n",
      "Gradient Descent(236/499): loss=2.212315837766651, w0=73.242, w1=13.525226114498155\n",
      "Gradient Descent(237/499): loss=2.2123158258684117, w0=73.242, w1=13.524738298090218\n",
      "Gradient Descent(238/499): loss=2.2123158167928767, w0=73.24000000000001, w1=13.524555593783546\n",
      "Gradient Descent(239/499): loss=2.2123158095138638, w0=73.24000000000001, w1=13.52406777737561\n",
      "Gradient Descent(240/499): loss=2.2123157976156107, w0=73.24000000000001, w1=13.523579960967673\n",
      "Gradient Descent(241/499): loss=2.2123157857173736, w0=73.24000000000001, w1=13.523092144559737\n",
      "Gradient Descent(242/499): loss=2.2123158610783666, w0=73.242, w1=13.52627656778637\n",
      "Gradient Descent(243/499): loss=2.2123158514898216, w0=73.242, w1=13.525788751378434\n",
      "Gradient Descent(244/499): loss=2.2123158395915823, w0=73.242, w1=13.525300934970497\n",
      "Gradient Descent(245/499): loss=2.212315827693343, w0=73.242, w1=13.52481311856256\n",
      "Gradient Descent(246/499): loss=2.2123158174763833, w0=73.24000000000001, w1=13.524630414255888\n",
      "Gradient Descent(247/499): loss=2.212315811338794, w0=73.24000000000001, w1=13.524142597847952\n",
      "Gradient Descent(248/499): loss=2.212315799440557, w0=73.24000000000001, w1=13.523654781440015\n",
      "Gradient Descent(249/499): loss=2.212315787542306, w0=73.24000000000001, w1=13.523166965032079\n",
      "Gradient Descent(250/499): loss=2.212315849165364, w0=73.242, w1=13.526351388258712\n",
      "Gradient Descent(251/499): loss=2.2123158533147618, w0=73.242, w1=13.525863571850776\n",
      "Gradient Descent(252/499): loss=2.2123158414165225, w0=73.242, w1=13.52537575544284\n",
      "Gradient Descent(253/499): loss=2.2123158295182734, w0=73.242, w1=13.524887939034903\n",
      "Gradient Descent(254/499): loss=2.212315818159875, w0=73.24000000000001, w1=13.52470523472823\n",
      "Gradient Descent(255/499): loss=2.2123158131637224, w0=73.24000000000001, w1=13.524217418320294\n",
      "Gradient Descent(256/499): loss=2.212315801265479, w0=73.24000000000001, w1=13.523729601912358\n",
      "Gradient Descent(257/499): loss=2.212315789367237, w0=73.24000000000001, w1=13.523241785504421\n",
      "Gradient Descent(258/499): loss=2.2123158372523677, w0=73.242, w1=13.526426208731055\n",
      "Gradient Descent(259/499): loss=2.2123158551396878, w0=73.242, w1=13.525938392323118\n",
      "Gradient Descent(260/499): loss=2.2123158432414503, w0=73.242, w1=13.525450575915182\n",
      "Gradient Descent(261/499): loss=2.212315831343202, w0=73.242, w1=13.524962759507245\n",
      "Gradient Descent(262/499): loss=2.2123158194449606, w0=73.242, w1=13.524474943099309\n",
      "Gradient Descent(263/499): loss=2.212315814387076, w0=73.24000000000001, w1=13.524292238792636\n",
      "Gradient Descent(264/499): loss=2.21231580309042, w0=73.24000000000001, w1=13.5238044223847\n",
      "Gradient Descent(265/499): loss=2.2123157911921756, w0=73.24000000000001, w1=13.523316605976763\n",
      "Gradient Descent(266/499): loss=2.2123158253393456, w0=73.242, w1=13.526501029203397\n",
      "Gradient Descent(267/499): loss=2.2123158569646324, w0=73.242, w1=13.52601321279546\n",
      "Gradient Descent(268/499): loss=2.2123158450663785, w0=73.242, w1=13.525525396387524\n",
      "Gradient Descent(269/499): loss=2.2123158331681347, w0=73.242, w1=13.525037579979587\n",
      "Gradient Descent(270/499): loss=2.212315821269896, w0=73.242, w1=13.52454976357165\n",
      "Gradient Descent(271/499): loss=2.212315815070587, w0=73.24000000000001, w1=13.524367059264979\n",
      "Gradient Descent(272/499): loss=2.2123158049153453, w0=73.24000000000001, w1=13.523879242857042\n",
      "Gradient Descent(273/499): loss=2.212315793017103, w0=73.24000000000001, w1=13.523391426449106\n",
      "Gradient Descent(274/499): loss=2.212315813426358, w0=73.242, w1=13.526575849675739\n",
      "Gradient Descent(275/499): loss=2.2123158587895606, w0=73.242, w1=13.526088033267802\n",
      "Gradient Descent(276/499): loss=2.2123158468913315, w0=73.242, w1=13.525600216859866\n",
      "Gradient Descent(277/499): loss=2.2123158349930736, w0=73.242, w1=13.52511240045193\n",
      "Gradient Descent(278/499): loss=2.2123158230948303, w0=73.242, w1=13.524624584043993\n",
      "Gradient Descent(279/499): loss=2.2123158157540765, w0=73.24000000000001, w1=13.52444187973732\n",
      "Gradient Descent(280/499): loss=2.2123158067402784, w0=73.24000000000001, w1=13.523954063329384\n",
      "Gradient Descent(281/499): loss=2.2123157948420364, w0=73.24000000000001, w1=13.523466246921448\n",
      "Gradient Descent(282/499): loss=2.212315801513358, w0=73.242, w1=13.526650670148081\n",
      "Gradient Descent(283/499): loss=2.2123158606145017, w0=73.242, w1=13.526162853740145\n",
      "Gradient Descent(284/499): loss=2.2123158487162473, w0=73.242, w1=13.525675037332208\n",
      "Gradient Descent(285/499): loss=2.2123158368180116, w0=73.242, w1=13.525187220924272\n",
      "Gradient Descent(286/499): loss=2.2123158249197576, w0=73.242, w1=13.524699404516335\n",
      "Gradient Descent(287/499): loss=2.2123158164375876, w0=73.24000000000001, w1=13.524516700209663\n",
      "Gradient Descent(288/499): loss=2.2123158085652186, w0=73.24000000000001, w1=13.524028883801726\n",
      "Gradient Descent(289/499): loss=2.2123157966669678, w0=73.24000000000001, w1=13.52354106739379\n",
      "Gradient Descent(290/499): loss=2.2123157896003485, w0=73.242, w1=13.526725490620423\n",
      "Gradient Descent(291/499): loss=2.212315862439422, w0=73.242, w1=13.526237674212487\n",
      "Gradient Descent(292/499): loss=2.212315850541184, w0=73.242, w1=13.52574985780455\n",
      "Gradient Descent(293/499): loss=2.212315838642938, w0=73.242, w1=13.525262041396614\n",
      "Gradient Descent(294/499): loss=2.212315826744707, w0=73.242, w1=13.524774224988677\n",
      "Gradient Descent(295/499): loss=2.2123158171210786, w0=73.24000000000001, w1=13.524591520682005\n",
      "Gradient Descent(296/499): loss=2.2123158103901446, w0=73.24000000000001, w1=13.524103704274069\n",
      "Gradient Descent(297/499): loss=2.2123157984918977, w0=73.24000000000001, w1=13.523615887866132\n",
      "Gradient Descent(298/499): loss=2.212315786593662, w0=73.24000000000001, w1=13.523128071458196\n",
      "Gradient Descent(299/499): loss=2.212315855358041, w0=73.242, w1=13.526312494684829\n",
      "Gradient Descent(300/499): loss=2.2123158523661064, w0=73.242, w1=13.525824678276892\n",
      "Gradient Descent(301/499): loss=2.2123158404678667, w0=73.242, w1=13.525336861868956\n",
      "Gradient Descent(302/499): loss=2.212315828569625, w0=73.242, w1=13.52484904546102\n",
      "Gradient Descent(303/499): loss=2.2123158178045816, w0=73.24000000000001, w1=13.524666341154347\n",
      "Gradient Descent(304/499): loss=2.2123158122150772, w0=73.24000000000001, w1=13.52417852474641\n",
      "Gradient Descent(305/499): loss=2.212315800316844, w0=73.24000000000001, w1=13.523690708338474\n",
      "Gradient Descent(306/499): loss=2.212315788418598, w0=73.24000000000001, w1=13.523202891930538\n",
      "Gradient Descent(307/499): loss=2.2123158434450505, w0=73.242, w1=13.526387315157171\n",
      "Gradient Descent(308/499): loss=2.2123158541910466, w0=73.242, w1=13.525899498749235\n",
      "Gradient Descent(309/499): loss=2.2123158422928166, w0=73.242, w1=13.525411682341298\n",
      "Gradient Descent(310/499): loss=2.2123158303945645, w0=73.242, w1=13.524923865933362\n",
      "Gradient Descent(311/499): loss=2.21231581849632, w0=73.242, w1=13.524436049525425\n",
      "Gradient Descent(312/499): loss=2.21231581403177, w0=73.24000000000001, w1=13.524253345218753\n",
      "Gradient Descent(313/499): loss=2.2123158021417635, w0=73.24000000000001, w1=13.523765528810817\n",
      "Gradient Descent(314/499): loss=2.2123157902435167, w0=73.24000000000001, w1=13.52327771240288\n",
      "Gradient Descent(315/499): loss=2.212315831532039, w0=73.242, w1=13.526462135629513\n",
      "Gradient Descent(316/499): loss=2.2123158560159673, w0=73.242, w1=13.525974319221577\n",
      "Gradient Descent(317/499): loss=2.2123158441177213, w0=73.242, w1=13.52548650281364\n",
      "Gradient Descent(318/499): loss=2.2123158322194976, w0=73.242, w1=13.524998686405704\n",
      "Gradient Descent(319/499): loss=2.2123158203212454, w0=73.242, w1=13.524510869997767\n",
      "Gradient Descent(320/499): loss=2.21231581471527, w0=73.24000000000001, w1=13.524328165691095\n",
      "Gradient Descent(321/499): loss=2.2123158039666926, w0=73.24000000000001, w1=13.523840349283159\n",
      "Gradient Descent(322/499): loss=2.2123157920684666, w0=73.24000000000001, w1=13.523352532875222\n",
      "Gradient Descent(323/499): loss=2.212315819619054, w0=73.242, w1=13.526536956101856\n",
      "Gradient Descent(324/499): loss=2.212315857840913, w0=73.242, w1=13.526049139693919\n",
      "Gradient Descent(325/499): loss=2.21231584594266, w0=73.242, w1=13.525561323285983\n",
      "Gradient Descent(326/499): loss=2.212315834044429, w0=73.242, w1=13.525073506878046\n",
      "Gradient Descent(327/499): loss=2.2123158221461905, w0=73.242, w1=13.52458569047011\n",
      "Gradient Descent(328/499): loss=2.2123158153987696, w0=73.24000000000001, w1=13.524402986163437\n",
      "Gradient Descent(329/499): loss=2.212315805791642, w0=73.24000000000001, w1=13.523915169755501\n",
      "Gradient Descent(330/499): loss=2.21231579389339, w0=73.24000000000001, w1=13.523427353347564\n",
      "Gradient Descent(331/499): loss=2.2123158077060348, w0=73.242, w1=13.526611776574198\n",
      "Gradient Descent(332/499): loss=2.2123158596658494, w0=73.242, w1=13.526123960166261\n",
      "Gradient Descent(333/499): loss=2.212315847767596, w0=73.242, w1=13.525636143758325\n",
      "Gradient Descent(334/499): loss=2.212315835869368, w0=73.242, w1=13.525148327350388\n",
      "Gradient Descent(335/499): loss=2.2123158239711174, w0=73.242, w1=13.524660510942452\n",
      "Gradient Descent(336/499): loss=2.2123158160822807, w0=73.24000000000001, w1=13.52447780663578\n",
      "Gradient Descent(337/499): loss=2.212315807616559, w0=73.24000000000001, w1=13.523989990227843\n",
      "Gradient Descent(338/499): loss=2.212315795718331, w0=73.24000000000001, w1=13.523502173819907\n",
      "Gradient Descent(339/499): loss=2.2123157957930366, w0=73.242, w1=13.52668659704654\n",
      "Gradient Descent(340/499): loss=2.212315861490781, w0=73.242, w1=13.526198780638603\n",
      "Gradient Descent(341/499): loss=2.212315849592532, w0=73.242, w1=13.525710964230667\n",
      "Gradient Descent(342/499): loss=2.2123158376942995, w0=73.242, w1=13.52522314782273\n",
      "Gradient Descent(343/499): loss=2.2123158257960505, w0=73.242, w1=13.524735331414794\n",
      "Gradient Descent(344/499): loss=2.21231581676578, w0=73.24000000000001, w1=13.524552627108122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(345/499): loss=2.212315809441507, w0=73.24000000000001, w1=13.524064810700185\n",
      "Gradient Descent(346/499): loss=2.2123157975432637, w0=73.24000000000001, w1=13.523576994292249\n",
      "Gradient Descent(347/499): loss=2.2123157856450106, w0=73.24000000000001, w1=13.523089177884312\n",
      "Gradient Descent(348/499): loss=2.212315861550751, w0=73.242, w1=13.526273601110946\n",
      "Gradient Descent(349/499): loss=2.212315851417465, w0=73.242, w1=13.52578578470301\n",
      "Gradient Descent(350/499): loss=2.2123158395192255, w0=73.242, w1=13.525297968295073\n",
      "Gradient Descent(351/499): loss=2.2123158276209858, w0=73.242, w1=13.524810151887136\n",
      "Gradient Descent(352/499): loss=2.2123158174492805, w0=73.24000000000001, w1=13.524627447580464\n",
      "Gradient Descent(353/499): loss=2.2123158112664267, w0=73.24000000000001, w1=13.524139631172527\n",
      "Gradient Descent(354/499): loss=2.2123157993681946, w0=73.24000000000001, w1=13.523651814764591\n",
      "Gradient Descent(355/499): loss=2.2123157874699397, w0=73.24000000000001, w1=13.523163998356655\n",
      "Gradient Descent(356/499): loss=2.2123158496377275, w0=73.242, w1=13.526348421583288\n",
      "Gradient Descent(357/499): loss=2.212315853242399, w0=73.242, w1=13.525860605175351\n",
      "Gradient Descent(358/499): loss=2.212315841344155, w0=73.242, w1=13.525372788767415\n",
      "Gradient Descent(359/499): loss=2.212315829445925, w0=73.242, w1=13.524884972359478\n",
      "Gradient Descent(360/499): loss=2.212315818132782, w0=73.24000000000001, w1=13.524702268052806\n",
      "Gradient Descent(361/499): loss=2.2123158130913647, w0=73.24000000000001, w1=13.52421445164487\n",
      "Gradient Descent(362/499): loss=2.2123158011931197, w0=73.24000000000001, w1=13.523726635236933\n",
      "Gradient Descent(363/499): loss=2.2123157892948764, w0=73.24000000000001, w1=13.523238818828997\n",
      "Gradient Descent(364/499): loss=2.2123158377247245, w0=73.242, w1=13.52642324205563\n",
      "Gradient Descent(365/499): loss=2.2123158550673385, w0=73.242, w1=13.525935425647694\n",
      "Gradient Descent(366/499): loss=2.212315843169094, w0=73.242, w1=13.525447609239757\n",
      "Gradient Descent(367/499): loss=2.2123158312708506, w0=73.242, w1=13.52495979283182\n",
      "Gradient Descent(368/499): loss=2.2123158193725985, w0=73.242, w1=13.524471976423884\n",
      "Gradient Descent(369/499): loss=2.2123158143599637, w0=73.24000000000001, w1=13.524289272117212\n",
      "Gradient Descent(370/499): loss=2.212315803018063, w0=73.24000000000001, w1=13.523801455709275\n",
      "Gradient Descent(371/499): loss=2.2123157911198095, w0=73.24000000000001, w1=13.523313639301339\n",
      "Gradient Descent(372/499): loss=2.2123158258117233, w0=73.242, w1=13.526498062527972\n",
      "Gradient Descent(373/499): loss=2.212315856892268, w0=73.242, w1=13.526010246120036\n",
      "Gradient Descent(374/499): loss=2.2123158449940177, w0=73.242, w1=13.5255224297121\n",
      "Gradient Descent(375/499): loss=2.2123158330957877, w0=73.242, w1=13.525034613304163\n",
      "Gradient Descent(376/499): loss=2.212315821197534, w0=73.242, w1=13.524546796896226\n",
      "Gradient Descent(377/499): loss=2.2123158150434814, w0=73.24000000000001, w1=13.524364092589554\n",
      "Gradient Descent(378/499): loss=2.2123158048429983, w0=73.24000000000001, w1=13.523876276181618\n",
      "Gradient Descent(379/499): loss=2.2123157929447435, w0=73.24000000000001, w1=13.523388459773681\n",
      "Gradient Descent(380/499): loss=2.212315813898716, w0=73.242, w1=13.526572883000314\n",
      "Gradient Descent(381/499): loss=2.212315858717193, w0=73.242, w1=13.526085066592378\n",
      "Gradient Descent(382/499): loss=2.2123158468189574, w0=73.242, w1=13.525597250184441\n",
      "Gradient Descent(383/499): loss=2.2123158349207066, w0=73.242, w1=13.525109433776505\n",
      "Gradient Descent(384/499): loss=2.212315823022469, w0=73.242, w1=13.524621617368568\n",
      "Gradient Descent(385/499): loss=2.2123158157269605, w0=73.24000000000001, w1=13.524438913061896\n",
      "Gradient Descent(386/499): loss=2.212315806667929, w0=73.24000000000001, w1=13.52395109665396\n",
      "Gradient Descent(387/499): loss=2.2123157947696805, w0=73.24000000000001, w1=13.523463280246023\n",
      "Gradient Descent(388/499): loss=2.2123158019856994, w0=73.242, w1=13.526647703472657\n",
      "Gradient Descent(389/499): loss=2.212315860542136, w0=73.242, w1=13.52615988706472\n",
      "Gradient Descent(390/499): loss=2.2123158486438963, w0=73.242, w1=13.525672070656784\n",
      "Gradient Descent(391/499): loss=2.212315836745655, w0=73.242, w1=13.525184254248847\n",
      "Gradient Descent(392/499): loss=2.2123158248473933, w0=73.242, w1=13.52469643784091\n",
      "Gradient Descent(393/499): loss=2.212315816410479, w0=73.24000000000001, w1=13.524513733534238\n",
      "Gradient Descent(394/499): loss=2.2123158084928596, w0=73.24000000000001, w1=13.524025917126302\n",
      "Gradient Descent(395/499): loss=2.2123157965946114, w0=73.24000000000001, w1=13.523538100718365\n",
      "Gradient Descent(396/499): loss=2.2123157900727266, w0=73.242, w1=13.526722523944999\n",
      "Gradient Descent(397/499): loss=2.2123158623670687, w0=73.242, w1=13.526234707537062\n",
      "Gradient Descent(398/499): loss=2.2123158504688223, w0=73.242, w1=13.525746891129126\n",
      "Gradient Descent(399/499): loss=2.212315838570574, w0=73.242, w1=13.52525907472119\n",
      "Gradient Descent(400/499): loss=2.2123158266723366, w0=73.242, w1=13.524771258313253\n",
      "Gradient Descent(401/499): loss=2.212315817093979, w0=73.24000000000001, w1=13.52458855400658\n",
      "Gradient Descent(402/499): loss=2.212315810317779, w0=73.24000000000001, w1=13.524100737598644\n",
      "Gradient Descent(403/499): loss=2.212315798419545, w0=73.24000000000001, w1=13.523612921190708\n",
      "Gradient Descent(404/499): loss=2.212315786521299, w0=73.24000000000001, w1=13.523125104782771\n",
      "Gradient Descent(405/499): loss=2.212315855830407, w0=73.242, w1=13.526309528009405\n",
      "Gradient Descent(406/499): loss=2.2123158522937527, w0=73.242, w1=13.525821711601468\n",
      "Gradient Descent(407/499): loss=2.21231584039551, w0=73.242, w1=13.525333895193532\n",
      "Gradient Descent(408/499): loss=2.212315828497281, w0=73.242, w1=13.524846078785595\n",
      "Gradient Descent(409/499): loss=2.21231581777748, w0=73.24000000000001, w1=13.524663374478923\n",
      "Gradient Descent(410/499): loss=2.212315812142712, w0=73.24000000000001, w1=13.524175558070986\n",
      "Gradient Descent(411/499): loss=2.2123158002444754, w0=73.24000000000001, w1=13.52368774166305\n",
      "Gradient Descent(412/499): loss=2.21231578834623, w0=73.24000000000001, w1=13.523199925255113\n",
      "Gradient Descent(413/499): loss=2.212315843917417, w0=73.242, w1=13.526384348481747\n",
      "Gradient Descent(414/499): loss=2.2123158541186774, w0=73.242, w1=13.52589653207381\n",
      "Gradient Descent(415/499): loss=2.212315842220448, w0=73.242, w1=13.525408715665874\n",
      "Gradient Descent(416/499): loss=2.2123158303221997, w0=73.242, w1=13.524920899257937\n",
      "Gradient Descent(417/499): loss=2.21231581846099, w0=73.24000000000001, w1=13.524738194951265\n",
      "Gradient Descent(418/499): loss=2.212315813967641, w0=73.24000000000001, w1=13.524250378543329\n",
      "Gradient Descent(419/499): loss=2.2123158020694023, w0=73.24000000000001, w1=13.523762562135392\n",
      "Gradient Descent(420/499): loss=2.212315790171164, w0=73.24000000000001, w1=13.523274745727456\n",
      "Gradient Descent(421/499): loss=2.212315832004398, w0=73.242, w1=13.526459168954089\n",
      "Gradient Descent(422/499): loss=2.212315855943625, w0=73.242, w1=13.525971352546152\n",
      "Gradient Descent(423/499): loss=2.212315844045383, w0=73.242, w1=13.525483536138216\n",
      "Gradient Descent(424/499): loss=2.2123158321471355, w0=73.242, w1=13.52499571973028\n",
      "Gradient Descent(425/499): loss=2.212315820248886, w0=73.242, w1=13.524507903322343\n",
      "Gradient Descent(426/499): loss=2.2123158146881705, w0=73.24000000000001, w1=13.52432519901567\n",
      "Gradient Descent(427/499): loss=2.2123158038943407, w0=73.24000000000001, w1=13.523837382607734\n",
      "Gradient Descent(428/499): loss=2.2123157919960987, w0=73.24000000000001, w1=13.523349566199798\n",
      "Gradient Descent(429/499): loss=2.2123158200913937, w0=73.242, w1=13.526533989426431\n",
      "Gradient Descent(430/499): loss=2.2123158577685564, w0=73.242, w1=13.526046173018495\n",
      "Gradient Descent(431/499): loss=2.2123158458703154, w0=73.242, w1=13.525558356610558\n",
      "Gradient Descent(432/499): loss=2.2123158339720597, w0=73.242, w1=13.525070540202622\n",
      "Gradient Descent(433/499): loss=2.2123158220738244, w0=73.242, w1=13.524582723794685\n",
      "Gradient Descent(434/499): loss=2.2123158153716775, w0=73.24000000000001, w1=13.524400019488013\n",
      "Gradient Descent(435/499): loss=2.212315805719271, w0=73.24000000000001, w1=13.523912203080076\n",
      "Gradient Descent(436/499): loss=2.2123157938210247, w0=73.24000000000001, w1=13.52342438667214\n",
      "Gradient Descent(437/499): loss=2.212315808178399, w0=73.242, w1=13.526608809898773\n",
      "Gradient Descent(438/499): loss=2.2123158595934744, w0=73.242, w1=13.526120993490837\n",
      "Gradient Descent(439/499): loss=2.2123158476952423, w0=73.242, w1=13.5256331770829\n",
      "Gradient Descent(440/499): loss=2.212315835796996, w0=73.242, w1=13.525145360674964\n",
      "Gradient Descent(441/499): loss=2.212315823898751, w0=73.242, w1=13.524657544267027\n",
      "Gradient Descent(442/499): loss=2.212315816055177, w0=73.24000000000001, w1=13.524474839960355\n",
      "Gradient Descent(443/499): loss=2.2123158075442086, w0=73.24000000000001, w1=13.523987023552419\n",
      "Gradient Descent(444/499): loss=2.212315795645967, w0=73.24000000000001, w1=13.523499207144482\n",
      "Gradient Descent(445/499): loss=2.2123157962653868, w0=73.242, w1=13.526683630371116\n",
      "Gradient Descent(446/499): loss=2.212315861418417, w0=73.242, w1=13.526195813963179\n",
      "Gradient Descent(447/499): loss=2.2123158495201674, w0=73.242, w1=13.525707997555243\n",
      "Gradient Descent(448/499): loss=2.21231583762193, w0=73.242, w1=13.525220181147306\n",
      "Gradient Descent(449/499): loss=2.2123158257236972, w0=73.242, w1=13.52473236473937\n",
      "Gradient Descent(450/499): loss=2.2123158167386845, w0=73.24000000000001, w1=13.524549660432697\n",
      "Gradient Descent(451/499): loss=2.212315809369147, w0=73.24000000000001, w1=13.52406184402476\n",
      "Gradient Descent(452/499): loss=2.2123157974709073, w0=73.24000000000001, w1=13.523574027616824\n",
      "Gradient Descent(453/499): loss=2.2123157855726467, w0=73.24000000000001, w1=13.523086211208888\n",
      "Gradient Descent(454/499): loss=2.212315862023083, w0=73.242, w1=13.526270634435521\n",
      "Gradient Descent(455/499): loss=2.2123158513451027, w0=73.242, w1=13.525782818027585\n",
      "Gradient Descent(456/499): loss=2.21231583944687, w0=73.242, w1=13.525295001619648\n",
      "Gradient Descent(457/499): loss=2.2123158275486197, w0=73.242, w1=13.524807185211712\n",
      "Gradient Descent(458/499): loss=2.2123158174221746, w0=73.24000000000001, w1=13.52462448090504\n",
      "Gradient Descent(459/499): loss=2.2123158111940784, w0=73.24000000000001, w1=13.524136664497103\n",
      "Gradient Descent(460/499): loss=2.2123157992958307, w0=73.24000000000001, w1=13.523648848089167\n",
      "Gradient Descent(461/499): loss=2.212315787397581, w0=73.24000000000001, w1=13.52316103168123\n",
      "Gradient Descent(462/499): loss=2.2123158501100795, w0=73.242, w1=13.526345454907863\n",
      "Gradient Descent(463/499): loss=2.212315853170046, w0=73.242, w1=13.525857638499927\n",
      "Gradient Descent(464/499): loss=2.2123158412717974, w0=73.242, w1=13.52536982209199\n",
      "Gradient Descent(465/499): loss=2.2123158293735643, w0=73.242, w1=13.524882005684054\n",
      "Gradient Descent(466/499): loss=2.2123158181056706, w0=73.24000000000001, w1=13.524699301377382\n",
      "Gradient Descent(467/499): loss=2.2123158130190044, w0=73.24000000000001, w1=13.524211484969445\n",
      "Gradient Descent(468/499): loss=2.2123158011207695, w0=73.24000000000001, w1=13.523723668561509\n",
      "Gradient Descent(469/499): loss=2.2123157892225302, w0=73.24000000000001, w1=13.523235852153572\n",
      "Gradient Descent(470/499): loss=2.212315838197089, w0=73.242, w1=13.526420275380206\n",
      "Gradient Descent(471/499): loss=2.2123158549949706, w0=73.242, w1=13.52593245897227\n",
      "Gradient Descent(472/499): loss=2.2123158430967265, w0=73.242, w1=13.525444642564333\n",
      "Gradient Descent(473/499): loss=2.212315831198476, w0=73.242, w1=13.524956826156396\n",
      "Gradient Descent(474/499): loss=2.2123158193002572, w0=73.242, w1=13.52446900974846\n",
      "Gradient Descent(475/499): loss=2.2123158143328796, w0=73.24000000000001, w1=13.524286305441787\n",
      "Gradient Descent(476/499): loss=2.212315802945701, w0=73.24000000000001, w1=13.523798489033851\n",
      "Gradient Descent(477/499): loss=2.2123157910474593, w0=73.24000000000001, w1=13.523310672625914\n",
      "Gradient Descent(478/499): loss=2.212315826284077, w0=73.242, w1=13.526495095852548\n",
      "Gradient Descent(479/499): loss=2.21231585681991, w0=73.242, w1=13.526007279444611\n",
      "Gradient Descent(480/499): loss=2.2123158449216778, w0=73.242, w1=13.525519463036675\n",
      "Gradient Descent(481/499): loss=2.212315833023421, w0=73.242, w1=13.525031646628738\n",
      "Gradient Descent(482/499): loss=2.2123158211251823, w0=73.242, w1=13.524543830220802\n",
      "Gradient Descent(483/499): loss=2.212315815016367, w0=73.24000000000001, w1=13.52436112591413\n",
      "Gradient Descent(484/499): loss=2.2123158047706273, w0=73.24000000000001, w1=13.523873309506193\n",
      "Gradient Descent(485/499): loss=2.2123157928723884, w0=73.24000000000001, w1=13.523385493098257\n",
      "Gradient Descent(486/499): loss=2.2123158143710637, w0=73.242, w1=13.52656991632489\n",
      "Gradient Descent(487/499): loss=2.2123158586448453, w0=73.242, w1=13.526082099916954\n",
      "Gradient Descent(488/499): loss=2.212315846746593, w0=73.242, w1=13.525594283509017\n",
      "Gradient Descent(489/499): loss=2.212315834848346, w0=73.242, w1=13.52510646710108\n",
      "Gradient Descent(490/499): loss=2.212315822950111, w0=73.242, w1=13.524618650693144\n",
      "Gradient Descent(491/499): loss=2.212315815699876, w0=73.24000000000001, w1=13.524435946386472\n",
      "Gradient Descent(492/499): loss=2.212315806595571, w0=73.24000000000001, w1=13.523948129978535\n",
      "Gradient Descent(493/499): loss=2.2123157946973198, w0=73.24000000000001, w1=13.523460313570599\n",
      "Gradient Descent(494/499): loss=2.2123158024580816, w0=73.242, w1=13.526644736797232\n",
      "Gradient Descent(495/499): loss=2.2123158604697646, w0=73.242, w1=13.526156920389296\n",
      "Gradient Descent(496/499): loss=2.212315848571542, w0=73.242, w1=13.52566910398136\n",
      "Gradient Descent(497/499): loss=2.21231583667329, w0=73.242, w1=13.525181287573423\n",
      "Gradient Descent(498/499): loss=2.2123158247750476, w0=73.242, w1=13.524693471165486\n",
      "Gradient Descent(499/499): loss=2.212315816383379, w0=73.24000000000001, w1=13.524510766858814\n",
      "Gradient Descent: execution time=1.085 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 10\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([100, 10])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62dad664cbc54433a9093f166a102872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89.4701687]\n",
      "[[ 1.          0.90764841]]\n"
     ]
    }
   ],
   "source": [
    "for minibatch_y, minibatch_tx in batch_iter(y, tx, 1, num_batches=1):\n",
    "    print(minibatch_y)\n",
    "    print(minibatch_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    dh = y - np.dot(tx,w)\n",
    "    dh[dh ==0] = random.uniform(-1,1)\n",
    "    dh[dh>0] = 1\n",
    "    dh[dh<0] = -1\n",
    "    Gd = [np.repeat(-1,tx.shape[0]),-tx[:,1]]\n",
    "    return 1/(len(y))*np.sum((dh*Gd), axis = 1)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size):\n",
    "            y_i = minibatch_y\n",
    "            tx_i = minibatch_tx\n",
    "        G = compute_stoch_gradient(y_i, tx_i,w)\n",
    "        loss = compute_loss(y_i, tx_i,w)\n",
    "        w = w-gamma*G\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/9999): loss=36.46182313041229, w0=0.9, w1=0.10621323476136003\n",
      "Gradient Descent(1/9999): loss=34.719429066353925, w0=1.8, w1=-0.17082917764567668\n",
      "Gradient Descent(2/9999): loss=36.72095075646453, w0=2.7, w1=-0.13414895010592992\n",
      "Gradient Descent(3/9999): loss=34.26330927143814, w0=3.6, w1=-0.24428341248733426\n",
      "Gradient Descent(4/9999): loss=32.94412038001646, w0=4.5, w1=-0.5219780193393821\n",
      "Gradient Descent(5/9999): loss=34.81173565900275, w0=5.4, w1=-0.5785636023366431\n",
      "Gradient Descent(6/9999): loss=33.59265488339688, w0=6.300000000000001, w1=-0.5781935533195695\n",
      "Gradient Descent(7/9999): loss=32.63689082307092, w0=7.200000000000001, w1=-0.6702518630347838\n",
      "Gradient Descent(8/9999): loss=32.85364922286803, w0=8.100000000000001, w1=-0.6273222717193376\n",
      "Gradient Descent(9/9999): loss=33.53720432138777, w0=9.000000000000002, w1=-0.5347708092717431\n",
      "Gradient Descent(10/9999): loss=32.59862134203556, w0=9.900000000000002, w1=-0.4799098424186732\n",
      "Gradient Descent(11/9999): loss=30.981461387370757, w0=10.800000000000002, w1=-0.5667388411132231\n",
      "Gradient Descent(12/9999): loss=31.568999248911624, w0=11.700000000000003, w1=-0.5217465469084969\n",
      "Gradient Descent(13/9999): loss=31.432585503682166, w0=12.600000000000003, w1=-0.4959371162202912\n",
      "Gradient Descent(14/9999): loss=30.26832836432793, w0=13.500000000000004, w1=-0.5350472479031692\n",
      "Gradient Descent(15/9999): loss=28.836726407221178, w0=14.400000000000004, w1=-0.5789902575476086\n",
      "Gradient Descent(16/9999): loss=30.645124648624904, w0=15.300000000000004, w1=-0.5224254530557298\n",
      "Gradient Descent(17/9999): loss=29.518843973245392, w0=16.200000000000003, w1=-0.5125659346713263\n",
      "Gradient Descent(18/9999): loss=28.036879124372653, w0=17.1, w1=-0.5533084926725108\n",
      "Gradient Descent(19/9999): loss=27.112551319010983, w0=18.0, w1=-0.6298553401238807\n",
      "Gradient Descent(20/9999): loss=26.443834192005696, w0=18.9, w1=-0.8337437362262845\n",
      "Gradient Descent(21/9999): loss=28.103733726000055, w0=19.799999999999997, w1=-0.7126029799337176\n",
      "Gradient Descent(22/9999): loss=26.96107576843511, w0=20.699999999999996, w1=-0.6996290032391045\n",
      "Gradient Descent(23/9999): loss=26.219417607848584, w0=21.599999999999994, w1=-0.6903678739857134\n",
      "Gradient Descent(24/9999): loss=25.92600631059842, w0=22.499999999999993, w1=-0.699412187290271\n",
      "Gradient Descent(25/9999): loss=24.014342817319942, w0=23.39999999999999, w1=-0.8441465864031471\n",
      "Gradient Descent(26/9999): loss=24.284836873851194, w0=24.29999999999999, w1=-0.9645580209588294\n",
      "Gradient Descent(27/9999): loss=24.411459223470754, w0=25.19999999999999, w1=-0.8537641947020441\n",
      "Gradient Descent(28/9999): loss=24.09277182329933, w0=26.099999999999987, w1=-0.8112946934118322\n",
      "Gradient Descent(29/9999): loss=23.044533257627112, w0=26.999999999999986, w1=-0.8506301471451182\n",
      "Gradient Descent(30/9999): loss=21.586371484392675, w0=27.899999999999984, w1=-1.067059200867404\n",
      "Gradient Descent(31/9999): loss=24.52625980177746, w0=28.799999999999983, w1=-0.8997955649550937\n",
      "Gradient Descent(32/9999): loss=23.03964295756151, w0=29.69999999999998, w1=-0.7876659873266987\n",
      "Gradient Descent(33/9999): loss=21.853244792093275, w0=30.59999999999998, w1=-0.7998190870091016\n",
      "Gradient Descent(34/9999): loss=22.395472182006422, w0=31.49999999999998, w1=-0.6454700668727955\n",
      "Gradient Descent(35/9999): loss=21.848152955133564, w0=32.39999999999998, w1=-0.5669160016683482\n",
      "Gradient Descent(36/9999): loss=19.88700203013723, w0=33.299999999999976, w1=-0.6174463241284646\n",
      "Gradient Descent(37/9999): loss=19.188514154845603, w0=34.199999999999974, w1=-0.7153894786276489\n",
      "Gradient Descent(38/9999): loss=20.08902976592712, w0=35.09999999999997, w1=-0.5659258920679618\n",
      "Gradient Descent(39/9999): loss=20.046210896892653, w0=35.99999999999997, w1=-0.47780440697783255\n",
      "Gradient Descent(40/9999): loss=18.152745258962618, w0=36.89999999999997, w1=-0.5136709492477503\n",
      "Gradient Descent(41/9999): loss=18.89386369551836, w0=37.79999999999997, w1=-0.47767407456314825\n",
      "Gradient Descent(42/9999): loss=17.124603708364635, w0=38.69999999999997, w1=-0.5211151315273351\n",
      "Gradient Descent(43/9999): loss=16.35371380440474, w0=39.54374999999997, w1=-0.5810779991153409\n",
      "Gradient Descent(44/9999): loss=17.018451808882475, w0=40.443749999999966, w1=-0.6111360565603926\n",
      "Gradient Descent(45/9999): loss=18.062427627900405, w0=41.343749999999964, w1=-0.501838869413842\n",
      "Gradient Descent(46/9999): loss=16.286095388862655, w0=42.24374999999996, w1=-0.4646341140381654\n",
      "Gradient Descent(47/9999): loss=14.917784711180746, w0=43.14374999999996, w1=-0.5350837626180673\n",
      "Gradient Descent(48/9999): loss=15.18636914594267, w0=44.04374999999996, w1=-0.45405516756209463\n",
      "Gradient Descent(49/9999): loss=12.518833244045577, w0=44.88749999999996, w1=-0.5558429660168429\n",
      "Gradient Descent(50/9999): loss=13.076585509866497, w0=45.78749999999996, w1=-0.6948838999859357\n",
      "Gradient Descent(51/9999): loss=12.364198030975013, w0=46.68749999999996, w1=-0.8483296014395718\n",
      "Gradient Descent(52/9999): loss=14.732411702400398, w0=47.53124999999996, w1=-0.5077212238351767\n",
      "Gradient Descent(53/9999): loss=13.655976285675234, w0=48.431249999999956, w1=-0.39075703729052486\n",
      "Gradient Descent(54/9999): loss=10.193798531445667, w0=49.331249999999955, w1=-0.6784521499766396\n",
      "Gradient Descent(55/9999): loss=10.74557411408604, w0=50.118749999999956, w1=-0.6338569860279398\n",
      "Gradient Descent(56/9999): loss=11.64438303076226, w0=50.962499999999956, w1=-0.5584990467977804\n",
      "Gradient Descent(57/9999): loss=12.722418624280229, w0=51.806249999999956, w1=-0.23447124531262126\n",
      "Gradient Descent(58/9999): loss=12.174821620474546, w0=52.706249999999955, w1=-0.14055048056883318\n",
      "Gradient Descent(59/9999): loss=12.940058833143818, w0=53.493749999999956, w1=0.30294080585838523\n",
      "Gradient Descent(60/9999): loss=9.720025883926205, w0=54.22499999999996, w1=0.6118288790969394\n",
      "Gradient Descent(61/9999): loss=8.809859874492052, w0=54.95624999999996, w1=0.8300363652981673\n",
      "Gradient Descent(62/9999): loss=9.881880180816259, w0=55.74374999999996, w1=1.0628740196671156\n",
      "Gradient Descent(63/9999): loss=8.15687125214076, w0=56.64374999999996, w1=1.0498035078279673\n",
      "Gradient Descent(64/9999): loss=8.791207501110176, w0=57.31874999999996, w1=1.384002147461464\n",
      "Gradient Descent(65/9999): loss=8.128101119258954, w0=57.993749999999956, w1=1.5512925800517192\n",
      "Gradient Descent(66/9999): loss=6.454188507949229, w0=58.66874999999995, w1=1.725354700652052\n",
      "Gradient Descent(67/9999): loss=7.987997721694505, w0=59.28749999999995, w1=2.0391254808219936\n",
      "Gradient Descent(68/9999): loss=8.49166209686324, w0=59.84999999999995, w1=2.6098890325202246\n",
      "Gradient Descent(69/9999): loss=7.622992796631878, w0=60.46874999999995, w1=3.0318957164704834\n",
      "Gradient Descent(70/9999): loss=7.790173060719091, w0=61.19999999999995, w1=3.318708944980356\n",
      "Gradient Descent(71/9999): loss=5.3824020657669, w0=61.87499999999995, w1=3.387038500563869\n",
      "Gradient Descent(72/9999): loss=6.544921234357957, w0=62.43749999999995, w1=3.73992278009469\n",
      "Gradient Descent(73/9999): loss=6.637298345442769, w0=62.88749999999995, w1=4.36098354857568\n",
      "Gradient Descent(74/9999): loss=7.311203386243046, w0=63.56249999999995, w1=4.992559645532958\n",
      "Gradient Descent(75/9999): loss=5.758487591901446, w0=64.12499999999994, w1=5.209998348087932\n",
      "Gradient Descent(76/9999): loss=5.83257125535792, w0=64.68749999999994, w1=5.581233427425931\n",
      "Gradient Descent(77/9999): loss=4.801939457431868, w0=65.30624999999995, w1=5.939240824430524\n",
      "Gradient Descent(78/9999): loss=4.634918306280207, w0=65.53124999999994, w1=6.564853749561719\n",
      "Gradient Descent(79/9999): loss=4.914521856528593, w0=65.86874999999995, w1=7.236538455262707\n",
      "Gradient Descent(80/9999): loss=4.636084616476814, w0=66.37499999999994, w1=7.571490133287228\n",
      "Gradient Descent(81/9999): loss=4.357103912360518, w0=66.76874999999994, w1=7.922616336716061\n",
      "Gradient Descent(82/9999): loss=4.08260403656192, w0=67.49999999999994, w1=8.304761526693566\n",
      "Gradient Descent(83/9999): loss=5.036597264292486, w0=68.23124999999995, w1=8.83302309776548\n",
      "Gradient Descent(84/9999): loss=3.7845161149651916, w0=68.56874999999995, w1=9.35596370131079\n",
      "Gradient Descent(85/9999): loss=3.159260802448017, w0=68.84999999999995, w1=9.701164546856434\n",
      "Gradient Descent(86/9999): loss=3.4258290006717154, w0=69.41249999999995, w1=10.103849908274801\n",
      "Gradient Descent(87/9999): loss=3.426498552031655, w0=69.97499999999995, w1=10.35979422132752\n",
      "Gradient Descent(88/9999): loss=2.8271820661450646, w0=70.31249999999996, w1=10.594090722630783\n",
      "Gradient Descent(89/9999): loss=3.164201090207688, w0=70.76249999999996, w1=10.810750498605163\n",
      "Gradient Descent(90/9999): loss=2.602871197056298, w0=70.98749999999995, w1=11.15183274207742\n",
      "Gradient Descent(91/9999): loss=2.4838846354702673, w0=71.09999999999995, w1=11.255755296031264\n",
      "Gradient Descent(92/9999): loss=2.2854652978627907, w0=71.66249999999995, w1=11.642202714514882\n",
      "Gradient Descent(93/9999): loss=2.1664081211009214, w0=71.88749999999995, w1=11.960885361439981\n",
      "Gradient Descent(94/9999): loss=3.1999243898096985, w0=71.99999999999994, w1=12.259094538592617\n",
      "Gradient Descent(95/9999): loss=1.8903469205926668, w0=72.05624999999995, w1=12.51925883463113\n",
      "Gradient Descent(96/9999): loss=2.5478394222198055, w0=72.44999999999995, w1=12.601040738478542\n",
      "Gradient Descent(97/9999): loss=2.7613867061053083, w0=72.73124999999995, w1=12.76164652618606\n",
      "Gradient Descent(98/9999): loss=2.297620734055184, w0=72.84374999999994, w1=13.110740306246624\n",
      "Gradient Descent(99/9999): loss=2.2194881759977156, w0=72.73124999999995, w1=12.965611093982414\n",
      "Gradient Descent(100/9999): loss=2.558490332759507, w0=72.84374999999994, w1=13.166389827719383\n",
      "Gradient Descent(101/9999): loss=2.2125253017481468, w0=72.95624999999994, w1=13.455999533526267\n",
      "Gradient Descent(102/9999): loss=2.1310830488124055, w0=72.89999999999993, w1=13.286061328279695\n",
      "Gradient Descent(103/9999): loss=1.9465983163367035, w0=72.73124999999993, w1=13.286074293987827\n",
      "Gradient Descent(104/9999): loss=2.359800984959885, w0=72.84374999999993, w1=13.111490698880177\n",
      "Gradient Descent(105/9999): loss=1.5829406390335816, w0=73.06874999999992, w1=13.266215444308958\n",
      "Gradient Descent(106/9999): loss=1.956973090051254, w0=73.12499999999993, w1=13.410956862320205\n",
      "Gradient Descent(107/9999): loss=2.2462901048104436, w0=72.84374999999993, w1=13.354802294011446\n",
      "Gradient Descent(108/9999): loss=2.4652851284575834, w0=73.06874999999992, w1=13.403021508199371\n",
      "Gradient Descent(109/9999): loss=2.196759229650191, w0=73.12499999999993, w1=13.406841760075432\n",
      "Gradient Descent(110/9999): loss=2.40064832181037, w0=73.12499999999993, w1=13.598384283792837\n",
      "Gradient Descent(111/9999): loss=2.6403739886144084, w0=73.18124999999993, w1=13.897995370125262\n",
      "Gradient Descent(112/9999): loss=2.522560020203815, w0=73.01249999999993, w1=13.784140056023663\n",
      "Gradient Descent(113/9999): loss=2.276111132426117, w0=72.95624999999993, w1=13.416432079711178\n",
      "Gradient Descent(114/9999): loss=2.7973834172779792, w0=73.12499999999993, w1=13.59099275907667\n",
      "Gradient Descent(115/9999): loss=1.8417336917956075, w0=73.23749999999993, w1=13.803592090499547\n",
      "Gradient Descent(116/9999): loss=2.4118945454043326, w0=73.40624999999993, w1=13.852252955155686\n",
      "Gradient Descent(117/9999): loss=2.202338907854569, w0=73.46249999999993, w1=13.790937981654325\n",
      "Gradient Descent(118/9999): loss=2.0172156288144434, w0=73.40624999999993, w1=13.68696248388107\n",
      "Gradient Descent(119/9999): loss=2.8031774559107263, w0=73.46249999999993, w1=13.590419625079516\n",
      "Gradient Descent(120/9999): loss=2.257453110165549, w0=73.34999999999994, w1=13.530973774010587\n",
      "Gradient Descent(121/9999): loss=2.246756476581358, w0=73.46249999999993, w1=13.37297736952747\n",
      "Gradient Descent(122/9999): loss=2.591265767096573, w0=73.23749999999994, w1=13.356537338720956\n",
      "Gradient Descent(123/9999): loss=1.9436200566940796, w0=73.46249999999993, w1=13.30634614098835\n",
      "Gradient Descent(124/9999): loss=2.205418888738814, w0=73.29374999999993, w1=12.957522567090063\n",
      "Gradient Descent(125/9999): loss=2.5876905987676833, w0=73.23749999999993, w1=12.953349060966177\n",
      "Gradient Descent(126/9999): loss=2.4595170639291815, w0=73.18124999999992, w1=13.035142675611587\n",
      "Gradient Descent(127/9999): loss=2.4120750460150333, w0=73.23749999999993, w1=13.05254772782547\n",
      "Gradient Descent(128/9999): loss=2.3520764255876294, w0=73.29374999999993, w1=12.69183231178411\n",
      "Gradient Descent(129/9999): loss=2.4164971776068653, w0=73.23749999999993, w1=13.001589172151151\n",
      "Gradient Descent(130/9999): loss=2.7943999407796167, w0=73.29374999999993, w1=13.178329655886051\n",
      "Gradient Descent(131/9999): loss=2.041235444129561, w0=73.29374999999993, w1=13.327443682367377\n",
      "Gradient Descent(132/9999): loss=2.419702766333158, w0=73.34999999999994, w1=13.323617782914335\n",
      "Gradient Descent(133/9999): loss=1.9541504050521394, w0=73.46249999999993, w1=13.436890116154107\n",
      "Gradient Descent(134/9999): loss=1.6748414968724643, w0=73.51874999999994, w1=13.350772189689845\n",
      "Gradient Descent(135/9999): loss=2.469654423581785, w0=73.46249999999993, w1=13.35336218786434\n",
      "Gradient Descent(136/9999): loss=2.2849115089401946, w0=73.51874999999994, w1=13.454838367874347\n",
      "Gradient Descent(137/9999): loss=2.43450401647901, w0=73.57499999999995, w1=13.223196281035762\n",
      "Gradient Descent(138/9999): loss=2.5391488598906156, w0=73.51874999999994, w1=13.227898725269565\n",
      "Gradient Descent(139/9999): loss=1.9546750735973073, w0=73.29374999999995, w1=13.38369504829645\n",
      "Gradient Descent(140/9999): loss=2.118398030373963, w0=73.18124999999995, w1=13.432669321428941\n",
      "Gradient Descent(141/9999): loss=2.4629364989981113, w0=73.34999999999995, w1=13.351080735060455\n",
      "Gradient Descent(142/9999): loss=1.9182731800614365, w0=73.34999999999995, w1=13.348065610170222\n",
      "Gradient Descent(143/9999): loss=1.8691105180071368, w0=73.29374999999995, w1=13.327329135956372\n",
      "Gradient Descent(144/9999): loss=2.211813636334253, w0=73.29374999999995, w1=13.331457145225052\n",
      "Gradient Descent(145/9999): loss=1.9716145019818174, w0=73.40624999999994, w1=13.40184881688936\n",
      "Gradient Descent(146/9999): loss=2.176379120589152, w0=73.34999999999994, w1=13.552982927276865\n",
      "Gradient Descent(147/9999): loss=2.2843526086131494, w0=73.51874999999994, w1=13.402822037289573\n",
      "Gradient Descent(148/9999): loss=2.9169717748941015, w0=73.46249999999993, w1=13.555141465697814\n",
      "Gradient Descent(149/9999): loss=2.6581858460228447, w0=73.18124999999993, w1=13.723212155756677\n",
      "Gradient Descent(150/9999): loss=2.118850062933842, w0=73.01249999999993, w1=13.928066912437245\n",
      "Gradient Descent(151/9999): loss=2.763972946647627, w0=73.18124999999993, w1=14.066533251371707\n",
      "Gradient Descent(152/9999): loss=2.081994308794649, w0=73.18124999999993, w1=14.095462663267346\n",
      "Gradient Descent(153/9999): loss=1.52319532656495, w0=72.95624999999994, w1=14.125595453358827\n",
      "Gradient Descent(154/9999): loss=2.2651199373120545, w0=73.01249999999995, w1=13.794124905044624\n",
      "Gradient Descent(155/9999): loss=2.6632845951552193, w0=72.89999999999995, w1=13.583392240620697\n",
      "Gradient Descent(156/9999): loss=2.2337237553753613, w0=73.01249999999995, w1=13.469221469611695\n",
      "Gradient Descent(157/9999): loss=2.2634903188287687, w0=72.89999999999995, w1=13.239557204030492\n",
      "Gradient Descent(158/9999): loss=2.670029485069011, w0=72.89999999999995, w1=13.225679033298361\n",
      "Gradient Descent(159/9999): loss=1.5349152556585195, w0=73.12499999999994, w1=13.378699121466122\n",
      "Gradient Descent(160/9999): loss=2.433475437270157, w0=73.29374999999995, w1=13.448954760269899\n",
      "Gradient Descent(161/9999): loss=2.1586461211712527, w0=73.34999999999995, w1=13.561673137900351\n",
      "Gradient Descent(162/9999): loss=1.9179227803565568, w0=73.34999999999995, w1=13.74500724432206\n",
      "Gradient Descent(163/9999): loss=1.9252504713753666, w0=73.40624999999996, w1=13.575852712279696\n",
      "Gradient Descent(164/9999): loss=2.2086804440950507, w0=73.40624999999996, w1=13.563020590115174\n",
      "Gradient Descent(165/9999): loss=2.564681223907685, w0=73.40624999999996, w1=13.39955114423191\n",
      "Gradient Descent(166/9999): loss=2.6503961964382037, w0=73.23749999999995, w1=13.54717658310632\n",
      "Gradient Descent(167/9999): loss=2.245049797182001, w0=73.12499999999996, w1=13.557369728496708\n",
      "Gradient Descent(168/9999): loss=2.2932509298037926, w0=73.01249999999996, w1=13.716003510578478\n",
      "Gradient Descent(169/9999): loss=2.544632623090436, w0=73.18124999999996, w1=13.38404435418408\n",
      "Gradient Descent(170/9999): loss=2.4834965801220155, w0=73.18124999999996, w1=13.637810814731132\n",
      "Gradient Descent(171/9999): loss=2.3426210352418524, w0=73.18124999999996, w1=13.713264896282125\n",
      "Gradient Descent(172/9999): loss=2.180165108190688, w0=73.29374999999996, w1=13.71395645261459\n",
      "Gradient Descent(173/9999): loss=2.1826634465611128, w0=73.18124999999996, w1=13.673654457533527\n",
      "Gradient Descent(174/9999): loss=2.4047816602040504, w0=73.01249999999996, w1=13.571704599396368\n",
      "Gradient Descent(175/9999): loss=2.433229357416797, w0=72.89999999999996, w1=13.449482985739298\n",
      "Gradient Descent(176/9999): loss=2.3897029832595305, w0=73.06874999999997, w1=13.33857862412237\n",
      "Gradient Descent(177/9999): loss=2.4671074671058726, w0=72.84374999999997, w1=13.668409893200682\n",
      "Gradient Descent(178/9999): loss=2.224408236331932, w0=72.89999999999998, w1=13.76974121429217\n",
      "Gradient Descent(179/9999): loss=2.0635457503967096, w0=72.89999999999998, w1=13.75089012096128\n",
      "Gradient Descent(180/9999): loss=1.5089190782641486, w0=72.84374999999997, w1=13.552701204487677\n",
      "Gradient Descent(181/9999): loss=2.598825258849316, w0=73.01249999999997, w1=13.90763960126466\n",
      "Gradient Descent(182/9999): loss=2.4827804138667466, w0=73.01249999999997, w1=13.767071127087043\n",
      "Gradient Descent(183/9999): loss=2.337182387827961, w0=72.95624999999997, w1=13.758753677978195\n",
      "Gradient Descent(184/9999): loss=2.143426019449532, w0=72.73124999999997, w1=13.98670951329673\n",
      "Gradient Descent(185/9999): loss=2.1533970774321984, w0=72.89999999999998, w1=14.102490713915053\n",
      "Gradient Descent(186/9999): loss=2.2152291899109002, w0=72.84374999999997, w1=14.118680315445454\n",
      "Gradient Descent(187/9999): loss=2.10022233379974, w0=72.84374999999997, w1=13.870021955531108\n",
      "Gradient Descent(188/9999): loss=2.1187875765182027, w0=72.84374999999997, w1=13.998913309281853\n",
      "Gradient Descent(189/9999): loss=2.4708026091460114, w0=72.89999999999998, w1=14.049388202897575\n",
      "Gradient Descent(190/9999): loss=2.7535501959266693, w0=72.78749999999998, w1=13.935180329372487\n",
      "Gradient Descent(191/9999): loss=1.924933315265632, w0=72.67499999999998, w1=13.667068927342301\n",
      "Gradient Descent(192/9999): loss=2.1320464843996896, w0=72.67499999999998, w1=13.394496656052041\n",
      "Gradient Descent(193/9999): loss=2.1826820208558058, w0=72.56249999999999, w1=13.667584399684829\n",
      "Gradient Descent(194/9999): loss=1.983155118566759, w0=72.56249999999999, w1=13.708857489855902\n",
      "Gradient Descent(195/9999): loss=2.5223214503035623, w0=72.56249999999999, w1=13.48974540767649\n",
      "Gradient Descent(196/9999): loss=2.5691802235437415, w0=72.44999999999999, w1=13.316002036199551\n",
      "Gradient Descent(197/9999): loss=2.29891359715095, w0=72.50625, w1=13.273930003760809\n",
      "Gradient Descent(198/9999): loss=2.3025123362532214, w0=72.7875, w1=13.414397247799995\n",
      "Gradient Descent(199/9999): loss=2.1153189491054105, w0=72.89999999999999, w1=13.400100214885518\n",
      "Gradient Descent(200/9999): loss=2.1441259698335893, w0=73.06875, w1=13.417446437552597\n",
      "Gradient Descent(201/9999): loss=2.213859102494333, w0=73.2375, w1=13.308710807052217\n",
      "Gradient Descent(202/9999): loss=2.33985263267631, w0=73.8, w1=13.26367324748488\n",
      "Gradient Descent(203/9999): loss=2.1583450873126786, w0=73.63125, w1=13.298186881106009\n",
      "Gradient Descent(204/9999): loss=2.2942571147579867, w0=73.57499999999999, w1=13.276856555724583\n",
      "Gradient Descent(205/9999): loss=1.9190002457874278, w0=73.96874999999999, w1=13.368498820381772\n",
      "Gradient Descent(206/9999): loss=1.69983345629093, w0=73.85624999999999, w1=13.36897207610952\n",
      "Gradient Descent(207/9999): loss=2.309996803336163, w0=73.63125, w1=13.204561058313312\n",
      "Gradient Descent(208/9999): loss=2.4456222793449127, w0=73.40625, w1=13.504130662094864\n",
      "Gradient Descent(209/9999): loss=2.5378913691009073, w0=73.575, w1=13.661555897125295\n",
      "Gradient Descent(210/9999): loss=2.019564233810963, w0=73.4625, w1=13.616243757645886\n",
      "Gradient Descent(211/9999): loss=2.5697694366536634, w0=73.35000000000001, w1=13.898968268688071\n",
      "Gradient Descent(212/9999): loss=1.7257896624652276, w0=73.35000000000001, w1=13.68083013705074\n",
      "Gradient Descent(213/9999): loss=2.33056956718237, w0=73.18125, w1=13.233185156945446\n",
      "Gradient Descent(214/9999): loss=2.5956999472662345, w0=73.125, w1=13.032488745745734\n",
      "Gradient Descent(215/9999): loss=2.1804643070250584, w0=73.06875, w1=13.206872647183369\n",
      "Gradient Descent(216/9999): loss=2.2812805469540582, w0=73.01249999999999, w1=12.985359098762935\n",
      "Gradient Descent(217/9999): loss=2.007499304357627, w0=72.95624999999998, w1=13.173286602833056\n",
      "Gradient Descent(218/9999): loss=1.9874934137660816, w0=73.06874999999998, w1=13.389824601011387\n",
      "Gradient Descent(219/9999): loss=2.437103134740215, w0=73.23749999999998, w1=13.423333673087786\n",
      "Gradient Descent(220/9999): loss=2.1956873028761468, w0=73.18124999999998, w1=13.380706089154925\n",
      "Gradient Descent(221/9999): loss=1.5074939862886991, w0=73.18124999999998, w1=13.402523814047981\n",
      "Gradient Descent(222/9999): loss=2.5009953905027107, w0=72.95624999999998, w1=13.434650570557123\n",
      "Gradient Descent(223/9999): loss=2.3200781755511004, w0=73.12499999999999, w1=13.311261094397022\n",
      "Gradient Descent(224/9999): loss=2.026126409933587, w0=73.34999999999998, w1=13.32027671168323\n",
      "Gradient Descent(225/9999): loss=2.009295328929614, w0=73.46249999999998, w1=13.04698850590863\n",
      "Gradient Descent(226/9999): loss=2.4572539661020243, w0=73.34999999999998, w1=13.200393937091045\n",
      "Gradient Descent(227/9999): loss=2.71966970228472, w0=73.46249999999998, w1=12.97624644797818\n",
      "Gradient Descent(228/9999): loss=1.9078284863342583, w0=73.18124999999998, w1=13.255874673823188\n",
      "Gradient Descent(229/9999): loss=2.0497965170733305, w0=73.23749999999998, w1=13.299082261828962\n",
      "Gradient Descent(230/9999): loss=2.232227540142544, w0=73.29374999999999, w1=13.239429567825782\n",
      "Gradient Descent(231/9999): loss=2.149475734331352, w0=73.29374999999999, w1=13.34073507069\n",
      "Gradient Descent(232/9999): loss=2.3707511993371977, w0=73.29374999999999, w1=13.46759162111857\n",
      "Gradient Descent(233/9999): loss=2.5189143245838923, w0=73.40624999999999, w1=13.672409941137726\n",
      "Gradient Descent(234/9999): loss=2.3131475916986872, w0=73.51874999999998, w1=13.533250366630037\n",
      "Gradient Descent(235/9999): loss=2.413275177321815, w0=73.74374999999998, w1=13.54679302040135\n",
      "Gradient Descent(236/9999): loss=2.038168038367201, w0=73.51874999999998, w1=13.56360553254908\n",
      "Gradient Descent(237/9999): loss=2.120110847758797, w0=73.57499999999999, w1=13.553751659413022\n",
      "Gradient Descent(238/9999): loss=2.2399846159891483, w0=73.74374999999999, w1=13.846852085882128\n",
      "Gradient Descent(239/9999): loss=2.5871149590032694, w0=73.96874999999999, w1=13.899787905052474\n",
      "Gradient Descent(240/9999): loss=1.9878221375225527, w0=73.74374999999999, w1=13.960902237488863\n",
      "Gradient Descent(241/9999): loss=1.6256395139564608, w0=73.46249999999999, w1=14.068837823396189\n",
      "Gradient Descent(242/9999): loss=1.6711823511257067, w0=73.29374999999999, w1=13.82922874763958\n",
      "Gradient Descent(243/9999): loss=1.7255612256853992, w0=73.35, w1=13.648847393007435\n",
      "Gradient Descent(244/9999): loss=2.6618030423818477, w0=73.40625, w1=13.534397917481256\n",
      "Gradient Descent(245/9999): loss=1.8529924052382007, w0=73.4625, w1=13.533394779093049\n",
      "Gradient Descent(246/9999): loss=2.2332965051095233, w0=73.40625, w1=13.731579771702233\n",
      "Gradient Descent(247/9999): loss=2.1535189041116354, w0=73.35, w1=13.740619417611507\n",
      "Gradient Descent(248/9999): loss=2.2412474692319524, w0=73.46249999999999, w1=14.102575411094698\n",
      "Gradient Descent(249/9999): loss=2.024649192988838, w0=73.12499999999999, w1=13.745619706342646\n",
      "Gradient Descent(250/9999): loss=2.2066809636897675, w0=72.89999999999999, w1=13.603364234772581\n",
      "Gradient Descent(251/9999): loss=1.997881618225593, w0=72.84374999999999, w1=13.504720331815786\n",
      "Gradient Descent(252/9999): loss=1.8967480282234899, w0=72.89999999999999, w1=13.562001578796085\n",
      "Gradient Descent(253/9999): loss=3.039871319208721, w0=72.7875, w1=13.564529132184278\n",
      "Gradient Descent(254/9999): loss=2.9322766813426164, w0=72.7875, w1=13.454071024123415\n",
      "Gradient Descent(255/9999): loss=2.518190194120043, w0=72.675, w1=13.601398001442938\n",
      "Gradient Descent(256/9999): loss=2.7284215920427735, w0=72.7875, w1=13.33962283709398\n",
      "Gradient Descent(257/9999): loss=2.143117319653358, w0=72.89999999999999, w1=13.35015004685839\n",
      "Gradient Descent(258/9999): loss=2.3541386336990975, w0=73.06875, w1=13.389506130699933\n",
      "Gradient Descent(259/9999): loss=2.0955543361810514, w0=72.84375, w1=13.065071443140162\n",
      "Gradient Descent(260/9999): loss=1.4707599128230744, w0=72.675, w1=13.172502655441962\n",
      "Gradient Descent(261/9999): loss=2.5052101248632335, w0=72.61874999999999, w1=13.261178298418026\n",
      "Gradient Descent(262/9999): loss=2.242953952866402, w0=72.84374999999999, w1=13.411466812738405\n",
      "Gradient Descent(263/9999): loss=2.086350373589397, w0=73.01249999999999, w1=13.201209518320237\n",
      "Gradient Descent(264/9999): loss=1.9009186319683264, w0=72.89999999999999, w1=13.155765504043858\n",
      "Gradient Descent(265/9999): loss=2.2688601760323532, w0=73.01249999999999, w1=13.209431169592944\n",
      "Gradient Descent(266/9999): loss=2.3376570388942746, w0=72.89999999999999, w1=12.94737870851907\n",
      "Gradient Descent(267/9999): loss=2.270813144087832, w0=72.7875, w1=13.292607673361855\n",
      "Gradient Descent(268/9999): loss=2.5539410725613827, w0=72.73124999999999, w1=13.339029109361604\n",
      "Gradient Descent(269/9999): loss=1.7663191786808208, w0=72.7875, w1=13.235273122970888\n",
      "Gradient Descent(270/9999): loss=1.7821032717744318, w0=72.675, w1=13.38177524038472\n",
      "Gradient Descent(271/9999): loss=2.431360199987056, w0=72.73125, w1=13.145554236444939\n",
      "Gradient Descent(272/9999): loss=2.367917932628604, w0=72.84375, w1=13.048024613383074\n",
      "Gradient Descent(273/9999): loss=1.9590528894583332, w0=72.95625, w1=13.02841243451851\n",
      "Gradient Descent(274/9999): loss=2.4649018384900945, w0=73.125, w1=13.080368007578194\n",
      "Gradient Descent(275/9999): loss=2.2717048746128263, w0=73.0125, w1=13.066124163762431\n",
      "Gradient Descent(276/9999): loss=2.1657638080855235, w0=73.0125, w1=13.188827455286521\n",
      "Gradient Descent(277/9999): loss=2.329763840527595, w0=73.06875000000001, w1=13.253487022344277\n",
      "Gradient Descent(278/9999): loss=2.5394075076347598, w0=73.12500000000001, w1=13.2051249794987\n",
      "Gradient Descent(279/9999): loss=2.4231497808507694, w0=72.95625000000001, w1=13.500851249650047\n",
      "Gradient Descent(280/9999): loss=2.724607267174381, w0=73.06875000000001, w1=13.52704082187301\n",
      "Gradient Descent(281/9999): loss=2.020076170780331, w0=72.9, w1=13.582904023894555\n",
      "Gradient Descent(282/9999): loss=2.7397452968405616, w0=73.125, w1=13.727684908551657\n",
      "Gradient Descent(283/9999): loss=2.278777231734373, w0=73.35, w1=13.67406732223588\n",
      "Gradient Descent(284/9999): loss=2.0973058176103727, w0=73.35, w1=13.353332298576715\n",
      "Gradient Descent(285/9999): loss=2.227149010252478, w0=73.35, w1=13.062041253448086\n",
      "Gradient Descent(286/9999): loss=1.9779978515914352, w0=73.46249999999999, w1=13.032785549890342\n",
      "Gradient Descent(287/9999): loss=2.336637563644716, w0=73.40624999999999, w1=13.305763369815757\n",
      "Gradient Descent(288/9999): loss=2.314293849758047, w0=73.34999999999998, w1=13.304709937246736\n",
      "Gradient Descent(289/9999): loss=1.8290708903292767, w0=73.18124999999998, w1=13.57013744474553\n",
      "Gradient Descent(290/9999): loss=2.3170956234875364, w0=72.95624999999998, w1=13.677367127277847\n",
      "Gradient Descent(291/9999): loss=2.5511753322010104, w0=72.84374999999999, w1=13.393727098748979\n",
      "Gradient Descent(292/9999): loss=2.1420340309349424, w0=72.84374999999999, w1=13.504149741133288\n",
      "Gradient Descent(293/9999): loss=1.9948539550733995, w0=72.89999999999999, w1=13.408566786959229\n",
      "Gradient Descent(294/9999): loss=2.0278525610570357, w0=72.56249999999999, w1=13.256195917834008\n",
      "Gradient Descent(295/9999): loss=1.9721515792747293, w0=72.50624999999998, w1=13.818120012690196\n",
      "Gradient Descent(296/9999): loss=2.485598584596757, w0=72.50624999999998, w1=13.743181195336827\n",
      "Gradient Descent(297/9999): loss=1.9440119113909915, w0=72.56249999999999, w1=13.640982633506217\n",
      "Gradient Descent(298/9999): loss=2.598269597533922, w0=72.67499999999998, w1=13.741888337366218\n",
      "Gradient Descent(299/9999): loss=2.2971677706205234, w0=72.89999999999998, w1=14.006579133656683\n",
      "Gradient Descent(300/9999): loss=2.278940067493716, w0=73.12499999999997, w1=13.962536039333111\n",
      "Gradient Descent(301/9999): loss=1.905882600885588, w0=73.23749999999997, w1=14.135434682978278\n",
      "Gradient Descent(302/9999): loss=1.8603360337926158, w0=73.06874999999997, w1=13.955106074882076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(303/9999): loss=2.1761459663042095, w0=73.23749999999997, w1=13.797683951439621\n",
      "Gradient Descent(304/9999): loss=2.14646755445415, w0=73.01249999999997, w1=13.953804198100858\n",
      "Gradient Descent(305/9999): loss=2.063195425768477, w0=73.01249999999997, w1=14.163187872823428\n",
      "Gradient Descent(306/9999): loss=2.4006868652720614, w0=73.29374999999997, w1=14.183786418180844\n",
      "Gradient Descent(307/9999): loss=2.601167834314133, w0=73.40624999999997, w1=13.857874575164862\n",
      "Gradient Descent(308/9999): loss=2.4102861631682613, w0=73.18124999999998, w1=13.792094048141932\n",
      "Gradient Descent(309/9999): loss=1.7584869990045722, w0=73.18124999999998, w1=13.854203369978048\n",
      "Gradient Descent(310/9999): loss=1.483763640879923, w0=73.01249999999997, w1=13.904697181676068\n",
      "Gradient Descent(311/9999): loss=1.8634653917364834, w0=73.06874999999998, w1=13.982387774451087\n",
      "Gradient Descent(312/9999): loss=2.3716043491875007, w0=72.89999999999998, w1=14.161107596491702\n",
      "Gradient Descent(313/9999): loss=1.8331345777043366, w0=73.01249999999997, w1=14.013451856976525\n",
      "Gradient Descent(314/9999): loss=2.268355988683435, w0=72.78749999999998, w1=14.302238081780594\n",
      "Gradient Descent(315/9999): loss=3.0210440533603773, w0=72.84374999999999, w1=14.149593882714722\n",
      "Gradient Descent(316/9999): loss=2.0128871832264954, w0=73.12499999999999, w1=14.15212117309235\n",
      "Gradient Descent(317/9999): loss=2.0443453633657525, w0=73.12499999999999, w1=14.319508539083708\n",
      "Gradient Descent(318/9999): loss=2.241247151460097, w0=73.18124999999999, w1=14.064392765078878\n",
      "Gradient Descent(319/9999): loss=2.100434431017179, w0=73.29374999999999, w1=14.061048962261777\n",
      "Gradient Descent(320/9999): loss=1.8989809327596332, w0=73.46249999999999, w1=14.0578618133217\n",
      "Gradient Descent(321/9999): loss=2.032667707593052, w0=73.2375, w1=13.85638168116661\n",
      "Gradient Descent(322/9999): loss=2.26120148360087, w0=73.35, w1=13.601811291132925\n",
      "Gradient Descent(323/9999): loss=2.322879063461246, w0=73.29374999999999, w1=13.083675791281072\n",
      "Gradient Descent(324/9999): loss=2.027377231709248, w0=73.06875, w1=13.20951407077185\n",
      "Gradient Descent(325/9999): loss=2.0907478624182607, w0=73.125, w1=13.168321777598308\n",
      "Gradient Descent(326/9999): loss=2.468479678396724, w0=73.18125, w1=13.261923274053462\n",
      "Gradient Descent(327/9999): loss=1.8618296595019381, w0=72.84375, w1=13.216511033941245\n",
      "Gradient Descent(328/9999): loss=2.1254044500662266, w0=73.06875, w1=13.143172103471784\n",
      "Gradient Descent(329/9999): loss=1.8992687161810937, w0=72.675, w1=13.427542737226492\n",
      "Gradient Descent(330/9999): loss=2.5413066867238903, w0=72.7875, w1=13.461042616357176\n",
      "Gradient Descent(331/9999): loss=2.625031627878014, w0=72.89999999999999, w1=13.26165676083537\n",
      "Gradient Descent(332/9999): loss=2.4231879623296444, w0=73.06875, w1=13.06800748931314\n",
      "Gradient Descent(333/9999): loss=1.8420261506980884, w0=72.84375, w1=13.184206525569964\n",
      "Gradient Descent(334/9999): loss=2.2926014797743304, w0=72.7875, w1=13.210893461379232\n",
      "Gradient Descent(335/9999): loss=2.521805357216058, w0=72.89999999999999, w1=13.242372181646504\n",
      "Gradient Descent(336/9999): loss=1.7728323414715925, w0=73.2375, w1=12.960323688290286\n",
      "Gradient Descent(337/9999): loss=1.6674629843831144, w0=73.46249999999999, w1=13.157166016807551\n",
      "Gradient Descent(338/9999): loss=2.334534172845313, w0=73.35, w1=13.18424384751227\n",
      "Gradient Descent(339/9999): loss=2.4992962480372976, w0=73.74374999999999, w1=13.181289476412918\n",
      "Gradient Descent(340/9999): loss=2.051732303334021, w0=73.74374999999999, w1=13.192895095791055\n",
      "Gradient Descent(341/9999): loss=2.904067479942748, w0=73.74374999999999, w1=13.483350759449646\n",
      "Gradient Descent(342/9999): loss=2.3224717007722036, w0=73.57499999999999, w1=13.432216026564134\n",
      "Gradient Descent(343/9999): loss=2.758710072864194, w0=73.46249999999999, w1=13.35307100171438\n",
      "Gradient Descent(344/9999): loss=1.893449580755142, w0=73.40624999999999, w1=13.250494401083206\n",
      "Gradient Descent(345/9999): loss=2.1216291265375773, w0=73.51874999999998, w1=13.233337541014187\n",
      "Gradient Descent(346/9999): loss=2.280031731145778, w0=73.51874999999998, w1=13.033308312937875\n",
      "Gradient Descent(347/9999): loss=2.1100754447479666, w0=73.57499999999999, w1=12.944723308067358\n",
      "Gradient Descent(348/9999): loss=2.542579011387702, w0=73.46249999999999, w1=13.306022368207588\n",
      "Gradient Descent(349/9999): loss=1.9279176337251624, w0=73.35, w1=13.529534123622463\n",
      "Gradient Descent(350/9999): loss=2.2380744587824597, w0=73.125, w1=13.403618391540547\n",
      "Gradient Descent(351/9999): loss=2.335580469371516, w0=72.95625, w1=13.424815728779313\n",
      "Gradient Descent(352/9999): loss=2.256448396970467, w0=73.18124999999999, w1=13.482334337872961\n",
      "Gradient Descent(353/9999): loss=2.1190861095080082, w0=73.06875, w1=13.596952086511704\n",
      "Gradient Descent(354/9999): loss=2.112326101796259, w0=73.01249999999999, w1=13.485840444225635\n",
      "Gradient Descent(355/9999): loss=2.417149690783188, w0=73.01249999999999, w1=13.492210629806511\n",
      "Gradient Descent(356/9999): loss=2.1364119532606445, w0=73.01249999999999, w1=13.906904462749077\n",
      "Gradient Descent(357/9999): loss=2.7407262955764247, w0=73.23749999999998, w1=13.989955736976055\n",
      "Gradient Descent(358/9999): loss=2.4945334034428424, w0=73.29374999999999, w1=14.178749946861956\n",
      "Gradient Descent(359/9999): loss=2.7791837155451278, w0=73.06875, w1=14.256030760977469\n",
      "Gradient Descent(360/9999): loss=2.248527727373757, w0=72.95625, w1=14.220520987408213\n",
      "Gradient Descent(361/9999): loss=2.090103509150258, w0=72.89999999999999, w1=14.107849176666344\n",
      "Gradient Descent(362/9999): loss=1.9700724805022523, w0=72.89999999999999, w1=13.993265040432066\n",
      "Gradient Descent(363/9999): loss=2.6249348422944436, w0=73.01249999999999, w1=14.038001267200158\n",
      "Gradient Descent(364/9999): loss=2.8395872502579804, w0=72.95624999999998, w1=13.736887432389866\n",
      "Gradient Descent(365/9999): loss=2.441825443811785, w0=72.95624999999998, w1=13.914537584979993\n",
      "Gradient Descent(366/9999): loss=2.250193877589818, w0=72.84374999999999, w1=13.935603181999491\n",
      "Gradient Descent(367/9999): loss=2.4341075925211526, w0=73.01249999999999, w1=13.885065095702343\n",
      "Gradient Descent(368/9999): loss=1.8289951332091137, w0=73.23749999999998, w1=13.930972276641208\n",
      "Gradient Descent(369/9999): loss=3.542726947924181, w0=73.18124999999998, w1=13.738691374408505\n",
      "Gradient Descent(370/9999): loss=2.7378120335288267, w0=73.51874999999998, w1=13.355953702150822\n",
      "Gradient Descent(371/9999): loss=2.274040984176574, w0=73.46249999999998, w1=13.608869437249504\n",
      "Gradient Descent(372/9999): loss=2.7718556434716115, w0=73.40624999999997, w1=13.595779633480076\n",
      "Gradient Descent(373/9999): loss=1.7628453718860864, w0=73.63124999999997, w1=13.668418832528458\n",
      "Gradient Descent(374/9999): loss=2.663418142096027, w0=73.57499999999996, w1=13.774210734524326\n",
      "Gradient Descent(375/9999): loss=2.2219337091501554, w0=73.63124999999997, w1=13.667159725924199\n",
      "Gradient Descent(376/9999): loss=1.8338650942946, w0=73.85624999999996, w1=13.861304907167211\n",
      "Gradient Descent(377/9999): loss=2.5221877416941796, w0=73.85624999999996, w1=13.577370420647224\n",
      "Gradient Descent(378/9999): loss=1.9104870553817304, w0=73.46249999999996, w1=13.545550908498821\n",
      "Gradient Descent(379/9999): loss=2.395461812612737, w0=73.40624999999996, w1=13.328789725700243\n",
      "Gradient Descent(380/9999): loss=2.160283470823021, w0=73.46249999999996, w1=13.413171387773257\n",
      "Gradient Descent(381/9999): loss=1.9945418705347815, w0=73.63124999999997, w1=13.342223586869725\n",
      "Gradient Descent(382/9999): loss=2.159093311756256, w0=73.57499999999996, w1=13.2572635005764\n",
      "Gradient Descent(383/9999): loss=2.6198541790677474, w0=73.40624999999996, w1=13.420265854720807\n",
      "Gradient Descent(384/9999): loss=2.002149315413702, w0=73.12499999999996, w1=13.690444153183599\n",
      "Gradient Descent(385/9999): loss=2.7589333773325664, w0=72.89999999999996, w1=13.58023273001884\n",
      "Gradient Descent(386/9999): loss=2.1053816682878463, w0=72.89999999999996, w1=13.613679447381205\n",
      "Gradient Descent(387/9999): loss=2.2610030080652717, w0=72.73124999999996, w1=13.410857119404945\n",
      "Gradient Descent(388/9999): loss=1.8543863877996047, w0=72.56249999999996, w1=13.513117528285383\n",
      "Gradient Descent(389/9999): loss=2.152749032721138, w0=72.78749999999995, w1=13.617331036047341\n",
      "Gradient Descent(390/9999): loss=2.3761396946460156, w0=72.95624999999995, w1=13.585984261233655\n",
      "Gradient Descent(391/9999): loss=2.1718934065584037, w0=73.12499999999996, w1=13.696223782119757\n",
      "Gradient Descent(392/9999): loss=2.308806201216116, w0=72.78749999999995, w1=13.622937861463413\n",
      "Gradient Descent(393/9999): loss=2.669168682338311, w0=72.78749999999995, w1=13.784241043718932\n",
      "Gradient Descent(394/9999): loss=2.3978708908040955, w0=73.01249999999995, w1=13.862628264043684\n",
      "Gradient Descent(395/9999): loss=1.7577016348118502, w0=72.89999999999995, w1=14.185031893138737\n",
      "Gradient Descent(396/9999): loss=2.112149679703374, w0=72.73124999999995, w1=14.2781738868292\n",
      "Gradient Descent(397/9999): loss=3.0751090469330578, w0=73.06874999999995, w1=13.976943308205312\n",
      "Gradient Descent(398/9999): loss=2.163342697927095, w0=73.01249999999995, w1=13.93522274383485\n",
      "Gradient Descent(399/9999): loss=1.8731223445253318, w0=72.95624999999994, w1=13.757381711926465\n",
      "Gradient Descent(400/9999): loss=2.0283659109819814, w0=73.18124999999993, w1=13.809714585296934\n",
      "Gradient Descent(401/9999): loss=2.0770697961600253, w0=73.34999999999994, w1=13.893131457887273\n",
      "Gradient Descent(402/9999): loss=2.250186201957524, w0=73.01249999999993, w1=13.852970543365172\n",
      "Gradient Descent(403/9999): loss=1.852425931265954, w0=72.89999999999993, w1=13.574011194393496\n",
      "Gradient Descent(404/9999): loss=2.4575639499467536, w0=73.01249999999993, w1=13.716118322162576\n",
      "Gradient Descent(405/9999): loss=2.3808859055643246, w0=72.89999999999993, w1=13.612186166839908\n",
      "Gradient Descent(406/9999): loss=1.9286519815889471, w0=73.06874999999994, w1=13.633543557772796\n",
      "Gradient Descent(407/9999): loss=2.058977529847402, w0=73.18124999999993, w1=13.603780474899231\n",
      "Gradient Descent(408/9999): loss=1.908177768187695, w0=73.12499999999993, w1=13.688064310467208\n",
      "Gradient Descent(409/9999): loss=2.1217973853635343, w0=73.23749999999993, w1=13.282563346663526\n",
      "Gradient Descent(410/9999): loss=2.631576572392029, w0=73.23749999999993, w1=13.420306392350655\n",
      "Gradient Descent(411/9999): loss=2.444175260616246, w0=73.12499999999993, w1=13.157693760532208\n",
      "Gradient Descent(412/9999): loss=2.1371143083890263, w0=72.84374999999993, w1=13.239176489853609\n",
      "Gradient Descent(413/9999): loss=2.4914848332901895, w0=73.06874999999992, w1=13.195436913766683\n",
      "Gradient Descent(414/9999): loss=2.3557085169049987, w0=73.12499999999993, w1=13.23189756262979\n",
      "Gradient Descent(415/9999): loss=2.1090678337560966, w0=73.40624999999993, w1=13.28086108211889\n",
      "Gradient Descent(416/9999): loss=2.2233548779985624, w0=73.34999999999992, w1=13.529560418641763\n",
      "Gradient Descent(417/9999): loss=2.3778949085373444, w0=73.06874999999992, w1=13.45469234625788\n",
      "Gradient Descent(418/9999): loss=2.3667876859557477, w0=72.95624999999993, w1=13.284035879715292\n",
      "Gradient Descent(419/9999): loss=2.487848391539364, w0=72.67499999999993, w1=13.178787019534186\n",
      "Gradient Descent(420/9999): loss=2.440787153408816, w0=72.84374999999993, w1=13.243894332629633\n",
      "Gradient Descent(421/9999): loss=1.9051906017539932, w0=72.84374999999993, w1=13.195161907754938\n",
      "Gradient Descent(422/9999): loss=2.38731647070988, w0=73.01249999999993, w1=13.140463680221918\n",
      "Gradient Descent(423/9999): loss=2.538073231357222, w0=72.89999999999993, w1=13.176527831376847\n",
      "Gradient Descent(424/9999): loss=2.114595606428328, w0=72.84374999999993, w1=13.32214588603577\n",
      "Gradient Descent(425/9999): loss=2.4180321150596864, w0=73.01249999999993, w1=13.279565611874524\n",
      "Gradient Descent(426/9999): loss=2.3128012999694496, w0=73.12499999999993, w1=13.441406054745684\n",
      "Gradient Descent(427/9999): loss=2.5646142104105865, w0=73.06874999999992, w1=13.643886271207688\n",
      "Gradient Descent(428/9999): loss=1.8497763158181904, w0=73.06874999999992, w1=13.371340973329819\n",
      "Gradient Descent(429/9999): loss=2.1394841044344517, w0=73.06874999999992, w1=13.376560027984137\n",
      "Gradient Descent(430/9999): loss=2.342517969651312, w0=73.06874999999992, w1=13.238550420646355\n",
      "Gradient Descent(431/9999): loss=2.065706363456953, w0=73.06874999999992, w1=13.336530047501933\n",
      "Gradient Descent(432/9999): loss=2.067631119656884, w0=73.23749999999993, w1=13.501490303858281\n",
      "Gradient Descent(433/9999): loss=2.261397726832845, w0=73.06874999999992, w1=13.830759494859494\n",
      "Gradient Descent(434/9999): loss=2.6617296044495444, w0=73.06874999999992, w1=13.514495877690614\n",
      "Gradient Descent(435/9999): loss=1.9659109598177857, w0=73.01249999999992, w1=13.764993735267568\n",
      "Gradient Descent(436/9999): loss=2.280931664797402, w0=73.18124999999992, w1=13.669373830584806\n",
      "Gradient Descent(437/9999): loss=2.568136917235629, w0=72.95624999999993, w1=13.746134057879908\n",
      "Gradient Descent(438/9999): loss=2.2345252824328945, w0=73.01249999999993, w1=13.839611890042473\n",
      "Gradient Descent(439/9999): loss=2.167536524352884, w0=73.18124999999993, w1=13.770531508026512\n",
      "Gradient Descent(440/9999): loss=2.3458262587201086, w0=73.29374999999993, w1=13.677638084343016\n",
      "Gradient Descent(441/9999): loss=2.1227466956306067, w0=73.23749999999993, w1=13.825088562015058\n",
      "Gradient Descent(442/9999): loss=2.0275929963206516, w0=73.01249999999993, w1=13.724605126074907\n",
      "Gradient Descent(443/9999): loss=2.165287406343979, w0=72.78749999999994, w1=13.738435259272865\n",
      "Gradient Descent(444/9999): loss=2.395893648413786, w0=73.12499999999994, w1=13.733479385994737\n",
      "Gradient Descent(445/9999): loss=2.602473181540582, w0=73.06874999999994, w1=13.71284785924501\n",
      "Gradient Descent(446/9999): loss=2.7588916422039986, w0=72.89999999999993, w1=13.632053619905461\n",
      "Gradient Descent(447/9999): loss=2.5209894871601106, w0=73.06874999999994, w1=13.597392631505032\n",
      "Gradient Descent(448/9999): loss=2.157007882261573, w0=72.89999999999993, w1=13.507014468877284\n",
      "Gradient Descent(449/9999): loss=2.2462443988937055, w0=73.06874999999994, w1=13.443975880594179\n",
      "Gradient Descent(450/9999): loss=2.31817300404468, w0=73.01249999999993, w1=13.459072581856828\n",
      "Gradient Descent(451/9999): loss=1.8788023669428624, w0=73.18124999999993, w1=13.514447726168537\n",
      "Gradient Descent(452/9999): loss=2.40852346594209, w0=73.40624999999993, w1=13.529892921977682\n",
      "Gradient Descent(453/9999): loss=2.0871342264515125, w0=73.68749999999993, w1=13.540521731655287\n",
      "Gradient Descent(454/9999): loss=2.188869845944088, w0=73.68749999999993, w1=13.645582599737962\n",
      "Gradient Descent(455/9999): loss=1.3366270047554163, w0=73.74374999999993, w1=13.674514467323789\n",
      "Gradient Descent(456/9999): loss=2.332666933900736, w0=73.68749999999993, w1=13.8184638684496\n",
      "Gradient Descent(457/9999): loss=2.121877243481994, w0=73.57499999999993, w1=13.510654584789346\n",
      "Gradient Descent(458/9999): loss=1.9275921445165292, w0=73.57499999999993, w1=13.320026082940446\n",
      "Gradient Descent(459/9999): loss=2.2462533579683743, w0=73.74374999999993, w1=13.469347651687013\n",
      "Gradient Descent(460/9999): loss=2.5660538938444843, w0=73.57499999999993, w1=13.409953378909885\n",
      "Gradient Descent(461/9999): loss=2.3741800651985754, w0=73.51874999999993, w1=13.639344853723427\n",
      "Gradient Descent(462/9999): loss=1.7788080016756647, w0=73.40624999999993, w1=13.611700990645101\n",
      "Gradient Descent(463/9999): loss=2.5694392761233793, w0=73.29374999999993, w1=13.459892826300116\n",
      "Gradient Descent(464/9999): loss=1.8867402652289424, w0=73.34999999999994, w1=13.536661652163566\n",
      "Gradient Descent(465/9999): loss=2.482049620541793, w0=73.51874999999994, w1=13.43091124165738\n",
      "Gradient Descent(466/9999): loss=2.4715941533034718, w0=73.12499999999994, w1=13.583849794794881\n",
      "Gradient Descent(467/9999): loss=2.7493534191952693, w0=73.01249999999995, w1=13.641915572459897\n",
      "Gradient Descent(468/9999): loss=2.47098311943133, w0=73.29374999999995, w1=13.783203249432804\n",
      "Gradient Descent(469/9999): loss=2.3468933353803747, w0=73.12499999999994, w1=13.822861358112233\n",
      "Gradient Descent(470/9999): loss=2.9718934792998244, w0=72.89999999999995, w1=13.580805371918327\n",
      "Gradient Descent(471/9999): loss=2.1186832320142317, w0=72.84374999999994, w1=13.665772466558352\n",
      "Gradient Descent(472/9999): loss=2.424866290278776, w0=72.67499999999994, w1=13.758230263593777\n",
      "Gradient Descent(473/9999): loss=2.427178697650719, w0=72.84374999999994, w1=13.998365070362258\n",
      "Gradient Descent(474/9999): loss=2.257981734089574, w0=72.89999999999995, w1=13.731720617168271\n",
      "Gradient Descent(475/9999): loss=2.6094894625095852, w0=73.01249999999995, w1=13.652574650488216\n",
      "Gradient Descent(476/9999): loss=2.069918478854658, w0=72.89999999999995, w1=13.894426275805285\n",
      "Gradient Descent(477/9999): loss=2.2941307974333904, w0=72.78749999999995, w1=13.77111344327193\n",
      "Gradient Descent(478/9999): loss=2.02278021960551, w0=73.01249999999995, w1=13.73340794793865\n",
      "Gradient Descent(479/9999): loss=2.1390917976822363, w0=73.01249999999995, w1=13.481021106387905\n",
      "Gradient Descent(480/9999): loss=1.5812944343467474, w0=73.34999999999995, w1=13.74884576971743\n",
      "Gradient Descent(481/9999): loss=2.6506383963170115, w0=73.18124999999995, w1=13.485600164233821\n",
      "Gradient Descent(482/9999): loss=1.8515163394004572, w0=73.12499999999994, w1=13.264347633454243\n",
      "Gradient Descent(483/9999): loss=2.565742888493574, w0=73.06874999999994, w1=13.285484389814238\n",
      "Gradient Descent(484/9999): loss=2.1333511441424755, w0=73.18124999999993, w1=13.072752852922463\n",
      "Gradient Descent(485/9999): loss=2.4940210164984746, w0=73.01249999999993, w1=13.330430042408773\n",
      "Gradient Descent(486/9999): loss=2.2658006480931503, w0=73.23749999999993, w1=13.152335423767175\n",
      "Gradient Descent(487/9999): loss=2.3260944742116023, w0=73.29374999999993, w1=13.073706111817843\n",
      "Gradient Descent(488/9999): loss=2.3803572306944782, w0=73.34999999999994, w1=13.32256135817814\n",
      "Gradient Descent(489/9999): loss=2.1218648381473715, w0=73.01249999999993, w1=13.281192721396787\n",
      "Gradient Descent(490/9999): loss=2.001472245601125, w0=73.12499999999993, w1=13.396674765745823\n",
      "Gradient Descent(491/9999): loss=2.1150935003840736, w0=73.34999999999992, w1=13.529675299331132\n",
      "Gradient Descent(492/9999): loss=2.7031596086775664, w0=73.40624999999993, w1=13.629764798495874\n",
      "Gradient Descent(493/9999): loss=2.145703649658474, w0=73.12499999999993, w1=13.536890981488918\n",
      "Gradient Descent(494/9999): loss=2.256451702475276, w0=73.01249999999993, w1=13.730238665291536\n",
      "Gradient Descent(495/9999): loss=2.344389541746296, w0=73.23749999999993, w1=13.783284334498289\n",
      "Gradient Descent(496/9999): loss=2.209760721864051, w0=73.23749999999993, w1=13.975924500145782\n",
      "Gradient Descent(497/9999): loss=2.792484663307814, w0=73.18124999999992, w1=13.799991805006634\n",
      "Gradient Descent(498/9999): loss=2.5482075007915848, w0=73.29374999999992, w1=13.513340469408078\n",
      "Gradient Descent(499/9999): loss=2.086649831812867, w0=73.12499999999991, w1=13.81868086184277\n",
      "Gradient Descent(500/9999): loss=2.1800640816638976, w0=73.34999999999991, w1=13.862949489841927\n",
      "Gradient Descent(501/9999): loss=2.24878771677224, w0=73.51874999999991, w1=13.915498816440365\n",
      "Gradient Descent(502/9999): loss=2.241890101725157, w0=73.4624999999999, w1=13.925172467868997\n",
      "Gradient Descent(503/9999): loss=2.3147166102859535, w0=73.34999999999991, w1=14.004647355816017\n",
      "Gradient Descent(504/9999): loss=2.4675083579590953, w0=73.40624999999991, w1=14.1049411619276\n",
      "Gradient Descent(505/9999): loss=2.635617228832256, w0=73.57499999999992, w1=14.207233669296238\n",
      "Gradient Descent(506/9999): loss=2.19491659972002, w0=73.40624999999991, w1=14.315901068345637\n",
      "Gradient Descent(507/9999): loss=2.466658332876506, w0=73.23749999999991, w1=14.389748879288833\n",
      "Gradient Descent(508/9999): loss=2.1930254210833766, w0=73.34999999999991, w1=14.161917925101745\n",
      "Gradient Descent(509/9999): loss=2.487388918974446, w0=72.95624999999991, w1=13.877960039779033\n",
      "Gradient Descent(510/9999): loss=2.145889700149696, w0=73.1812499999999, w1=13.552404260051462\n",
      "Gradient Descent(511/9999): loss=2.097872510817619, w0=73.2937499999999, w1=13.591524150704434\n",
      "Gradient Descent(512/9999): loss=2.04561399828227, w0=73.5749999999999, w1=13.411901755329959\n",
      "Gradient Descent(513/9999): loss=2.351032886948749, w0=73.7999999999999, w1=13.591051236964693\n",
      "Gradient Descent(514/9999): loss=2.1157218976386734, w0=73.6874999999999, w1=13.565484085022543\n",
      "Gradient Descent(515/9999): loss=2.403771336821637, w0=73.6874999999999, w1=13.709325276757001\n",
      "Gradient Descent(516/9999): loss=2.2768818272691496, w0=73.5749999999999, w1=13.546132167965549\n",
      "Gradient Descent(517/9999): loss=2.0204391676981395, w0=73.5749999999999, w1=13.374004117471875\n",
      "Gradient Descent(518/9999): loss=2.788052924840291, w0=73.4624999999999, w1=13.46341027879489\n",
      "Gradient Descent(519/9999): loss=2.2313362630655798, w0=73.34999999999991, w1=13.231136989158047\n",
      "Gradient Descent(520/9999): loss=2.0776127488867058, w0=73.23749999999991, w1=13.578774036081615\n",
      "Gradient Descent(521/9999): loss=2.0030466000464306, w0=73.12499999999991, w1=13.450906013105019\n",
      "Gradient Descent(522/9999): loss=1.551788364098667, w0=73.23749999999991, w1=13.64891255893783\n",
      "Gradient Descent(523/9999): loss=2.5051416218113736, w0=73.23749999999991, w1=13.580177137637673\n",
      "Gradient Descent(524/9999): loss=2.6544038677094632, w0=73.34999999999991, w1=13.407474039454668\n",
      "Gradient Descent(525/9999): loss=2.7333772982173263, w0=73.51874999999991, w1=13.410052215845658\n",
      "Gradient Descent(526/9999): loss=2.1869479091726807, w0=73.68749999999991, w1=13.71853529071002\n",
      "Gradient Descent(527/9999): loss=1.7610968119523907, w0=73.85624999999992, w1=13.892520513113054\n",
      "Gradient Descent(528/9999): loss=2.412473148153008, w0=73.91249999999992, w1=13.638031740595492\n",
      "Gradient Descent(529/9999): loss=2.4911323532559537, w0=73.85624999999992, w1=13.89473620907184\n",
      "Gradient Descent(530/9999): loss=2.0539691856678033, w0=73.74374999999992, w1=13.744646601373296\n",
      "Gradient Descent(531/9999): loss=2.092162076940765, w0=73.79999999999993, w1=13.703262218887168\n",
      "Gradient Descent(532/9999): loss=2.609279748947381, w0=73.74374999999992, w1=13.596775930434328\n",
      "Gradient Descent(533/9999): loss=2.595307607201257, w0=73.63124999999992, w1=13.61110255174445\n",
      "Gradient Descent(534/9999): loss=1.8726278989145468, w0=73.29374999999992, w1=13.61475047810605\n",
      "Gradient Descent(535/9999): loss=2.3445727069868347, w0=73.01249999999992, w1=13.510777575622779\n",
      "Gradient Descent(536/9999): loss=2.336234070401068, w0=72.89999999999992, w1=13.403049004285537\n",
      "Gradient Descent(537/9999): loss=1.720343240117394, w0=72.95624999999993, w1=13.699476292935357\n",
      "Gradient Descent(538/9999): loss=1.9756163898236065, w0=73.40624999999993, w1=13.692468650954343\n",
      "Gradient Descent(539/9999): loss=1.7367790163281513, w0=73.46249999999993, w1=13.634424664597983\n",
      "Gradient Descent(540/9999): loss=1.8331153577425732, w0=73.34999999999994, w1=13.481820086338967\n",
      "Gradient Descent(541/9999): loss=2.2422225115959096, w0=73.29374999999993, w1=13.45602463956987\n",
      "Gradient Descent(542/9999): loss=2.40035864177902, w0=73.63124999999994, w1=13.687076470123097\n",
      "Gradient Descent(543/9999): loss=2.3096019505096237, w0=73.46249999999993, w1=13.865596175729966\n",
      "Gradient Descent(544/9999): loss=2.2661698322860078, w0=73.40624999999993, w1=13.692018201460677\n",
      "Gradient Descent(545/9999): loss=2.2324580211744873, w0=73.46249999999993, w1=13.753263357217028\n",
      "Gradient Descent(546/9999): loss=1.7970753592144058, w0=73.51874999999994, w1=13.761372890140668\n",
      "Gradient Descent(547/9999): loss=2.446342879252352, w0=73.51874999999994, w1=13.819067002996416\n",
      "Gradient Descent(548/9999): loss=1.96252331491801, w0=73.29374999999995, w1=13.804040514972629\n",
      "Gradient Descent(549/9999): loss=2.345616414078866, w0=73.34999999999995, w1=13.974166232982911\n",
      "Gradient Descent(550/9999): loss=2.6131124853974157, w0=73.40624999999996, w1=13.83275862662729\n",
      "Gradient Descent(551/9999): loss=2.1776437278264003, w0=73.51874999999995, w1=13.737462833546031\n",
      "Gradient Descent(552/9999): loss=1.9445118821450216, w0=73.63124999999995, w1=13.853597431161392\n",
      "Gradient Descent(553/9999): loss=2.0179087488949032, w0=73.63124999999995, w1=13.564873948065204\n",
      "Gradient Descent(554/9999): loss=2.463930449062624, w0=73.57499999999995, w1=13.75101948672227\n",
      "Gradient Descent(555/9999): loss=1.860574803976844, w0=73.46249999999995, w1=13.661953847215692\n",
      "Gradient Descent(556/9999): loss=1.872384357529217, w0=73.18124999999995, w1=13.632019058956777\n",
      "Gradient Descent(557/9999): loss=1.8024106415833883, w0=73.34999999999995, w1=13.88737156247867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(558/9999): loss=2.2468632769475576, w0=73.40624999999996, w1=13.838319707351937\n",
      "Gradient Descent(559/9999): loss=2.455911005989102, w0=73.40624999999996, w1=13.755457408030498\n",
      "Gradient Descent(560/9999): loss=1.951803814333142, w0=73.46249999999996, w1=13.702599916054332\n",
      "Gradient Descent(561/9999): loss=2.3682876077306445, w0=73.18124999999996, w1=13.699953370748796\n",
      "Gradient Descent(562/9999): loss=2.2339260397171454, w0=73.18124999999996, w1=13.697561774282944\n",
      "Gradient Descent(563/9999): loss=2.3709794010453087, w0=73.18124999999996, w1=13.918546077097817\n",
      "Gradient Descent(564/9999): loss=2.489243494406865, w0=73.40624999999996, w1=13.930023983429223\n",
      "Gradient Descent(565/9999): loss=2.361299484751866, w0=73.40624999999996, w1=13.872843405631675\n",
      "Gradient Descent(566/9999): loss=2.258479057675567, w0=73.46249999999996, w1=13.730259805263497\n",
      "Gradient Descent(567/9999): loss=1.8570267834010603, w0=73.34999999999997, w1=13.471718336366353\n",
      "Gradient Descent(568/9999): loss=2.0474383582896785, w0=73.40624999999997, w1=13.496980821090547\n",
      "Gradient Descent(569/9999): loss=2.0694719741330356, w0=73.63124999999997, w1=13.629842459516444\n",
      "Gradient Descent(570/9999): loss=2.833158814083475, w0=73.57499999999996, w1=13.563104237354867\n",
      "Gradient Descent(571/9999): loss=1.9413247605893278, w0=73.40624999999996, w1=13.532139386905053\n",
      "Gradient Descent(572/9999): loss=1.98938424933279, w0=73.68749999999996, w1=13.572052873444012\n",
      "Gradient Descent(573/9999): loss=2.1579007223716467, w0=73.68749999999996, w1=13.481354640658013\n",
      "Gradient Descent(574/9999): loss=1.9321043985069772, w0=73.46249999999996, w1=13.507983353653861\n",
      "Gradient Descent(575/9999): loss=2.2341210018269093, w0=73.23749999999997, w1=13.639431993503537\n",
      "Gradient Descent(576/9999): loss=1.9662670185455127, w0=73.40624999999997, w1=13.709267471243606\n",
      "Gradient Descent(577/9999): loss=2.6796394461399355, w0=73.23749999999997, w1=13.524869316084843\n",
      "Gradient Descent(578/9999): loss=2.6714873890927913, w0=73.34999999999997, w1=13.709642061678705\n",
      "Gradient Descent(579/9999): loss=2.2388726804355983, w0=73.18124999999996, w1=13.59279275514814\n",
      "Gradient Descent(580/9999): loss=2.36227068767832, w0=73.34999999999997, w1=13.569868908174668\n",
      "Gradient Descent(581/9999): loss=2.463123814854977, w0=73.29374999999996, w1=13.589053358840017\n",
      "Gradient Descent(582/9999): loss=2.3446030372804123, w0=72.95624999999995, w1=13.695224462784138\n",
      "Gradient Descent(583/9999): loss=2.569783261785262, w0=72.78749999999995, w1=13.548650517265044\n",
      "Gradient Descent(584/9999): loss=2.1839550641486722, w0=72.61874999999995, w1=13.884283628299766\n",
      "Gradient Descent(585/9999): loss=2.188474592620214, w0=72.95624999999995, w1=13.998453972038812\n",
      "Gradient Descent(586/9999): loss=2.427014107087805, w0=72.95624999999995, w1=14.171174689736269\n",
      "Gradient Descent(587/9999): loss=2.4455866596614824, w0=72.78749999999995, w1=14.279751181208287\n",
      "Gradient Descent(588/9999): loss=2.1019381737299665, w0=72.73124999999995, w1=13.963879224734097\n",
      "Gradient Descent(589/9999): loss=2.5357152571458323, w0=72.89999999999995, w1=13.824243145688701\n",
      "Gradient Descent(590/9999): loss=1.9345602667437374, w0=73.12499999999994, w1=13.832606252319232\n",
      "Gradient Descent(591/9999): loss=1.6543827319996363, w0=73.01249999999995, w1=13.755406834234567\n",
      "Gradient Descent(592/9999): loss=2.517083403761773, w0=73.01249999999995, w1=13.922433531483659\n",
      "Gradient Descent(593/9999): loss=2.400813446275622, w0=73.18124999999995, w1=13.939754067429565\n",
      "Gradient Descent(594/9999): loss=2.5702759849017607, w0=73.34999999999995, w1=13.723179079376312\n",
      "Gradient Descent(595/9999): loss=2.8156265099747517, w0=73.46249999999995, w1=13.86298863271231\n",
      "Gradient Descent(596/9999): loss=1.8564850690234034, w0=73.51874999999995, w1=14.212681066850134\n",
      "Gradient Descent(597/9999): loss=1.8337086777644112, w0=73.40624999999996, w1=14.244967549756723\n",
      "Gradient Descent(598/9999): loss=2.0493497331989703, w0=73.18124999999996, w1=14.398531447827747\n",
      "Gradient Descent(599/9999): loss=2.247383607111602, w0=73.29374999999996, w1=14.391147901806821\n",
      "Gradient Descent(600/9999): loss=2.432019191669803, w0=73.34999999999997, w1=14.312802872911744\n",
      "Gradient Descent(601/9999): loss=1.995578096585703, w0=73.23749999999997, w1=14.329966040154245\n",
      "Gradient Descent(602/9999): loss=2.4581659470880783, w0=73.34999999999997, w1=14.135422501013023\n",
      "Gradient Descent(603/9999): loss=2.1686195441751357, w0=73.12499999999997, w1=14.386896735521088\n",
      "Gradient Descent(604/9999): loss=1.94284229142939, w0=73.23749999999997, w1=14.473815306869566\n",
      "Gradient Descent(605/9999): loss=2.4719538610539407, w0=73.23749999999997, w1=14.041824520785674\n",
      "Gradient Descent(606/9999): loss=2.4857450499812366, w0=73.23749999999997, w1=13.931057435795388\n",
      "Gradient Descent(607/9999): loss=1.8051354878276564, w0=73.06874999999997, w1=13.90567823951553\n",
      "Gradient Descent(608/9999): loss=2.127057467512094, w0=72.89999999999996, w1=13.910793347917306\n",
      "Gradient Descent(609/9999): loss=2.1624814553033316, w0=73.06874999999997, w1=13.766190322369253\n",
      "Gradient Descent(610/9999): loss=2.472439756294941, w0=73.06874999999997, w1=13.956412701666851\n",
      "Gradient Descent(611/9999): loss=2.9049186204768827, w0=73.18124999999996, w1=13.854780685869738\n",
      "Gradient Descent(612/9999): loss=2.164717299433616, w0=73.18124999999996, w1=13.782394025432172\n",
      "Gradient Descent(613/9999): loss=2.3553557111748242, w0=73.06874999999997, w1=13.766597748233641\n",
      "Gradient Descent(614/9999): loss=2.255491051782622, w0=73.18124999999996, w1=13.471320750790143\n",
      "Gradient Descent(615/9999): loss=2.5149984128672496, w0=73.18124999999996, w1=13.595503474025188\n",
      "Gradient Descent(616/9999): loss=2.741655615510153, w0=73.23749999999997, w1=13.427560405225654\n",
      "Gradient Descent(617/9999): loss=2.260029227491737, w0=73.12499999999997, w1=13.489430866081616\n",
      "Gradient Descent(618/9999): loss=1.8600300878809974, w0=72.95624999999997, w1=13.75871818618323\n",
      "Gradient Descent(619/9999): loss=2.6204374232794807, w0=73.12499999999997, w1=13.696712795589011\n",
      "Gradient Descent(620/9999): loss=2.400547122578537, w0=73.18124999999998, w1=13.703501962428277\n",
      "Gradient Descent(621/9999): loss=2.285432707145742, w0=73.46249999999998, w1=13.47218939826434\n",
      "Gradient Descent(622/9999): loss=1.8764173496811554, w0=73.51874999999998, w1=13.606770005075168\n",
      "Gradient Descent(623/9999): loss=1.938125995095065, w0=73.40624999999999, w1=13.589630593258146\n",
      "Gradient Descent(624/9999): loss=2.4014178395170833, w0=73.23749999999998, w1=13.312085876952347\n",
      "Gradient Descent(625/9999): loss=2.314351529621389, w0=73.57499999999999, w1=13.325624714448436\n",
      "Gradient Descent(626/9999): loss=2.6502350584096153, w0=73.40624999999999, w1=13.514408054763951\n",
      "Gradient Descent(627/9999): loss=2.1790222581539584, w0=73.06874999999998, w1=13.27352642610673\n",
      "Gradient Descent(628/9999): loss=2.1496527667357235, w0=73.18124999999998, w1=13.252279170551954\n",
      "Gradient Descent(629/9999): loss=1.7277305744373068, w0=72.95624999999998, w1=13.270171557192677\n",
      "Gradient Descent(630/9999): loss=2.863183130774832, w0=73.01249999999999, w1=13.420254117357675\n",
      "Gradient Descent(631/9999): loss=2.0728254093721583, w0=73.12499999999999, w1=13.441182861811516\n",
      "Gradient Descent(632/9999): loss=2.096242651775362, w0=73.01249999999999, w1=13.627758827502232\n",
      "Gradient Descent(633/9999): loss=2.2259932860511857, w0=73.12499999999999, w1=13.616485051442108\n",
      "Gradient Descent(634/9999): loss=1.8878878866330548, w0=73.12499999999999, w1=13.507212466832492\n",
      "Gradient Descent(635/9999): loss=2.309665599245947, w0=73.18124999999999, w1=13.373421451606747\n",
      "Gradient Descent(636/9999): loss=1.7975103330115536, w0=72.89999999999999, w1=13.522285971437125\n",
      "Gradient Descent(637/9999): loss=2.339482624593601, w0=72.95625, w1=13.6858120111808\n",
      "Gradient Descent(638/9999): loss=1.8757222238656837, w0=72.84375, w1=13.813964388737602\n",
      "Gradient Descent(639/9999): loss=2.382811253573864, w0=73.06875, w1=14.155101294675346\n",
      "Gradient Descent(640/9999): loss=2.479319967706396, w0=73.01249999999999, w1=14.296430426646854\n",
      "Gradient Descent(641/9999): loss=2.2165733086968027, w0=72.7875, w1=14.171184951434261\n",
      "Gradient Descent(642/9999): loss=2.8467379260698045, w0=72.7875, w1=14.102309492241979\n",
      "Gradient Descent(643/9999): loss=2.256720193281784, w0=72.5625, w1=14.109547101085262\n",
      "Gradient Descent(644/9999): loss=2.592298574103697, w0=72.675, w1=14.113623406423976\n",
      "Gradient Descent(645/9999): loss=1.8662450950125482, w0=72.95625, w1=14.131237993927307\n",
      "Gradient Descent(646/9999): loss=2.1360969699218595, w0=73.125, w1=14.07048325079131\n",
      "Gradient Descent(647/9999): loss=2.028672138314117, w0=73.18125, w1=13.666122972809294\n",
      "Gradient Descent(648/9999): loss=2.488866799081355, w0=73.0125, w1=13.458330386182443\n",
      "Gradient Descent(649/9999): loss=2.0871522497240647, w0=72.84375, w1=13.746929889514519\n",
      "Gradient Descent(650/9999): loss=1.6823621298064486, w0=72.7875, w1=13.700746701423343\n",
      "Gradient Descent(651/9999): loss=1.972614766447813, w0=72.84375, w1=13.541383670078986\n",
      "Gradient Descent(652/9999): loss=2.382131850708438, w0=72.9, w1=13.427786430414939\n",
      "Gradient Descent(653/9999): loss=2.3633249062507113, w0=73.06875000000001, w1=13.338745957232382\n",
      "Gradient Descent(654/9999): loss=1.914758980887198, w0=73.0125, w1=13.397258302860727\n",
      "Gradient Descent(655/9999): loss=1.7409978220929432, w0=73.40625, w1=13.409092830521667\n",
      "Gradient Descent(656/9999): loss=2.2501405460990638, w0=72.9, w1=13.282484507983357\n",
      "Gradient Descent(657/9999): loss=2.6865790861525136, w0=72.78750000000001, w1=13.50509460691793\n",
      "Gradient Descent(658/9999): loss=2.4795273592670894, w0=73.12500000000001, w1=13.53423370724279\n",
      "Gradient Descent(659/9999): loss=1.9626686235483486, w0=72.90000000000002, w1=13.377043193489953\n",
      "Gradient Descent(660/9999): loss=1.6567595612187722, w0=72.84375000000001, w1=13.655404499276793\n",
      "Gradient Descent(661/9999): loss=1.863542566995665, w0=72.56250000000001, w1=13.472101944015806\n",
      "Gradient Descent(662/9999): loss=2.4952476500174035, w0=72.50625000000001, w1=13.695417797350709\n",
      "Gradient Descent(663/9999): loss=1.9345108876548918, w0=72.67500000000001, w1=13.629484714776636\n",
      "Gradient Descent(664/9999): loss=2.2897244396630225, w0=72.61875, w1=13.703354462581924\n",
      "Gradient Descent(665/9999): loss=1.739863765552881, w0=72.50625000000001, w1=13.779104463832201\n",
      "Gradient Descent(666/9999): loss=2.1721082527501716, w0=72.50625000000001, w1=13.661102385570093\n",
      "Gradient Descent(667/9999): loss=2.187410311322713, w0=72.61875, w1=13.51189268703322\n",
      "Gradient Descent(668/9999): loss=2.157194960386726, w0=72.9, w1=13.554510509634202\n",
      "Gradient Descent(669/9999): loss=1.8103682006389805, w0=72.84375, w1=13.798674129695064\n",
      "Gradient Descent(670/9999): loss=2.3752751030368198, w0=72.73125, w1=14.023275483404705\n",
      "Gradient Descent(671/9999): loss=1.962164201772592, w0=72.9, w1=13.942021313344986\n",
      "Gradient Descent(672/9999): loss=2.324674286772102, w0=72.9, w1=13.725631046848077\n",
      "Gradient Descent(673/9999): loss=2.2359347551627957, w0=72.9, w1=13.39323407432286\n",
      "Gradient Descent(674/9999): loss=2.564803859636778, w0=73.06875000000001, w1=13.539700826989767\n",
      "Gradient Descent(675/9999): loss=1.73057796054739, w0=73.23750000000001, w1=13.292135017611866\n",
      "Gradient Descent(676/9999): loss=2.0210022362259585, w0=73.29375000000002, w1=13.405061865510284\n",
      "Gradient Descent(677/9999): loss=2.5775676920321438, w0=73.06875000000002, w1=13.567947186607828\n",
      "Gradient Descent(678/9999): loss=2.4122428783419236, w0=73.18125000000002, w1=13.602109673647377\n",
      "Gradient Descent(679/9999): loss=2.285474422929208, w0=73.35000000000002, w1=13.60914814375814\n",
      "Gradient Descent(680/9999): loss=2.057580028143061, w0=73.18125000000002, w1=13.502091274616413\n",
      "Gradient Descent(681/9999): loss=2.6301153269990363, w0=73.12500000000001, w1=13.795276556931023\n",
      "Gradient Descent(682/9999): loss=2.1640521768855185, w0=73.40625000000001, w1=13.375376654414524\n",
      "Gradient Descent(683/9999): loss=2.368369454054538, w0=73.35000000000001, w1=13.547235363146767\n",
      "Gradient Descent(684/9999): loss=1.991226990228355, w0=73.35000000000001, w1=13.458700356204824\n",
      "Gradient Descent(685/9999): loss=2.192880872435971, w0=73.23750000000001, w1=13.357167714555805\n",
      "Gradient Descent(686/9999): loss=2.5042464708941234, w0=73.01250000000002, w1=13.243381472659394\n",
      "Gradient Descent(687/9999): loss=2.596287757230559, w0=73.46250000000002, w1=13.23561694876876\n",
      "Gradient Descent(688/9999): loss=2.1992264984839176, w0=73.40625000000001, w1=13.467009252895018\n",
      "Gradient Descent(689/9999): loss=1.8731476191273118, w0=73.35000000000001, w1=13.693795838678378\n",
      "Gradient Descent(690/9999): loss=2.3110096719914335, w0=73.23750000000001, w1=13.63036735022637\n",
      "Gradient Descent(691/9999): loss=2.2550459875739683, w0=73.01250000000002, w1=13.637616188864325\n",
      "Gradient Descent(692/9999): loss=2.512637379257489, w0=73.23750000000001, w1=13.638043741240303\n",
      "Gradient Descent(693/9999): loss=2.141010322008389, w0=73.4625, w1=13.664357166781356\n",
      "Gradient Descent(694/9999): loss=2.595429117516308, w0=73.125, w1=13.338491545039574\n",
      "Gradient Descent(695/9999): loss=2.539865156404214, w0=73.0125, w1=13.290595955378059\n",
      "Gradient Descent(696/9999): loss=2.5052362823020395, w0=73.0125, w1=13.101395466889384\n",
      "Gradient Descent(697/9999): loss=2.412633302351314, w0=73.0125, w1=12.976313733284357\n",
      "Gradient Descent(698/9999): loss=2.0910167786879126, w0=73.06875000000001, w1=13.08790355985969\n",
      "Gradient Descent(699/9999): loss=1.9601181793367304, w0=72.95625000000001, w1=13.450766007588653\n",
      "Gradient Descent(700/9999): loss=2.137132816464754, w0=72.95625000000001, w1=13.582401721275138\n",
      "Gradient Descent(701/9999): loss=1.9533301209613338, w0=73.01250000000002, w1=13.412604274325027\n",
      "Gradient Descent(702/9999): loss=2.0278901154984177, w0=72.95625000000001, w1=13.341249821738318\n",
      "Gradient Descent(703/9999): loss=2.9560028778889396, w0=73.06875000000001, w1=13.430952653340656\n",
      "Gradient Descent(704/9999): loss=2.3066966652083103, w0=72.95625000000001, w1=13.435364337204897\n",
      "Gradient Descent(705/9999): loss=2.5494910847226, w0=73.06875000000001, w1=13.309013550832946\n",
      "Gradient Descent(706/9999): loss=2.3663778311362114, w0=72.9, w1=13.353719856345835\n",
      "Gradient Descent(707/9999): loss=1.9493250001788018, w0=72.84375, w1=13.241573791479608\n",
      "Gradient Descent(708/9999): loss=2.1346302777085606, w0=72.9, w1=13.197608898986532\n",
      "Gradient Descent(709/9999): loss=2.0596229909707806, w0=72.78750000000001, w1=13.51317922928945\n",
      "Gradient Descent(710/9999): loss=2.4787532738171465, w0=72.78750000000001, w1=13.432300391397364\n",
      "Gradient Descent(711/9999): loss=2.1757442241143687, w0=72.73125, w1=13.446226171122168\n",
      "Gradient Descent(712/9999): loss=2.354890578113289, w0=72.78750000000001, w1=13.553192760444725\n",
      "Gradient Descent(713/9999): loss=2.1730609395811706, w0=73.0125, w1=13.374625970883937\n",
      "Gradient Descent(714/9999): loss=2.5906584438613516, w0=73.2375, w1=13.434752767142516\n",
      "Gradient Descent(715/9999): loss=2.5027309250211074, w0=73.29375, w1=13.5402168587438\n",
      "Gradient Descent(716/9999): loss=2.1586585082068463, w0=73.18125, w1=13.430103968201205\n",
      "Gradient Descent(717/9999): loss=1.8388825064118213, w0=73.06875000000001, w1=13.674942202485006\n",
      "Gradient Descent(718/9999): loss=2.7366990501953508, w0=73.23750000000001, w1=13.516789872540505\n",
      "Gradient Descent(719/9999): loss=2.4563587950675005, w0=73.29375000000002, w1=13.5514453865018\n",
      "Gradient Descent(720/9999): loss=1.916001939551005, w0=73.06875000000002, w1=13.290110526163419\n",
      "Gradient Descent(721/9999): loss=2.0376601825065688, w0=73.18125000000002, w1=13.169501705414952\n",
      "Gradient Descent(722/9999): loss=2.4350845269638413, w0=73.12500000000001, w1=13.687245773829641\n",
      "Gradient Descent(723/9999): loss=2.044328072295241, w0=72.90000000000002, w1=13.577124634197865\n",
      "Gradient Descent(724/9999): loss=2.2572603729200527, w0=72.90000000000002, w1=13.60071238109209\n",
      "Gradient Descent(725/9999): loss=2.0724576777534005, w0=72.73125000000002, w1=13.622711479352395\n",
      "Gradient Descent(726/9999): loss=1.969660232432156, w0=72.61875000000002, w1=13.60922694971078\n",
      "Gradient Descent(727/9999): loss=2.0737976745839655, w0=72.78750000000002, w1=13.453743487842663\n",
      "Gradient Descent(728/9999): loss=2.0730284486017037, w0=72.73125000000002, w1=13.545005447800564\n",
      "Gradient Descent(729/9999): loss=2.320713817687888, w0=72.67500000000001, w1=13.237978858194118\n",
      "Gradient Descent(730/9999): loss=2.361028669783022, w0=72.67500000000001, w1=13.17286032575284\n",
      "Gradient Descent(731/9999): loss=2.4779121583354726, w0=72.73125000000002, w1=13.433786866718972\n",
      "Gradient Descent(732/9999): loss=2.36398583756817, w0=72.95625000000001, w1=13.412798377105801\n",
      "Gradient Descent(733/9999): loss=2.1931085174259177, w0=73.06875000000001, w1=13.737365662964544\n",
      "Gradient Descent(734/9999): loss=2.2431043194413967, w0=73.29375, w1=13.88330064736106\n",
      "Gradient Descent(735/9999): loss=2.5449555307352476, w0=73.29375, w1=14.047374472939088\n",
      "Gradient Descent(736/9999): loss=2.3028467334500684, w0=73.29375, w1=14.070849380303198\n",
      "Gradient Descent(737/9999): loss=2.524807493456639, w0=73.18125, w1=14.10873612652436\n",
      "Gradient Descent(738/9999): loss=2.6260278989026027, w0=73.63125000000001, w1=13.956419719738285\n",
      "Gradient Descent(739/9999): loss=2.827306514003469, w0=73.4625, w1=14.07612094530805\n",
      "Gradient Descent(740/9999): loss=2.0625534499965665, w0=73.29375, w1=13.95426011885856\n",
      "Gradient Descent(741/9999): loss=1.7814055916013776, w0=73.40625, w1=13.82789566816298\n",
      "Gradient Descent(742/9999): loss=2.067074640336179, w0=73.40625, w1=13.628793648466205\n",
      "Gradient Descent(743/9999): loss=2.001006097155826, w0=73.63125, w1=13.842935615446203\n",
      "Gradient Descent(744/9999): loss=2.361890891362978, w0=73.40625, w1=13.893866076741718\n",
      "Gradient Descent(745/9999): loss=2.6552870052185042, w0=73.06875, w1=14.005904001124911\n",
      "Gradient Descent(746/9999): loss=2.4765137235137926, w0=72.84375, w1=14.043005443140846\n",
      "Gradient Descent(747/9999): loss=2.3899554032517907, w0=73.0125, w1=14.138531007323358\n",
      "Gradient Descent(748/9999): loss=2.500247442548633, w0=72.95625, w1=14.187858490628313\n",
      "Gradient Descent(749/9999): loss=1.5920998916780422, w0=73.0125, w1=14.21382551587479\n",
      "Gradient Descent(750/9999): loss=2.204374161454164, w0=73.125, w1=13.941202652947329\n",
      "Gradient Descent(751/9999): loss=2.0996187318874977, w0=72.95625, w1=14.19162518253016\n",
      "Gradient Descent(752/9999): loss=1.8906339781867783, w0=72.84375, w1=13.82020604363644\n",
      "Gradient Descent(753/9999): loss=2.5453109552624413, w0=72.9, w1=13.61369777951489\n",
      "Gradient Descent(754/9999): loss=2.38921231241024, w0=72.95625000000001, w1=13.711660678511443\n",
      "Gradient Descent(755/9999): loss=2.2722530006068964, w0=73.06875000000001, w1=13.513626111708385\n",
      "Gradient Descent(756/9999): loss=1.9545830968639548, w0=73.0125, w1=13.657590028551095\n",
      "Gradient Descent(757/9999): loss=1.7745235669365986, w0=72.84375, w1=13.399714733849573\n",
      "Gradient Descent(758/9999): loss=2.34794127144621, w0=73.06875, w1=13.36016849237342\n",
      "Gradient Descent(759/9999): loss=2.4439192644753174, w0=73.06875, w1=13.635260089702586\n",
      "Gradient Descent(760/9999): loss=2.5635039088950355, w0=73.125, w1=13.705547686624815\n",
      "Gradient Descent(761/9999): loss=2.4669502307744944, w0=73.06875, w1=13.690018396860852\n",
      "Gradient Descent(762/9999): loss=2.0516051038289964, w0=73.06875, w1=13.61088838181028\n",
      "Gradient Descent(763/9999): loss=2.2169562188787117, w0=73.125, w1=13.543625991476896\n",
      "Gradient Descent(764/9999): loss=1.928649269010194, w0=73.29375, w1=13.47192810627727\n",
      "Gradient Descent(765/9999): loss=2.4976905085197436, w0=73.29375, w1=13.29876748302673\n",
      "Gradient Descent(766/9999): loss=2.0921616538421475, w0=72.9, w1=13.249236765660664\n",
      "Gradient Descent(767/9999): loss=1.8145623211953636, w0=73.06875000000001, w1=13.2164664472764\n",
      "Gradient Descent(768/9999): loss=2.3163389747280756, w0=73.0125, w1=13.312666694444275\n",
      "Gradient Descent(769/9999): loss=2.832588829714793, w0=73.2375, w1=13.288729822460493\n",
      "Gradient Descent(770/9999): loss=1.9786911001605347, w0=73.2375, w1=13.152817071544819\n",
      "Gradient Descent(771/9999): loss=2.0362644579746236, w0=73.29375, w1=13.281083278407124\n",
      "Gradient Descent(772/9999): loss=2.4257979381896315, w0=73.29375, w1=13.260836297670501\n",
      "Gradient Descent(773/9999): loss=2.4182121618898496, w0=73.29375, w1=13.21899456229447\n",
      "Gradient Descent(774/9999): loss=2.206529622163207, w0=73.125, w1=13.071723485544327\n",
      "Gradient Descent(775/9999): loss=2.073037626334072, w0=73.29375, w1=13.544635905573514\n",
      "Gradient Descent(776/9999): loss=2.1331321405535153, w0=73.29375, w1=13.404371079445363\n",
      "Gradient Descent(777/9999): loss=2.1440933549105248, w0=73.2375, w1=13.284449190234577\n",
      "Gradient Descent(778/9999): loss=2.419290292813173, w0=73.18124999999999, w1=13.21601458236138\n",
      "Gradient Descent(779/9999): loss=2.5666561306176696, w0=73.12499999999999, w1=13.161639906802472\n",
      "Gradient Descent(780/9999): loss=2.335316110552043, w0=73.18124999999999, w1=13.16184429950112\n",
      "Gradient Descent(781/9999): loss=2.6080165998903606, w0=73.12499999999999, w1=13.371338902186345\n",
      "Gradient Descent(782/9999): loss=1.9694839876453862, w0=73.34999999999998, w1=13.495955095932691\n",
      "Gradient Descent(783/9999): loss=2.220663928211618, w0=73.29374999999997, w1=13.468849398922298\n",
      "Gradient Descent(784/9999): loss=2.0186269144765543, w0=73.34999999999998, w1=13.49998411786426\n",
      "Gradient Descent(785/9999): loss=2.020948484221461, w0=73.23749999999998, w1=13.616030378260794\n",
      "Gradient Descent(786/9999): loss=1.9020055370138147, w0=73.40624999999999, w1=13.694518923581336\n",
      "Gradient Descent(787/9999): loss=2.5288826179360715, w0=73.12499999999999, w1=13.73578321432715\n",
      "Gradient Descent(788/9999): loss=2.5410561781691374, w0=73.18124999999999, w1=13.87713112925512\n",
      "Gradient Descent(789/9999): loss=1.9746097824076194, w0=73.18124999999999, w1=13.611452141908345\n",
      "Gradient Descent(790/9999): loss=1.8532674901164383, w0=73.12499999999999, w1=13.390037049648205\n",
      "Gradient Descent(791/9999): loss=2.2698715254997746, w0=73.12499999999999, w1=13.345403056281263\n",
      "Gradient Descent(792/9999): loss=2.0010691778678504, w0=73.01249999999999, w1=13.335066650330756\n",
      "Gradient Descent(793/9999): loss=2.22158151048788, w0=72.95624999999998, w1=13.321506272086566\n",
      "Gradient Descent(794/9999): loss=2.573280901874567, w0=72.67499999999998, w1=13.206237649243832\n",
      "Gradient Descent(795/9999): loss=2.3818358330431106, w0=72.67499999999998, w1=13.332379437218679\n",
      "Gradient Descent(796/9999): loss=1.9012515547609985, w0=72.84374999999999, w1=13.307687857993201\n",
      "Gradient Descent(797/9999): loss=1.9281130632722456, w0=72.89999999999999, w1=13.543580755662468\n",
      "Gradient Descent(798/9999): loss=2.5357843860860503, w0=72.7875, w1=13.430751731805389\n",
      "Gradient Descent(799/9999): loss=2.2699485709323044, w0=72.675, w1=13.337805977079487\n",
      "Gradient Descent(800/9999): loss=2.066161390205515, w0=72.61874999999999, w1=13.169892174243829\n",
      "Gradient Descent(801/9999): loss=2.2532384008646997, w0=72.39375, w1=13.136418147228305\n",
      "Gradient Descent(802/9999): loss=1.8714960617799496, w0=72.50625, w1=13.192035210884214\n",
      "Gradient Descent(803/9999): loss=2.2402412803888554, w0=72.73124999999999, w1=13.281986529460989\n",
      "Gradient Descent(804/9999): loss=2.0430724188534075, w0=72.7875, w1=13.156725097702314\n",
      "Gradient Descent(805/9999): loss=1.8075941484191882, w0=72.84375, w1=13.051293929580693\n",
      "Gradient Descent(806/9999): loss=2.3363673917805134, w0=72.675, w1=13.352214124009917\n",
      "Gradient Descent(807/9999): loss=2.4146240572349247, w0=72.7875, w1=13.370884582095401\n",
      "Gradient Descent(808/9999): loss=2.2124772963114836, w0=73.01249999999999, w1=13.472822435121593\n",
      "Gradient Descent(809/9999): loss=2.427122736760796, w0=72.95624999999998, w1=13.492359473139933\n",
      "Gradient Descent(810/9999): loss=1.8369785277845823, w0=72.73124999999999, w1=13.469113724201588\n",
      "Gradient Descent(811/9999): loss=2.3286310660113037, w0=72.56249999999999, w1=13.362370263438066\n",
      "Gradient Descent(812/9999): loss=2.5299553201811658, w0=72.84374999999999, w1=13.32013661095733\n",
      "Gradient Descent(813/9999): loss=2.1331028437529196, w0=73.18124999999999, w1=13.39613424707846\n",
      "Gradient Descent(814/9999): loss=2.3780878089633886, w0=73.01249999999999, w1=13.02146510880253\n",
      "Gradient Descent(815/9999): loss=2.5095673775585734, w0=73.01249999999999, w1=13.087303426458632\n",
      "Gradient Descent(816/9999): loss=2.094069009319968, w0=73.01249999999999, w1=13.088945556711703\n",
      "Gradient Descent(817/9999): loss=2.690229336048803, w0=73.35, w1=13.114241635133295\n",
      "Gradient Descent(818/9999): loss=2.272359652690237, w0=73.46249999999999, w1=13.100654966556714\n",
      "Gradient Descent(819/9999): loss=2.215533415080613, w0=73.63125, w1=12.989887196174607\n",
      "Gradient Descent(820/9999): loss=2.4602819214865854, w0=73.40625, w1=13.23306129493416\n",
      "Gradient Descent(821/9999): loss=2.5447734030679303, w0=73.18125, w1=13.524504885023799\n",
      "Gradient Descent(822/9999): loss=2.3447610365395244, w0=73.18125, w1=13.629328227330289\n",
      "Gradient Descent(823/9999): loss=2.0865936159892935, w0=73.23750000000001, w1=13.663787995406278\n",
      "Gradient Descent(824/9999): loss=1.760600307733004, w0=73.01250000000002, w1=13.781996624449166\n",
      "Gradient Descent(825/9999): loss=2.33374422247473, w0=73.12500000000001, w1=14.029687393335614\n",
      "Gradient Descent(826/9999): loss=2.1094900197407354, w0=72.84375000000001, w1=14.022376266872499\n",
      "Gradient Descent(827/9999): loss=1.7790041081364312, w0=72.90000000000002, w1=14.082909738249151\n",
      "Gradient Descent(828/9999): loss=2.1913336658407934, w0=73.06875000000002, w1=13.85349835068495\n",
      "Gradient Descent(829/9999): loss=2.4789640047133013, w0=73.01250000000002, w1=13.747208371082198\n",
      "Gradient Descent(830/9999): loss=2.5475601678574935, w0=72.67500000000001, w1=13.936458779031113\n",
      "Gradient Descent(831/9999): loss=1.981835243037596, w0=72.67500000000001, w1=13.913686631726776\n",
      "Gradient Descent(832/9999): loss=2.41103543350459, w0=72.9, w1=14.215653936566179\n",
      "Gradient Descent(833/9999): loss=2.2573039505826378, w0=72.95625000000001, w1=14.160491896590056\n",
      "Gradient Descent(834/9999): loss=1.9291117737587058, w0=73.23750000000001, w1=13.954350006631083\n",
      "Gradient Descent(835/9999): loss=2.4045091321987044, w0=73.29375000000002, w1=14.10707882306343\n",
      "Gradient Descent(836/9999): loss=2.2168744740682964, w0=73.18125000000002, w1=13.928862911763613\n",
      "Gradient Descent(837/9999): loss=2.464926793458993, w0=72.95625000000003, w1=13.825411975111345\n",
      "Gradient Descent(838/9999): loss=1.953262679178188, w0=73.01250000000003, w1=13.923678307097324\n",
      "Gradient Descent(839/9999): loss=1.8019910020322019, w0=72.61875000000003, w1=13.800884691396595\n",
      "Gradient Descent(840/9999): loss=1.7278044906224195, w0=72.45000000000003, w1=13.74984417863593\n",
      "Gradient Descent(841/9999): loss=1.8759386680149772, w0=72.45000000000003, w1=13.655459570668517\n",
      "Gradient Descent(842/9999): loss=2.449699289190649, w0=72.61875000000003, w1=13.631569509330438\n",
      "Gradient Descent(843/9999): loss=2.306172253256059, w0=72.78750000000004, w1=13.646551069731222\n",
      "Gradient Descent(844/9999): loss=2.2593354353849486, w0=72.73125000000003, w1=13.315849981690748\n",
      "Gradient Descent(845/9999): loss=2.2431156147815394, w0=72.73125000000003, w1=13.426569655349946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(846/9999): loss=2.149605753412464, w0=72.78750000000004, w1=13.426710612165106\n",
      "Gradient Descent(847/9999): loss=1.9734999488416616, w0=72.73125000000003, w1=13.267135783185278\n",
      "Gradient Descent(848/9999): loss=2.0049014544583894, w0=72.73125000000003, w1=13.426901340927028\n",
      "Gradient Descent(849/9999): loss=2.660681054957565, w0=72.78750000000004, w1=13.260878884846719\n",
      "Gradient Descent(850/9999): loss=2.2736899124272774, w0=72.95625000000004, w1=13.142248342376377\n",
      "Gradient Descent(851/9999): loss=2.2214986172926814, w0=73.01250000000005, w1=13.291289954663291\n",
      "Gradient Descent(852/9999): loss=2.1463263616881156, w0=73.06875000000005, w1=13.23543458108276\n",
      "Gradient Descent(853/9999): loss=1.9986676662990641, w0=73.01250000000005, w1=13.2476548907605\n",
      "Gradient Descent(854/9999): loss=2.188338033607155, w0=72.84375000000004, w1=13.086645283873946\n",
      "Gradient Descent(855/9999): loss=2.223224596710486, w0=72.61875000000005, w1=13.142556183083157\n",
      "Gradient Descent(856/9999): loss=2.3716273040158455, w0=72.95625000000005, w1=13.259748582331733\n",
      "Gradient Descent(857/9999): loss=2.1289185501240744, w0=73.01250000000006, w1=13.263216521313872\n",
      "Gradient Descent(858/9999): loss=2.142606868139405, w0=73.01250000000006, w1=13.20433596208744\n",
      "Gradient Descent(859/9999): loss=2.2545807546137295, w0=72.90000000000006, w1=13.25024671799711\n",
      "Gradient Descent(860/9999): loss=1.7718755648036724, w0=72.90000000000006, w1=13.583018971006808\n",
      "Gradient Descent(861/9999): loss=1.659983566059947, w0=73.12500000000006, w1=13.552846744818373\n",
      "Gradient Descent(862/9999): loss=2.325467625016949, w0=73.01250000000006, w1=13.737668548041523\n",
      "Gradient Descent(863/9999): loss=2.6528360594150695, w0=73.12500000000006, w1=13.803133143832955\n",
      "Gradient Descent(864/9999): loss=2.205344766021642, w0=73.18125000000006, w1=13.481191356795431\n",
      "Gradient Descent(865/9999): loss=2.4843630398998346, w0=73.12500000000006, w1=13.771516084509921\n",
      "Gradient Descent(866/9999): loss=2.7384651339884187, w0=72.84375000000006, w1=13.687086728390112\n",
      "Gradient Descent(867/9999): loss=2.63119013723745, w0=73.01250000000006, w1=13.49317563283286\n",
      "Gradient Descent(868/9999): loss=2.1208729419898864, w0=73.46250000000006, w1=13.402183009762064\n",
      "Gradient Descent(869/9999): loss=2.2375662615515166, w0=73.35000000000007, w1=13.589357065653383\n",
      "Gradient Descent(870/9999): loss=2.195700231225011, w0=73.46250000000006, w1=13.54657729664426\n",
      "Gradient Descent(871/9999): loss=2.2949855196694764, w0=73.40625000000006, w1=13.679825561728382\n",
      "Gradient Descent(872/9999): loss=2.1827423528003944, w0=73.46250000000006, w1=13.822081647237214\n",
      "Gradient Descent(873/9999): loss=2.9447074210827666, w0=73.29375000000006, w1=13.65417426713923\n",
      "Gradient Descent(874/9999): loss=2.1851893768613673, w0=73.06875000000007, w1=13.428991651103727\n",
      "Gradient Descent(875/9999): loss=2.0082749873187287, w0=72.95625000000007, w1=13.506399473401727\n",
      "Gradient Descent(876/9999): loss=2.475011214953182, w0=72.78750000000007, w1=13.489393113159739\n",
      "Gradient Descent(877/9999): loss=2.2778342819161006, w0=73.06875000000007, w1=13.39107288625958\n",
      "Gradient Descent(878/9999): loss=1.9704706166347667, w0=73.29375000000006, w1=13.656328949542708\n",
      "Gradient Descent(879/9999): loss=2.3032873964130363, w0=73.06875000000007, w1=13.454874837074831\n",
      "Gradient Descent(880/9999): loss=2.5057332107492316, w0=73.18125000000006, w1=13.471118942502532\n",
      "Gradient Descent(881/9999): loss=1.8698767330336363, w0=73.35000000000007, w1=13.693967416577197\n",
      "Gradient Descent(882/9999): loss=1.6831525145558106, w0=73.51875000000007, w1=13.727848044830338\n",
      "Gradient Descent(883/9999): loss=2.4686305564851527, w0=73.68750000000007, w1=13.495764879995408\n",
      "Gradient Descent(884/9999): loss=2.592416740653623, w0=73.51875000000007, w1=13.22319591610748\n",
      "Gradient Descent(885/9999): loss=2.535181292948816, w0=73.23750000000007, w1=13.231867159072443\n",
      "Gradient Descent(886/9999): loss=1.8378423538548811, w0=72.90000000000006, w1=13.244955881981818\n",
      "Gradient Descent(887/9999): loss=2.006402154120128, w0=72.95625000000007, w1=13.369770200166345\n",
      "Gradient Descent(888/9999): loss=2.61556886611242, w0=73.01250000000007, w1=13.179514035826644\n",
      "Gradient Descent(889/9999): loss=2.6770288789142622, w0=72.90000000000008, w1=13.266427796986678\n",
      "Gradient Descent(890/9999): loss=2.086564863441457, w0=72.78750000000008, w1=13.157318833761277\n",
      "Gradient Descent(891/9999): loss=1.6903213118059235, w0=72.84375000000009, w1=12.924131332722848\n",
      "Gradient Descent(892/9999): loss=2.146943975591676, w0=72.50625000000008, w1=13.414719463285438\n",
      "Gradient Descent(893/9999): loss=1.9882848782719125, w0=72.61875000000008, w1=13.577665639867563\n",
      "Gradient Descent(894/9999): loss=2.0731789754308787, w0=72.84375000000007, w1=13.434487176404406\n",
      "Gradient Descent(895/9999): loss=2.3603996645625998, w0=72.90000000000008, w1=13.668273669517218\n",
      "Gradient Descent(896/9999): loss=2.357036720320978, w0=73.01250000000007, w1=13.80898776465785\n",
      "Gradient Descent(897/9999): loss=2.070412453072369, w0=72.84375000000007, w1=13.682491255083578\n",
      "Gradient Descent(898/9999): loss=2.2570295015991384, w0=72.67500000000007, w1=13.863926901550483\n",
      "Gradient Descent(899/9999): loss=1.700049429338918, w0=72.90000000000006, w1=13.854751296220588\n",
      "Gradient Descent(900/9999): loss=2.0311244263541397, w0=72.95625000000007, w1=13.683121119703005\n",
      "Gradient Descent(901/9999): loss=1.94084060370988, w0=72.78750000000007, w1=13.817574611441866\n",
      "Gradient Descent(902/9999): loss=2.539447801754416, w0=72.78750000000007, w1=13.658459878077174\n",
      "Gradient Descent(903/9999): loss=2.389284848723854, w0=72.95625000000007, w1=13.736183739041879\n",
      "Gradient Descent(904/9999): loss=2.122676173256027, w0=73.23750000000007, w1=13.631789832375981\n",
      "Gradient Descent(905/9999): loss=2.383822284339999, w0=73.12500000000007, w1=13.285100053293675\n",
      "Gradient Descent(906/9999): loss=2.0697816777638294, w0=73.06875000000007, w1=13.42656628321721\n",
      "Gradient Descent(907/9999): loss=2.3860148088708053, w0=72.95625000000007, w1=13.524928952375674\n",
      "Gradient Descent(908/9999): loss=2.4982723452977416, w0=73.01250000000007, w1=13.39368870906496\n",
      "Gradient Descent(909/9999): loss=1.813318539950995, w0=73.18125000000008, w1=13.361903353032623\n",
      "Gradient Descent(910/9999): loss=2.0913188608042983, w0=73.29375000000007, w1=13.322333129960352\n",
      "Gradient Descent(911/9999): loss=2.3212285805327504, w0=73.12500000000007, w1=13.349198682481957\n",
      "Gradient Descent(912/9999): loss=2.9410932649299832, w0=72.84375000000007, w1=13.401434882068965\n",
      "Gradient Descent(913/9999): loss=1.616869403212685, w0=72.78750000000007, w1=13.417000447006522\n",
      "Gradient Descent(914/9999): loss=2.1929754477228167, w0=72.84375000000007, w1=13.667973369069458\n",
      "Gradient Descent(915/9999): loss=1.964629070812906, w0=72.95625000000007, w1=13.38018102010194\n",
      "Gradient Descent(916/9999): loss=2.5041254101551296, w0=72.95625000000007, w1=13.285586906357908\n",
      "Gradient Descent(917/9999): loss=2.4981388043227843, w0=73.23750000000007, w1=13.351657002661277\n",
      "Gradient Descent(918/9999): loss=2.3181536941816656, w0=73.18125000000006, w1=13.2159102990444\n",
      "Gradient Descent(919/9999): loss=2.302045471783015, w0=73.12500000000006, w1=13.297624323441365\n",
      "Gradient Descent(920/9999): loss=2.244173662178408, w0=73.12500000000006, w1=13.497759753620763\n",
      "Gradient Descent(921/9999): loss=2.375696159669043, w0=72.78750000000005, w1=13.618882796009366\n",
      "Gradient Descent(922/9999): loss=2.569818375158841, w0=72.50625000000005, w1=13.687320980132442\n",
      "Gradient Descent(923/9999): loss=2.1333438684299066, w0=72.90000000000005, w1=13.581167444460483\n",
      "Gradient Descent(924/9999): loss=1.4800865589476397, w0=72.78750000000005, w1=13.548300292991536\n",
      "Gradient Descent(925/9999): loss=2.0938469503758697, w0=72.78750000000005, w1=13.563802341406408\n",
      "Gradient Descent(926/9999): loss=1.854433170829917, w0=72.67500000000005, w1=13.610040508345326\n",
      "Gradient Descent(927/9999): loss=2.071187129772179, w0=72.95625000000005, w1=13.242120581369646\n",
      "Gradient Descent(928/9999): loss=1.9846600156726568, w0=72.95625000000005, w1=13.25497699267844\n",
      "Gradient Descent(929/9999): loss=2.2596522151635723, w0=72.84375000000006, w1=13.495986972712188\n",
      "Gradient Descent(930/9999): loss=2.1346699586192877, w0=72.90000000000006, w1=13.589628803810447\n",
      "Gradient Descent(931/9999): loss=2.206035352496519, w0=72.95625000000007, w1=13.373966727419429\n",
      "Gradient Descent(932/9999): loss=2.054590263722029, w0=72.84375000000007, w1=13.341531753252532\n",
      "Gradient Descent(933/9999): loss=2.3318151086063477, w0=72.67500000000007, w1=13.316425345404982\n",
      "Gradient Descent(934/9999): loss=1.901844310658364, w0=72.73125000000007, w1=13.414644879356345\n",
      "Gradient Descent(935/9999): loss=1.7845468142413787, w0=72.95625000000007, w1=13.232728228046904\n",
      "Gradient Descent(936/9999): loss=2.4979977127896866, w0=73.01250000000007, w1=13.141756644738951\n",
      "Gradient Descent(937/9999): loss=2.3227952944856467, w0=72.95625000000007, w1=12.995643447881836\n",
      "Gradient Descent(938/9999): loss=2.0849238709426943, w0=73.01250000000007, w1=12.931138353560597\n",
      "Gradient Descent(939/9999): loss=2.382531093309142, w0=72.84375000000007, w1=12.942740065210579\n",
      "Gradient Descent(940/9999): loss=1.635320020245685, w0=72.84375000000007, w1=13.038115668360325\n",
      "Gradient Descent(941/9999): loss=1.8012424852066966, w0=72.78750000000007, w1=13.05916021877408\n",
      "Gradient Descent(942/9999): loss=2.3031011669513406, w0=72.90000000000006, w1=12.984943598065769\n",
      "Gradient Descent(943/9999): loss=1.9754228327654315, w0=73.01250000000006, w1=13.03500621788252\n",
      "Gradient Descent(944/9999): loss=2.4659202946027383, w0=72.84375000000006, w1=13.057333422586346\n",
      "Gradient Descent(945/9999): loss=2.1196398044859355, w0=72.67500000000005, w1=13.307743426223052\n",
      "Gradient Descent(946/9999): loss=2.4358789290373952, w0=72.95625000000005, w1=13.589269496123165\n",
      "Gradient Descent(947/9999): loss=2.142441912409698, w0=73.01250000000006, w1=13.673009974369075\n",
      "Gradient Descent(948/9999): loss=1.9254715494631944, w0=72.95625000000005, w1=13.707294671296173\n",
      "Gradient Descent(949/9999): loss=2.3746752665667037, w0=72.84375000000006, w1=13.685712154345019\n",
      "Gradient Descent(950/9999): loss=2.294775996054956, w0=72.84375000000006, w1=13.567312805052595\n",
      "Gradient Descent(951/9999): loss=2.5365854413730298, w0=72.95625000000005, w1=13.72199506982177\n",
      "Gradient Descent(952/9999): loss=1.9032274831594505, w0=72.78750000000005, w1=13.541551539905507\n",
      "Gradient Descent(953/9999): loss=1.8302068083026173, w0=73.06875000000005, w1=13.24984244742416\n",
      "Gradient Descent(954/9999): loss=1.9937977617677678, w0=73.12500000000006, w1=13.361201596265248\n",
      "Gradient Descent(955/9999): loss=2.129078139461778, w0=73.29375000000006, w1=13.301664706649813\n",
      "Gradient Descent(956/9999): loss=1.780799695083166, w0=73.51875000000005, w1=12.84481533758962\n",
      "Gradient Descent(957/9999): loss=2.013530383513385, w0=73.40625000000006, w1=13.037479463488461\n",
      "Gradient Descent(958/9999): loss=2.0776641708276857, w0=73.40625000000006, w1=13.159795098600142\n",
      "Gradient Descent(959/9999): loss=2.5603694272608486, w0=73.29375000000006, w1=13.329538128834447\n",
      "Gradient Descent(960/9999): loss=1.948549906534904, w0=73.29375000000006, w1=13.651134745369433\n",
      "Gradient Descent(961/9999): loss=2.248838824747641, w0=73.35000000000007, w1=13.42892241361468\n",
      "Gradient Descent(962/9999): loss=2.06513759286213, w0=73.29375000000006, w1=13.683997157092913\n",
      "Gradient Descent(963/9999): loss=2.5549690061764254, w0=73.12500000000006, w1=13.557227065745153\n",
      "Gradient Descent(964/9999): loss=2.0866356130573647, w0=72.90000000000006, w1=13.620127583827221\n",
      "Gradient Descent(965/9999): loss=1.9836105470681322, w0=73.06875000000007, w1=13.615686860304052\n",
      "Gradient Descent(966/9999): loss=2.207792722128602, w0=72.84375000000007, w1=14.10536633619331\n",
      "Gradient Descent(967/9999): loss=2.433387702906078, w0=72.84375000000007, w1=13.65982312812327\n",
      "Gradient Descent(968/9999): loss=2.2572774148392183, w0=72.95625000000007, w1=13.437762788346163\n",
      "Gradient Descent(969/9999): loss=2.2043093375743226, w0=73.12500000000007, w1=13.670090978130016\n",
      "Gradient Descent(970/9999): loss=2.2742339556518316, w0=73.46250000000008, w1=13.94722558340057\n",
      "Gradient Descent(971/9999): loss=2.3552013810663324, w0=73.51875000000008, w1=13.875676688393206\n",
      "Gradient Descent(972/9999): loss=2.491573353979762, w0=73.51875000000008, w1=13.619478378191511\n",
      "Gradient Descent(973/9999): loss=2.1763100487672213, w0=73.57500000000009, w1=13.251808904189318\n",
      "Gradient Descent(974/9999): loss=1.8930679999590663, w0=73.51875000000008, w1=13.303631640897793\n",
      "Gradient Descent(975/9999): loss=2.495193216645092, w0=73.40625000000009, w1=13.639343944747027\n",
      "Gradient Descent(976/9999): loss=1.7739252632550506, w0=73.12500000000009, w1=13.579791568939813\n",
      "Gradient Descent(977/9999): loss=2.5316711242918126, w0=73.01250000000009, w1=13.293398016065552\n",
      "Gradient Descent(978/9999): loss=1.7598135967812105, w0=72.7875000000001, w1=13.248053033765897\n",
      "Gradient Descent(979/9999): loss=2.9735770571161884, w0=72.5625000000001, w1=13.307552492962234\n",
      "Gradient Descent(980/9999): loss=2.045447350894235, w0=72.5062500000001, w1=13.36249473607064\n",
      "Gradient Descent(981/9999): loss=2.277864457745661, w0=72.2812500000001, w1=13.278974153837945\n",
      "Gradient Descent(982/9999): loss=1.8683597234614715, w0=72.5062500000001, w1=13.385195222348086\n",
      "Gradient Descent(983/9999): loss=2.0562697007873667, w0=72.61875000000009, w1=13.305444408015486\n",
      "Gradient Descent(984/9999): loss=2.5986302116788833, w0=72.84375000000009, w1=12.998389132367171\n",
      "Gradient Descent(985/9999): loss=1.9740566480741966, w0=73.12500000000009, w1=13.26392908628051\n",
      "Gradient Descent(986/9999): loss=2.167907187972625, w0=73.01250000000009, w1=13.239336360027492\n",
      "Gradient Descent(987/9999): loss=2.0471941162625598, w0=73.18125000000009, w1=13.67749617602987\n",
      "Gradient Descent(988/9999): loss=2.672533770287659, w0=73.5187500000001, w1=13.874609053878347\n",
      "Gradient Descent(989/9999): loss=2.333409977505195, w0=73.6312500000001, w1=13.950265249658374\n",
      "Gradient Descent(990/9999): loss=2.289766707138681, w0=73.8000000000001, w1=13.518631481366663\n",
      "Gradient Descent(991/9999): loss=1.5813019437317037, w0=73.74375000000009, w1=13.56370291008064\n",
      "Gradient Descent(992/9999): loss=2.189015832289693, w0=73.8000000000001, w1=13.433234662459263\n",
      "Gradient Descent(993/9999): loss=2.350823074901183, w0=73.6875000000001, w1=13.439399417083795\n",
      "Gradient Descent(994/9999): loss=2.243047576397358, w0=73.4625000000001, w1=13.291178656916841\n",
      "Gradient Descent(995/9999): loss=2.17793070133725, w0=73.2937500000001, w1=13.203437241412225\n",
      "Gradient Descent(996/9999): loss=2.098276007175129, w0=73.2937500000001, w1=13.104948908732307\n",
      "Gradient Descent(997/9999): loss=2.185793939916492, w0=73.35000000000011, w1=13.22275934672514\n",
      "Gradient Descent(998/9999): loss=1.9192538801292285, w0=73.40625000000011, w1=13.262713712934103\n",
      "Gradient Descent(999/9999): loss=2.0348810931107417, w0=73.57500000000012, w1=13.149386711998202\n",
      "Gradient Descent(1000/9999): loss=2.1596529959488913, w0=73.51875000000011, w1=13.219348978061412\n",
      "Gradient Descent(1001/9999): loss=1.7663581842287435, w0=73.51875000000011, w1=12.916775633859634\n",
      "Gradient Descent(1002/9999): loss=1.9280633948254613, w0=73.7437500000001, w1=12.92672739443764\n",
      "Gradient Descent(1003/9999): loss=2.477175493184959, w0=73.8562500000001, w1=13.021561791757206\n",
      "Gradient Descent(1004/9999): loss=1.9467592874148216, w0=73.8000000000001, w1=13.13665708072974\n",
      "Gradient Descent(1005/9999): loss=2.025721942458366, w0=73.46250000000009, w1=13.214978951998875\n",
      "Gradient Descent(1006/9999): loss=1.9623017810467174, w0=73.5187500000001, w1=13.199194352523769\n",
      "Gradient Descent(1007/9999): loss=1.6876197308157217, w0=73.5187500000001, w1=13.30501226388669\n",
      "Gradient Descent(1008/9999): loss=1.956845215364599, w0=73.6312500000001, w1=13.237386646403417\n",
      "Gradient Descent(1009/9999): loss=2.4591828369354194, w0=73.4062500000001, w1=13.203696537732357\n",
      "Gradient Descent(1010/9999): loss=2.8863599344359328, w0=73.2937500000001, w1=13.429105882966768\n",
      "Gradient Descent(1011/9999): loss=2.436500080413376, w0=73.2375000000001, w1=13.158367321829049\n",
      "Gradient Descent(1012/9999): loss=1.811709185011352, w0=73.1250000000001, w1=13.28627141283399\n",
      "Gradient Descent(1013/9999): loss=2.6578209754648903, w0=73.4625000000001, w1=13.384089721229895\n",
      "Gradient Descent(1014/9999): loss=2.04038626727233, w0=73.35000000000011, w1=13.39050505660032\n",
      "Gradient Descent(1015/9999): loss=1.9856568455325838, w0=73.1812500000001, w1=13.345266002890616\n",
      "Gradient Descent(1016/9999): loss=1.5627541888949688, w0=73.23750000000011, w1=13.228135906217123\n",
      "Gradient Descent(1017/9999): loss=1.8166432497779832, w0=72.95625000000011, w1=13.218893283341828\n",
      "Gradient Descent(1018/9999): loss=2.1848434457508183, w0=73.23750000000011, w1=13.254167471777595\n",
      "Gradient Descent(1019/9999): loss=2.282410288576553, w0=73.29375000000012, w1=13.311341795609575\n",
      "Gradient Descent(1020/9999): loss=1.9212378452847731, w0=73.06875000000012, w1=13.29967785893651\n",
      "Gradient Descent(1021/9999): loss=2.403638693762094, w0=73.23750000000013, w1=13.391310871474905\n",
      "Gradient Descent(1022/9999): loss=2.39633910318939, w0=73.23750000000013, w1=13.405968084144298\n",
      "Gradient Descent(1023/9999): loss=2.5811980647831643, w0=73.18125000000012, w1=13.49934117072854\n",
      "Gradient Descent(1024/9999): loss=2.5545788613385647, w0=73.23750000000013, w1=13.301678329432036\n",
      "Gradient Descent(1025/9999): loss=1.6839738038167449, w0=73.35000000000012, w1=13.297061640453288\n",
      "Gradient Descent(1026/9999): loss=2.302672065922538, w0=73.46250000000012, w1=13.237676214365115\n",
      "Gradient Descent(1027/9999): loss=2.1839977466869636, w0=73.51875000000013, w1=13.261681245505402\n",
      "Gradient Descent(1028/9999): loss=2.5212581988711067, w0=73.35000000000012, w1=13.270095796091223\n",
      "Gradient Descent(1029/9999): loss=2.4229117381630574, w0=73.57500000000012, w1=13.211659824612413\n",
      "Gradient Descent(1030/9999): loss=2.4869322761304584, w0=73.51875000000011, w1=13.529206761064938\n",
      "Gradient Descent(1031/9999): loss=1.923967360446878, w0=73.29375000000012, w1=13.388092346035167\n",
      "Gradient Descent(1032/9999): loss=2.156799081107276, w0=73.06875000000012, w1=13.305588320935193\n",
      "Gradient Descent(1033/9999): loss=2.15143115767815, w0=73.23750000000013, w1=13.309093459932345\n",
      "Gradient Descent(1034/9999): loss=2.007085024441249, w0=73.18125000000012, w1=13.202588570426025\n",
      "Gradient Descent(1035/9999): loss=2.167460467839146, w0=73.12500000000011, w1=13.419728868905985\n",
      "Gradient Descent(1036/9999): loss=2.7839074574798217, w0=73.06875000000011, w1=13.3663289118761\n",
      "Gradient Descent(1037/9999): loss=2.5357093092804455, w0=72.95625000000011, w1=13.44219009525301\n",
      "Gradient Descent(1038/9999): loss=2.368210951584316, w0=72.95625000000011, w1=13.439355357611735\n",
      "Gradient Descent(1039/9999): loss=2.046689956948037, w0=73.23750000000011, w1=13.365742921763706\n",
      "Gradient Descent(1040/9999): loss=2.661376743444157, w0=73.06875000000011, w1=13.731453197886777\n",
      "Gradient Descent(1041/9999): loss=1.9504121212028003, w0=73.12500000000011, w1=13.63762158800106\n",
      "Gradient Descent(1042/9999): loss=2.0733462556570856, w0=73.01250000000012, w1=13.584210964417942\n",
      "Gradient Descent(1043/9999): loss=2.1247685027932377, w0=73.01250000000012, w1=13.401870188289546\n",
      "Gradient Descent(1044/9999): loss=2.0790844006868263, w0=72.90000000000012, w1=13.422907077286549\n",
      "Gradient Descent(1045/9999): loss=1.959060012747111, w0=72.90000000000012, w1=13.429618162333256\n",
      "Gradient Descent(1046/9999): loss=2.31102227622867, w0=72.90000000000012, w1=13.358879205291212\n",
      "Gradient Descent(1047/9999): loss=2.1586697068954286, w0=72.90000000000012, w1=13.523066966393346\n",
      "Gradient Descent(1048/9999): loss=2.1444807941564723, w0=73.29375000000012, w1=13.766988001231995\n",
      "Gradient Descent(1049/9999): loss=1.961992373386623, w0=73.12500000000011, w1=13.72956625313068\n",
      "Gradient Descent(1050/9999): loss=1.7057905979035533, w0=73.06875000000011, w1=13.916118096382892\n",
      "Gradient Descent(1051/9999): loss=2.6041169392780046, w0=72.95625000000011, w1=13.85313171003394\n",
      "Gradient Descent(1052/9999): loss=2.527581097433546, w0=72.78750000000011, w1=14.075826217008263\n",
      "Gradient Descent(1053/9999): loss=2.0027373846239804, w0=72.78750000000011, w1=14.037373480241579\n",
      "Gradient Descent(1054/9999): loss=1.9507412036328247, w0=72.67500000000011, w1=14.099428730973083\n",
      "Gradient Descent(1055/9999): loss=2.4240513500176943, w0=73.12500000000011, w1=14.176926125804046\n",
      "Gradient Descent(1056/9999): loss=2.283812214290135, w0=73.23750000000011, w1=13.936551538265922\n",
      "Gradient Descent(1057/9999): loss=2.0040364850191534, w0=73.06875000000011, w1=13.69124401267266\n",
      "Gradient Descent(1058/9999): loss=2.118164869133931, w0=73.0125000000001, w1=13.637593987560312\n",
      "Gradient Descent(1059/9999): loss=1.9385005386097374, w0=73.06875000000011, w1=13.552507576768486\n",
      "Gradient Descent(1060/9999): loss=1.865705193429736, w0=72.9000000000001, w1=13.656274936032144\n",
      "Gradient Descent(1061/9999): loss=2.172078167648519, w0=73.06875000000011, w1=13.529560574354397\n",
      "Gradient Descent(1062/9999): loss=2.1528331404770378, w0=73.2937500000001, w1=13.120053112778336\n",
      "Gradient Descent(1063/9999): loss=2.196480549031854, w0=73.1812500000001, w1=12.940528014442476\n",
      "Gradient Descent(1064/9999): loss=2.0801879387564512, w0=73.1250000000001, w1=13.227816126700425\n",
      "Gradient Descent(1065/9999): loss=2.722568074511246, w0=73.1812500000001, w1=13.02734971609587\n",
      "Gradient Descent(1066/9999): loss=2.077635321280651, w0=73.06875000000011, w1=13.1308608246372\n",
      "Gradient Descent(1067/9999): loss=2.991593093356041, w0=73.2937500000001, w1=13.268091513997817\n",
      "Gradient Descent(1068/9999): loss=2.1091497525007377, w0=73.2375000000001, w1=13.44335607462415\n",
      "Gradient Descent(1069/9999): loss=2.5960980234420274, w0=73.18125000000009, w1=13.387868098588221\n",
      "Gradient Descent(1070/9999): loss=2.4549574802388525, w0=73.18125000000009, w1=13.466069600299042\n",
      "Gradient Descent(1071/9999): loss=2.0951841482679274, w0=73.3500000000001, w1=13.112477881039638\n",
      "Gradient Descent(1072/9999): loss=2.4563730270794735, w0=73.29375000000009, w1=13.396980283427549\n",
      "Gradient Descent(1073/9999): loss=2.5269273561652925, w0=73.0687500000001, w1=13.316363344759127\n",
      "Gradient Descent(1074/9999): loss=2.3063086229008753, w0=73.2375000000001, w1=13.367691507223102\n",
      "Gradient Descent(1075/9999): loss=3.1073074229276085, w0=73.2375000000001, w1=13.417601053284969\n",
      "Gradient Descent(1076/9999): loss=2.1813049005882243, w0=73.18125000000009, w1=13.541450370621758\n",
      "Gradient Descent(1077/9999): loss=2.3757263794456005, w0=73.0687500000001, w1=13.43690838704314\n",
      "Gradient Descent(1078/9999): loss=2.1754508624307047, w0=73.29375000000009, w1=13.681988363416352\n",
      "Gradient Descent(1079/9999): loss=2.2230008273457607, w0=73.12500000000009, w1=13.5262373914048\n",
      "Gradient Descent(1080/9999): loss=2.1282232727589907, w0=73.23750000000008, w1=13.345728209739493\n",
      "Gradient Descent(1081/9999): loss=2.1600673255305463, w0=73.18125000000008, w1=13.280301154292586\n",
      "Gradient Descent(1082/9999): loss=2.0711901433496562, w0=73.40625000000007, w1=13.264115615050516\n",
      "Gradient Descent(1083/9999): loss=2.5698604719290583, w0=73.35000000000007, w1=13.33653217349525\n",
      "Gradient Descent(1084/9999): loss=2.1580952432373097, w0=73.23750000000007, w1=13.179978916355548\n",
      "Gradient Descent(1085/9999): loss=2.3282455531172426, w0=73.18125000000006, w1=13.036550161693288\n",
      "Gradient Descent(1086/9999): loss=2.363352250425171, w0=72.84375000000006, w1=13.149591341853286\n",
      "Gradient Descent(1087/9999): loss=2.506157132888249, w0=72.67500000000005, w1=13.247909987334294\n",
      "Gradient Descent(1088/9999): loss=2.5175922077741797, w0=72.90000000000005, w1=13.19180807716312\n",
      "Gradient Descent(1089/9999): loss=2.6503380607071567, w0=72.90000000000005, w1=13.118714007794612\n",
      "Gradient Descent(1090/9999): loss=2.289955088640295, w0=72.90000000000005, w1=13.19836824115701\n",
      "Gradient Descent(1091/9999): loss=2.2479079359197507, w0=72.73125000000005, w1=13.418599310330036\n",
      "Gradient Descent(1092/9999): loss=2.458014664492641, w0=72.56250000000004, w1=13.425090122749555\n",
      "Gradient Descent(1093/9999): loss=2.415477669805717, w0=72.45000000000005, w1=13.273473194829707\n",
      "Gradient Descent(1094/9999): loss=2.1232378411142285, w0=72.56250000000004, w1=13.344030205037688\n",
      "Gradient Descent(1095/9999): loss=2.2610572174365617, w0=72.56250000000004, w1=13.07636239219267\n",
      "Gradient Descent(1096/9999): loss=2.1124790007716587, w0=72.56250000000004, w1=13.294481449012387\n",
      "Gradient Descent(1097/9999): loss=2.471553609162328, w0=72.61875000000005, w1=13.39661020225818\n",
      "Gradient Descent(1098/9999): loss=2.456215806893674, w0=72.67500000000005, w1=13.391325855896115\n",
      "Gradient Descent(1099/9999): loss=2.2391606859069286, w0=72.84375000000006, w1=13.399315220585493\n",
      "Gradient Descent(1100/9999): loss=2.8362973517731267, w0=72.67500000000005, w1=13.44759199677844\n",
      "Gradient Descent(1101/9999): loss=1.917394000193446, w0=72.95625000000005, w1=13.383545728653726\n",
      "Gradient Descent(1102/9999): loss=2.23658547954325, w0=73.01250000000006, w1=13.303573784850437\n",
      "Gradient Descent(1103/9999): loss=2.228521580368012, w0=73.18125000000006, w1=13.326940074065613\n",
      "Gradient Descent(1104/9999): loss=2.474668787648472, w0=73.23750000000007, w1=13.504684688164307\n",
      "Gradient Descent(1105/9999): loss=2.3845399153090407, w0=73.35000000000007, w1=13.676039124128986\n",
      "Gradient Descent(1106/9999): loss=2.7265724645456215, w0=73.46250000000006, w1=13.618494347961951\n",
      "Gradient Descent(1107/9999): loss=2.700748306832803, w0=73.63125000000007, w1=13.61134468602196\n",
      "Gradient Descent(1108/9999): loss=1.4913371400194086, w0=73.46250000000006, w1=13.397176755924736\n",
      "Gradient Descent(1109/9999): loss=2.4096691882897447, w0=73.63125000000007, w1=13.522592799085725\n",
      "Gradient Descent(1110/9999): loss=1.892726484141371, w0=73.63125000000007, w1=13.404314644829116\n",
      "Gradient Descent(1111/9999): loss=2.0306866811636612, w0=73.74375000000006, w1=13.327747865749926\n",
      "Gradient Descent(1112/9999): loss=2.48171447692254, w0=73.74375000000006, w1=13.420366446309279\n",
      "Gradient Descent(1113/9999): loss=1.9986843488442414, w0=73.63125000000007, w1=13.43451552163419\n",
      "Gradient Descent(1114/9999): loss=2.019517633994141, w0=73.74375000000006, w1=13.445580710481714\n",
      "Gradient Descent(1115/9999): loss=2.0164228339416175, w0=73.68750000000006, w1=13.47081331821178\n",
      "Gradient Descent(1116/9999): loss=2.2520953117296756, w0=73.74375000000006, w1=13.305197301186144\n",
      "Gradient Descent(1117/9999): loss=2.0663804329394457, w0=73.68750000000006, w1=13.527955291645297\n",
      "Gradient Descent(1118/9999): loss=2.289041883573868, w0=73.57500000000006, w1=13.168400634075914\n",
      "Gradient Descent(1119/9999): loss=2.2423892938188605, w0=73.68750000000006, w1=13.409323807905976\n",
      "Gradient Descent(1120/9999): loss=2.4345679007109484, w0=73.74375000000006, w1=13.153784404955385\n",
      "Gradient Descent(1121/9999): loss=2.0965764285495463, w0=73.74375000000006, w1=13.389059734456728\n",
      "Gradient Descent(1122/9999): loss=2.2337371773794588, w0=73.63125000000007, w1=13.497323341994376\n",
      "Gradient Descent(1123/9999): loss=2.0369273244604615, w0=73.74375000000006, w1=13.105001989985213\n",
      "Gradient Descent(1124/9999): loss=1.7049736112335228, w0=73.80000000000007, w1=13.11312478645685\n",
      "Gradient Descent(1125/9999): loss=2.3870569295586876, w0=73.46250000000006, w1=13.198844875239725\n",
      "Gradient Descent(1126/9999): loss=2.3328535286404186, w0=73.35000000000007, w1=13.293417601767294\n",
      "Gradient Descent(1127/9999): loss=2.4612900726061135, w0=73.18125000000006, w1=13.289819620071398\n",
      "Gradient Descent(1128/9999): loss=2.2372106491858155, w0=72.90000000000006, w1=13.324639312695451\n",
      "Gradient Descent(1129/9999): loss=2.4073424308004947, w0=72.84375000000006, w1=13.301350195565595\n",
      "Gradient Descent(1130/9999): loss=2.6794894397640823, w0=73.01250000000006, w1=13.546176918038961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1131/9999): loss=2.480354738794685, w0=73.29375000000006, w1=13.864129910043333\n",
      "Gradient Descent(1132/9999): loss=2.17785566654947, w0=73.29375000000006, w1=13.549327466014923\n",
      "Gradient Descent(1133/9999): loss=2.4842159517516094, w0=73.23750000000005, w1=13.560546489089267\n",
      "Gradient Descent(1134/9999): loss=2.860344026367787, w0=73.23750000000005, w1=13.5278524496221\n",
      "Gradient Descent(1135/9999): loss=2.586990589565617, w0=73.35000000000005, w1=13.344739257956128\n",
      "Gradient Descent(1136/9999): loss=2.701308917825244, w0=73.40625000000006, w1=13.465481165193106\n",
      "Gradient Descent(1137/9999): loss=2.205845500297972, w0=73.57500000000006, w1=13.612555447848159\n",
      "Gradient Descent(1138/9999): loss=2.6232524875311203, w0=73.35000000000007, w1=13.635058990423493\n",
      "Gradient Descent(1139/9999): loss=2.261842061129948, w0=73.23750000000007, w1=13.392205938297662\n",
      "Gradient Descent(1140/9999): loss=1.9974672943481022, w0=73.18125000000006, w1=13.496898306309687\n",
      "Gradient Descent(1141/9999): loss=2.259857528447401, w0=73.23750000000007, w1=13.472300240950938\n",
      "Gradient Descent(1142/9999): loss=1.770165104181676, w0=73.40625000000007, w1=13.72445774066266\n",
      "Gradient Descent(1143/9999): loss=2.2438731461029415, w0=73.23750000000007, w1=13.74957665032254\n",
      "Gradient Descent(1144/9999): loss=1.9685088801518356, w0=73.23750000000007, w1=13.686953446097466\n",
      "Gradient Descent(1145/9999): loss=1.827890915491983, w0=73.40625000000007, w1=13.463103983216287\n",
      "Gradient Descent(1146/9999): loss=2.23384605330328, w0=73.23750000000007, w1=13.561725556907085\n",
      "Gradient Descent(1147/9999): loss=1.548757553868918, w0=73.18125000000006, w1=13.835992553031934\n",
      "Gradient Descent(1148/9999): loss=2.090999260051078, w0=73.18125000000006, w1=13.73592135770442\n",
      "Gradient Descent(1149/9999): loss=2.152424313478641, w0=73.18125000000006, w1=13.628501788925384\n",
      "Gradient Descent(1150/9999): loss=2.008890910695289, w0=73.29375000000006, w1=13.693751809681267\n",
      "Gradient Descent(1151/9999): loss=1.8941432091930177, w0=73.18125000000006, w1=13.420027858964911\n",
      "Gradient Descent(1152/9999): loss=1.994586073432316, w0=73.29375000000006, w1=13.379049489858579\n",
      "Gradient Descent(1153/9999): loss=2.605864890686915, w0=73.12500000000006, w1=13.410173253301066\n",
      "Gradient Descent(1154/9999): loss=2.1907185836803023, w0=73.01250000000006, w1=13.496967580991205\n",
      "Gradient Descent(1155/9999): loss=2.072262571424933, w0=73.12500000000006, w1=13.642991406991538\n",
      "Gradient Descent(1156/9999): loss=2.0264937269619034, w0=73.12500000000006, w1=13.61273535170384\n",
      "Gradient Descent(1157/9999): loss=2.095002247706606, w0=73.12500000000006, w1=13.648108778157969\n",
      "Gradient Descent(1158/9999): loss=1.8573331059140088, w0=73.18125000000006, w1=13.73588995510124\n",
      "Gradient Descent(1159/9999): loss=1.9790251212432977, w0=73.18125000000006, w1=13.38866460148312\n",
      "Gradient Descent(1160/9999): loss=2.322924754181866, w0=73.01250000000006, w1=13.689865356247978\n",
      "Gradient Descent(1161/9999): loss=2.07449906561701, w0=73.06875000000007, w1=13.556286333588531\n",
      "Gradient Descent(1162/9999): loss=2.25782120441898, w0=73.23750000000007, w1=13.63750598546128\n",
      "Gradient Descent(1163/9999): loss=2.0268937062049153, w0=73.18125000000006, w1=13.515884365194616\n",
      "Gradient Descent(1164/9999): loss=2.067376308336409, w0=73.18125000000006, w1=13.29329122077103\n",
      "Gradient Descent(1165/9999): loss=2.746091318243619, w0=73.18125000000006, w1=13.353264910793321\n",
      "Gradient Descent(1166/9999): loss=2.4198289673096376, w0=73.29375000000006, w1=13.455535041058585\n",
      "Gradient Descent(1167/9999): loss=1.5701260728750932, w0=73.40625000000006, w1=13.48179906714629\n",
      "Gradient Descent(1168/9999): loss=2.23593357798228, w0=73.23750000000005, w1=13.494401816718247\n",
      "Gradient Descent(1169/9999): loss=2.380697034664018, w0=73.23750000000005, w1=13.666349247774605\n",
      "Gradient Descent(1170/9999): loss=2.421136325837369, w0=72.90000000000005, w1=13.881495870192065\n",
      "Gradient Descent(1171/9999): loss=2.378310640046513, w0=73.01250000000005, w1=13.735544326043343\n",
      "Gradient Descent(1172/9999): loss=2.199564055350945, w0=73.01250000000005, w1=13.750647977649855\n",
      "Gradient Descent(1173/9999): loss=1.8645778145265908, w0=73.06875000000005, w1=13.884807003409643\n",
      "Gradient Descent(1174/9999): loss=2.1253340333357467, w0=73.18125000000005, w1=13.740777317421873\n",
      "Gradient Descent(1175/9999): loss=2.4895031580874303, w0=73.18125000000005, w1=13.586546514236533\n",
      "Gradient Descent(1176/9999): loss=2.3461403015115074, w0=73.23750000000005, w1=13.782527086093301\n",
      "Gradient Descent(1177/9999): loss=2.192590610997457, w0=73.29375000000006, w1=13.803968004168679\n",
      "Gradient Descent(1178/9999): loss=2.0377948880250365, w0=73.57500000000006, w1=13.79215175234206\n",
      "Gradient Descent(1179/9999): loss=2.0626358487384633, w0=73.68750000000006, w1=13.672973271024265\n",
      "Gradient Descent(1180/9999): loss=2.3152010039593396, w0=73.63125000000005, w1=13.683194928897278\n",
      "Gradient Descent(1181/9999): loss=1.9257421253088183, w0=73.63125000000005, w1=13.568826816534363\n",
      "Gradient Descent(1182/9999): loss=2.013804478367196, w0=73.63125000000005, w1=13.630913182475842\n",
      "Gradient Descent(1183/9999): loss=2.5073974479417376, w0=73.46250000000005, w1=13.541047270735248\n",
      "Gradient Descent(1184/9999): loss=2.2882427162364793, w0=73.40625000000004, w1=13.576116150563447\n",
      "Gradient Descent(1185/9999): loss=1.7200917424944493, w0=73.29375000000005, w1=13.33913261175524\n",
      "Gradient Descent(1186/9999): loss=2.2916138737685996, w0=73.35000000000005, w1=13.288463663161638\n",
      "Gradient Descent(1187/9999): loss=2.62399778186703, w0=73.35000000000005, w1=13.254469171666853\n",
      "Gradient Descent(1188/9999): loss=2.361426173858851, w0=73.57500000000005, w1=13.398974633231095\n",
      "Gradient Descent(1189/9999): loss=2.008058198086638, w0=73.57500000000005, w1=13.575467591934993\n",
      "Gradient Descent(1190/9999): loss=1.8273693748702393, w0=73.46250000000005, w1=13.506868556147175\n",
      "Gradient Descent(1191/9999): loss=2.9760398070337577, w0=73.40625000000004, w1=13.704163846263556\n",
      "Gradient Descent(1192/9999): loss=2.2828949759550294, w0=73.40625000000004, w1=13.510774296240529\n",
      "Gradient Descent(1193/9999): loss=2.0092159269098495, w0=73.46250000000005, w1=13.390008511937724\n",
      "Gradient Descent(1194/9999): loss=2.4157383633136513, w0=73.23750000000005, w1=13.550608137900218\n",
      "Gradient Descent(1195/9999): loss=2.1498247254088714, w0=72.95625000000005, w1=13.383163903333255\n",
      "Gradient Descent(1196/9999): loss=2.172401209499834, w0=73.29375000000006, w1=13.210610193033105\n",
      "Gradient Descent(1197/9999): loss=2.348144437300577, w0=73.12500000000006, w1=13.435291725085994\n",
      "Gradient Descent(1198/9999): loss=2.464147140961675, w0=73.29375000000006, w1=13.602071023289495\n",
      "Gradient Descent(1199/9999): loss=1.8491986360529726, w0=73.23750000000005, w1=13.65122991572203\n",
      "Gradient Descent(1200/9999): loss=2.389850666036552, w0=73.29375000000006, w1=13.395927205509446\n",
      "Gradient Descent(1201/9999): loss=2.6861952154693487, w0=73.12500000000006, w1=13.500416727516837\n",
      "Gradient Descent(1202/9999): loss=2.504205673761155, w0=73.35000000000005, w1=13.465256420624284\n",
      "Gradient Descent(1203/9999): loss=1.64266802486531, w0=73.46250000000005, w1=13.605667770104551\n",
      "Gradient Descent(1204/9999): loss=2.5796206643365878, w0=73.51875000000005, w1=13.579031301333876\n",
      "Gradient Descent(1205/9999): loss=1.966745267642021, w0=73.40625000000006, w1=13.435679585318914\n",
      "Gradient Descent(1206/9999): loss=2.065828093788516, w0=73.23750000000005, w1=13.71678318644124\n",
      "Gradient Descent(1207/9999): loss=2.64219635404607, w0=73.23750000000005, w1=13.53073927406337\n",
      "Gradient Descent(1208/9999): loss=1.4240053136015889, w0=72.84375000000006, w1=13.422213746371492\n",
      "Gradient Descent(1209/9999): loss=2.041123175339695, w0=72.73125000000006, w1=13.278389773298388\n",
      "Gradient Descent(1210/9999): loss=1.8525869768161827, w0=72.78750000000007, w1=13.67281148983771\n",
      "Gradient Descent(1211/9999): loss=1.740490764800332, w0=72.50625000000007, w1=13.367434880035619\n",
      "Gradient Descent(1212/9999): loss=2.0552905025510118, w0=72.73125000000006, w1=13.322528698158619\n",
      "Gradient Descent(1213/9999): loss=2.109972418761739, w0=72.67500000000005, w1=13.241204854941374\n",
      "Gradient Descent(1214/9999): loss=1.6436226605341173, w0=72.90000000000005, w1=13.1427622257581\n",
      "Gradient Descent(1215/9999): loss=2.1738626444852005, w0=73.01250000000005, w1=13.328030962458524\n",
      "Gradient Descent(1216/9999): loss=2.2098451799738754, w0=72.73125000000005, w1=13.461128025935308\n",
      "Gradient Descent(1217/9999): loss=2.6501526784788902, w0=72.78750000000005, w1=13.546775096230874\n",
      "Gradient Descent(1218/9999): loss=2.383037086187417, w0=72.67500000000005, w1=13.554224399872972\n",
      "Gradient Descent(1219/9999): loss=2.2842439744659284, w0=72.67500000000005, w1=13.66608130834486\n",
      "Gradient Descent(1220/9999): loss=2.27464993157676, w0=72.90000000000005, w1=13.74194005286765\n",
      "Gradient Descent(1221/9999): loss=2.346485050195425, w0=72.95625000000005, w1=13.779202118126381\n",
      "Gradient Descent(1222/9999): loss=2.4982008169526293, w0=72.95625000000005, w1=14.00754705760725\n",
      "Gradient Descent(1223/9999): loss=1.8243286654176945, w0=73.06875000000005, w1=14.104401820144892\n",
      "Gradient Descent(1224/9999): loss=1.8017001858311692, w0=72.84375000000006, w1=14.040465134494212\n",
      "Gradient Descent(1225/9999): loss=2.4606192431837, w0=72.95625000000005, w1=13.669468531502067\n",
      "Gradient Descent(1226/9999): loss=2.578566238107406, w0=73.01250000000006, w1=13.734796628268096\n",
      "Gradient Descent(1227/9999): loss=2.137007251207026, w0=73.18125000000006, w1=13.506581574619188\n",
      "Gradient Descent(1228/9999): loss=1.821882092163867, w0=73.06875000000007, w1=13.370943102860734\n",
      "Gradient Descent(1229/9999): loss=2.890933748995399, w0=73.01250000000006, w1=13.560204901372124\n",
      "Gradient Descent(1230/9999): loss=2.4097273835678066, w0=73.06875000000007, w1=13.394834203144503\n",
      "Gradient Descent(1231/9999): loss=1.7695916187349507, w0=73.06875000000007, w1=13.353369605200228\n",
      "Gradient Descent(1232/9999): loss=2.495637122387293, w0=73.01250000000006, w1=13.888642381549419\n",
      "Gradient Descent(1233/9999): loss=2.268216415006573, w0=73.06875000000007, w1=13.914459781092352\n",
      "Gradient Descent(1234/9999): loss=2.6226321486158093, w0=72.95625000000007, w1=13.723832596770453\n",
      "Gradient Descent(1235/9999): loss=2.6617773180742277, w0=72.84375000000007, w1=13.697912773083495\n",
      "Gradient Descent(1236/9999): loss=2.4683050464826444, w0=72.84375000000007, w1=13.60235559950381\n",
      "Gradient Descent(1237/9999): loss=2.0402954355197958, w0=72.95625000000007, w1=13.611045842021596\n",
      "Gradient Descent(1238/9999): loss=2.2136752380676126, w0=72.84375000000007, w1=13.424408341427098\n",
      "Gradient Descent(1239/9999): loss=1.7349788298979958, w0=72.95625000000007, w1=13.612525315597603\n",
      "Gradient Descent(1240/9999): loss=2.201853301171956, w0=72.78750000000007, w1=13.436380764995652\n",
      "Gradient Descent(1241/9999): loss=2.0112442978538136, w0=72.67500000000007, w1=13.298725562703355\n",
      "Gradient Descent(1242/9999): loss=2.0400419666375296, w0=72.67500000000007, w1=13.39894375771785\n",
      "Gradient Descent(1243/9999): loss=2.5034142936843207, w0=72.90000000000006, w1=13.09652356572088\n",
      "Gradient Descent(1244/9999): loss=2.012542140683329, w0=73.06875000000007, w1=13.386868699078656\n",
      "Gradient Descent(1245/9999): loss=2.564615198076969, w0=73.18125000000006, w1=13.191373217818994\n",
      "Gradient Descent(1246/9999): loss=1.9452521832665397, w0=73.18125000000006, w1=13.141977650566854\n",
      "Gradient Descent(1247/9999): loss=2.227256561997649, w0=72.90000000000006, w1=13.116853777495294\n",
      "Gradient Descent(1248/9999): loss=1.9352491521927102, w0=72.78750000000007, w1=13.378170003897102\n",
      "Gradient Descent(1249/9999): loss=2.507123305900882, w0=73.06875000000007, w1=13.26941853672621\n",
      "Gradient Descent(1250/9999): loss=1.795543161973615, w0=73.06875000000007, w1=13.432318881433172\n",
      "Gradient Descent(1251/9999): loss=2.3196822447856325, w0=73.18125000000006, w1=13.490895978805487\n",
      "Gradient Descent(1252/9999): loss=2.1472037820778924, w0=73.18125000000006, w1=13.436353849336829\n",
      "Gradient Descent(1253/9999): loss=2.172959873079843, w0=73.18125000000006, w1=13.4642362419094\n",
      "Gradient Descent(1254/9999): loss=2.8090954181249597, w0=73.46250000000006, w1=13.499261662487237\n",
      "Gradient Descent(1255/9999): loss=2.140270054241416, w0=73.23750000000007, w1=13.495992583426561\n",
      "Gradient Descent(1256/9999): loss=2.212444532139509, w0=73.40625000000007, w1=13.534552175803926\n",
      "Gradient Descent(1257/9999): loss=2.1139691054193674, w0=73.51875000000007, w1=13.369561170963895\n",
      "Gradient Descent(1258/9999): loss=1.7106212551073492, w0=73.40625000000007, w1=13.259746577476834\n",
      "Gradient Descent(1259/9999): loss=2.3626917261844236, w0=73.74375000000008, w1=13.514728021746183\n",
      "Gradient Descent(1260/9999): loss=2.433023485661815, w0=74.02500000000008, w1=13.562481252828889\n",
      "Gradient Descent(1261/9999): loss=2.089933892224786, w0=73.91250000000008, w1=13.361794811107238\n",
      "Gradient Descent(1262/9999): loss=2.355942023142675, w0=73.96875000000009, w1=13.38806556864838\n",
      "Gradient Descent(1263/9999): loss=1.9415890938043996, w0=74.25000000000009, w1=13.406121421727741\n",
      "Gradient Descent(1264/9999): loss=2.7331367409317444, w0=74.13750000000009, w1=13.476471049776077\n",
      "Gradient Descent(1265/9999): loss=2.3891743535582948, w0=74.30625000000009, w1=13.418828881072583\n",
      "Gradient Descent(1266/9999): loss=2.2471861734764875, w0=73.9125000000001, w1=13.627634831108434\n",
      "Gradient Descent(1267/9999): loss=2.189284513515682, w0=73.6875000000001, w1=13.723014527255033\n",
      "Gradient Descent(1268/9999): loss=2.054559342471902, w0=73.4625000000001, w1=13.593109910322656\n",
      "Gradient Descent(1269/9999): loss=1.781358809676569, w0=73.4625000000001, w1=13.878581416626059\n",
      "Gradient Descent(1270/9999): loss=2.1378476465273737, w0=73.4062500000001, w1=13.926960358036162\n",
      "Gradient Descent(1271/9999): loss=2.4584260154120643, w0=73.6312500000001, w1=13.839382627152915\n",
      "Gradient Descent(1272/9999): loss=2.4243112399109927, w0=73.4062500000001, w1=13.783529112298076\n",
      "Gradient Descent(1273/9999): loss=2.66180465090154, w0=73.5187500000001, w1=13.746817481428197\n",
      "Gradient Descent(1274/9999): loss=2.282684366810318, w0=73.46250000000009, w1=13.848112624361637\n",
      "Gradient Descent(1275/9999): loss=1.7972721843628015, w0=73.46250000000009, w1=13.715226801989653\n",
      "Gradient Descent(1276/9999): loss=1.8350652103384686, w0=73.18125000000009, w1=13.387425804628318\n",
      "Gradient Descent(1277/9999): loss=2.1971421107314493, w0=73.0687500000001, w1=13.453478083771083\n",
      "Gradient Descent(1278/9999): loss=2.129899134680084, w0=73.18125000000009, w1=13.524793233732362\n",
      "Gradient Descent(1279/9999): loss=2.084973100923306, w0=73.2375000000001, w1=13.252359837509086\n",
      "Gradient Descent(1280/9999): loss=2.508634224804007, w0=73.46250000000009, w1=13.325851354076752\n",
      "Gradient Descent(1281/9999): loss=2.1936989583222832, w0=73.29375000000009, w1=13.413543404597243\n",
      "Gradient Descent(1282/9999): loss=2.0274496557623465, w0=73.23750000000008, w1=13.51675899360299\n",
      "Gradient Descent(1283/9999): loss=2.1629162114677767, w0=72.95625000000008, w1=13.39837641157496\n",
      "Gradient Descent(1284/9999): loss=2.5988395753293965, w0=72.95625000000008, w1=13.47914956544264\n",
      "Gradient Descent(1285/9999): loss=2.1220866882959184, w0=72.90000000000008, w1=13.560360279076432\n",
      "Gradient Descent(1286/9999): loss=2.605166654733475, w0=73.18125000000008, w1=13.765944621509874\n",
      "Gradient Descent(1287/9999): loss=2.4542548106802125, w0=73.29375000000007, w1=13.907706525172387\n",
      "Gradient Descent(1288/9999): loss=1.8691883577899582, w0=73.23750000000007, w1=13.877265720824138\n",
      "Gradient Descent(1289/9999): loss=2.730913080834042, w0=73.40625000000007, w1=13.733630222248797\n",
      "Gradient Descent(1290/9999): loss=2.1897253145945994, w0=73.46250000000008, w1=13.835290315065794\n",
      "Gradient Descent(1291/9999): loss=1.9450444385350214, w0=73.51875000000008, w1=13.728622258212935\n",
      "Gradient Descent(1292/9999): loss=2.4181191293524527, w0=73.46250000000008, w1=13.708958075156353\n",
      "Gradient Descent(1293/9999): loss=2.4654125634190596, w0=73.06875000000008, w1=13.721942586888515\n",
      "Gradient Descent(1294/9999): loss=2.163338082659508, w0=73.06875000000008, w1=13.774738735949034\n",
      "Gradient Descent(1295/9999): loss=1.914500508837783, w0=73.18125000000008, w1=13.640587027554432\n",
      "Gradient Descent(1296/9999): loss=2.770569785404839, w0=73.29375000000007, w1=13.325408697852628\n",
      "Gradient Descent(1297/9999): loss=2.0184072391192123, w0=73.51875000000007, w1=13.586644016124968\n",
      "Gradient Descent(1298/9999): loss=2.354140899600196, w0=73.51875000000007, w1=13.670002514112795\n",
      "Gradient Descent(1299/9999): loss=1.7528175151868717, w0=73.68750000000007, w1=13.401468737475213\n",
      "Gradient Descent(1300/9999): loss=2.5594758900571133, w0=73.57500000000007, w1=13.651656187791803\n",
      "Gradient Descent(1301/9999): loss=2.255963233874734, w0=73.18125000000008, w1=13.520520033184008\n",
      "Gradient Descent(1302/9999): loss=2.273021995972356, w0=73.01250000000007, w1=13.41743468944039\n",
      "Gradient Descent(1303/9999): loss=2.40528438292284, w0=72.90000000000008, w1=13.385634685038843\n",
      "Gradient Descent(1304/9999): loss=2.166176941607521, w0=73.01250000000007, w1=13.51598857829582\n",
      "Gradient Descent(1305/9999): loss=2.4803253035074233, w0=73.06875000000008, w1=13.523197738040665\n",
      "Gradient Descent(1306/9999): loss=1.751542442631563, w0=73.06875000000008, w1=13.49392518301658\n",
      "Gradient Descent(1307/9999): loss=2.3055629741073282, w0=73.18125000000008, w1=13.66441123021209\n",
      "Gradient Descent(1308/9999): loss=1.802029542899855, w0=73.01250000000007, w1=13.724913771304646\n",
      "Gradient Descent(1309/9999): loss=2.104025378353616, w0=73.12500000000007, w1=13.843339293296966\n",
      "Gradient Descent(1310/9999): loss=1.7092270455077703, w0=72.95625000000007, w1=13.930826750343572\n",
      "Gradient Descent(1311/9999): loss=2.1163914252673637, w0=72.90000000000006, w1=14.186612911503108\n",
      "Gradient Descent(1312/9999): loss=2.2036724650125885, w0=72.73125000000006, w1=14.146499870598847\n",
      "Gradient Descent(1313/9999): loss=1.9274339252869621, w0=72.73125000000006, w1=14.229766494261444\n",
      "Gradient Descent(1314/9999): loss=2.5129658753984643, w0=72.45000000000006, w1=14.068759560144734\n",
      "Gradient Descent(1315/9999): loss=2.094415771997345, w0=72.61875000000006, w1=14.345566863060727\n",
      "Gradient Descent(1316/9999): loss=2.1917410501126438, w0=72.45000000000006, w1=14.477022278348826\n",
      "Gradient Descent(1317/9999): loss=2.4927141265428165, w0=72.45000000000006, w1=14.378407971629823\n",
      "Gradient Descent(1318/9999): loss=2.1936089929531852, w0=72.33750000000006, w1=14.199952711742515\n",
      "Gradient Descent(1319/9999): loss=2.3768945141168016, w0=72.50625000000007, w1=14.441172972667358\n",
      "Gradient Descent(1320/9999): loss=2.0969329259795186, w0=72.45000000000006, w1=14.308451150735547\n",
      "Gradient Descent(1321/9999): loss=2.2551832552617297, w0=72.73125000000006, w1=14.255333656628567\n",
      "Gradient Descent(1322/9999): loss=2.515335868459903, w0=72.95625000000005, w1=14.374319695830906\n",
      "Gradient Descent(1323/9999): loss=2.4997744339810155, w0=73.01250000000006, w1=14.03126030432354\n",
      "Gradient Descent(1324/9999): loss=2.7110152086453914, w0=73.06875000000007, w1=13.946649152101264\n",
      "Gradient Descent(1325/9999): loss=2.343363590679491, w0=72.90000000000006, w1=14.093690129706781\n",
      "Gradient Descent(1326/9999): loss=2.0625537985387417, w0=73.18125000000006, w1=13.896463285785364\n",
      "Gradient Descent(1327/9999): loss=2.0967873198736724, w0=73.29375000000006, w1=14.07142668638471\n",
      "Gradient Descent(1328/9999): loss=1.938160970613033, w0=73.18125000000006, w1=14.007569301476488\n",
      "Gradient Descent(1329/9999): loss=2.300305903257545, w0=73.35000000000007, w1=13.832673305439439\n",
      "Gradient Descent(1330/9999): loss=2.095007570625638, w0=73.51875000000007, w1=13.837707701748046\n",
      "Gradient Descent(1331/9999): loss=1.973316801739758, w0=73.57500000000007, w1=13.636006848506561\n",
      "Gradient Descent(1332/9999): loss=2.398888442860734, w0=73.35000000000008, w1=13.623284834199495\n",
      "Gradient Descent(1333/9999): loss=2.399599658013142, w0=73.35000000000008, w1=13.83493669477857\n",
      "Gradient Descent(1334/9999): loss=2.3232387535404166, w0=73.23750000000008, w1=13.982320370965601\n",
      "Gradient Descent(1335/9999): loss=1.9609680315023004, w0=73.18125000000008, w1=13.963082941110681\n",
      "Gradient Descent(1336/9999): loss=2.327641280739752, w0=72.73125000000007, w1=13.942811307835603\n",
      "Gradient Descent(1337/9999): loss=2.3612082533398713, w0=72.73125000000007, w1=13.79456597908849\n",
      "Gradient Descent(1338/9999): loss=1.7464085643194094, w0=72.67500000000007, w1=13.837463820498794\n",
      "Gradient Descent(1339/9999): loss=2.51722265768621, w0=72.67500000000007, w1=13.796500304060853\n",
      "Gradient Descent(1340/9999): loss=2.407266080959904, w0=72.73125000000007, w1=13.639834127091477\n",
      "Gradient Descent(1341/9999): loss=2.3557352633817965, w0=72.73125000000007, w1=13.715138443504996\n",
      "Gradient Descent(1342/9999): loss=2.1559427958617734, w0=72.67500000000007, w1=13.705976387572456\n",
      "Gradient Descent(1343/9999): loss=1.837032419020168, w0=72.56250000000007, w1=13.691279622117133\n",
      "Gradient Descent(1344/9999): loss=2.148917666846421, w0=72.61875000000008, w1=13.539114313476512\n",
      "Gradient Descent(1345/9999): loss=2.198355579308352, w0=72.50625000000008, w1=13.468494137317519\n",
      "Gradient Descent(1346/9999): loss=2.486029430919857, w0=72.67500000000008, w1=13.212305450403077\n",
      "Gradient Descent(1347/9999): loss=2.204219923492533, w0=72.50625000000008, w1=13.136071754612601\n",
      "Gradient Descent(1348/9999): loss=2.121518312293291, w0=72.50625000000008, w1=13.211165153109373\n",
      "Gradient Descent(1349/9999): loss=2.2616858547635115, w0=72.61875000000008, w1=13.161319714561309\n",
      "Gradient Descent(1350/9999): loss=1.8919931655216564, w0=72.78750000000008, w1=13.204854776751288\n",
      "Gradient Descent(1351/9999): loss=2.2516270187253453, w0=72.61875000000008, w1=13.137329443415734\n",
      "Gradient Descent(1352/9999): loss=3.271980199943198, w0=72.84375000000007, w1=13.500568143564895\n",
      "Gradient Descent(1353/9999): loss=2.28401967306808, w0=73.06875000000007, w1=13.495347509411415\n",
      "Gradient Descent(1354/9999): loss=2.3174645684651054, w0=72.95625000000007, w1=13.65638427690506\n",
      "Gradient Descent(1355/9999): loss=2.570021119315556, w0=73.01250000000007, w1=13.62948832314947\n",
      "Gradient Descent(1356/9999): loss=1.979831821040223, w0=73.01250000000007, w1=13.50873711795031\n",
      "Gradient Descent(1357/9999): loss=2.2634528260851035, w0=73.23750000000007, w1=13.624424685958228\n",
      "Gradient Descent(1358/9999): loss=2.27572408988848, w0=73.18125000000006, w1=13.691176870457987\n",
      "Gradient Descent(1359/9999): loss=2.6602154424409203, w0=73.01250000000006, w1=13.458552113959124\n",
      "Gradient Descent(1360/9999): loss=3.0045739589026175, w0=73.01250000000006, w1=13.537365716317932\n",
      "Gradient Descent(1361/9999): loss=2.5291185724212673, w0=73.18125000000006, w1=13.422449777983369\n",
      "Gradient Descent(1362/9999): loss=2.4833592506216764, w0=73.40625000000006, w1=13.720111922228694\n",
      "Gradient Descent(1363/9999): loss=2.4380649223021025, w0=73.40625000000006, w1=13.63053315350166\n",
      "Gradient Descent(1364/9999): loss=2.388050362126031, w0=73.51875000000005, w1=13.298814348972451\n",
      "Gradient Descent(1365/9999): loss=2.6572782899784357, w0=73.63125000000005, w1=13.33924183219735\n",
      "Gradient Descent(1366/9999): loss=1.8943788049862316, w0=73.68750000000006, w1=13.278773529307939\n",
      "Gradient Descent(1367/9999): loss=2.9784320728785136, w0=73.63125000000005, w1=13.29142998812954\n",
      "Gradient Descent(1368/9999): loss=2.3763575303765396, w0=73.46250000000005, w1=13.195144184071316\n",
      "Gradient Descent(1369/9999): loss=1.9568603623897876, w0=73.63125000000005, w1=13.170332906771742\n",
      "Gradient Descent(1370/9999): loss=1.6423843069506006, w0=73.63125000000005, w1=13.322755845234113\n",
      "Gradient Descent(1371/9999): loss=2.346277752142467, w0=73.40625000000006, w1=13.404310918015323\n",
      "Gradient Descent(1372/9999): loss=1.735220370385366, w0=73.63125000000005, w1=13.31048561544798\n",
      "Gradient Descent(1373/9999): loss=2.4801544379172142, w0=73.57500000000005, w1=13.112797002504475\n",
      "Gradient Descent(1374/9999): loss=1.7014234813396127, w0=73.23750000000004, w1=13.28130766853912\n",
      "Gradient Descent(1375/9999): loss=2.1465342317544516, w0=73.18125000000003, w1=13.408307966653993\n",
      "Gradient Descent(1376/9999): loss=2.11692534160039, w0=73.06875000000004, w1=13.422731688600916\n",
      "Gradient Descent(1377/9999): loss=2.3722392052611037, w0=73.06875000000004, w1=13.659047770345461\n",
      "Gradient Descent(1378/9999): loss=2.293848648622047, w0=72.90000000000003, w1=13.709296131866136\n",
      "Gradient Descent(1379/9999): loss=2.104940378015389, w0=73.18125000000003, w1=13.646770269695063\n",
      "Gradient Descent(1380/9999): loss=1.9028687604179444, w0=73.18125000000003, w1=13.61473490030307\n",
      "Gradient Descent(1381/9999): loss=1.9053709286762857, w0=73.12500000000003, w1=13.548603374394695\n",
      "Gradient Descent(1382/9999): loss=1.6990676740301485, w0=73.06875000000002, w1=13.696400011390242\n",
      "Gradient Descent(1383/9999): loss=2.1480225644570927, w0=73.12500000000003, w1=13.995215737894553\n",
      "Gradient Descent(1384/9999): loss=2.4668645004142844, w0=73.01250000000003, w1=13.979075662247723\n",
      "Gradient Descent(1385/9999): loss=1.9607538229867623, w0=73.06875000000004, w1=13.922612196285145\n",
      "Gradient Descent(1386/9999): loss=2.38761387843677, w0=73.06875000000004, w1=13.868145426367695\n",
      "Gradient Descent(1387/9999): loss=1.523049545590449, w0=73.01250000000003, w1=13.817732387073779\n",
      "Gradient Descent(1388/9999): loss=2.2708690255974875, w0=73.01250000000003, w1=13.840020190780843\n",
      "Gradient Descent(1389/9999): loss=2.004828559054194, w0=72.90000000000003, w1=13.980131291449739\n",
      "Gradient Descent(1390/9999): loss=2.75501192852725, w0=72.95625000000004, w1=13.960812826164032\n",
      "Gradient Descent(1391/9999): loss=1.9257066843943913, w0=73.06875000000004, w1=13.99648131194942\n",
      "Gradient Descent(1392/9999): loss=2.522398382054115, w0=73.29375000000003, w1=13.83822691626268\n",
      "Gradient Descent(1393/9999): loss=2.380443130610214, w0=73.23750000000003, w1=13.865001157589974\n",
      "Gradient Descent(1394/9999): loss=1.9820263307885415, w0=73.35000000000002, w1=14.03569795771765\n",
      "Gradient Descent(1395/9999): loss=2.067929585636586, w0=73.12500000000003, w1=14.39478701888009\n",
      "Gradient Descent(1396/9999): loss=2.2469817774853387, w0=73.12500000000003, w1=14.047283737185591\n",
      "Gradient Descent(1397/9999): loss=2.512913486050685, w0=72.84375000000003, w1=14.050802398811397\n",
      "Gradient Descent(1398/9999): loss=2.1468309161874104, w0=72.84375000000003, w1=13.870830605855126\n",
      "Gradient Descent(1399/9999): loss=1.878958314044486, w0=73.12500000000003, w1=13.773141739487293\n",
      "Gradient Descent(1400/9999): loss=2.1106741086627445, w0=73.35000000000002, w1=13.619879884123497\n",
      "Gradient Descent(1401/9999): loss=2.4294373388003425, w0=73.12500000000003, w1=13.605946514304236\n",
      "Gradient Descent(1402/9999): loss=2.1776910251364323, w0=73.01250000000003, w1=13.797987488455698\n",
      "Gradient Descent(1403/9999): loss=2.232979624905378, w0=72.90000000000003, w1=13.756751092486919\n",
      "Gradient Descent(1404/9999): loss=2.1382432436168797, w0=73.01250000000003, w1=13.600806601235435\n",
      "Gradient Descent(1405/9999): loss=2.561082028035216, w0=73.12500000000003, w1=13.708372845557163\n",
      "Gradient Descent(1406/9999): loss=1.8316599137315719, w0=73.06875000000002, w1=13.5835785224243\n",
      "Gradient Descent(1407/9999): loss=2.5556817917367978, w0=73.01250000000002, w1=13.458253281614677\n",
      "Gradient Descent(1408/9999): loss=2.2279647989882934, w0=73.06875000000002, w1=13.239392266656612\n",
      "Gradient Descent(1409/9999): loss=2.6463628227200497, w0=73.12500000000003, w1=13.203428785569773\n",
      "Gradient Descent(1410/9999): loss=2.2063342045326713, w0=72.73125000000003, w1=13.310789118492924\n",
      "Gradient Descent(1411/9999): loss=1.88107294074938, w0=72.95625000000003, w1=13.220135505932477\n",
      "Gradient Descent(1412/9999): loss=1.9382235205217007, w0=73.01250000000003, w1=13.403875549448129\n",
      "Gradient Descent(1413/9999): loss=1.6867026505820608, w0=73.23750000000003, w1=13.513080625100145\n",
      "Gradient Descent(1414/9999): loss=1.8603006891227558, w0=73.18125000000002, w1=13.448405773577592\n",
      "Gradient Descent(1415/9999): loss=2.404266649934339, w0=73.23750000000003, w1=13.35024420002934\n",
      "Gradient Descent(1416/9999): loss=2.1516108882672205, w0=73.18125000000002, w1=13.797991544794652\n",
      "Gradient Descent(1417/9999): loss=2.066943155445677, w0=73.23750000000003, w1=13.83702545188435\n",
      "Gradient Descent(1418/9999): loss=1.8643831860859956, w0=73.18125000000002, w1=13.7065540051693\n",
      "Gradient Descent(1419/9999): loss=2.466893605951494, w0=73.12500000000001, w1=13.888454045447386\n",
      "Gradient Descent(1420/9999): loss=2.239591402517368, w0=73.29375000000002, w1=13.958401581991716\n",
      "Gradient Descent(1421/9999): loss=2.52863933389075, w0=73.23750000000001, w1=14.021617993632594\n",
      "Gradient Descent(1422/9999): loss=2.6490868531322525, w0=73.06875000000001, w1=14.024713714501237\n",
      "Gradient Descent(1423/9999): loss=2.2747238716280824, w0=73.12500000000001, w1=13.874863193601952\n",
      "Gradient Descent(1424/9999): loss=2.5183715811046685, w0=73.46250000000002, w1=13.944022924521931\n",
      "Gradient Descent(1425/9999): loss=1.8396259248204547, w0=73.51875000000003, w1=13.686791788376903\n",
      "Gradient Descent(1426/9999): loss=2.09562970096302, w0=73.57500000000003, w1=13.647113300078493\n",
      "Gradient Descent(1427/9999): loss=2.202859324671773, w0=73.68750000000003, w1=13.702414963040733\n",
      "Gradient Descent(1428/9999): loss=2.531377595284285, w0=73.74375000000003, w1=13.455130138906782\n",
      "Gradient Descent(1429/9999): loss=2.5294052562425846, w0=73.51875000000004, w1=13.343514611489079\n",
      "Gradient Descent(1430/9999): loss=2.4079236824847, w0=73.40625000000004, w1=13.230057007180768\n",
      "Gradient Descent(1431/9999): loss=2.094274890002206, w0=73.63125000000004, w1=13.516999620232118\n",
      "Gradient Descent(1432/9999): loss=1.9137430548069772, w0=73.51875000000004, w1=13.725893194185016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1433/9999): loss=2.332081227599149, w0=73.63125000000004, w1=13.612080400052832\n",
      "Gradient Descent(1434/9999): loss=2.1149374372284395, w0=73.51875000000004, w1=14.013752156336938\n",
      "Gradient Descent(1435/9999): loss=2.053114128977865, w0=73.51875000000004, w1=14.043380511847719\n",
      "Gradient Descent(1436/9999): loss=2.5603909279437214, w0=73.57500000000005, w1=14.071677166239219\n",
      "Gradient Descent(1437/9999): loss=1.9315963318237512, w0=73.46250000000005, w1=13.863212809310387\n",
      "Gradient Descent(1438/9999): loss=2.550075730369096, w0=73.35000000000005, w1=13.904046610617563\n",
      "Gradient Descent(1439/9999): loss=2.223673602657258, w0=73.18125000000005, w1=13.736049431326002\n",
      "Gradient Descent(1440/9999): loss=2.473743169210736, w0=73.06875000000005, w1=13.692569413984819\n",
      "Gradient Descent(1441/9999): loss=2.519241116153198, w0=72.90000000000005, w1=13.649167454506335\n",
      "Gradient Descent(1442/9999): loss=2.1163189261114415, w0=72.84375000000004, w1=13.69078419572675\n",
      "Gradient Descent(1443/9999): loss=2.0460698581280092, w0=72.84375000000004, w1=13.821849160740014\n",
      "Gradient Descent(1444/9999): loss=2.456095150940166, w0=72.95625000000004, w1=13.819082439750138\n",
      "Gradient Descent(1445/9999): loss=2.1355331570404665, w0=73.01250000000005, w1=13.743811858835683\n",
      "Gradient Descent(1446/9999): loss=2.679056062040871, w0=72.95625000000004, w1=13.467714945973805\n",
      "Gradient Descent(1447/9999): loss=2.5834935037313143, w0=73.12500000000004, w1=13.460095518101706\n",
      "Gradient Descent(1448/9999): loss=1.8757247572756006, w0=73.35000000000004, w1=13.03745782831515\n",
      "Gradient Descent(1449/9999): loss=2.012222294851999, w0=73.35000000000004, w1=13.369879908138133\n",
      "Gradient Descent(1450/9999): loss=1.70351875189525, w0=73.12500000000004, w1=13.57769256329168\n",
      "Gradient Descent(1451/9999): loss=1.8356382641579583, w0=73.23750000000004, w1=13.515904174264175\n",
      "Gradient Descent(1452/9999): loss=1.3139693660088831, w0=73.01250000000005, w1=13.720732157061931\n",
      "Gradient Descent(1453/9999): loss=2.371467163883202, w0=73.06875000000005, w1=13.391838717050826\n",
      "Gradient Descent(1454/9999): loss=2.2040825716031076, w0=73.01250000000005, w1=13.472648225638526\n",
      "Gradient Descent(1455/9999): loss=2.245142248951448, w0=72.78750000000005, w1=13.476063235572326\n",
      "Gradient Descent(1456/9999): loss=2.2871923105820064, w0=72.95625000000005, w1=13.609562610332931\n",
      "Gradient Descent(1457/9999): loss=2.05955258812424, w0=73.06875000000005, w1=13.761777055724165\n",
      "Gradient Descent(1458/9999): loss=2.6787174417473327, w0=73.18125000000005, w1=13.707822431887784\n",
      "Gradient Descent(1459/9999): loss=2.5571307276684045, w0=73.23750000000005, w1=13.38130255751288\n",
      "Gradient Descent(1460/9999): loss=2.0107449140367284, w0=73.29375000000006, w1=13.601726548936833\n",
      "Gradient Descent(1461/9999): loss=2.030549351645691, w0=73.01250000000006, w1=13.573680620579184\n",
      "Gradient Descent(1462/9999): loss=2.156329386491493, w0=73.23750000000005, w1=13.536195014483836\n",
      "Gradient Descent(1463/9999): loss=1.9567840833726011, w0=73.23750000000005, w1=13.698199166278474\n",
      "Gradient Descent(1464/9999): loss=2.2798878248307965, w0=73.35000000000005, w1=13.632681237380593\n",
      "Gradient Descent(1465/9999): loss=2.123885180164434, w0=73.23750000000005, w1=13.400811121125255\n",
      "Gradient Descent(1466/9999): loss=2.3407823279332916, w0=73.23750000000005, w1=13.325730706104917\n",
      "Gradient Descent(1467/9999): loss=2.326884933197678, w0=73.23750000000005, w1=13.446206581111072\n",
      "Gradient Descent(1468/9999): loss=2.4164660073186535, w0=73.06875000000005, w1=13.33845911265784\n",
      "Gradient Descent(1469/9999): loss=1.9683696321578341, w0=73.29375000000005, w1=13.330322483085494\n",
      "Gradient Descent(1470/9999): loss=2.2699693044002385, w0=73.46250000000005, w1=13.151112821872509\n",
      "Gradient Descent(1471/9999): loss=2.248123237512678, w0=73.29375000000005, w1=13.117111566733227\n",
      "Gradient Descent(1472/9999): loss=1.980470885826972, w0=73.01250000000005, w1=13.574596002856875\n",
      "Gradient Descent(1473/9999): loss=2.540347365140203, w0=73.29375000000005, w1=13.562577978118524\n",
      "Gradient Descent(1474/9999): loss=2.094213682515184, w0=73.51875000000004, w1=13.59256595042406\n",
      "Gradient Descent(1475/9999): loss=2.1395203704485133, w0=73.51875000000004, w1=13.40622033859034\n",
      "Gradient Descent(1476/9999): loss=2.486807270546604, w0=73.68750000000004, w1=13.525546500588291\n",
      "Gradient Descent(1477/9999): loss=2.4266921783282407, w0=73.63125000000004, w1=13.497368521346097\n",
      "Gradient Descent(1478/9999): loss=1.9146830951363498, w0=73.40625000000004, w1=13.680072521620762\n",
      "Gradient Descent(1479/9999): loss=2.4454001442917357, w0=73.57500000000005, w1=13.835468952046657\n",
      "Gradient Descent(1480/9999): loss=2.2355750120787796, w0=73.29375000000005, w1=13.856186823257596\n",
      "Gradient Descent(1481/9999): loss=2.4197160078755284, w0=73.40625000000004, w1=13.716390906389872\n",
      "Gradient Descent(1482/9999): loss=2.4784706495252804, w0=73.46250000000005, w1=13.640524151913787\n",
      "Gradient Descent(1483/9999): loss=1.9929803069923009, w0=73.57500000000005, w1=13.341375939074899\n",
      "Gradient Descent(1484/9999): loss=2.4373775649535876, w0=73.63125000000005, w1=13.366954797924148\n",
      "Gradient Descent(1485/9999): loss=1.8536678894923602, w0=73.57500000000005, w1=13.229169146854382\n",
      "Gradient Descent(1486/9999): loss=2.092190245295771, w0=73.51875000000004, w1=13.311828552272035\n",
      "Gradient Descent(1487/9999): loss=2.2133818564139887, w0=73.46250000000003, w1=13.25060240573758\n",
      "Gradient Descent(1488/9999): loss=2.5005479599584746, w0=73.51875000000004, w1=13.250064217555668\n",
      "Gradient Descent(1489/9999): loss=2.427510058351377, w0=73.23750000000004, w1=13.115936556345172\n",
      "Gradient Descent(1490/9999): loss=2.557065447502815, w0=73.23750000000004, w1=12.897075391491661\n",
      "Gradient Descent(1491/9999): loss=2.610965583959953, w0=73.23750000000004, w1=13.319684662974439\n",
      "Gradient Descent(1492/9999): loss=1.7587053254278646, w0=73.23750000000004, w1=13.402187553603296\n",
      "Gradient Descent(1493/9999): loss=2.2014820335801, w0=73.18125000000003, w1=13.530286063686749\n",
      "Gradient Descent(1494/9999): loss=2.260079700199035, w0=73.12500000000003, w1=13.610802726258624\n",
      "Gradient Descent(1495/9999): loss=2.1118116485637355, w0=73.18125000000003, w1=13.38430274771552\n",
      "Gradient Descent(1496/9999): loss=2.0292434810747393, w0=73.01250000000003, w1=13.371333058302142\n",
      "Gradient Descent(1497/9999): loss=2.380580210322174, w0=73.35000000000004, w1=13.508944813336576\n",
      "Gradient Descent(1498/9999): loss=2.0120409584516956, w0=73.29375000000003, w1=13.408312899581347\n",
      "Gradient Descent(1499/9999): loss=2.645234521152641, w0=73.01250000000003, w1=13.615500538296127\n",
      "Gradient Descent(1500/9999): loss=2.185038639881962, w0=72.90000000000003, w1=13.579888499148744\n",
      "Gradient Descent(1501/9999): loss=2.643009253789298, w0=72.95625000000004, w1=13.496911439873415\n",
      "Gradient Descent(1502/9999): loss=1.7500644894742348, w0=73.01250000000005, w1=13.217834720968042\n",
      "Gradient Descent(1503/9999): loss=2.5298684150080772, w0=73.01250000000005, w1=13.246344191088246\n",
      "Gradient Descent(1504/9999): loss=1.7144306728859542, w0=72.95625000000004, w1=13.41761541536903\n",
      "Gradient Descent(1505/9999): loss=2.593878782406082, w0=72.90000000000003, w1=13.63763099555151\n",
      "Gradient Descent(1506/9999): loss=1.8851252830719054, w0=73.06875000000004, w1=13.683538107301379\n",
      "Gradient Descent(1507/9999): loss=1.8436282483566793, w0=73.18125000000003, w1=13.691308828149493\n",
      "Gradient Descent(1508/9999): loss=1.985684036970662, w0=73.01250000000003, w1=13.876227885479416\n",
      "Gradient Descent(1509/9999): loss=2.235945939084801, w0=72.95625000000003, w1=13.826958717386274\n",
      "Gradient Descent(1510/9999): loss=2.3269946024076313, w0=73.06875000000002, w1=13.504558733778135\n",
      "Gradient Descent(1511/9999): loss=2.1562223716780338, w0=73.18125000000002, w1=13.178698944962743\n",
      "Gradient Descent(1512/9999): loss=2.259580275321007, w0=73.06875000000002, w1=13.2673306104065\n",
      "Gradient Descent(1513/9999): loss=2.49887041467838, w0=73.01250000000002, w1=13.597756123189669\n",
      "Gradient Descent(1514/9999): loss=2.2409930137163423, w0=72.95625000000001, w1=13.789584265058133\n",
      "Gradient Descent(1515/9999): loss=2.115904685343393, w0=72.78750000000001, w1=13.969789121158744\n",
      "Gradient Descent(1516/9999): loss=2.389358105540599, w0=72.9, w1=13.682361332899635\n",
      "Gradient Descent(1517/9999): loss=2.5299291180339516, w0=72.84375, w1=13.358127700001171\n",
      "Gradient Descent(1518/9999): loss=2.343949464790235, w0=73.06875, w1=13.27098742632066\n",
      "Gradient Descent(1519/9999): loss=2.481526788234487, w0=73.18124999999999, w1=13.122306465963604\n",
      "Gradient Descent(1520/9999): loss=2.1159056806938965, w0=73.35, w1=13.121584617278668\n",
      "Gradient Descent(1521/9999): loss=2.2330733868694033, w0=73.35, w1=13.216422437046164\n",
      "Gradient Descent(1522/9999): loss=1.7753908984265212, w0=73.06875, w1=13.315624028334652\n",
      "Gradient Descent(1523/9999): loss=2.699835678300421, w0=72.7875, w1=13.255133404343148\n",
      "Gradient Descent(1524/9999): loss=2.7053970093578084, w0=72.84375, w1=13.311201444620673\n",
      "Gradient Descent(1525/9999): loss=1.6823824651269077, w0=72.84375, w1=13.267095946295761\n",
      "Gradient Descent(1526/9999): loss=1.8678505346419643, w0=72.7875, w1=13.195307630793957\n",
      "Gradient Descent(1527/9999): loss=2.4827434202922802, w0=73.18124999999999, w1=12.986747974559346\n",
      "Gradient Descent(1528/9999): loss=2.050154679446544, w0=73.18124999999999, w1=13.345644268299285\n",
      "Gradient Descent(1529/9999): loss=2.3554110649150646, w0=73.46249999999999, w1=13.27506206063255\n",
      "Gradient Descent(1530/9999): loss=2.2235147071252244, w0=73.35, w1=13.427607790916511\n",
      "Gradient Descent(1531/9999): loss=2.3179916782641192, w0=73.2375, w1=13.55309159349425\n",
      "Gradient Descent(1532/9999): loss=2.6534582726412337, w0=73.29375, w1=13.799827411462656\n",
      "Gradient Descent(1533/9999): loss=2.2147413412948755, w0=73.40625, w1=13.519400652258984\n",
      "Gradient Descent(1534/9999): loss=2.3406916406458853, w0=73.35, w1=13.371130101842025\n",
      "Gradient Descent(1535/9999): loss=2.239867743565874, w0=73.35, w1=13.4936771409805\n",
      "Gradient Descent(1536/9999): loss=2.223792364599014, w0=73.51875, w1=13.675667387029682\n",
      "Gradient Descent(1537/9999): loss=2.4235357806543445, w0=73.46249999999999, w1=13.343951188841391\n",
      "Gradient Descent(1538/9999): loss=2.459940503712692, w0=73.46249999999999, w1=13.190983173539589\n",
      "Gradient Descent(1539/9999): loss=2.2297821468489616, w0=73.29374999999999, w1=13.188067681805352\n",
      "Gradient Descent(1540/9999): loss=2.4360000481382214, w0=73.40624999999999, w1=13.118883070918198\n",
      "Gradient Descent(1541/9999): loss=1.8550618177454976, w0=73.51874999999998, w1=13.233199312542961\n",
      "Gradient Descent(1542/9999): loss=1.9922288813143014, w0=73.51874999999998, w1=13.304704157018897\n",
      "Gradient Descent(1543/9999): loss=2.231401606737885, w0=73.23749999999998, w1=13.220365602116495\n",
      "Gradient Descent(1544/9999): loss=2.1119276754037335, w0=73.12499999999999, w1=13.181238344106966\n",
      "Gradient Descent(1545/9999): loss=1.910911143610935, w0=72.95624999999998, w1=13.262961212342164\n",
      "Gradient Descent(1546/9999): loss=2.084509133049934, w0=73.18124999999998, w1=13.46255899387854\n",
      "Gradient Descent(1547/9999): loss=2.077140975620643, w0=73.18124999999998, w1=13.418106466024428\n",
      "Gradient Descent(1548/9999): loss=1.7866967965956477, w0=73.18124999999998, w1=13.602501895494473\n",
      "Gradient Descent(1549/9999): loss=2.5552940662427717, w0=73.12499999999997, w1=13.715217173995974\n",
      "Gradient Descent(1550/9999): loss=1.8994762869773942, w0=73.29374999999997, w1=13.697931708793508\n",
      "Gradient Descent(1551/9999): loss=1.9034530000021277, w0=73.46249999999998, w1=13.676966973155245\n",
      "Gradient Descent(1552/9999): loss=2.42426644834331, w0=73.40624999999997, w1=13.504192797381027\n",
      "Gradient Descent(1553/9999): loss=2.069101890219648, w0=73.06874999999997, w1=13.558962128088696\n",
      "Gradient Descent(1554/9999): loss=2.6467198489675203, w0=73.18124999999996, w1=13.854792107649544\n",
      "Gradient Descent(1555/9999): loss=2.0134800266132595, w0=73.40624999999996, w1=13.89369402598755\n",
      "Gradient Descent(1556/9999): loss=2.1934780090134005, w0=73.40624999999996, w1=13.915133075007583\n",
      "Gradient Descent(1557/9999): loss=2.406707587420186, w0=73.29374999999996, w1=13.630716325520389\n",
      "Gradient Descent(1558/9999): loss=1.4566895113922163, w0=73.51874999999995, w1=13.68835366831879\n",
      "Gradient Descent(1559/9999): loss=2.659700263168831, w0=73.40624999999996, w1=13.680788754733744\n",
      "Gradient Descent(1560/9999): loss=2.075613108722947, w0=73.06874999999995, w1=13.670980155411655\n",
      "Gradient Descent(1561/9999): loss=1.9177353218808189, w0=73.18124999999995, w1=13.656603785982515\n",
      "Gradient Descent(1562/9999): loss=1.9689155465230326, w0=73.06874999999995, w1=14.009564925280737\n",
      "Gradient Descent(1563/9999): loss=2.1203359178566563, w0=73.40624999999996, w1=14.009977146276377\n",
      "Gradient Descent(1564/9999): loss=2.4694483389576316, w0=73.29374999999996, w1=13.938349685991406\n",
      "Gradient Descent(1565/9999): loss=2.4703827962299125, w0=73.57499999999996, w1=13.976129398412548\n",
      "Gradient Descent(1566/9999): loss=2.3637418928920906, w0=73.57499999999996, w1=14.079726508240471\n",
      "Gradient Descent(1567/9999): loss=2.185391910567124, w0=73.29374999999996, w1=13.787503147925126\n",
      "Gradient Descent(1568/9999): loss=2.488528132620706, w0=73.12499999999996, w1=13.807721989567392\n",
      "Gradient Descent(1569/9999): loss=2.2430349615591245, w0=73.12499999999996, w1=13.881875890090658\n",
      "Gradient Descent(1570/9999): loss=1.991524075683779, w0=72.89999999999996, w1=13.976345011624387\n",
      "Gradient Descent(1571/9999): loss=2.4441433668179995, w0=72.67499999999997, w1=13.70807666940638\n",
      "Gradient Descent(1572/9999): loss=2.2841776435897954, w0=72.67499999999997, w1=13.716521495671987\n",
      "Gradient Descent(1573/9999): loss=2.6650635433438064, w0=72.78749999999997, w1=13.609527188722923\n",
      "Gradient Descent(1574/9999): loss=2.629162333823098, w0=72.50624999999997, w1=13.373489204603178\n",
      "Gradient Descent(1575/9999): loss=2.1445682008944367, w0=72.50624999999997, w1=13.488338723045995\n",
      "Gradient Descent(1576/9999): loss=2.580633936524541, w0=72.39374999999997, w1=13.363275637463445\n",
      "Gradient Descent(1577/9999): loss=1.8659805225976795, w0=72.50624999999997, w1=13.599953189946573\n",
      "Gradient Descent(1578/9999): loss=2.32004699109171, w0=72.67499999999997, w1=13.366496796908176\n",
      "Gradient Descent(1579/9999): loss=2.104460702927301, w0=72.89999999999996, w1=13.005286478523844\n",
      "Gradient Descent(1580/9999): loss=2.4034814606651684, w0=72.67499999999997, w1=13.01227866242852\n",
      "Gradient Descent(1581/9999): loss=2.656949975439499, w0=72.73124999999997, w1=13.183224671991345\n",
      "Gradient Descent(1582/9999): loss=1.76058845256025, w0=72.84374999999997, w1=13.261314008042678\n",
      "Gradient Descent(1583/9999): loss=2.2916667958259476, w0=72.95624999999997, w1=13.14512662116026\n",
      "Gradient Descent(1584/9999): loss=2.4122435090838827, w0=72.89999999999996, w1=13.091814612958341\n",
      "Gradient Descent(1585/9999): loss=2.4788058416365115, w0=73.01249999999996, w1=13.300898166490228\n",
      "Gradient Descent(1586/9999): loss=2.9767054818194274, w0=73.18124999999996, w1=13.461499083324528\n",
      "Gradient Descent(1587/9999): loss=2.5993498836990714, w0=73.40624999999996, w1=13.394144959324993\n",
      "Gradient Descent(1588/9999): loss=1.8821571171537772, w0=73.34999999999995, w1=13.46142099614806\n",
      "Gradient Descent(1589/9999): loss=1.925000614054714, w0=73.34999999999995, w1=13.618713642089515\n",
      "Gradient Descent(1590/9999): loss=2.4952490349308913, w0=73.12499999999996, w1=13.73698884572669\n",
      "Gradient Descent(1591/9999): loss=2.2109448740459765, w0=72.95624999999995, w1=13.816290462727656\n",
      "Gradient Descent(1592/9999): loss=2.392692512716601, w0=72.89999999999995, w1=13.791413488818616\n",
      "Gradient Descent(1593/9999): loss=2.5966642272362863, w0=72.95624999999995, w1=13.669710224020829\n",
      "Gradient Descent(1594/9999): loss=2.665412106661789, w0=73.12499999999996, w1=13.571676135886309\n",
      "Gradient Descent(1595/9999): loss=2.293112921873503, w0=73.06874999999995, w1=13.84580431578434\n",
      "Gradient Descent(1596/9999): loss=1.9555017102772125, w0=73.29374999999995, w1=13.985241837295325\n",
      "Gradient Descent(1597/9999): loss=2.1798872466201376, w0=73.57499999999995, w1=13.987192382529564\n",
      "Gradient Descent(1598/9999): loss=2.1395857466379695, w0=73.57499999999995, w1=14.15842917009271\n",
      "Gradient Descent(1599/9999): loss=1.7720622802328219, w0=73.46249999999995, w1=13.948794635874616\n",
      "Gradient Descent(1600/9999): loss=2.9613419110225223, w0=73.51874999999995, w1=13.985166264634152\n",
      "Gradient Descent(1601/9999): loss=2.5852312375584514, w0=73.51874999999995, w1=13.72218147700727\n",
      "Gradient Descent(1602/9999): loss=1.9997357981128963, w0=73.40624999999996, w1=13.714655181690894\n",
      "Gradient Descent(1603/9999): loss=2.2809751812627916, w0=73.29374999999996, w1=13.863040210787641\n",
      "Gradient Descent(1604/9999): loss=2.4203058898945313, w0=73.18124999999996, w1=13.685363936660062\n",
      "Gradient Descent(1605/9999): loss=1.9556309133882346, w0=73.34999999999997, w1=13.575290363084298\n",
      "Gradient Descent(1606/9999): loss=2.304122082969279, w0=73.34999999999997, w1=13.51953742591379\n",
      "Gradient Descent(1607/9999): loss=1.9121530437055054, w0=73.46249999999996, w1=13.372407818134054\n",
      "Gradient Descent(1608/9999): loss=2.075641215334979, w0=73.23749999999997, w1=13.211286575459432\n",
      "Gradient Descent(1609/9999): loss=2.6521688675109254, w0=73.34999999999997, w1=13.116043149473036\n",
      "Gradient Descent(1610/9999): loss=2.0911475749300723, w0=73.46249999999996, w1=13.101867534998474\n",
      "Gradient Descent(1611/9999): loss=2.0664706256194947, w0=73.51874999999997, w1=13.354139075406323\n",
      "Gradient Descent(1612/9999): loss=2.00543447952916, w0=73.40624999999997, w1=13.724080564604243\n",
      "Gradient Descent(1613/9999): loss=2.020903428360117, w0=73.23749999999997, w1=13.806804796199312\n",
      "Gradient Descent(1614/9999): loss=2.182481165921235, w0=73.23749999999997, w1=13.753356779600153\n",
      "Gradient Descent(1615/9999): loss=2.382072000672567, w0=73.29374999999997, w1=13.87456563697365\n",
      "Gradient Descent(1616/9999): loss=2.1879077035600814, w0=73.29374999999997, w1=13.93006929933025\n",
      "Gradient Descent(1617/9999): loss=2.339905966843441, w0=73.23749999999997, w1=14.008564964840469\n",
      "Gradient Descent(1618/9999): loss=2.0900134140473003, w0=73.18124999999996, w1=13.997456505177011\n",
      "Gradient Descent(1619/9999): loss=2.039262498196146, w0=73.18124999999996, w1=14.178783887932775\n",
      "Gradient Descent(1620/9999): loss=2.1185563505390554, w0=73.06874999999997, w1=13.805211096273196\n",
      "Gradient Descent(1621/9999): loss=2.6799813079507406, w0=73.06874999999997, w1=13.934636664451277\n",
      "Gradient Descent(1622/9999): loss=2.436294003037447, w0=73.18124999999996, w1=13.82022895120724\n",
      "Gradient Descent(1623/9999): loss=2.274349896604482, w0=73.34999999999997, w1=14.031991152143846\n",
      "Gradient Descent(1624/9999): loss=2.034441671445242, w0=73.46249999999996, w1=13.62101670153715\n",
      "Gradient Descent(1625/9999): loss=2.142239496294476, w0=73.51874999999997, w1=13.699717153360208\n",
      "Gradient Descent(1626/9999): loss=2.6753696010848107, w0=73.51874999999997, w1=13.66389707761476\n",
      "Gradient Descent(1627/9999): loss=2.3864157445231826, w0=73.57499999999997, w1=13.70260519299976\n",
      "Gradient Descent(1628/9999): loss=2.0387441500230836, w0=73.63124999999998, w1=13.88574538665726\n",
      "Gradient Descent(1629/9999): loss=2.327797315825908, w0=73.40624999999999, w1=13.694289390928235\n",
      "Gradient Descent(1630/9999): loss=2.5397442296549935, w0=73.46249999999999, w1=13.663346383104082\n",
      "Gradient Descent(1631/9999): loss=1.6210301259187572, w0=73.57499999999999, w1=13.7973725669782\n",
      "Gradient Descent(1632/9999): loss=2.564663932457977, w0=73.68749999999999, w1=13.729509189644139\n",
      "Gradient Descent(1633/9999): loss=2.0465557772030936, w0=73.85624999999999, w1=13.735957103821377\n",
      "Gradient Descent(1634/9999): loss=2.418759128482481, w0=73.57499999999999, w1=13.489186887634853\n",
      "Gradient Descent(1635/9999): loss=1.9806568369997657, w0=73.57499999999999, w1=13.672748612588073\n",
      "Gradient Descent(1636/9999): loss=2.5107014304539366, w0=73.74374999999999, w1=13.705379196425731\n",
      "Gradient Descent(1637/9999): loss=2.2322097920033244, w0=73.40624999999999, w1=13.93982930262668\n",
      "Gradient Descent(1638/9999): loss=2.096822969139147, w0=73.29374999999999, w1=13.984623521054884\n",
      "Gradient Descent(1639/9999): loss=2.731640783330528, w0=73.35, w1=14.012584747973497\n",
      "Gradient Descent(1640/9999): loss=2.42884523522384, w0=73.29374999999999, w1=13.94841241206749\n",
      "Gradient Descent(1641/9999): loss=2.466109894326115, w0=73.46249999999999, w1=14.012091535786341\n",
      "Gradient Descent(1642/9999): loss=2.0223551273588987, w0=73.57499999999999, w1=14.019855827150856\n",
      "Gradient Descent(1643/9999): loss=2.444864002178431, w0=73.85624999999999, w1=13.86631497734309\n",
      "Gradient Descent(1644/9999): loss=2.0728247756790132, w0=73.85624999999999, w1=13.980962120002431\n",
      "Gradient Descent(1645/9999): loss=2.21233837665494, w0=73.68749999999999, w1=13.842669569669326\n",
      "Gradient Descent(1646/9999): loss=2.162388649266, w0=73.68749999999999, w1=13.543206194373358\n",
      "Gradient Descent(1647/9999): loss=2.6040815935048567, w0=73.46249999999999, w1=13.700656441621348\n",
      "Gradient Descent(1648/9999): loss=2.9545888446145026, w0=73.40624999999999, w1=13.73030187495111\n",
      "Gradient Descent(1649/9999): loss=2.3054422564719204, w0=73.23749999999998, w1=13.545519461447945\n",
      "Gradient Descent(1650/9999): loss=2.127456277897024, w0=73.46249999999998, w1=13.69081123334439\n",
      "Gradient Descent(1651/9999): loss=2.1616298441759474, w0=73.40624999999997, w1=13.399981801033942\n",
      "Gradient Descent(1652/9999): loss=1.8814569939514734, w0=73.34999999999997, w1=13.416646241775906\n",
      "Gradient Descent(1653/9999): loss=2.295044678307975, w0=73.34999999999997, w1=13.596346734973789\n",
      "Gradient Descent(1654/9999): loss=1.688304807225702, w0=73.23749999999997, w1=13.526344932023145\n",
      "Gradient Descent(1655/9999): loss=1.943849452694348, w0=73.12499999999997, w1=13.582495388185986\n",
      "Gradient Descent(1656/9999): loss=2.802646108631906, w0=73.29374999999997, w1=13.490531288679806\n",
      "Gradient Descent(1657/9999): loss=1.860140773131929, w0=73.29374999999997, w1=13.723107295541705\n",
      "Gradient Descent(1658/9999): loss=2.577566969400217, w0=73.18124999999998, w1=13.699020820866048\n",
      "Gradient Descent(1659/9999): loss=2.1488618347692157, w0=72.95624999999998, w1=13.385478589345478\n",
      "Gradient Descent(1660/9999): loss=2.6115268668946254, w0=72.95624999999998, w1=13.58750211972881\n",
      "Gradient Descent(1661/9999): loss=2.453106094628527, w0=73.18124999999998, w1=13.78820598275232\n",
      "Gradient Descent(1662/9999): loss=2.7357969769519648, w0=73.40624999999997, w1=13.797178287768439\n",
      "Gradient Descent(1663/9999): loss=2.244275689883765, w0=73.23749999999997, w1=13.564091396223725\n",
      "Gradient Descent(1664/9999): loss=2.3471436998555975, w0=73.23749999999997, w1=13.467780954508632\n",
      "Gradient Descent(1665/9999): loss=2.651355181547264, w0=73.06874999999997, w1=13.266450882692546\n",
      "Gradient Descent(1666/9999): loss=2.081966613969132, w0=72.89999999999996, w1=13.375155441991787\n",
      "Gradient Descent(1667/9999): loss=2.445678122813553, w0=72.78749999999997, w1=13.46787579868762\n",
      "Gradient Descent(1668/9999): loss=2.263331707035196, w0=72.73124999999996, w1=13.461721852407472\n",
      "Gradient Descent(1669/9999): loss=2.1149520116348635, w0=72.89999999999996, w1=13.637562576076496\n",
      "Gradient Descent(1670/9999): loss=2.299259107390328, w0=72.78749999999997, w1=13.812486192140513\n",
      "Gradient Descent(1671/9999): loss=2.094644891195576, w0=73.01249999999996, w1=13.847379788304329\n",
      "Gradient Descent(1672/9999): loss=2.452329480469164, w0=73.01249999999996, w1=13.592910075505948\n",
      "Gradient Descent(1673/9999): loss=2.577202568114088, w0=73.29374999999996, w1=13.566036117057228\n",
      "Gradient Descent(1674/9999): loss=2.0728762756892216, w0=73.29374999999996, w1=13.752374632469422\n",
      "Gradient Descent(1675/9999): loss=2.5826845033488874, w0=73.40624999999996, w1=13.757270218201837\n",
      "Gradient Descent(1676/9999): loss=1.9975307593873062, w0=73.46249999999996, w1=13.881616514780715\n",
      "Gradient Descent(1677/9999): loss=1.807703398769063, w0=73.46249999999996, w1=13.874137692334886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1678/9999): loss=2.1381694888406146, w0=73.40624999999996, w1=13.921686267163137\n",
      "Gradient Descent(1679/9999): loss=2.431085292642641, w0=73.12499999999996, w1=14.073828893663839\n",
      "Gradient Descent(1680/9999): loss=2.4244383126700724, w0=73.06874999999995, w1=14.058399549931481\n",
      "Gradient Descent(1681/9999): loss=2.1850644055201505, w0=73.23749999999995, w1=14.175372751788563\n",
      "Gradient Descent(1682/9999): loss=2.13724392560422, w0=73.18124999999995, w1=14.292658163284154\n",
      "Gradient Descent(1683/9999): loss=2.1368495770629425, w0=73.23749999999995, w1=14.241962954301265\n",
      "Gradient Descent(1684/9999): loss=2.3257491222800226, w0=73.01249999999996, w1=14.04351105602544\n",
      "Gradient Descent(1685/9999): loss=2.288508469062528, w0=73.29374999999996, w1=14.256255769109138\n",
      "Gradient Descent(1686/9999): loss=1.8631659307532464, w0=73.46249999999996, w1=14.380528090133051\n",
      "Gradient Descent(1687/9999): loss=2.2732977892430615, w0=73.40624999999996, w1=14.209535873292202\n",
      "Gradient Descent(1688/9999): loss=2.1583755671990064, w0=73.63124999999995, w1=14.37988743661168\n",
      "Gradient Descent(1689/9999): loss=2.048981018533876, w0=73.40624999999996, w1=14.307687888888209\n",
      "Gradient Descent(1690/9999): loss=2.4564806370995496, w0=73.23749999999995, w1=14.168029674268055\n",
      "Gradient Descent(1691/9999): loss=2.5595007389405433, w0=72.95624999999995, w1=14.08778635708343\n",
      "Gradient Descent(1692/9999): loss=2.731877035093553, w0=72.95624999999995, w1=13.807888964120455\n",
      "Gradient Descent(1693/9999): loss=2.305079055980441, w0=72.89999999999995, w1=13.72437746893315\n",
      "Gradient Descent(1694/9999): loss=2.391256059248798, w0=72.84374999999994, w1=14.06725056882536\n",
      "Gradient Descent(1695/9999): loss=2.179703987214001, w0=72.78749999999994, w1=14.042564231106612\n",
      "Gradient Descent(1696/9999): loss=1.9982929332850028, w0=72.84374999999994, w1=14.150083842413133\n",
      "Gradient Descent(1697/9999): loss=2.399583488872083, w0=72.89999999999995, w1=14.115108670136266\n",
      "Gradient Descent(1698/9999): loss=1.8363228700659397, w0=73.18124999999995, w1=14.149743868533378\n",
      "Gradient Descent(1699/9999): loss=1.589457716809472, w0=73.40624999999994, w1=14.217320619985527\n",
      "Gradient Descent(1700/9999): loss=2.7761215614054984, w0=73.51874999999994, w1=14.276761824267533\n",
      "Gradient Descent(1701/9999): loss=2.3144796056390797, w0=73.68749999999994, w1=14.250578669265286\n",
      "Gradient Descent(1702/9999): loss=2.483912474734094, w0=73.74374999999995, w1=14.325378079198787\n",
      "Gradient Descent(1703/9999): loss=2.9919551615526654, w0=74.08124999999995, w1=14.185419304791692\n",
      "Gradient Descent(1704/9999): loss=2.4754899161982222, w0=74.02499999999995, w1=14.099748387137527\n",
      "Gradient Descent(1705/9999): loss=2.2832312620446253, w0=73.96874999999994, w1=13.925283334405654\n",
      "Gradient Descent(1706/9999): loss=2.5110325569330794, w0=73.74374999999995, w1=13.813856465128731\n",
      "Gradient Descent(1707/9999): loss=1.8563781493523135, w0=73.51874999999995, w1=13.916218912327755\n",
      "Gradient Descent(1708/9999): loss=2.2669921989327664, w0=73.40624999999996, w1=13.976737857861707\n",
      "Gradient Descent(1709/9999): loss=2.7290932908116936, w0=73.34999999999995, w1=14.036122127753698\n",
      "Gradient Descent(1710/9999): loss=1.904025050442812, w0=73.40624999999996, w1=13.950607122015711\n",
      "Gradient Descent(1711/9999): loss=2.5348074349064538, w0=73.74374999999996, w1=14.204787587155288\n",
      "Gradient Descent(1712/9999): loss=2.8086192663219864, w0=73.63124999999997, w1=14.101046984816195\n",
      "Gradient Descent(1713/9999): loss=2.0658037485432077, w0=73.74374999999996, w1=13.747443287780133\n",
      "Gradient Descent(1714/9999): loss=2.6094751600590382, w0=73.63124999999997, w1=13.782968080874154\n",
      "Gradient Descent(1715/9999): loss=1.8489564841012616, w0=73.34999999999997, w1=13.916065228830705\n",
      "Gradient Descent(1716/9999): loss=2.218201785872484, w0=73.12499999999997, w1=13.798594803212099\n",
      "Gradient Descent(1717/9999): loss=2.095220072984161, w0=73.23749999999997, w1=13.96816076121982\n",
      "Gradient Descent(1718/9999): loss=1.9176555090987584, w0=72.73124999999997, w1=13.925452195867972\n",
      "Gradient Descent(1719/9999): loss=1.9532773618518056, w0=72.89999999999998, w1=14.258137176542075\n",
      "Gradient Descent(1720/9999): loss=2.4727419515523987, w0=73.06874999999998, w1=13.854418289047032\n",
      "Gradient Descent(1721/9999): loss=1.8720160310890328, w0=73.12499999999999, w1=13.998530884431743\n",
      "Gradient Descent(1722/9999): loss=2.3331215680019834, w0=73.12499999999999, w1=13.991225127299254\n",
      "Gradient Descent(1723/9999): loss=2.362437605389458, w0=73.18124999999999, w1=13.914828146853749\n",
      "Gradient Descent(1724/9999): loss=2.513318566078894, w0=72.95625, w1=13.440146389619533\n",
      "Gradient Descent(1725/9999): loss=2.298104426398364, w0=72.84375, w1=13.43691862159174\n",
      "Gradient Descent(1726/9999): loss=2.2904432631068268, w0=72.61875, w1=13.83700780134991\n",
      "Gradient Descent(1727/9999): loss=2.358361541482644, w0=72.78750000000001, w1=13.463078471728561\n",
      "Gradient Descent(1728/9999): loss=1.6132146218574084, w0=72.78750000000001, w1=13.493375598697213\n",
      "Gradient Descent(1729/9999): loss=1.8122869560640473, w0=72.84375000000001, w1=13.598558037074488\n",
      "Gradient Descent(1730/9999): loss=2.4338104027193115, w0=72.73125000000002, w1=13.75768804914117\n",
      "Gradient Descent(1731/9999): loss=2.2455166943852953, w0=72.56250000000001, w1=13.86640628791808\n",
      "Gradient Descent(1732/9999): loss=2.534691475143517, w0=72.61875000000002, w1=13.649460527955059\n",
      "Gradient Descent(1733/9999): loss=2.267740247006497, w0=72.50625000000002, w1=13.85979749260254\n",
      "Gradient Descent(1734/9999): loss=2.1175553754046375, w0=72.56250000000003, w1=13.81375201456424\n",
      "Gradient Descent(1735/9999): loss=1.9074899399494427, w0=72.50625000000002, w1=13.501812151025543\n",
      "Gradient Descent(1736/9999): loss=2.4460730298533107, w0=72.95625000000003, w1=13.527907995691926\n",
      "Gradient Descent(1737/9999): loss=2.0301157386725137, w0=72.84375000000003, w1=13.575530117868963\n",
      "Gradient Descent(1738/9999): loss=1.8634284509162353, w0=72.50625000000002, w1=13.35749123558732\n",
      "Gradient Descent(1739/9999): loss=2.5301248502738014, w0=72.73125000000002, w1=13.185194371485457\n",
      "Gradient Descent(1740/9999): loss=2.245590138663609, w0=72.73125000000002, w1=13.494921149216397\n",
      "Gradient Descent(1741/9999): loss=2.6945999058713315, w0=73.06875000000002, w1=13.608831200370625\n",
      "Gradient Descent(1742/9999): loss=2.4990689367132743, w0=73.29375000000002, w1=13.740117908536519\n",
      "Gradient Descent(1743/9999): loss=2.138491308767618, w0=73.29375000000002, w1=14.039056437280248\n",
      "Gradient Descent(1744/9999): loss=2.1520937534126476, w0=73.18125000000002, w1=14.025250305071063\n",
      "Gradient Descent(1745/9999): loss=1.9075192054175696, w0=73.40625000000001, w1=13.742401255886175\n",
      "Gradient Descent(1746/9999): loss=2.3866084699007284, w0=73.40625000000001, w1=13.953413532004788\n",
      "Gradient Descent(1747/9999): loss=1.6529507755425779, w0=73.63125000000001, w1=14.037649383473687\n",
      "Gradient Descent(1748/9999): loss=1.84505257014148, w0=73.29375, w1=13.93903762908114\n",
      "Gradient Descent(1749/9999): loss=2.621408296498513, w0=73.18125, w1=14.030309394192315\n",
      "Gradient Descent(1750/9999): loss=1.9909591237231379, w0=73.51875000000001, w1=13.903518066365939\n",
      "Gradient Descent(1751/9999): loss=2.0376876846617042, w0=73.23750000000001, w1=13.85585380754978\n",
      "Gradient Descent(1752/9999): loss=2.440174072637045, w0=73.29375000000002, w1=13.580595564527414\n",
      "Gradient Descent(1753/9999): loss=1.7446528138293078, w0=73.40625000000001, w1=13.741554112508842\n",
      "Gradient Descent(1754/9999): loss=2.0485014512698463, w0=73.68750000000001, w1=13.69562288510272\n",
      "Gradient Descent(1755/9999): loss=2.109929509900707, w0=73.57500000000002, w1=13.75062250977418\n",
      "Gradient Descent(1756/9999): loss=2.031081763957518, w0=73.35000000000002, w1=13.857791479038971\n",
      "Gradient Descent(1757/9999): loss=2.7939471123646253, w0=73.40625000000003, w1=13.883198488101373\n",
      "Gradient Descent(1758/9999): loss=2.370537240799733, w0=73.40625000000003, w1=13.749321714617526\n",
      "Gradient Descent(1759/9999): loss=2.5579825173218866, w0=73.29375000000003, w1=13.623104175255659\n",
      "Gradient Descent(1760/9999): loss=2.3046554492989175, w0=73.23750000000003, w1=13.741024129319518\n",
      "Gradient Descent(1761/9999): loss=2.556725992339912, w0=73.29375000000003, w1=13.91348158868207\n",
      "Gradient Descent(1762/9999): loss=2.3825532497167163, w0=73.18125000000003, w1=13.831324772902938\n",
      "Gradient Descent(1763/9999): loss=2.6776138207699036, w0=73.35000000000004, w1=13.744723037224695\n",
      "Gradient Descent(1764/9999): loss=2.1473927943932996, w0=73.40625000000004, w1=14.034393201958274\n",
      "Gradient Descent(1765/9999): loss=2.288789225200514, w0=73.40625000000004, w1=13.938608965385408\n",
      "Gradient Descent(1766/9999): loss=2.173378491183466, w0=73.29375000000005, w1=14.03876004424259\n",
      "Gradient Descent(1767/9999): loss=2.3651880721439618, w0=73.29375000000005, w1=13.881366713982828\n",
      "Gradient Descent(1768/9999): loss=2.3059153597449837, w0=73.23750000000004, w1=13.997707714423107\n",
      "Gradient Descent(1769/9999): loss=2.3285657696701545, w0=73.29375000000005, w1=13.905747463117649\n",
      "Gradient Descent(1770/9999): loss=2.022922144012737, w0=73.51875000000004, w1=14.090157949346475\n",
      "Gradient Descent(1771/9999): loss=2.2465931198529434, w0=73.63125000000004, w1=13.927315600669777\n",
      "Gradient Descent(1772/9999): loss=2.1715449120425236, w0=73.29375000000003, w1=13.568629277674075\n",
      "Gradient Descent(1773/9999): loss=2.0047126352918134, w0=73.46250000000003, w1=13.602580108438861\n",
      "Gradient Descent(1774/9999): loss=2.285116278634917, w0=73.46250000000003, w1=13.491537950960629\n",
      "Gradient Descent(1775/9999): loss=2.298293658841765, w0=73.35000000000004, w1=13.470581568114005\n",
      "Gradient Descent(1776/9999): loss=2.028126770843689, w0=73.29375000000003, w1=13.614772749370193\n",
      "Gradient Descent(1777/9999): loss=2.438959133137411, w0=73.29375000000003, w1=13.764370964735594\n",
      "Gradient Descent(1778/9999): loss=2.257798598110898, w0=73.23750000000003, w1=13.845112731103532\n",
      "Gradient Descent(1779/9999): loss=2.507481200617678, w0=73.40625000000003, w1=13.785985979347892\n",
      "Gradient Descent(1780/9999): loss=2.3598996804820596, w0=73.23750000000003, w1=13.883353023718373\n",
      "Gradient Descent(1781/9999): loss=2.2655343372053585, w0=73.35000000000002, w1=13.852982966301179\n",
      "Gradient Descent(1782/9999): loss=2.327708870213382, w0=73.23750000000003, w1=13.969758282732014\n",
      "Gradient Descent(1783/9999): loss=2.2635319212751175, w0=73.23750000000003, w1=13.663007810726985\n",
      "Gradient Descent(1784/9999): loss=2.5310216210575396, w0=72.95625000000003, w1=13.671562541913195\n",
      "Gradient Descent(1785/9999): loss=2.60749410284559, w0=73.01250000000003, w1=13.702732698618544\n",
      "Gradient Descent(1786/9999): loss=1.79265246875951, w0=72.90000000000003, w1=13.697379237427885\n",
      "Gradient Descent(1787/9999): loss=2.0014035613427015, w0=73.12500000000003, w1=13.799503857385405\n",
      "Gradient Descent(1788/9999): loss=1.9396731882161906, w0=73.06875000000002, w1=13.762654612150259\n",
      "Gradient Descent(1789/9999): loss=2.400854200599837, w0=73.06875000000002, w1=13.770180378293745\n",
      "Gradient Descent(1790/9999): loss=1.7926399694388422, w0=73.06875000000002, w1=13.804330709377496\n",
      "Gradient Descent(1791/9999): loss=1.9031049197149206, w0=72.95625000000003, w1=13.552493207517875\n",
      "Gradient Descent(1792/9999): loss=2.0430982134140288, w0=73.18125000000002, w1=13.908144493301293\n",
      "Gradient Descent(1793/9999): loss=1.9309278993418646, w0=73.40625000000001, w1=13.780691593180581\n",
      "Gradient Descent(1794/9999): loss=2.3134064293426, w0=73.63125000000001, w1=13.69453092977616\n",
      "Gradient Descent(1795/9999): loss=2.0380337659593994, w0=73.63125000000001, w1=13.697345869469547\n",
      "Gradient Descent(1796/9999): loss=2.8667390765510543, w0=73.35000000000001, w1=13.81776743310992\n",
      "Gradient Descent(1797/9999): loss=1.687449261386957, w0=73.40625000000001, w1=14.018717814129431\n",
      "Gradient Descent(1798/9999): loss=1.8124483072992192, w0=73.40625000000001, w1=14.10645963514681\n",
      "Gradient Descent(1799/9999): loss=2.128413771751932, w0=73.12500000000001, w1=13.979975696880725\n",
      "Gradient Descent(1800/9999): loss=1.7665377295682043, w0=73.01250000000002, w1=14.053665742721224\n",
      "Gradient Descent(1801/9999): loss=2.2345671645683276, w0=72.90000000000002, w1=13.885424446465434\n",
      "Gradient Descent(1802/9999): loss=2.322335981520082, w0=73.06875000000002, w1=13.769991243261648\n",
      "Gradient Descent(1803/9999): loss=1.9763272147963402, w0=72.95625000000003, w1=13.600464412477026\n",
      "Gradient Descent(1804/9999): loss=2.0716178062325774, w0=72.95625000000003, w1=13.679428947793449\n",
      "Gradient Descent(1805/9999): loss=2.5988326219417743, w0=72.95625000000003, w1=13.761772487552204\n",
      "Gradient Descent(1806/9999): loss=2.1080993234381613, w0=73.06875000000002, w1=13.832271143736982\n",
      "Gradient Descent(1807/9999): loss=2.482382805093457, w0=73.06875000000002, w1=13.961225319472256\n",
      "Gradient Descent(1808/9999): loss=2.1223071190995, w0=73.46250000000002, w1=13.917449311146887\n",
      "Gradient Descent(1809/9999): loss=2.473117529164723, w0=73.40625000000001, w1=13.808297749054558\n",
      "Gradient Descent(1810/9999): loss=2.5001223337566962, w0=73.12500000000001, w1=13.968815600791007\n",
      "Gradient Descent(1811/9999): loss=1.9247888575335894, w0=73.01250000000002, w1=14.334203739149773\n",
      "Gradient Descent(1812/9999): loss=2.0880846306329586, w0=73.01250000000002, w1=13.98115651148919\n",
      "Gradient Descent(1813/9999): loss=2.3089693756007916, w0=73.12500000000001, w1=13.77202196432196\n",
      "Gradient Descent(1814/9999): loss=2.291336388386898, w0=73.01250000000002, w1=13.711452184508781\n",
      "Gradient Descent(1815/9999): loss=2.0925353310900547, w0=73.01250000000002, w1=13.898721575789898\n",
      "Gradient Descent(1816/9999): loss=2.6731855479367743, w0=73.06875000000002, w1=13.905654305629348\n",
      "Gradient Descent(1817/9999): loss=2.436334062824374, w0=73.29375000000002, w1=13.536827481795635\n",
      "Gradient Descent(1818/9999): loss=3.043981112834805, w0=73.35000000000002, w1=13.42233108538277\n",
      "Gradient Descent(1819/9999): loss=1.8926573503379687, w0=73.35000000000002, w1=13.693387028724517\n",
      "Gradient Descent(1820/9999): loss=2.0711370724819, w0=73.01250000000002, w1=13.67962956302038\n",
      "Gradient Descent(1821/9999): loss=2.1725720364919425, w0=73.06875000000002, w1=13.52415593746285\n",
      "Gradient Descent(1822/9999): loss=2.467534459659081, w0=72.90000000000002, w1=13.485697386438346\n",
      "Gradient Descent(1823/9999): loss=2.4827036716525566, w0=73.18125000000002, w1=13.419913387447817\n",
      "Gradient Descent(1824/9999): loss=2.394697343877559, w0=73.12500000000001, w1=13.283273809988913\n",
      "Gradient Descent(1825/9999): loss=1.9949962455674308, w0=73.01250000000002, w1=13.74279439432104\n",
      "Gradient Descent(1826/9999): loss=2.0381464559664453, w0=73.06875000000002, w1=13.623045208646287\n",
      "Gradient Descent(1827/9999): loss=2.2811936617876065, w0=73.01250000000002, w1=13.631878383693886\n",
      "Gradient Descent(1828/9999): loss=2.250821498911958, w0=73.06875000000002, w1=13.650792232485351\n",
      "Gradient Descent(1829/9999): loss=2.166123548424152, w0=72.90000000000002, w1=13.6952612915948\n",
      "Gradient Descent(1830/9999): loss=2.2420621083534287, w0=72.90000000000002, w1=13.513594780086818\n",
      "Gradient Descent(1831/9999): loss=2.26670973838365, w0=73.01250000000002, w1=13.540673387253019\n",
      "Gradient Descent(1832/9999): loss=2.653355905527583, w0=73.35000000000002, w1=13.534025988003856\n",
      "Gradient Descent(1833/9999): loss=1.7670972158722098, w0=73.12500000000003, w1=13.37908317882054\n",
      "Gradient Descent(1834/9999): loss=2.316805776407073, w0=73.35000000000002, w1=13.468037905036159\n",
      "Gradient Descent(1835/9999): loss=2.262631247257085, w0=73.06875000000002, w1=13.72653323858981\n",
      "Gradient Descent(1836/9999): loss=2.082919778825093, w0=72.73125000000002, w1=13.74555778932266\n",
      "Gradient Descent(1837/9999): loss=1.8974298007039263, w0=72.61875000000002, w1=13.600954628318378\n",
      "Gradient Descent(1838/9999): loss=2.696462439047724, w0=72.45000000000002, w1=13.516609792878032\n",
      "Gradient Descent(1839/9999): loss=2.2227639353753066, w0=72.39375000000001, w1=13.507915378726254\n",
      "Gradient Descent(1840/9999): loss=2.3502192760741263, w0=72.45000000000002, w1=13.586340512170922\n",
      "Gradient Descent(1841/9999): loss=2.2454966281880973, w0=72.67500000000001, w1=13.234351305508715\n",
      "Gradient Descent(1842/9999): loss=2.458775203136135, w0=73.01250000000002, w1=13.172723249968515\n",
      "Gradient Descent(1843/9999): loss=1.7064218384041794, w0=73.46250000000002, w1=13.139653677909298\n",
      "Gradient Descent(1844/9999): loss=2.1018992875876426, w0=73.35000000000002, w1=13.434181720390656\n",
      "Gradient Descent(1845/9999): loss=2.269913494106355, w0=73.29375000000002, w1=13.386976215865852\n",
      "Gradient Descent(1846/9999): loss=2.6833512358682308, w0=73.46250000000002, w1=13.208302267782456\n",
      "Gradient Descent(1847/9999): loss=1.762339188246258, w0=73.40625000000001, w1=13.158329286425902\n",
      "Gradient Descent(1848/9999): loss=1.7239426256861057, w0=73.35000000000001, w1=13.4867405482605\n",
      "Gradient Descent(1849/9999): loss=2.1898062044517834, w0=73.575, w1=13.516250612902846\n",
      "Gradient Descent(1850/9999): loss=2.1306501011586048, w0=73.575, w1=13.299573518907636\n",
      "Gradient Descent(1851/9999): loss=1.825192002766133, w0=73.575, w1=13.177721185095477\n",
      "Gradient Descent(1852/9999): loss=2.3358818835330095, w0=73.35000000000001, w1=13.415384769302479\n",
      "Gradient Descent(1853/9999): loss=2.701489576246717, w0=73.29375, w1=13.103985898547188\n",
      "Gradient Descent(1854/9999): loss=1.9326934619815022, w0=73.29375, w1=13.093689126587384\n",
      "Gradient Descent(1855/9999): loss=2.3586175882502864, w0=73.2375, w1=13.314262222962487\n",
      "Gradient Descent(1856/9999): loss=2.0659383521598595, w0=73.46249999999999, w1=13.33820361844964\n",
      "Gradient Descent(1857/9999): loss=2.4339792595035985, w0=73.29374999999999, w1=13.197443705940197\n",
      "Gradient Descent(1858/9999): loss=2.2362754328358023, w0=73.06875, w1=13.319585577521597\n",
      "Gradient Descent(1859/9999): loss=2.35333758906925, w0=72.89999999999999, w1=13.320487065297476\n",
      "Gradient Descent(1860/9999): loss=1.8795871829415047, w0=72.89999999999999, w1=13.573670068169527\n",
      "Gradient Descent(1861/9999): loss=1.9938633514603752, w0=72.95625, w1=13.626303189899962\n",
      "Gradient Descent(1862/9999): loss=2.2717246589063405, w0=73.2375, w1=13.726569443657317\n",
      "Gradient Descent(1863/9999): loss=2.3561389734912304, w0=73.125, w1=13.606763408172737\n",
      "Gradient Descent(1864/9999): loss=2.3013119985925057, w0=73.06875, w1=13.568184394844316\n",
      "Gradient Descent(1865/9999): loss=2.4120581570411153, w0=73.06875, w1=13.498112962441148\n",
      "Gradient Descent(1866/9999): loss=2.0150885631014477, w0=73.01249999999999, w1=13.357157926341618\n",
      "Gradient Descent(1867/9999): loss=2.1729585761289805, w0=73.12499999999999, w1=13.14042485799955\n",
      "Gradient Descent(1868/9999): loss=2.071456415085525, w0=72.95624999999998, w1=13.195207955519521\n",
      "Gradient Descent(1869/9999): loss=1.6855894104400693, w0=72.95624999999998, w1=13.159357470491443\n",
      "Gradient Descent(1870/9999): loss=2.12237614735668, w0=73.12499999999999, w1=13.522943147549508\n",
      "Gradient Descent(1871/9999): loss=2.1512866512664024, w0=72.95624999999998, w1=13.851586495564169\n",
      "Gradient Descent(1872/9999): loss=2.3920555913952155, w0=72.95624999999998, w1=13.965026884666576\n",
      "Gradient Descent(1873/9999): loss=1.7182909605574062, w0=73.01249999999999, w1=14.002411463960108\n",
      "Gradient Descent(1874/9999): loss=2.502636326495746, w0=73.29374999999999, w1=13.81738934662768\n",
      "Gradient Descent(1875/9999): loss=1.9312273308310293, w0=73.06875, w1=13.7651740665105\n",
      "Gradient Descent(1876/9999): loss=3.2135444744703983, w0=73.125, w1=13.690865883318432\n",
      "Gradient Descent(1877/9999): loss=2.326732795010656, w0=73.29375, w1=13.722377054199121\n",
      "Gradient Descent(1878/9999): loss=2.5546004209106474, w0=73.6875, w1=13.595150357841359\n",
      "Gradient Descent(1879/9999): loss=2.019423546054468, w0=73.4625, w1=13.515311744479456\n",
      "Gradient Descent(1880/9999): loss=2.71705575318418, w0=73.51875000000001, w1=13.659321162892578\n",
      "Gradient Descent(1881/9999): loss=2.2786900013820417, w0=73.4625, w1=13.556142906106988\n",
      "Gradient Descent(1882/9999): loss=2.062193601887477, w0=73.23750000000001, w1=13.536097967302249\n",
      "Gradient Descent(1883/9999): loss=2.2340837610753477, w0=72.95625000000001, w1=13.776793144781074\n",
      "Gradient Descent(1884/9999): loss=2.2876212840093713, w0=72.95625000000001, w1=13.846570495054507\n",
      "Gradient Descent(1885/9999): loss=1.6676815901033424, w0=72.84375000000001, w1=13.511760200217534\n",
      "Gradient Descent(1886/9999): loss=1.6060836388625364, w0=72.78750000000001, w1=13.538062187258905\n",
      "Gradient Descent(1887/9999): loss=2.195075758645939, w0=72.73125, w1=13.319794459143253\n",
      "Gradient Descent(1888/9999): loss=2.0753367285885806, w0=72.675, w1=13.067443205653635\n",
      "Gradient Descent(1889/9999): loss=2.851210419845862, w0=72.7875, w1=13.118259013573441\n",
      "Gradient Descent(1890/9999): loss=2.2572206739420864, w0=72.675, w1=13.369037967289078\n",
      "Gradient Descent(1891/9999): loss=2.208919232634139, w0=72.95625, w1=13.488877857808397\n",
      "Gradient Descent(1892/9999): loss=2.1595358600088748, w0=73.18124999999999, w1=13.272420034793221\n",
      "Gradient Descent(1893/9999): loss=2.3939022319265018, w0=73.35, w1=13.166894301948652\n",
      "Gradient Descent(1894/9999): loss=2.081490498605663, w0=73.29374999999999, w1=13.182781803570052\n",
      "Gradient Descent(1895/9999): loss=1.9704688072172645, w0=73.23749999999998, w1=13.474190730073413\n",
      "Gradient Descent(1896/9999): loss=1.9875551921709553, w0=73.12499999999999, w1=13.525426849479432\n",
      "Gradient Descent(1897/9999): loss=2.8210282914578335, w0=73.06874999999998, w1=13.504092087392143\n",
      "Gradient Descent(1898/9999): loss=2.1636100544822296, w0=73.18124999999998, w1=13.449840052576002\n",
      "Gradient Descent(1899/9999): loss=2.5768706231751306, w0=73.23749999999998, w1=13.48150451494298\n",
      "Gradient Descent(1900/9999): loss=2.063204159622088, w0=73.40624999999999, w1=13.756127685190396\n",
      "Gradient Descent(1901/9999): loss=2.448458697801109, w0=73.29374999999999, w1=13.688566629705745\n",
      "Gradient Descent(1902/9999): loss=1.694094706679804, w0=73.18124999999999, w1=13.711014668308469\n",
      "Gradient Descent(1903/9999): loss=2.333217831013972, w0=73.2375, w1=13.446893825508315\n",
      "Gradient Descent(1904/9999): loss=2.396734575133803, w0=73.2375, w1=13.428459962636705\n",
      "Gradient Descent(1905/9999): loss=2.045881083266313, w0=73.18124999999999, w1=13.532580951023544\n",
      "Gradient Descent(1906/9999): loss=2.0717059070798687, w0=73.2375, w1=13.63828380303877\n",
      "Gradient Descent(1907/9999): loss=2.5581989130972524, w0=73.35, w1=13.45419644822176\n",
      "Gradient Descent(1908/9999): loss=2.018002897386995, w0=73.29374999999999, w1=13.270255155878912\n",
      "Gradient Descent(1909/9999): loss=1.977579826833636, w0=73.12499999999999, w1=13.067412867005629\n",
      "Gradient Descent(1910/9999): loss=1.960195487264419, w0=73.23749999999998, w1=13.148954938917646\n",
      "Gradient Descent(1911/9999): loss=2.1754123345685095, w0=73.57499999999999, w1=13.228101812405633\n",
      "Gradient Descent(1912/9999): loss=1.9214579388127868, w0=73.57499999999999, w1=13.081861392595153\n",
      "Gradient Descent(1913/9999): loss=2.1851862883173823, w0=73.46249999999999, w1=13.169074801860024\n",
      "Gradient Descent(1914/9999): loss=2.013109068965517, w0=73.40624999999999, w1=13.441092936579148\n",
      "Gradient Descent(1915/9999): loss=2.7375327551328574, w0=73.51874999999998, w1=13.288117679985655\n",
      "Gradient Descent(1916/9999): loss=2.2759267772409153, w0=73.34999999999998, w1=13.304864429166589\n",
      "Gradient Descent(1917/9999): loss=2.160350741061505, w0=73.34999999999998, w1=13.409356302210997\n",
      "Gradient Descent(1918/9999): loss=2.1258042278334957, w0=73.23749999999998, w1=13.54899915681396\n",
      "Gradient Descent(1919/9999): loss=1.9845091613058188, w0=73.06874999999998, w1=13.334667985618767\n",
      "Gradient Descent(1920/9999): loss=2.1795338804898536, w0=73.06874999999998, w1=13.40373841591881\n",
      "Gradient Descent(1921/9999): loss=1.7428994784936889, w0=73.18124999999998, w1=13.40960536433077\n",
      "Gradient Descent(1922/9999): loss=2.1358928492811966, w0=73.40624999999997, w1=13.230900855099406\n",
      "Gradient Descent(1923/9999): loss=2.4132267287038003, w0=73.23749999999997, w1=13.168763340432589\n",
      "Gradient Descent(1924/9999): loss=1.7743716637196751, w0=73.34999999999997, w1=13.268160844355439\n",
      "Gradient Descent(1925/9999): loss=2.0635509098294684, w0=73.40624999999997, w1=13.368917450159516\n",
      "Gradient Descent(1926/9999): loss=2.207100625089245, w0=73.51874999999997, w1=13.338658889899857\n",
      "Gradient Descent(1927/9999): loss=2.2106572946659844, w0=73.63124999999997, w1=13.449290131117166\n",
      "Gradient Descent(1928/9999): loss=2.0216304814021813, w0=73.23749999999997, w1=13.103179579883689\n",
      "Gradient Descent(1929/9999): loss=1.5842027000685661, w0=73.23749999999997, w1=13.091182328687102\n",
      "Gradient Descent(1930/9999): loss=1.80919654139033, w0=73.23749999999997, w1=13.036304113283926\n",
      "Gradient Descent(1931/9999): loss=2.2093539993277895, w0=73.06874999999997, w1=13.242653926244381\n",
      "Gradient Descent(1932/9999): loss=2.6098041383864623, w0=73.06874999999997, w1=13.562530882640983\n",
      "Gradient Descent(1933/9999): loss=1.6311367796127227, w0=73.01249999999996, w1=13.293515684945636\n",
      "Gradient Descent(1934/9999): loss=2.3696809602843265, w0=72.78749999999997, w1=13.355074973785046\n",
      "Gradient Descent(1935/9999): loss=1.5926604062857876, w0=72.78749999999997, w1=13.15969329095093\n",
      "Gradient Descent(1936/9999): loss=2.5577557889411437, w0=73.06874999999997, w1=13.435318047857118\n",
      "Gradient Descent(1937/9999): loss=2.218249212826443, w0=73.01249999999996, w1=13.401713675437618\n",
      "Gradient Descent(1938/9999): loss=2.4584490481029677, w0=72.89999999999996, w1=13.595902050027185\n",
      "Gradient Descent(1939/9999): loss=1.766334707380129, w0=72.89999999999996, w1=13.787034178339702\n",
      "Gradient Descent(1940/9999): loss=1.9433093076586214, w0=72.78749999999997, w1=13.86329795481351\n",
      "Gradient Descent(1941/9999): loss=2.4897234725566166, w0=72.84374999999997, w1=13.838895637324907\n",
      "Gradient Descent(1942/9999): loss=2.2710086493691226, w0=72.39374999999997, w1=13.639564372188115\n",
      "Gradient Descent(1943/9999): loss=2.4669598461886855, w0=72.50624999999997, w1=13.80306864197814\n",
      "Gradient Descent(1944/9999): loss=2.0754124254842736, w0=72.61874999999996, w1=13.769446650183161\n",
      "Gradient Descent(1945/9999): loss=2.301565475555488, w0=72.73124999999996, w1=13.933162389859305\n",
      "Gradient Descent(1946/9999): loss=2.0461290580969003, w0=72.44999999999996, w1=14.05832193198659\n",
      "Gradient Descent(1947/9999): loss=2.295239715867239, w0=72.73124999999996, w1=13.907833110511906\n",
      "Gradient Descent(1948/9999): loss=1.8838256968588498, w0=72.67499999999995, w1=14.005516818873808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(1949/9999): loss=2.515253307975021, w0=73.01249999999996, w1=14.04381407051236\n",
      "Gradient Descent(1950/9999): loss=2.369022996738029, w0=72.78749999999997, w1=13.790021802802027\n",
      "Gradient Descent(1951/9999): loss=2.0346928070605514, w0=72.73124999999996, w1=13.620568705195456\n",
      "Gradient Descent(1952/9999): loss=2.1639706373220533, w0=72.61874999999996, w1=13.671441482431897\n",
      "Gradient Descent(1953/9999): loss=2.1034860003309426, w0=72.89999999999996, w1=13.884311046603639\n",
      "Gradient Descent(1954/9999): loss=1.7224199662668538, w0=72.89999999999996, w1=14.06429469591128\n",
      "Gradient Descent(1955/9999): loss=2.6895752534394326, w0=73.01249999999996, w1=14.526167999448052\n",
      "Gradient Descent(1956/9999): loss=2.862420462179408, w0=72.95624999999995, w1=14.185229115197034\n",
      "Gradient Descent(1957/9999): loss=2.4216567529588584, w0=72.89999999999995, w1=14.381084745310048\n",
      "Gradient Descent(1958/9999): loss=1.865004017138737, w0=72.78749999999995, w1=14.291139718066864\n",
      "Gradient Descent(1959/9999): loss=2.074687587368466, w0=72.84374999999996, w1=14.429250171468258\n",
      "Gradient Descent(1960/9999): loss=2.1108919606759065, w0=73.46249999999996, w1=14.423389575483455\n",
      "Gradient Descent(1961/9999): loss=2.13834671029967, w0=73.34999999999997, w1=14.179404302777922\n",
      "Gradient Descent(1962/9999): loss=1.9824067758457193, w0=73.46249999999996, w1=14.06175302003512\n",
      "Gradient Descent(1963/9999): loss=2.2919341602384016, w0=73.29374999999996, w1=14.093938302229006\n",
      "Gradient Descent(1964/9999): loss=2.188172678266091, w0=73.12499999999996, w1=14.053381810415589\n",
      "Gradient Descent(1965/9999): loss=1.9384009135250369, w0=73.29374999999996, w1=13.946166958088117\n",
      "Gradient Descent(1966/9999): loss=1.8638683089867125, w0=73.01249999999996, w1=13.989573841601526\n",
      "Gradient Descent(1967/9999): loss=1.9761278548041292, w0=73.18124999999996, w1=13.781690635565548\n",
      "Gradient Descent(1968/9999): loss=2.151249343020411, w0=73.06874999999997, w1=13.725010016871966\n",
      "Gradient Descent(1969/9999): loss=2.2989149065065964, w0=73.34999999999997, w1=13.816365473053075\n",
      "Gradient Descent(1970/9999): loss=2.0137468028580563, w0=73.23749999999997, w1=13.498529625538\n",
      "Gradient Descent(1971/9999): loss=1.5895008595481797, w0=73.40624999999997, w1=13.666458217293268\n",
      "Gradient Descent(1972/9999): loss=2.1706316477580874, w0=73.40624999999997, w1=13.495655124951863\n",
      "Gradient Descent(1973/9999): loss=2.7171011935600937, w0=73.40624999999997, w1=13.262368637212605\n",
      "Gradient Descent(1974/9999): loss=2.102498003458768, w0=73.34999999999997, w1=13.353617251820566\n",
      "Gradient Descent(1975/9999): loss=2.305331698349642, w0=73.23749999999997, w1=12.984769047109701\n",
      "Gradient Descent(1976/9999): loss=1.8266665722114754, w0=73.12499999999997, w1=13.03644520761469\n",
      "Gradient Descent(1977/9999): loss=2.2516840872427237, w0=72.89999999999998, w1=13.035958504542407\n",
      "Gradient Descent(1978/9999): loss=1.9831701187929802, w0=72.95624999999998, w1=13.032168083143471\n",
      "Gradient Descent(1979/9999): loss=2.7791365460585267, w0=72.95624999999998, w1=13.107395198692485\n",
      "Gradient Descent(1980/9999): loss=2.439411995771509, w0=73.29374999999999, w1=13.157908676089262\n",
      "Gradient Descent(1981/9999): loss=2.5213947060076882, w0=73.46249999999999, w1=13.233419259175767\n",
      "Gradient Descent(1982/9999): loss=2.5548559176908205, w0=73.46249999999999, w1=13.297400392901828\n",
      "Gradient Descent(1983/9999): loss=1.9353674953237603, w0=73.51875, w1=13.194029077122998\n",
      "Gradient Descent(1984/9999): loss=2.1842448548596165, w0=73.29375, w1=13.095585754090067\n",
      "Gradient Descent(1985/9999): loss=2.167633336238544, w0=73.06875000000001, w1=13.26469182351737\n",
      "Gradient Descent(1986/9999): loss=1.963152595550659, w0=72.84375000000001, w1=13.325654339445714\n",
      "Gradient Descent(1987/9999): loss=2.2378447518757976, w0=72.67500000000001, w1=13.308177951510405\n",
      "Gradient Descent(1988/9999): loss=2.558766908882495, w0=72.67500000000001, w1=13.032225444553971\n",
      "Gradient Descent(1989/9999): loss=2.4293609032324293, w0=72.78750000000001, w1=13.189798072268017\n",
      "Gradient Descent(1990/9999): loss=2.3912495098659257, w0=72.78750000000001, w1=13.251255179683966\n",
      "Gradient Descent(1991/9999): loss=1.7825150234454723, w0=72.95625000000001, w1=13.113045447003351\n",
      "Gradient Descent(1992/9999): loss=2.209996317612041, w0=73.01250000000002, w1=13.347784252930941\n",
      "Gradient Descent(1993/9999): loss=2.382809998302142, w0=73.12500000000001, w1=13.09419185098937\n",
      "Gradient Descent(1994/9999): loss=2.264020608595531, w0=73.46250000000002, w1=13.396383763300852\n",
      "Gradient Descent(1995/9999): loss=2.1840219918626085, w0=73.18125000000002, w1=13.488680130921683\n",
      "Gradient Descent(1996/9999): loss=1.9520436328131865, w0=73.01250000000002, w1=13.587151088804392\n",
      "Gradient Descent(1997/9999): loss=1.814148406943986, w0=73.12500000000001, w1=13.617777059173903\n",
      "Gradient Descent(1998/9999): loss=2.3910894051915488, w0=73.18125000000002, w1=13.508988053485252\n",
      "Gradient Descent(1999/9999): loss=2.046264390517659, w0=73.23750000000003, w1=13.42361651617104\n",
      "Gradient Descent(2000/9999): loss=2.1060386220484952, w0=73.18125000000002, w1=13.530860741251002\n",
      "Gradient Descent(2001/9999): loss=1.9674159913222975, w0=73.06875000000002, w1=13.616591517836287\n",
      "Gradient Descent(2002/9999): loss=2.374723548363117, w0=73.01250000000002, w1=13.567254982030693\n",
      "Gradient Descent(2003/9999): loss=1.9712962226160007, w0=72.95625000000001, w1=13.486015070992643\n",
      "Gradient Descent(2004/9999): loss=2.4853828740029367, w0=72.95625000000001, w1=13.558931265339897\n",
      "Gradient Descent(2005/9999): loss=2.037317783023309, w0=73.23750000000001, w1=13.319904763423015\n",
      "Gradient Descent(2006/9999): loss=2.1602450224368566, w0=73.06875000000001, w1=13.368675224949774\n",
      "Gradient Descent(2007/9999): loss=2.2770033191035015, w0=72.95625000000001, w1=13.376833474863911\n",
      "Gradient Descent(2008/9999): loss=2.059831655092372, w0=73.12500000000001, w1=13.57702438863932\n",
      "Gradient Descent(2009/9999): loss=2.0178003826285855, w0=72.84375000000001, w1=13.34512998584036\n",
      "Gradient Descent(2010/9999): loss=2.600323450361219, w0=72.84375000000001, w1=13.351212217406523\n",
      "Gradient Descent(2011/9999): loss=2.258273547450509, w0=72.61875000000002, w1=13.391277600734277\n",
      "Gradient Descent(2012/9999): loss=2.0631430328118716, w0=72.67500000000003, w1=13.565835970475797\n",
      "Gradient Descent(2013/9999): loss=2.545763745806546, w0=72.56250000000003, w1=13.314344140860541\n",
      "Gradient Descent(2014/9999): loss=2.2231813267061167, w0=72.39375000000003, w1=13.185466215005581\n",
      "Gradient Descent(2015/9999): loss=2.49964369228114, w0=72.33750000000002, w1=13.224071078705789\n",
      "Gradient Descent(2016/9999): loss=2.170770050900078, w0=72.45000000000002, w1=12.912404335868827\n",
      "Gradient Descent(2017/9999): loss=2.091805823704983, w0=72.50625000000002, w1=12.648709559444617\n",
      "Gradient Descent(2018/9999): loss=2.30282723633462, w0=72.56250000000003, w1=12.858034128098774\n",
      "Gradient Descent(2019/9999): loss=2.3222799542083683, w0=72.78750000000002, w1=12.759573435758387\n",
      "Gradient Descent(2020/9999): loss=2.6833901138516083, w0=72.95625000000003, w1=12.958350691480026\n",
      "Gradient Descent(2021/9999): loss=3.139446512664117, w0=73.23750000000003, w1=13.208100509861431\n",
      "Gradient Descent(2022/9999): loss=2.5899174732168437, w0=73.29375000000003, w1=13.251407980108972\n",
      "Gradient Descent(2023/9999): loss=2.0935410946844306, w0=73.29375000000003, w1=13.191708640210733\n",
      "Gradient Descent(2024/9999): loss=2.670232612680788, w0=73.18125000000003, w1=13.277583349010866\n",
      "Gradient Descent(2025/9999): loss=1.8022602162456014, w0=73.01250000000003, w1=13.037618923771566\n",
      "Gradient Descent(2026/9999): loss=1.6533882502772355, w0=72.78750000000004, w1=13.050205460966486\n",
      "Gradient Descent(2027/9999): loss=2.171899021427678, w0=72.90000000000003, w1=13.107633125824803\n",
      "Gradient Descent(2028/9999): loss=2.1953108624925806, w0=73.01250000000003, w1=13.413733833409387\n",
      "Gradient Descent(2029/9999): loss=2.2349977069724716, w0=73.01250000000003, w1=13.638297624553136\n",
      "Gradient Descent(2030/9999): loss=1.9493324461456607, w0=73.12500000000003, w1=13.508414359881494\n",
      "Gradient Descent(2031/9999): loss=2.5370971862472067, w0=73.06875000000002, w1=13.443875950756949\n",
      "Gradient Descent(2032/9999): loss=1.988033054329823, w0=72.78750000000002, w1=13.207067032012239\n",
      "Gradient Descent(2033/9999): loss=2.2986944285443327, w0=72.78750000000002, w1=13.321680697833875\n",
      "Gradient Descent(2034/9999): loss=2.0478501548156487, w0=72.95625000000003, w1=13.774798174250092\n",
      "Gradient Descent(2035/9999): loss=2.3771985174641386, w0=73.23750000000003, w1=13.811402943546094\n",
      "Gradient Descent(2036/9999): loss=2.3811029501859355, w0=73.12500000000003, w1=13.93893365665703\n",
      "Gradient Descent(2037/9999): loss=1.861895452340231, w0=73.01250000000003, w1=14.165967003449348\n",
      "Gradient Descent(2038/9999): loss=2.4641024397875535, w0=73.01250000000003, w1=14.020697842569763\n",
      "Gradient Descent(2039/9999): loss=1.989819964867347, w0=73.06875000000004, w1=14.03105303311392\n",
      "Gradient Descent(2040/9999): loss=2.013299765127327, w0=73.06875000000004, w1=14.03158436208826\n",
      "Gradient Descent(2041/9999): loss=2.321019564205617, w0=72.78750000000004, w1=13.86108477133284\n",
      "Gradient Descent(2042/9999): loss=1.693765950947511, w0=72.61875000000003, w1=13.913254574271248\n",
      "Gradient Descent(2043/9999): loss=2.2535831279568708, w0=72.45000000000003, w1=13.873364184191317\n",
      "Gradient Descent(2044/9999): loss=2.0358999171491483, w0=72.73125000000003, w1=13.968350551408816\n",
      "Gradient Descent(2045/9999): loss=2.461819641268756, w0=73.12500000000003, w1=13.96735895031821\n",
      "Gradient Descent(2046/9999): loss=2.2929209775558004, w0=73.06875000000002, w1=13.87084497379731\n",
      "Gradient Descent(2047/9999): loss=2.116749592313697, w0=72.84375000000003, w1=13.70991754534994\n",
      "Gradient Descent(2048/9999): loss=2.416628631518692, w0=72.73125000000003, w1=13.643839976396123\n",
      "Gradient Descent(2049/9999): loss=2.5593497341145355, w0=72.73125000000003, w1=13.68310725872214\n",
      "Gradient Descent(2050/9999): loss=2.5649394984672917, w0=72.84375000000003, w1=13.500747740382959\n",
      "Gradient Descent(2051/9999): loss=1.3379230814438974, w0=72.67500000000003, w1=13.537118263814635\n",
      "Gradient Descent(2052/9999): loss=1.8666854927812386, w0=72.61875000000002, w1=13.321067021822824\n",
      "Gradient Descent(2053/9999): loss=2.114835834380612, w0=73.06875000000002, w1=13.624095286499177\n",
      "Gradient Descent(2054/9999): loss=2.1960621256724746, w0=73.01250000000002, w1=13.617233349439893\n",
      "Gradient Descent(2055/9999): loss=1.9369587317928532, w0=72.95625000000001, w1=13.529210547707876\n",
      "Gradient Descent(2056/9999): loss=2.2787695309717737, w0=72.67500000000001, w1=13.65227057183776\n",
      "Gradient Descent(2057/9999): loss=2.420231927341206, w0=72.78750000000001, w1=13.60977003258532\n",
      "Gradient Descent(2058/9999): loss=2.234582552125742, w0=73.0125, w1=13.525131198984464\n",
      "Gradient Descent(2059/9999): loss=2.1626274138840973, w0=72.95625, w1=13.473216078404231\n",
      "Gradient Descent(2060/9999): loss=2.322675652471995, w0=73.125, w1=13.486906863561131\n",
      "Gradient Descent(2061/9999): loss=2.0269138985352178, w0=73.35, w1=13.162081195187175\n",
      "Gradient Descent(2062/9999): loss=1.901980735754392, w0=73.46249999999999, w1=13.118037608440558\n",
      "Gradient Descent(2063/9999): loss=1.793865628881893, w0=73.29374999999999, w1=13.212858410066065\n",
      "Gradient Descent(2064/9999): loss=2.8019966146088597, w0=73.35, w1=13.621616748241859\n",
      "Gradient Descent(2065/9999): loss=2.28923462806858, w0=73.18124999999999, w1=13.367129769722048\n",
      "Gradient Descent(2066/9999): loss=2.6727865329507567, w0=73.35, w1=13.544644873741822\n",
      "Gradient Descent(2067/9999): loss=2.2879767236896735, w0=73.35, w1=13.593805484371144\n",
      "Gradient Descent(2068/9999): loss=2.319993652361896, w0=73.29374999999999, w1=13.392431439586368\n",
      "Gradient Descent(2069/9999): loss=2.292559593191638, w0=73.06875, w1=13.357292674212665\n",
      "Gradient Descent(2070/9999): loss=1.9050003960170312, w0=72.95625, w1=13.247535202481663\n",
      "Gradient Descent(2071/9999): loss=2.1439530751813503, w0=73.35, w1=13.413824808499543\n",
      "Gradient Descent(2072/9999): loss=2.6502591718845503, w0=73.74374999999999, w1=13.56636542430772\n",
      "Gradient Descent(2073/9999): loss=2.510816987051411, w0=73.74374999999999, w1=13.539896636911394\n",
      "Gradient Descent(2074/9999): loss=2.5235916898556785, w0=73.57499999999999, w1=13.823762306864184\n",
      "Gradient Descent(2075/9999): loss=1.9046831203654393, w0=73.68749999999999, w1=13.6016926657958\n",
      "Gradient Descent(2076/9999): loss=1.840031793079705, w0=73.34999999999998, w1=13.531719869894339\n",
      "Gradient Descent(2077/9999): loss=2.5202690413856486, w0=73.18124999999998, w1=13.437341362900076\n",
      "Gradient Descent(2078/9999): loss=2.1561545240715416, w0=73.06874999999998, w1=13.223649782228879\n",
      "Gradient Descent(2079/9999): loss=2.0886061395642, w0=73.01249999999997, w1=13.281998081604863\n",
      "Gradient Descent(2080/9999): loss=2.4485663384262066, w0=73.01249999999997, w1=13.455050047617807\n",
      "Gradient Descent(2081/9999): loss=2.280369332181473, w0=72.84374999999997, w1=13.320257340418484\n",
      "Gradient Descent(2082/9999): loss=2.119872477952595, w0=72.89999999999998, w1=13.544590085045701\n",
      "Gradient Descent(2083/9999): loss=2.300946555126401, w0=73.01249999999997, w1=13.41356449554722\n",
      "Gradient Descent(2084/9999): loss=2.1052390272499526, w0=73.01249999999997, w1=13.334330518143117\n",
      "Gradient Descent(2085/9999): loss=2.0586193511976605, w0=72.89999999999998, w1=13.056991159324177\n",
      "Gradient Descent(2086/9999): loss=1.91046731914025, w0=72.78749999999998, w1=13.12229125532781\n",
      "Gradient Descent(2087/9999): loss=2.8181903661063186, w0=72.89999999999998, w1=12.969096477993887\n",
      "Gradient Descent(2088/9999): loss=2.1355406783453086, w0=72.84374999999997, w1=12.920734091527036\n",
      "Gradient Descent(2089/9999): loss=2.305990683259614, w0=72.89999999999998, w1=13.101682828908245\n",
      "Gradient Descent(2090/9999): loss=2.536052863721083, w0=72.73124999999997, w1=13.170645528601211\n",
      "Gradient Descent(2091/9999): loss=2.1555634629597, w0=72.89999999999998, w1=13.128404114931742\n",
      "Gradient Descent(2092/9999): loss=2.146164337035342, w0=73.34999999999998, w1=13.300987895586422\n",
      "Gradient Descent(2093/9999): loss=2.105775681129967, w0=73.40624999999999, w1=13.377376442312563\n",
      "Gradient Descent(2094/9999): loss=2.135739929654699, w0=73.12499999999999, w1=13.451376550046986\n",
      "Gradient Descent(2095/9999): loss=2.1318230939508087, w0=72.95624999999998, w1=13.466986747083428\n",
      "Gradient Descent(2096/9999): loss=1.9390421816523007, w0=73.06874999999998, w1=13.667918316620945\n",
      "Gradient Descent(2097/9999): loss=2.897503139495472, w0=72.89999999999998, w1=13.717888346472838\n",
      "Gradient Descent(2098/9999): loss=1.9166130569422641, w0=73.01249999999997, w1=13.56583597805236\n",
      "Gradient Descent(2099/9999): loss=2.179666390361498, w0=72.84374999999997, w1=13.496499157962674\n",
      "Gradient Descent(2100/9999): loss=1.852529137045107, w0=72.89999999999998, w1=13.75846857688675\n",
      "Gradient Descent(2101/9999): loss=1.9990401973992606, w0=72.89999999999998, w1=13.755772184297477\n",
      "Gradient Descent(2102/9999): loss=2.435244042378525, w0=72.84374999999997, w1=13.4804046982621\n",
      "Gradient Descent(2103/9999): loss=2.162137986742818, w0=72.84374999999997, w1=13.402881114019435\n",
      "Gradient Descent(2104/9999): loss=1.9814696541763328, w0=72.95624999999997, w1=13.631205507007472\n",
      "Gradient Descent(2105/9999): loss=2.0396669544475197, w0=72.84374999999997, w1=13.318022343724975\n",
      "Gradient Descent(2106/9999): loss=2.1481403434366446, w0=73.01249999999997, w1=13.512445825086491\n",
      "Gradient Descent(2107/9999): loss=2.0994072097647787, w0=73.18124999999998, w1=13.523777698581299\n",
      "Gradient Descent(2108/9999): loss=2.5603677210927778, w0=73.18124999999998, w1=13.669111870912799\n",
      "Gradient Descent(2109/9999): loss=2.1633484367594984, w0=73.29374999999997, w1=13.355984669117813\n",
      "Gradient Descent(2110/9999): loss=2.134962637916345, w0=73.23749999999997, w1=13.328323958532543\n",
      "Gradient Descent(2111/9999): loss=2.1283767505887896, w0=73.18124999999996, w1=13.644753772609057\n",
      "Gradient Descent(2112/9999): loss=2.1501833837876974, w0=73.06874999999997, w1=13.42285753141183\n",
      "Gradient Descent(2113/9999): loss=2.3419540151764573, w0=72.95624999999997, w1=13.356668035656568\n",
      "Gradient Descent(2114/9999): loss=1.8799100223071956, w0=73.06874999999997, w1=13.265625352393561\n",
      "Gradient Descent(2115/9999): loss=2.6358954380443276, w0=73.06874999999997, w1=13.412366455810101\n",
      "Gradient Descent(2116/9999): loss=1.5941648721535924, w0=73.29374999999996, w1=13.487548615954553\n",
      "Gradient Descent(2117/9999): loss=2.505338616227937, w0=73.40624999999996, w1=13.519706663462614\n",
      "Gradient Descent(2118/9999): loss=2.5227459327632356, w0=73.12499999999996, w1=13.572630244558962\n",
      "Gradient Descent(2119/9999): loss=2.29313991872767, w0=73.29374999999996, w1=13.741825925235831\n",
      "Gradient Descent(2120/9999): loss=2.4732828950510757, w0=73.46249999999996, w1=13.665023361712251\n",
      "Gradient Descent(2121/9999): loss=1.8472015544220068, w0=73.51874999999997, w1=13.54658384965958\n",
      "Gradient Descent(2122/9999): loss=2.002137325056858, w0=73.29374999999997, w1=13.69799907224765\n",
      "Gradient Descent(2123/9999): loss=2.1068610930552403, w0=73.23749999999997, w1=13.559281148391337\n",
      "Gradient Descent(2124/9999): loss=2.1745951088714315, w0=73.46249999999996, w1=13.626623543178532\n",
      "Gradient Descent(2125/9999): loss=2.496436501015904, w0=73.29374999999996, w1=13.81973140690905\n",
      "Gradient Descent(2126/9999): loss=1.7151111275329438, w0=73.23749999999995, w1=13.778968363069142\n",
      "Gradient Descent(2127/9999): loss=1.7418252676203194, w0=73.18124999999995, w1=14.128547208686115\n",
      "Gradient Descent(2128/9999): loss=1.9980689710099877, w0=73.40624999999994, w1=14.159081504906515\n",
      "Gradient Descent(2129/9999): loss=2.153190808362008, w0=73.68749999999994, w1=14.153832910822294\n",
      "Gradient Descent(2130/9999): loss=2.1036734620463875, w0=73.68749999999994, w1=14.269859352405826\n",
      "Gradient Descent(2131/9999): loss=2.4805654243082818, w0=73.85624999999995, w1=14.467028682154192\n",
      "Gradient Descent(2132/9999): loss=2.288824788602062, w0=73.51874999999994, w1=14.285740037251353\n",
      "Gradient Descent(2133/9999): loss=2.1346972683936674, w0=73.29374999999995, w1=14.537121136985657\n",
      "Gradient Descent(2134/9999): loss=2.1884126476743786, w0=73.34999999999995, w1=14.51791555319874\n",
      "Gradient Descent(2135/9999): loss=2.199638711939971, w0=73.34999999999995, w1=14.715988523578286\n",
      "Gradient Descent(2136/9999): loss=2.0938864689754944, w0=73.29374999999995, w1=14.364016936774155\n",
      "Gradient Descent(2137/9999): loss=2.075452188103917, w0=73.06874999999995, w1=14.289447172847689\n",
      "Gradient Descent(2138/9999): loss=2.5617503976105125, w0=73.06874999999995, w1=14.192294019375721\n",
      "Gradient Descent(2139/9999): loss=1.8700423799716959, w0=72.61874999999995, w1=14.142646384515746\n",
      "Gradient Descent(2140/9999): loss=2.781997668936187, w0=72.73124999999995, w1=14.02687806923792\n",
      "Gradient Descent(2141/9999): loss=2.4307610467660883, w0=72.78749999999995, w1=13.884564837426783\n",
      "Gradient Descent(2142/9999): loss=2.2793731400021926, w0=72.95624999999995, w1=14.007842528438216\n",
      "Gradient Descent(2143/9999): loss=1.894910484716928, w0=73.01249999999996, w1=13.971289797531979\n",
      "Gradient Descent(2144/9999): loss=2.3437006498502466, w0=73.12499999999996, w1=13.709384978610274\n",
      "Gradient Descent(2145/9999): loss=2.4740121529781693, w0=73.12499999999996, w1=14.104336080497655\n",
      "Gradient Descent(2146/9999): loss=1.889709425462957, w0=73.06874999999995, w1=13.964813074143226\n",
      "Gradient Descent(2147/9999): loss=2.1715967282431228, w0=73.29374999999995, w1=14.02904777038631\n",
      "Gradient Descent(2148/9999): loss=2.669594133178241, w0=73.46249999999995, w1=13.721830140039266\n",
      "Gradient Descent(2149/9999): loss=2.3080806292283667, w0=73.40624999999994, w1=13.558157877734354\n",
      "Gradient Descent(2150/9999): loss=2.2423206794970967, w0=73.29374999999995, w1=13.787329854819491\n",
      "Gradient Descent(2151/9999): loss=1.9540522374331113, w0=73.18124999999995, w1=13.781113342771945\n",
      "Gradient Descent(2152/9999): loss=1.6713638487058564, w0=73.18124999999995, w1=13.312635527006307\n",
      "Gradient Descent(2153/9999): loss=2.8845290400014667, w0=73.23749999999995, w1=13.450283144511543\n",
      "Gradient Descent(2154/9999): loss=1.8683804111524362, w0=72.95624999999995, w1=13.378733003718992\n",
      "Gradient Descent(2155/9999): loss=2.0564899364156632, w0=72.89999999999995, w1=13.231945511408405\n",
      "Gradient Descent(2156/9999): loss=2.11105418586523, w0=73.06874999999995, w1=13.507872875565294\n",
      "Gradient Descent(2157/9999): loss=2.526255186685251, w0=73.01249999999995, w1=13.431589803161572\n",
      "Gradient Descent(2158/9999): loss=2.275377305405446, w0=73.01249999999995, w1=13.428105565562326\n",
      "Gradient Descent(2159/9999): loss=2.1982540252524574, w0=73.01249999999995, w1=13.6894794443419\n",
      "Gradient Descent(2160/9999): loss=1.8828023441195665, w0=73.18124999999995, w1=13.623936966769172\n",
      "Gradient Descent(2161/9999): loss=1.9441535212717218, w0=73.18124999999995, w1=13.191221753720425\n",
      "Gradient Descent(2162/9999): loss=2.074408273398484, w0=73.18124999999995, w1=13.437469912329373\n",
      "Gradient Descent(2163/9999): loss=2.0686683758452435, w0=73.23749999999995, w1=13.771036541162385\n",
      "Gradient Descent(2164/9999): loss=2.2408199141948346, w0=73.29374999999996, w1=13.562461163690987\n",
      "Gradient Descent(2165/9999): loss=2.2528127028307985, w0=73.34999999999997, w1=13.54162150287101\n",
      "Gradient Descent(2166/9999): loss=1.9151996615204183, w0=73.29374999999996, w1=13.390609205472662\n",
      "Gradient Descent(2167/9999): loss=2.1421848980818776, w0=73.46249999999996, w1=13.401411096023436\n",
      "Gradient Descent(2168/9999): loss=2.2481187472390323, w0=73.51874999999997, w1=13.37223249320836\n",
      "Gradient Descent(2169/9999): loss=1.9981184424428429, w0=73.40624999999997, w1=13.626326801012514\n",
      "Gradient Descent(2170/9999): loss=2.1980321162392302, w0=73.34999999999997, w1=13.54924633387342\n",
      "Gradient Descent(2171/9999): loss=2.740738415318888, w0=73.29374999999996, w1=13.5953534042544\n",
      "Gradient Descent(2172/9999): loss=1.92999644467462, w0=73.18124999999996, w1=13.413613327278233\n",
      "Gradient Descent(2173/9999): loss=2.6994703609789803, w0=73.29374999999996, w1=13.62383244017234\n",
      "Gradient Descent(2174/9999): loss=2.333409445814487, w0=73.12499999999996, w1=13.406677353990743\n",
      "Gradient Descent(2175/9999): loss=1.872187515202543, w0=72.84374999999996, w1=13.463146592887492\n",
      "Gradient Descent(2176/9999): loss=2.273151210890763, w0=73.01249999999996, w1=13.451326006011314\n",
      "Gradient Descent(2177/9999): loss=2.294956376243112, w0=73.06874999999997, w1=13.405209877225074\n",
      "Gradient Descent(2178/9999): loss=2.3790881413366334, w0=72.67499999999997, w1=13.31023552422912\n",
      "Gradient Descent(2179/9999): loss=1.6796826555562463, w0=72.78749999999997, w1=13.253168423742975\n",
      "Gradient Descent(2180/9999): loss=2.5416557122272034, w0=72.95624999999997, w1=13.14127229662735\n",
      "Gradient Descent(2181/9999): loss=2.1486389653528066, w0=73.12499999999997, w1=13.277757402720614\n",
      "Gradient Descent(2182/9999): loss=2.245261950960672, w0=73.06874999999997, w1=13.505275762800343\n",
      "Gradient Descent(2183/9999): loss=1.91493568125798, w0=72.95624999999997, w1=13.520615605941162\n",
      "Gradient Descent(2184/9999): loss=1.8821687484176772, w0=73.01249999999997, w1=12.990290656577992\n",
      "Gradient Descent(2185/9999): loss=1.9756622816862786, w0=72.84374999999997, w1=13.173509744273336\n",
      "Gradient Descent(2186/9999): loss=2.0946912129348902, w0=73.01249999999997, w1=13.293198880571392\n",
      "Gradient Descent(2187/9999): loss=2.055953590381391, w0=73.12499999999997, w1=13.228683966524752\n",
      "Gradient Descent(2188/9999): loss=2.0830829677338993, w0=73.29374999999997, w1=13.485485757898699\n",
      "Gradient Descent(2189/9999): loss=2.398307752345536, w0=73.40624999999997, w1=13.224741623998106\n",
      "Gradient Descent(2190/9999): loss=2.445824249675892, w0=73.01249999999997, w1=13.179082888957906\n",
      "Gradient Descent(2191/9999): loss=2.3597215696257474, w0=73.40624999999997, w1=13.138017678866309\n",
      "Gradient Descent(2192/9999): loss=2.0520266178425635, w0=73.34999999999997, w1=13.01159513901422\n",
      "Gradient Descent(2193/9999): loss=2.2649318147903585, w0=73.40624999999997, w1=12.92333473924207\n",
      "Gradient Descent(2194/9999): loss=2.0710479350796627, w0=73.34999999999997, w1=13.080683896706152\n",
      "Gradient Descent(2195/9999): loss=2.3974704788119583, w0=73.34999999999997, w1=13.664024884700718\n",
      "Gradient Descent(2196/9999): loss=1.9086074603309797, w0=73.23749999999997, w1=14.0807137265074\n",
      "Gradient Descent(2197/9999): loss=2.0804707054028615, w0=73.29374999999997, w1=14.041713315238423\n",
      "Gradient Descent(2198/9999): loss=1.3755254727912392, w0=73.51874999999997, w1=14.258611846695253\n",
      "Gradient Descent(2199/9999): loss=2.0413244296623274, w0=73.46249999999996, w1=14.204113910993886\n",
      "Gradient Descent(2200/9999): loss=2.052147513921329, w0=73.23749999999997, w1=13.935947191678961\n",
      "Gradient Descent(2201/9999): loss=2.142779003113689, w0=73.34999999999997, w1=13.808246819125449\n",
      "Gradient Descent(2202/9999): loss=2.532488347883061, w0=73.40624999999997, w1=13.597563845615424\n",
      "Gradient Descent(2203/9999): loss=2.2190601333121176, w0=73.40624999999997, w1=13.711284131150435\n",
      "Gradient Descent(2204/9999): loss=1.4965673046755776, w0=73.46249999999998, w1=13.753642594595524\n",
      "Gradient Descent(2205/9999): loss=2.078846899000459, w0=73.40624999999997, w1=13.79239382041709\n",
      "Gradient Descent(2206/9999): loss=2.3455192653308785, w0=73.63124999999997, w1=13.745643577539697\n",
      "Gradient Descent(2207/9999): loss=2.499728941598486, w0=73.68749999999997, w1=13.565797946413952\n",
      "Gradient Descent(2208/9999): loss=2.359764484768223, w0=73.68749999999997, w1=13.571236770830076\n",
      "Gradient Descent(2209/9999): loss=1.984114738510939, w0=73.57499999999997, w1=13.730716036086763\n",
      "Gradient Descent(2210/9999): loss=2.2719092550350277, w0=73.57499999999997, w1=13.73832896678026\n",
      "Gradient Descent(2211/9999): loss=2.2611376800425624, w0=73.63124999999998, w1=13.72823227061372\n",
      "Gradient Descent(2212/9999): loss=2.093325084435733, w0=73.74374999999998, w1=13.591688751154855\n",
      "Gradient Descent(2213/9999): loss=1.8132811816693803, w0=73.74374999999998, w1=13.73793867623099\n",
      "Gradient Descent(2214/9999): loss=1.9754610920600315, w0=73.51874999999998, w1=13.781034136495089\n",
      "Gradient Descent(2215/9999): loss=1.8279577996521479, w0=73.40624999999999, w1=13.858770398735949\n",
      "Gradient Descent(2216/9999): loss=2.305764143070606, w0=73.57499999999999, w1=13.660810405342795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2217/9999): loss=1.9591355268044244, w0=73.35, w1=14.011203989131657\n",
      "Gradient Descent(2218/9999): loss=2.2923953322485158, w0=73.51875, w1=13.99299856957525\n",
      "Gradient Descent(2219/9999): loss=2.3209129420994143, w0=73.40625, w1=14.00819125849911\n",
      "Gradient Descent(2220/9999): loss=2.2603606212830476, w0=73.40625, w1=13.97379299269309\n",
      "Gradient Descent(2221/9999): loss=1.8554781197978127, w0=73.40625, w1=13.67027379303107\n",
      "Gradient Descent(2222/9999): loss=2.1734860510095784, w0=73.63125, w1=13.433729054796993\n",
      "Gradient Descent(2223/9999): loss=2.148465718686775, w0=73.35, w1=13.454458945807467\n",
      "Gradient Descent(2224/9999): loss=1.9805880805265095, w0=73.125, w1=13.688070018096315\n",
      "Gradient Descent(2225/9999): loss=1.8949648765432752, w0=72.9, w1=13.55163112012388\n",
      "Gradient Descent(2226/9999): loss=1.818622371724684, w0=72.73125, w1=13.527098343317656\n",
      "Gradient Descent(2227/9999): loss=2.4638576231438587, w0=73.0125, w1=13.496826283090384\n",
      "Gradient Descent(2228/9999): loss=1.7061495484901057, w0=73.06875000000001, w1=13.272797542072846\n",
      "Gradient Descent(2229/9999): loss=1.520356707092244, w0=73.12500000000001, w1=13.549763392120118\n",
      "Gradient Descent(2230/9999): loss=2.3014122413906444, w0=73.12500000000001, w1=13.54613305116079\n",
      "Gradient Descent(2231/9999): loss=2.1862519132937877, w0=73.18125000000002, w1=13.373160202671603\n",
      "Gradient Descent(2232/9999): loss=2.7221058064576003, w0=73.23750000000003, w1=13.505748195237922\n",
      "Gradient Descent(2233/9999): loss=2.476265853913521, w0=73.23750000000003, w1=13.541575127219621\n",
      "Gradient Descent(2234/9999): loss=2.5054153053630195, w0=73.18125000000002, w1=13.409625371344868\n",
      "Gradient Descent(2235/9999): loss=1.985233367606531, w0=73.40625000000001, w1=13.417761962120203\n",
      "Gradient Descent(2236/9999): loss=2.0755213113771607, w0=73.23750000000001, w1=13.330222948110444\n",
      "Gradient Descent(2237/9999): loss=2.0381768542780074, w0=73.23750000000001, w1=13.323783808587661\n",
      "Gradient Descent(2238/9999): loss=2.7458502235353808, w0=73.35000000000001, w1=13.390280393513045\n",
      "Gradient Descent(2239/9999): loss=2.5061736036254647, w0=73.29375, w1=13.332908354531824\n",
      "Gradient Descent(2240/9999): loss=2.1381609964524015, w0=73.125, w1=13.510670951960153\n",
      "Gradient Descent(2241/9999): loss=1.508875105215822, w0=73.06875, w1=13.632288421185821\n",
      "Gradient Descent(2242/9999): loss=1.6753022722686524, w0=73.01249999999999, w1=13.676166831734687\n",
      "Gradient Descent(2243/9999): loss=2.1693624958669444, w0=73.29374999999999, w1=13.533573279146825\n",
      "Gradient Descent(2244/9999): loss=2.1794730270738056, w0=73.35, w1=14.018737039563902\n",
      "Gradient Descent(2245/9999): loss=1.8115601493128768, w0=73.40625, w1=13.84535706423487\n",
      "Gradient Descent(2246/9999): loss=2.2350433212260663, w0=73.29375, w1=13.987043143673539\n",
      "Gradient Descent(2247/9999): loss=2.351334825417954, w0=73.18125, w1=14.2743674290037\n",
      "Gradient Descent(2248/9999): loss=1.9937149585762604, w0=73.06875000000001, w1=14.081395755771206\n",
      "Gradient Descent(2249/9999): loss=2.2987912685483876, w0=73.35000000000001, w1=13.898225933394176\n",
      "Gradient Descent(2250/9999): loss=2.820256246908489, w0=73.29375, w1=13.74857150519467\n",
      "Gradient Descent(2251/9999): loss=2.358105341973321, w0=73.35000000000001, w1=13.453889077332024\n",
      "Gradient Descent(2252/9999): loss=1.907574110792839, w0=73.29375, w1=13.50789759408659\n",
      "Gradient Descent(2253/9999): loss=2.427759462479043, w0=73.2375, w1=13.534101587507397\n",
      "Gradient Descent(2254/9999): loss=1.8328082119708466, w0=73.29375, w1=13.575082892550299\n",
      "Gradient Descent(2255/9999): loss=2.201791711630401, w0=73.2375, w1=13.703980714246043\n",
      "Gradient Descent(2256/9999): loss=1.920869212847076, w0=72.89999999999999, w1=13.9793615183947\n",
      "Gradient Descent(2257/9999): loss=1.7646359589651763, w0=73.01249999999999, w1=13.913183972988342\n",
      "Gradient Descent(2258/9999): loss=2.498591227469005, w0=73.06875, w1=13.698446190456654\n",
      "Gradient Descent(2259/9999): loss=2.4544382413747092, w0=73.06875, w1=13.903497110008667\n",
      "Gradient Descent(2260/9999): loss=2.2724612701102465, w0=73.18124999999999, w1=13.61126303479254\n",
      "Gradient Descent(2261/9999): loss=2.0164785296156564, w0=73.35, w1=13.66749881589903\n",
      "Gradient Descent(2262/9999): loss=2.5792261705955277, w0=73.35, w1=13.967506975323204\n",
      "Gradient Descent(2263/9999): loss=2.149819780150481, w0=73.29374999999999, w1=13.878410536675686\n",
      "Gradient Descent(2264/9999): loss=2.6512341511747985, w0=73.35, w1=13.921908145197285\n",
      "Gradient Descent(2265/9999): loss=2.1657001890905123, w0=73.35, w1=13.779417454565435\n",
      "Gradient Descent(2266/9999): loss=2.5835007252164273, w0=73.18124999999999, w1=13.634465048163023\n",
      "Gradient Descent(2267/9999): loss=2.1409792458855845, w0=73.18124999999999, w1=13.44819538215383\n",
      "Gradient Descent(2268/9999): loss=2.1712837301608356, w0=73.35, w1=13.661770979209873\n",
      "Gradient Descent(2269/9999): loss=2.112001333543381, w0=72.89999999999999, w1=13.613385421130687\n",
      "Gradient Descent(2270/9999): loss=2.2861870087252982, w0=72.89999999999999, w1=13.53442861462119\n",
      "Gradient Descent(2271/9999): loss=2.9910745765968043, w0=72.95625, w1=13.220142060648463\n",
      "Gradient Descent(2272/9999): loss=2.1266364030512652, w0=72.95625, w1=13.33825096775248\n",
      "Gradient Descent(2273/9999): loss=1.6617445287193622, w0=72.95625, w1=13.092205593231297\n",
      "Gradient Descent(2274/9999): loss=1.656107287773532, w0=73.06875, w1=13.203660339678278\n",
      "Gradient Descent(2275/9999): loss=1.8920527557829567, w0=72.73124999999999, w1=13.261018484496166\n",
      "Gradient Descent(2276/9999): loss=2.0344800913417003, w0=72.61874999999999, w1=13.60231964150384\n",
      "Gradient Descent(2277/9999): loss=2.4396821610400186, w0=72.84374999999999, w1=13.610324600641032\n",
      "Gradient Descent(2278/9999): loss=1.9812304031210601, w0=72.84374999999999, w1=13.478744619136757\n",
      "Gradient Descent(2279/9999): loss=3.012574747814768, w0=72.84374999999999, w1=13.602341326093738\n",
      "Gradient Descent(2280/9999): loss=2.1142979905046744, w0=72.67499999999998, w1=13.36077995779875\n",
      "Gradient Descent(2281/9999): loss=1.8156817704350747, w0=72.67499999999998, w1=13.602272089880056\n",
      "Gradient Descent(2282/9999): loss=2.4165845674332003, w0=72.78749999999998, w1=13.648019945270535\n",
      "Gradient Descent(2283/9999): loss=2.2481733908455874, w0=72.67499999999998, w1=13.694242078356535\n",
      "Gradient Descent(2284/9999): loss=1.9618415012173607, w0=72.78749999999998, w1=13.366317318411724\n",
      "Gradient Descent(2285/9999): loss=2.1754282737661006, w0=72.73124999999997, w1=13.072799529104218\n",
      "Gradient Descent(2286/9999): loss=2.148177447026305, w0=72.73124999999997, w1=12.958041761277583\n",
      "Gradient Descent(2287/9999): loss=1.8877856780628914, w0=72.78749999999998, w1=13.028701912177757\n",
      "Gradient Descent(2288/9999): loss=2.467159840441387, w0=72.84374999999999, w1=13.255890019283603\n",
      "Gradient Descent(2289/9999): loss=2.369733642826545, w0=72.56249999999999, w1=13.195687139018775\n",
      "Gradient Descent(2290/9999): loss=2.320944570929833, w0=72.73124999999999, w1=13.209033628440089\n",
      "Gradient Descent(2291/9999): loss=2.127228828071517, w0=72.73124999999999, w1=13.252306816118216\n",
      "Gradient Descent(2292/9999): loss=2.0380612085350913, w0=72.84374999999999, w1=13.560733320277347\n",
      "Gradient Descent(2293/9999): loss=2.2860011734451122, w0=72.84374999999999, w1=13.694052069611049\n",
      "Gradient Descent(2294/9999): loss=2.1681905566514565, w0=72.73124999999999, w1=13.67731397641007\n",
      "Gradient Descent(2295/9999): loss=1.9576613215595169, w0=72.67499999999998, w1=13.612729635176308\n",
      "Gradient Descent(2296/9999): loss=2.644797586123922, w0=72.84374999999999, w1=13.442221203658509\n",
      "Gradient Descent(2297/9999): loss=2.3628650212549145, w0=72.95624999999998, w1=13.52269019606028\n",
      "Gradient Descent(2298/9999): loss=2.551131676805214, w0=73.01249999999999, w1=13.582476465428224\n",
      "Gradient Descent(2299/9999): loss=1.8270172276512473, w0=73.18124999999999, w1=13.385161932318052\n",
      "Gradient Descent(2300/9999): loss=2.2489313460255644, w0=72.95625, w1=13.254907129063985\n",
      "Gradient Descent(2301/9999): loss=2.371044089522376, w0=73.125, w1=13.501282954784564\n",
      "Gradient Descent(2302/9999): loss=2.233326287788052, w0=73.06875, w1=13.498416306145526\n",
      "Gradient Descent(2303/9999): loss=2.068367220608889, w0=73.125, w1=13.281687788565012\n",
      "Gradient Descent(2304/9999): loss=1.9772444059560326, w0=73.0125, w1=13.318924174709464\n",
      "Gradient Descent(2305/9999): loss=2.432052935847472, w0=73.06875000000001, w1=13.341022593855579\n",
      "Gradient Descent(2306/9999): loss=1.9310440447631303, w0=73.40625000000001, w1=13.411836895087397\n",
      "Gradient Descent(2307/9999): loss=2.5737398352445506, w0=73.23750000000001, w1=13.462097562700114\n",
      "Gradient Descent(2308/9999): loss=2.4865378658112443, w0=73.35000000000001, w1=13.504735175432579\n",
      "Gradient Descent(2309/9999): loss=2.239041227259993, w0=73.40625000000001, w1=13.55256178686453\n",
      "Gradient Descent(2310/9999): loss=1.83901134373173, w0=73.40625000000001, w1=13.672199113597642\n",
      "Gradient Descent(2311/9999): loss=2.2403413974468007, w0=73.40625000000001, w1=13.342706983913757\n",
      "Gradient Descent(2312/9999): loss=2.1324027286387013, w0=73.35000000000001, w1=13.40761273693488\n",
      "Gradient Descent(2313/9999): loss=2.5960522808963793, w0=73.23750000000001, w1=13.448555440413227\n",
      "Gradient Descent(2314/9999): loss=1.8945148035656605, w0=73.01250000000002, w1=13.505780738712035\n",
      "Gradient Descent(2315/9999): loss=2.1245877846117844, w0=73.12500000000001, w1=13.579441174887258\n",
      "Gradient Descent(2316/9999): loss=2.4163065111379183, w0=73.12500000000001, w1=13.461749006713477\n",
      "Gradient Descent(2317/9999): loss=2.00278406998338, w0=73.06875000000001, w1=13.577198629040844\n",
      "Gradient Descent(2318/9999): loss=2.229381020231185, w0=73.23750000000001, w1=13.774105397748071\n",
      "Gradient Descent(2319/9999): loss=2.4133389703424832, w0=73.06875000000001, w1=13.721454755206697\n",
      "Gradient Descent(2320/9999): loss=1.8226348517103794, w0=73.29375, w1=13.763633117140047\n",
      "Gradient Descent(2321/9999): loss=2.2224703621002435, w0=73.18125, w1=13.514594031887341\n",
      "Gradient Descent(2322/9999): loss=2.0653235649428794, w0=73.35000000000001, w1=13.454470152826175\n",
      "Gradient Descent(2323/9999): loss=2.770757428948449, w0=73.40625000000001, w1=13.510919584727281\n",
      "Gradient Descent(2324/9999): loss=2.0875777297327143, w0=73.40625000000001, w1=13.70950302874916\n",
      "Gradient Descent(2325/9999): loss=2.1780323914950808, w0=73.68750000000001, w1=13.807195126568033\n",
      "Gradient Descent(2326/9999): loss=2.582139815842558, w0=73.63125000000001, w1=13.959106127587937\n",
      "Gradient Descent(2327/9999): loss=2.3270789328871926, w0=73.51875000000001, w1=13.94076199571948\n",
      "Gradient Descent(2328/9999): loss=2.392862958940495, w0=73.4625, w1=13.725546314409709\n",
      "Gradient Descent(2329/9999): loss=2.014362855139437, w0=73.51875000000001, w1=13.763956302724099\n",
      "Gradient Descent(2330/9999): loss=1.7946661596759546, w0=73.40625000000001, w1=13.725357536469748\n",
      "Gradient Descent(2331/9999): loss=2.7578047198789006, w0=73.51875000000001, w1=13.711318303287118\n",
      "Gradient Descent(2332/9999): loss=2.1033970683085883, w0=73.29375000000002, w1=13.81311698412993\n",
      "Gradient Descent(2333/9999): loss=2.2518412576692066, w0=73.01250000000002, w1=13.618332917214213\n",
      "Gradient Descent(2334/9999): loss=2.1845510658779705, w0=73.12500000000001, w1=13.687056531645375\n",
      "Gradient Descent(2335/9999): loss=2.4535623117735685, w0=72.95625000000001, w1=14.140420460617559\n",
      "Gradient Descent(2336/9999): loss=1.9667721521862491, w0=72.84375000000001, w1=14.012595375392038\n",
      "Gradient Descent(2337/9999): loss=2.474441415896949, w0=72.90000000000002, w1=13.886176891619295\n",
      "Gradient Descent(2338/9999): loss=2.159209927732372, w0=72.95625000000003, w1=13.889861658417994\n",
      "Gradient Descent(2339/9999): loss=2.5422944544256403, w0=72.90000000000002, w1=13.739624265091496\n",
      "Gradient Descent(2340/9999): loss=2.4340876820519792, w0=72.78750000000002, w1=13.620449314942183\n",
      "Gradient Descent(2341/9999): loss=1.538721512877378, w0=72.73125000000002, w1=13.76511503194492\n",
      "Gradient Descent(2342/9999): loss=2.737585261234961, w0=72.67500000000001, w1=13.756420475496377\n",
      "Gradient Descent(2343/9999): loss=2.678993535709436, w0=72.9, w1=13.594914775168803\n",
      "Gradient Descent(2344/9999): loss=2.1844938554421103, w0=73.125, w1=13.573056183179455\n",
      "Gradient Descent(2345/9999): loss=2.1516909141958496, w0=73.18125, w1=13.546475305383435\n",
      "Gradient Descent(2346/9999): loss=1.6862308093158291, w0=72.9, w1=13.281360769442777\n",
      "Gradient Descent(2347/9999): loss=2.397947131525636, w0=72.95625000000001, w1=13.339226379155495\n",
      "Gradient Descent(2348/9999): loss=2.172969935872194, w0=73.18125, w1=13.462679964617594\n",
      "Gradient Descent(2349/9999): loss=2.4729537430538135, w0=73.18125, w1=13.609258558521532\n",
      "Gradient Descent(2350/9999): loss=2.1396918158490843, w0=73.35000000000001, w1=13.717092970413445\n",
      "Gradient Descent(2351/9999): loss=2.510392607203773, w0=73.51875000000001, w1=13.597898972410187\n",
      "Gradient Descent(2352/9999): loss=1.9984870621884465, w0=73.40625000000001, w1=13.594655774615111\n",
      "Gradient Descent(2353/9999): loss=2.529467132119426, w0=73.46250000000002, w1=13.544444535472804\n",
      "Gradient Descent(2354/9999): loss=2.1812167266575497, w0=73.40625000000001, w1=13.702423897780957\n",
      "Gradient Descent(2355/9999): loss=2.4644362411864584, w0=72.95625000000001, w1=13.685539461265277\n",
      "Gradient Descent(2356/9999): loss=2.329588775877772, w0=72.9, w1=14.019486153991826\n",
      "Gradient Descent(2357/9999): loss=2.0124942077891177, w0=73.125, w1=14.387950684719845\n",
      "Gradient Descent(2358/9999): loss=2.6345279437546463, w0=73.06875, w1=14.204087666094724\n",
      "Gradient Descent(2359/9999): loss=2.675981656142344, w0=72.95625, w1=14.127322417908857\n",
      "Gradient Descent(2360/9999): loss=1.80902974164686, w0=72.84375, w1=14.09577952399861\n",
      "Gradient Descent(2361/9999): loss=2.4285970761441495, w0=72.84375, w1=13.872587233394048\n",
      "Gradient Descent(2362/9999): loss=2.364395663115423, w0=72.95625, w1=13.94457161342281\n",
      "Gradient Descent(2363/9999): loss=2.2961465157211647, w0=73.0125, w1=13.57799647949457\n",
      "Gradient Descent(2364/9999): loss=2.47017000067612, w0=73.06875000000001, w1=13.387489377495756\n",
      "Gradient Descent(2365/9999): loss=1.724028216071691, w0=72.95625000000001, w1=13.619032307016036\n",
      "Gradient Descent(2366/9999): loss=1.7470470414907684, w0=72.95625000000001, w1=13.486535234071965\n",
      "Gradient Descent(2367/9999): loss=1.7231156602045825, w0=73.06875000000001, w1=13.755464807815093\n",
      "Gradient Descent(2368/9999): loss=1.9984313139680379, w0=73.0125, w1=13.725793024752214\n",
      "Gradient Descent(2369/9999): loss=1.8835541295116505, w0=73.125, w1=13.810204081492934\n",
      "Gradient Descent(2370/9999): loss=2.00215128829162, w0=73.06875, w1=13.596456184867215\n",
      "Gradient Descent(2371/9999): loss=2.2567926786642456, w0=72.89999999999999, w1=13.487925647152464\n",
      "Gradient Descent(2372/9999): loss=2.021653091856737, w0=72.7875, w1=13.21976366550859\n",
      "Gradient Descent(2373/9999): loss=1.909463798902868, w0=72.675, w1=13.126372798932275\n",
      "Gradient Descent(2374/9999): loss=2.2593788126020895, w0=72.675, w1=12.941520567206947\n",
      "Gradient Descent(2375/9999): loss=2.480868526193805, w0=72.95625, w1=13.110617207866975\n",
      "Gradient Descent(2376/9999): loss=1.9183498122602491, w0=73.0125, w1=13.035244784434807\n",
      "Gradient Descent(2377/9999): loss=1.8926123894223084, w0=73.0125, w1=12.95018683304342\n",
      "Gradient Descent(2378/9999): loss=2.586218492875351, w0=73.06875000000001, w1=13.332925735724631\n",
      "Gradient Descent(2379/9999): loss=2.0327560475257656, w0=73.06875000000001, w1=13.46049828273372\n",
      "Gradient Descent(2380/9999): loss=2.2895286074850167, w0=72.84375000000001, w1=13.299555094623567\n",
      "Gradient Descent(2381/9999): loss=2.355329008810406, w0=72.84375000000001, w1=13.239002723311824\n",
      "Gradient Descent(2382/9999): loss=2.45467823840762, w0=72.90000000000002, w1=13.319211420445798\n",
      "Gradient Descent(2383/9999): loss=1.5864314188207345, w0=72.73125000000002, w1=13.44180307652329\n",
      "Gradient Descent(2384/9999): loss=2.220904068734647, w0=72.67500000000001, w1=13.42460468244784\n",
      "Gradient Descent(2385/9999): loss=1.9676528917388512, w0=72.84375000000001, w1=13.282322189415371\n",
      "Gradient Descent(2386/9999): loss=2.354431855369497, w0=72.90000000000002, w1=13.113549322852437\n",
      "Gradient Descent(2387/9999): loss=2.143418295381252, w0=72.78750000000002, w1=13.45153837320272\n",
      "Gradient Descent(2388/9999): loss=1.8782312749112104, w0=72.84375000000003, w1=13.601547567903793\n",
      "Gradient Descent(2389/9999): loss=2.2568040164664573, w0=73.01250000000003, w1=13.506202683579366\n",
      "Gradient Descent(2390/9999): loss=2.220478201279274, w0=73.01250000000003, w1=13.613156349127925\n",
      "Gradient Descent(2391/9999): loss=1.9900192261926877, w0=72.84375000000003, w1=13.853785070517933\n",
      "Gradient Descent(2392/9999): loss=1.9749535318376787, w0=72.67500000000003, w1=13.597539112778138\n",
      "Gradient Descent(2393/9999): loss=2.483700273193156, w0=72.78750000000002, w1=13.449580800692251\n",
      "Gradient Descent(2394/9999): loss=2.475410397650286, w0=72.90000000000002, w1=12.872017668880954\n",
      "Gradient Descent(2395/9999): loss=2.429584217276633, w0=72.95625000000003, w1=13.109581854528297\n",
      "Gradient Descent(2396/9999): loss=1.8382716206738927, w0=73.06875000000002, w1=13.112407164890492\n",
      "Gradient Descent(2397/9999): loss=2.090841855982023, w0=73.06875000000002, w1=13.344699082016929\n",
      "Gradient Descent(2398/9999): loss=2.151781518599422, w0=73.29375000000002, w1=13.245524184254531\n",
      "Gradient Descent(2399/9999): loss=2.396577829712522, w0=73.35000000000002, w1=13.31881316099413\n",
      "Gradient Descent(2400/9999): loss=2.448387439092569, w0=73.63125000000002, w1=13.056749686368658\n",
      "Gradient Descent(2401/9999): loss=2.09288355627279, w0=73.46250000000002, w1=13.312548946313347\n",
      "Gradient Descent(2402/9999): loss=2.256967640146745, w0=73.29375000000002, w1=13.109628126472249\n",
      "Gradient Descent(2403/9999): loss=1.6706285875718812, w0=73.12500000000001, w1=13.223717446624386\n",
      "Gradient Descent(2404/9999): loss=2.153644158912824, w0=73.18125000000002, w1=13.156565423760922\n",
      "Gradient Descent(2405/9999): loss=2.1615079200763585, w0=73.01250000000002, w1=13.335855832697971\n",
      "Gradient Descent(2406/9999): loss=1.7034947299468244, w0=72.95625000000001, w1=13.318575145945735\n",
      "Gradient Descent(2407/9999): loss=1.9823714813175026, w0=72.95625000000001, w1=13.099170828211928\n",
      "Gradient Descent(2408/9999): loss=1.8831339761567527, w0=73.18125, w1=12.995425125417098\n",
      "Gradient Descent(2409/9999): loss=1.93244743776523, w0=73.18125, w1=13.245928962665305\n",
      "Gradient Descent(2410/9999): loss=1.8728720060202462, w0=73.4625, w1=13.194835545063896\n",
      "Gradient Descent(2411/9999): loss=1.8667814736238006, w0=73.4625, w1=13.447369565542447\n",
      "Gradient Descent(2412/9999): loss=2.390288940393572, w0=73.29375, w1=13.220376483624742\n",
      "Gradient Descent(2413/9999): loss=1.9876571090068635, w0=73.35000000000001, w1=13.23303568713852\n",
      "Gradient Descent(2414/9999): loss=2.8494456571009943, w0=73.575, w1=13.509316802941326\n",
      "Gradient Descent(2415/9999): loss=1.9557347302585968, w0=73.575, w1=13.40003675632522\n",
      "Gradient Descent(2416/9999): loss=2.3663128308324106, w0=73.74375, w1=13.3529596299842\n",
      "Gradient Descent(2417/9999): loss=2.5845525105058185, w0=73.6875, w1=13.505706131253381\n",
      "Gradient Descent(2418/9999): loss=2.584201088946942, w0=73.51875, w1=13.54797096789596\n",
      "Gradient Descent(2419/9999): loss=2.6912695699693776, w0=73.18124999999999, w1=13.49495053318012\n",
      "Gradient Descent(2420/9999): loss=2.5649336362133726, w0=73.35, w1=13.337011523075482\n",
      "Gradient Descent(2421/9999): loss=2.108027999149793, w0=73.2375, w1=13.437632384587406\n",
      "Gradient Descent(2422/9999): loss=2.403662482889069, w0=73.125, w1=13.389912743083306\n",
      "Gradient Descent(2423/9999): loss=2.7767497397074106, w0=73.2375, w1=13.793823995000974\n",
      "Gradient Descent(2424/9999): loss=2.0947646328600324, w0=73.2375, w1=13.853140015770533\n",
      "Gradient Descent(2425/9999): loss=2.425879385114209, w0=73.125, w1=13.635099084047416\n",
      "Gradient Descent(2426/9999): loss=1.9873598973375624, w0=73.2375, w1=13.876929515182125\n",
      "Gradient Descent(2427/9999): loss=2.7302467307766225, w0=73.2375, w1=13.807267795057424\n",
      "Gradient Descent(2428/9999): loss=1.8842245395654316, w0=73.51875, w1=13.9338132032078\n",
      "Gradient Descent(2429/9999): loss=2.0226435660948106, w0=73.51875, w1=13.941917524878749\n",
      "Gradient Descent(2430/9999): loss=2.216021671925259, w0=73.46249999999999, w1=13.827311250112837\n",
      "Gradient Descent(2431/9999): loss=2.2157951653157815, w0=73.40624999999999, w1=13.545488759513251\n",
      "Gradient Descent(2432/9999): loss=1.8802892547532135, w0=73.34999999999998, w1=13.79578817323163\n",
      "Gradient Descent(2433/9999): loss=2.0104107923853194, w0=73.18124999999998, w1=13.83795846902554\n",
      "Gradient Descent(2434/9999): loss=1.7575631300004688, w0=73.18124999999998, w1=13.592385612186158\n",
      "Gradient Descent(2435/9999): loss=2.51547341768972, w0=73.40624999999997, w1=13.610842529945439\n",
      "Gradient Descent(2436/9999): loss=2.1938435605686806, w0=73.57499999999997, w1=13.846777376228674\n",
      "Gradient Descent(2437/9999): loss=2.1429908540443288, w0=73.51874999999997, w1=13.628436845813983\n",
      "Gradient Descent(2438/9999): loss=1.9545155510488557, w0=73.63124999999997, w1=13.175419228626424\n",
      "Gradient Descent(2439/9999): loss=2.3587444715917867, w0=73.63124999999997, w1=13.285486362015037\n",
      "Gradient Descent(2440/9999): loss=2.3926205488670025, w0=73.51874999999997, w1=13.443048226872067\n",
      "Gradient Descent(2441/9999): loss=2.1397236563648874, w0=73.51874999999997, w1=13.365499126385116\n",
      "Gradient Descent(2442/9999): loss=2.1773471845130894, w0=73.40624999999997, w1=13.727412543156586\n",
      "Gradient Descent(2443/9999): loss=2.1993325118237452, w0=73.18124999999998, w1=13.874559527521516\n",
      "Gradient Descent(2444/9999): loss=1.8508371329560174, w0=73.01249999999997, w1=13.494265281425763\n",
      "Gradient Descent(2445/9999): loss=1.9523135445612378, w0=72.84374999999997, w1=13.45647191553701\n",
      "Gradient Descent(2446/9999): loss=2.083662878714371, w0=72.61874999999998, w1=13.449139055867457\n",
      "Gradient Descent(2447/9999): loss=1.9066307215444924, w0=72.44999999999997, w1=13.507421289714285\n",
      "Gradient Descent(2448/9999): loss=2.521653148434649, w0=72.33749999999998, w1=13.50899952107752\n",
      "Gradient Descent(2449/9999): loss=2.2862820041441116, w0=72.39374999999998, w1=13.416249449375012\n",
      "Gradient Descent(2450/9999): loss=1.7854509904370006, w0=72.39374999999998, w1=13.645017666913303\n",
      "Gradient Descent(2451/9999): loss=2.956955392587078, w0=72.33749999999998, w1=13.557051983271677\n",
      "Gradient Descent(2452/9999): loss=2.5615893341483535, w0=72.61874999999998, w1=13.69496625478895\n",
      "Gradient Descent(2453/9999): loss=2.177205565689534, w0=72.67499999999998, w1=13.898467784574793\n",
      "Gradient Descent(2454/9999): loss=2.1810274634172693, w0=72.95624999999998, w1=13.878526179170066\n",
      "Gradient Descent(2455/9999): loss=2.384435496594626, w0=72.84374999999999, w1=13.688503780670183\n",
      "Gradient Descent(2456/9999): loss=2.88879071286461, w0=72.78749999999998, w1=13.5258497761433\n",
      "Gradient Descent(2457/9999): loss=2.378188897682219, w0=72.95624999999998, w1=13.882989186111846\n",
      "Gradient Descent(2458/9999): loss=2.3717932699268447, w0=72.78749999999998, w1=13.933620006956888\n",
      "Gradient Descent(2459/9999): loss=2.0450368854442025, w0=72.73124999999997, w1=13.888900637206437\n",
      "Gradient Descent(2460/9999): loss=1.8931190369745092, w0=72.78749999999998, w1=13.760662777158025\n",
      "Gradient Descent(2461/9999): loss=1.884260287487273, w0=73.01249999999997, w1=13.735490536895988\n",
      "Gradient Descent(2462/9999): loss=2.1696241865242016, w0=72.89999999999998, w1=13.77694383714177\n",
      "Gradient Descent(2463/9999): loss=2.081348870884884, w0=73.06874999999998, w1=13.418529996258489\n",
      "Gradient Descent(2464/9999): loss=2.228148126128156, w0=73.18124999999998, w1=13.226345381467874\n",
      "Gradient Descent(2465/9999): loss=1.7973266013924007, w0=73.12499999999997, w1=13.563215565436721\n",
      "Gradient Descent(2466/9999): loss=2.3656445172163947, w0=73.06874999999997, w1=13.457105667858793\n",
      "Gradient Descent(2467/9999): loss=2.179904048541019, w0=72.89999999999996, w1=13.778982540707249\n",
      "Gradient Descent(2468/9999): loss=2.41777607632668, w0=72.89999999999996, w1=13.551882920988804\n",
      "Gradient Descent(2469/9999): loss=2.0895347773831743, w0=72.84374999999996, w1=13.459748783932703\n",
      "Gradient Descent(2470/9999): loss=2.3788112654117217, w0=72.95624999999995, w1=13.606298518623262\n",
      "Gradient Descent(2471/9999): loss=2.4198277050576413, w0=72.95624999999995, w1=13.418478017262574\n",
      "Gradient Descent(2472/9999): loss=2.2713279322979285, w0=72.39374999999995, w1=13.43900080251073\n",
      "Gradient Descent(2473/9999): loss=1.8523144360147694, w0=72.56249999999996, w1=13.232813106552541\n",
      "Gradient Descent(2474/9999): loss=1.94966819608168, w0=72.56249999999996, w1=13.317831886270461\n",
      "Gradient Descent(2475/9999): loss=2.165489237172028, w0=72.89999999999996, w1=13.399036038071015\n",
      "Gradient Descent(2476/9999): loss=1.949082321671366, w0=73.01249999999996, w1=13.530648490055682\n",
      "Gradient Descent(2477/9999): loss=2.8437151414171358, w0=73.12499999999996, w1=13.449234387080978\n",
      "Gradient Descent(2478/9999): loss=2.4410218091340434, w0=73.01249999999996, w1=13.280465810458656\n",
      "Gradient Descent(2479/9999): loss=2.2404868561893436, w0=73.18124999999996, w1=13.144924401622818\n",
      "Gradient Descent(2480/9999): loss=2.163979873486647, w0=72.95624999999997, w1=13.329565490387202\n",
      "Gradient Descent(2481/9999): loss=1.8030703818464666, w0=73.06874999999997, w1=12.944700121336124\n",
      "Gradient Descent(2482/9999): loss=2.1307527278449836, w0=72.78749999999997, w1=13.168836697415458\n",
      "Gradient Descent(2483/9999): loss=2.528067923636024, w0=72.95624999999997, w1=13.103355920034458\n",
      "Gradient Descent(2484/9999): loss=1.7066526109127595, w0=73.18124999999996, w1=13.167103619019041\n",
      "Gradient Descent(2485/9999): loss=1.6802712273590865, w0=73.01249999999996, w1=13.450828016417232\n",
      "Gradient Descent(2486/9999): loss=2.375799813187503, w0=73.01249999999996, w1=13.459949105718389\n",
      "Gradient Descent(2487/9999): loss=1.9327607583797026, w0=72.78749999999997, w1=13.681720632731956\n",
      "Gradient Descent(2488/9999): loss=1.6921644351289795, w0=72.67499999999997, w1=13.232335088733253\n",
      "Gradient Descent(2489/9999): loss=2.8261192521209564, w0=72.73124999999997, w1=13.640163459484613\n",
      "Gradient Descent(2490/9999): loss=2.064797492387899, w0=72.67499999999997, w1=13.626486268911329\n",
      "Gradient Descent(2491/9999): loss=2.2636174375930795, w0=72.73124999999997, w1=13.286976744080771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2492/9999): loss=2.557015992374466, w0=73.12499999999997, w1=13.340099154715743\n",
      "Gradient Descent(2493/9999): loss=2.39094711139885, w0=73.06874999999997, w1=13.282229433605908\n",
      "Gradient Descent(2494/9999): loss=2.2554367852733277, w0=72.95624999999997, w1=13.223354840671306\n",
      "Gradient Descent(2495/9999): loss=2.744081903827335, w0=72.73124999999997, w1=13.365042293724771\n",
      "Gradient Descent(2496/9999): loss=2.2873185713105744, w0=72.73124999999997, w1=13.782956675543264\n",
      "Gradient Descent(2497/9999): loss=2.2366105305137367, w0=73.01249999999997, w1=13.74253554679832\n",
      "Gradient Descent(2498/9999): loss=2.410892473260998, w0=72.89999999999998, w1=14.021546116566135\n",
      "Gradient Descent(2499/9999): loss=2.466106993456256, w0=72.84374999999997, w1=13.935846203223639\n",
      "Gradient Descent(2500/9999): loss=2.3384589584086406, w0=73.06874999999997, w1=13.810077215424315\n",
      "Gradient Descent(2501/9999): loss=1.9235394415305036, w0=73.12499999999997, w1=14.126074317798755\n",
      "Gradient Descent(2502/9999): loss=2.0289031097500203, w0=73.01249999999997, w1=13.926471580096138\n",
      "Gradient Descent(2503/9999): loss=2.6234529799142527, w0=73.01249999999997, w1=13.79134593090417\n",
      "Gradient Descent(2504/9999): loss=2.1856427865047925, w0=73.34999999999998, w1=14.030786893986782\n",
      "Gradient Descent(2505/9999): loss=1.9452843913022884, w0=73.23749999999998, w1=13.769221901424666\n",
      "Gradient Descent(2506/9999): loss=2.2270235317418385, w0=73.23749999999998, w1=13.569814817761028\n",
      "Gradient Descent(2507/9999): loss=2.0061159895971974, w0=73.18124999999998, w1=13.687754419860388\n",
      "Gradient Descent(2508/9999): loss=2.6159905839136037, w0=73.12499999999997, w1=13.802704167771592\n",
      "Gradient Descent(2509/9999): loss=2.575411699620835, w0=73.12499999999997, w1=13.941543997038108\n",
      "Gradient Descent(2510/9999): loss=2.7078807427254654, w0=73.51874999999997, w1=13.995306586047455\n",
      "Gradient Descent(2511/9999): loss=2.243977524039126, w0=73.46249999999996, w1=13.789635647354746\n",
      "Gradient Descent(2512/9999): loss=1.7565923879039658, w0=73.74374999999996, w1=13.761481874118\n",
      "Gradient Descent(2513/9999): loss=3.093199923700669, w0=73.79999999999997, w1=13.717050015497358\n",
      "Gradient Descent(2514/9999): loss=2.136214854924006, w0=73.63124999999997, w1=13.401839649940321\n",
      "Gradient Descent(2515/9999): loss=1.8993717020469616, w0=73.63124999999997, w1=13.217962201587294\n",
      "Gradient Descent(2516/9999): loss=1.751120723315972, w0=73.68749999999997, w1=13.342481909534134\n",
      "Gradient Descent(2517/9999): loss=2.321621329751167, w0=73.46249999999998, w1=13.278384047943717\n",
      "Gradient Descent(2518/9999): loss=1.841376562330299, w0=73.68749999999997, w1=13.170951489900531\n",
      "Gradient Descent(2519/9999): loss=2.0840523277325964, w0=73.79999999999997, w1=13.309337049030706\n",
      "Gradient Descent(2520/9999): loss=2.0738602263981556, w0=73.46249999999996, w1=13.345088446829159\n",
      "Gradient Descent(2521/9999): loss=2.341414924580481, w0=73.57499999999996, w1=13.079635087874443\n",
      "Gradient Descent(2522/9999): loss=2.038789367528003, w0=73.06874999999997, w1=12.97003051959306\n",
      "Gradient Descent(2523/9999): loss=2.1420003566593118, w0=72.84374999999997, w1=12.893795750805712\n",
      "Gradient Descent(2524/9999): loss=1.8560657828406204, w0=72.95624999999997, w1=13.027029369677521\n",
      "Gradient Descent(2525/9999): loss=2.5590636979003745, w0=73.01249999999997, w1=13.356617218736305\n",
      "Gradient Descent(2526/9999): loss=2.586198362866977, w0=73.06874999999998, w1=13.636535241964355\n",
      "Gradient Descent(2527/9999): loss=1.5535820376883687, w0=72.89999999999998, w1=13.536674590102395\n",
      "Gradient Descent(2528/9999): loss=1.712079797222555, w0=72.73124999999997, w1=13.579630202932323\n",
      "Gradient Descent(2529/9999): loss=2.6558252696571465, w0=72.44999999999997, w1=13.596003265212532\n",
      "Gradient Descent(2530/9999): loss=2.424628856132419, w0=72.28124999999997, w1=13.529111681125084\n",
      "Gradient Descent(2531/9999): loss=2.2298969432797673, w0=72.28124999999997, w1=13.811780160128626\n",
      "Gradient Descent(2532/9999): loss=2.5496082484937164, w0=72.28124999999997, w1=13.64495746409189\n",
      "Gradient Descent(2533/9999): loss=2.5491653824615863, w0=72.22499999999997, w1=13.514906345097202\n",
      "Gradient Descent(2534/9999): loss=2.7440617271656267, w0=72.44999999999996, w1=13.49595592427563\n",
      "Gradient Descent(2535/9999): loss=2.712548264413009, w0=72.67499999999995, w1=13.360646929885672\n",
      "Gradient Descent(2536/9999): loss=2.0066554702530914, w0=72.39374999999995, w1=13.339691657165005\n",
      "Gradient Descent(2537/9999): loss=1.8608226373730021, w0=72.50624999999995, w1=13.328649987034472\n",
      "Gradient Descent(2538/9999): loss=2.743895458256865, w0=72.67499999999995, w1=13.062017074701048\n",
      "Gradient Descent(2539/9999): loss=2.6800745646587774, w0=72.84374999999996, w1=13.210072332881268\n",
      "Gradient Descent(2540/9999): loss=1.752662956085777, w0=73.01249999999996, w1=13.458857954165506\n",
      "Gradient Descent(2541/9999): loss=2.687460196816589, w0=72.78749999999997, w1=13.584888360304406\n",
      "Gradient Descent(2542/9999): loss=1.7915683709597452, w0=72.78749999999997, w1=13.414426026040244\n",
      "Gradient Descent(2543/9999): loss=2.1808342711105393, w0=72.73124999999996, w1=13.47094830052964\n",
      "Gradient Descent(2544/9999): loss=2.036662124985931, w0=72.89999999999996, w1=13.430505625419137\n",
      "Gradient Descent(2545/9999): loss=2.6259729686108324, w0=73.06874999999997, w1=13.219972966150122\n",
      "Gradient Descent(2546/9999): loss=2.3568186118669225, w0=72.89999999999996, w1=13.519550792383153\n",
      "Gradient Descent(2547/9999): loss=2.1169034541695466, w0=73.01249999999996, w1=13.505144469606238\n",
      "Gradient Descent(2548/9999): loss=2.4850553684962025, w0=73.06874999999997, w1=13.539325164415786\n",
      "Gradient Descent(2549/9999): loss=2.7568281534792876, w0=73.01249999999996, w1=13.299777518024484\n",
      "Gradient Descent(2550/9999): loss=2.0597861930098826, w0=73.01249999999996, w1=13.401086265227491\n",
      "Gradient Descent(2551/9999): loss=1.8751441363284904, w0=72.95624999999995, w1=13.38449937888037\n",
      "Gradient Descent(2552/9999): loss=2.6604667211881945, w0=72.78749999999995, w1=13.370316241455443\n",
      "Gradient Descent(2553/9999): loss=2.204638075939561, w0=73.12499999999996, w1=13.035791105580643\n",
      "Gradient Descent(2554/9999): loss=2.094878042810362, w0=73.18124999999996, w1=12.922591148219222\n",
      "Gradient Descent(2555/9999): loss=2.0486947037197387, w0=73.18124999999996, w1=12.71077101868648\n",
      "Gradient Descent(2556/9999): loss=2.6076374307720633, w0=73.12499999999996, w1=12.916563007779382\n",
      "Gradient Descent(2557/9999): loss=2.3019407854535974, w0=73.12499999999996, w1=12.898298108860422\n",
      "Gradient Descent(2558/9999): loss=2.629608457962669, w0=73.12499999999996, w1=13.015459437783566\n",
      "Gradient Descent(2559/9999): loss=2.163950205202818, w0=73.06874999999995, w1=12.916624400373443\n",
      "Gradient Descent(2560/9999): loss=1.7731882679745268, w0=73.01249999999995, w1=13.417431543959138\n",
      "Gradient Descent(2561/9999): loss=2.843178883252715, w0=72.89999999999995, w1=13.391915523304823\n",
      "Gradient Descent(2562/9999): loss=2.3536331618248476, w0=72.78749999999995, w1=13.236952356075069\n",
      "Gradient Descent(2563/9999): loss=2.339115345203199, w0=72.78749999999995, w1=12.966328804167865\n",
      "Gradient Descent(2564/9999): loss=2.832799939901565, w0=73.06874999999995, w1=13.19238322623373\n",
      "Gradient Descent(2565/9999): loss=2.369111149640715, w0=73.40624999999996, w1=13.233010732198041\n",
      "Gradient Descent(2566/9999): loss=1.724082068716404, w0=73.63124999999995, w1=13.348857164171449\n",
      "Gradient Descent(2567/9999): loss=2.1955517194807967, w0=73.51874999999995, w1=13.289438729122393\n",
      "Gradient Descent(2568/9999): loss=2.4623545936196685, w0=73.63124999999995, w1=13.253176554076141\n",
      "Gradient Descent(2569/9999): loss=2.444123483505776, w0=73.51874999999995, w1=13.188336246485983\n",
      "Gradient Descent(2570/9999): loss=2.070073443004919, w0=73.23749999999995, w1=13.224473159232877\n",
      "Gradient Descent(2571/9999): loss=2.3972762769260174, w0=73.18124999999995, w1=13.33738941140124\n",
      "Gradient Descent(2572/9999): loss=2.0746938215675637, w0=73.23749999999995, w1=13.112423512155564\n",
      "Gradient Descent(2573/9999): loss=1.826063658068788, w0=73.23749999999995, w1=13.319696300879396\n",
      "Gradient Descent(2574/9999): loss=2.3100082053877267, w0=73.18124999999995, w1=13.31987050039126\n",
      "Gradient Descent(2575/9999): loss=1.9040046346693122, w0=73.12499999999994, w1=13.51973049836561\n",
      "Gradient Descent(2576/9999): loss=2.531980680221252, w0=73.01249999999995, w1=13.544034654642903\n",
      "Gradient Descent(2577/9999): loss=1.7871485408953016, w0=72.95624999999994, w1=13.839198944808683\n",
      "Gradient Descent(2578/9999): loss=1.7930390432757068, w0=72.84374999999994, w1=13.762226435075364\n",
      "Gradient Descent(2579/9999): loss=1.7855934048918785, w0=72.73124999999995, w1=14.018874487577481\n",
      "Gradient Descent(2580/9999): loss=1.8469431560757676, w0=72.89999999999995, w1=14.063977238769162\n",
      "Gradient Descent(2581/9999): loss=2.4243000411080144, w0=73.06874999999995, w1=13.9790267902258\n",
      "Gradient Descent(2582/9999): loss=2.399630187077216, w0=72.95624999999995, w1=13.615367030001714\n",
      "Gradient Descent(2583/9999): loss=2.3816346829043153, w0=72.78749999999995, w1=13.818292742430458\n",
      "Gradient Descent(2584/9999): loss=2.1627722265244795, w0=72.95624999999995, w1=13.699181189029417\n",
      "Gradient Descent(2585/9999): loss=2.4395870741455727, w0=72.84374999999996, w1=13.598198798968399\n",
      "Gradient Descent(2586/9999): loss=1.8830564123829503, w0=72.78749999999995, w1=13.251843538939909\n",
      "Gradient Descent(2587/9999): loss=2.0258000051275036, w0=73.01249999999995, w1=13.03572117102547\n",
      "Gradient Descent(2588/9999): loss=2.116006726554824, w0=72.89999999999995, w1=13.141972095231818\n",
      "Gradient Descent(2589/9999): loss=2.2806498631981493, w0=72.73124999999995, w1=13.319516348993051\n",
      "Gradient Descent(2590/9999): loss=1.9034487724467697, w0=72.50624999999995, w1=13.216081494553809\n",
      "Gradient Descent(2591/9999): loss=1.7021852600842438, w0=72.50624999999995, w1=13.288958184364551\n",
      "Gradient Descent(2592/9999): loss=2.13216796959024, w0=72.67499999999995, w1=13.41855305474474\n",
      "Gradient Descent(2593/9999): loss=2.21849913787514, w0=72.84374999999996, w1=13.482677032895133\n",
      "Gradient Descent(2594/9999): loss=1.9660261638529692, w0=72.95624999999995, w1=13.616154423670205\n",
      "Gradient Descent(2595/9999): loss=2.4123654889432133, w0=73.01249999999996, w1=13.711328585916307\n",
      "Gradient Descent(2596/9999): loss=2.25711325533345, w0=73.06874999999997, w1=13.87663806199019\n",
      "Gradient Descent(2597/9999): loss=1.8490731779834948, w0=73.12499999999997, w1=13.903092749269533\n",
      "Gradient Descent(2598/9999): loss=2.600101067531523, w0=73.12499999999997, w1=13.70032030503262\n",
      "Gradient Descent(2599/9999): loss=1.7744902435895562, w0=73.18124999999998, w1=13.81533779055566\n",
      "Gradient Descent(2600/9999): loss=2.26476939275389, w0=73.12499999999997, w1=13.985515654210861\n",
      "Gradient Descent(2601/9999): loss=2.239403946732955, w0=73.06874999999997, w1=13.684929402103291\n",
      "Gradient Descent(2602/9999): loss=2.645157101231713, w0=72.95624999999997, w1=13.764069220251589\n",
      "Gradient Descent(2603/9999): loss=2.2530569109591316, w0=72.95624999999997, w1=13.936863506301947\n",
      "Gradient Descent(2604/9999): loss=2.234792726967852, w0=72.95624999999997, w1=14.19093900580758\n",
      "Gradient Descent(2605/9999): loss=2.059551722041679, w0=72.95624999999997, w1=14.024268063761381\n",
      "Gradient Descent(2606/9999): loss=1.6800183122779508, w0=73.12499999999997, w1=14.24764417435733\n",
      "Gradient Descent(2607/9999): loss=2.294921442330173, w0=73.06874999999997, w1=14.08985949136165\n",
      "Gradient Descent(2608/9999): loss=2.1988716338886913, w0=73.06874999999997, w1=14.293474739996908\n",
      "Gradient Descent(2609/9999): loss=2.1058084734559737, w0=73.12499999999997, w1=14.219968048516064\n",
      "Gradient Descent(2610/9999): loss=2.332895291282134, w0=73.18124999999998, w1=14.046982981334006\n",
      "Gradient Descent(2611/9999): loss=1.875121777050608, w0=72.95624999999998, w1=14.104545581407903\n",
      "Gradient Descent(2612/9999): loss=2.4947599245216354, w0=72.95624999999998, w1=14.024559411618574\n",
      "Gradient Descent(2613/9999): loss=2.2572525691527674, w0=72.67499999999998, w1=13.686188628980144\n",
      "Gradient Descent(2614/9999): loss=1.7948911659817597, w0=72.56249999999999, w1=13.767878317890707\n",
      "Gradient Descent(2615/9999): loss=2.352751518290695, w0=72.78749999999998, w1=13.708650023077523\n",
      "Gradient Descent(2616/9999): loss=1.442081759168008, w0=73.12499999999999, w1=13.790853226489343\n",
      "Gradient Descent(2617/9999): loss=2.3670170262679924, w0=73.34999999999998, w1=13.564944537081317\n",
      "Gradient Descent(2618/9999): loss=2.187235470212768, w0=73.29374999999997, w1=13.478891424070916\n",
      "Gradient Descent(2619/9999): loss=2.8167219882762504, w0=73.18124999999998, w1=13.408449053454833\n",
      "Gradient Descent(2620/9999): loss=2.8580507843014855, w0=73.29374999999997, w1=13.364263928986702\n",
      "Gradient Descent(2621/9999): loss=1.980995254478739, w0=73.46249999999998, w1=13.61572290644118\n",
      "Gradient Descent(2622/9999): loss=2.7888348162064576, w0=73.01249999999997, w1=13.716787944797716\n",
      "Gradient Descent(2623/9999): loss=2.7205055163453085, w0=73.23749999999997, w1=13.85805404379624\n",
      "Gradient Descent(2624/9999): loss=2.6328662975021775, w0=73.51874999999997, w1=13.757724941133745\n",
      "Gradient Descent(2625/9999): loss=1.9582006495269204, w0=73.34999999999997, w1=13.445195767230537\n",
      "Gradient Descent(2626/9999): loss=2.2813732503525657, w0=73.23749999999997, w1=13.32115748611674\n",
      "Gradient Descent(2627/9999): loss=2.536359435925042, w0=72.84374999999997, w1=13.465065084838336\n",
      "Gradient Descent(2628/9999): loss=2.583383896764561, w0=72.78749999999997, w1=13.62495200855321\n",
      "Gradient Descent(2629/9999): loss=2.531170232524077, w0=72.95624999999997, w1=13.716297110364076\n",
      "Gradient Descent(2630/9999): loss=2.3383893294033244, w0=73.18124999999996, w1=13.377640630992504\n",
      "Gradient Descent(2631/9999): loss=1.8851473406129322, w0=73.40624999999996, w1=13.308425469053816\n",
      "Gradient Descent(2632/9999): loss=2.2407435276599097, w0=72.95624999999995, w1=13.157675311675803\n",
      "Gradient Descent(2633/9999): loss=2.627071719999014, w0=73.29374999999996, w1=13.118259960971205\n",
      "Gradient Descent(2634/9999): loss=2.425199130987729, w0=73.40624999999996, w1=13.070054256193753\n",
      "Gradient Descent(2635/9999): loss=2.3149325958503137, w0=73.57499999999996, w1=13.343098820605997\n",
      "Gradient Descent(2636/9999): loss=1.7275395219383654, w0=73.63124999999997, w1=13.362152566811883\n",
      "Gradient Descent(2637/9999): loss=2.5789846382277046, w0=73.51874999999997, w1=12.940433455852709\n",
      "Gradient Descent(2638/9999): loss=2.086512933638564, w0=73.06874999999997, w1=13.153922553284477\n",
      "Gradient Descent(2639/9999): loss=2.0534237817123002, w0=73.01249999999996, w1=13.049756726378574\n",
      "Gradient Descent(2640/9999): loss=2.2694114124080866, w0=73.18124999999996, w1=13.287981927162202\n",
      "Gradient Descent(2641/9999): loss=2.401697468570431, w0=73.34999999999997, w1=13.321365275691605\n",
      "Gradient Descent(2642/9999): loss=2.211987784681545, w0=73.18124999999996, w1=13.144843447098227\n",
      "Gradient Descent(2643/9999): loss=2.4272028354420048, w0=73.12499999999996, w1=13.204675135580624\n",
      "Gradient Descent(2644/9999): loss=1.8697674466719958, w0=73.12499999999996, w1=13.104910001776581\n",
      "Gradient Descent(2645/9999): loss=2.223883563453403, w0=73.18124999999996, w1=13.099863622024023\n",
      "Gradient Descent(2646/9999): loss=2.114071082126423, w0=73.34999999999997, w1=13.076057221871803\n",
      "Gradient Descent(2647/9999): loss=2.5086811570362717, w0=73.18124999999996, w1=13.13445575123069\n",
      "Gradient Descent(2648/9999): loss=2.6584939342480975, w0=73.06874999999997, w1=13.17466638855655\n",
      "Gradient Descent(2649/9999): loss=2.0540425958420827, w0=73.01249999999996, w1=13.20022719868304\n",
      "Gradient Descent(2650/9999): loss=2.2735530847939627, w0=73.06874999999997, w1=13.36832673653915\n",
      "Gradient Descent(2651/9999): loss=2.275117609863087, w0=72.78749999999997, w1=13.366767314655638\n",
      "Gradient Descent(2652/9999): loss=2.5532225725332207, w0=72.84374999999997, w1=13.654954157323527\n",
      "Gradient Descent(2653/9999): loss=2.2497792722832224, w0=73.06874999999997, w1=13.533931017406555\n",
      "Gradient Descent(2654/9999): loss=1.7632180339975063, w0=73.12499999999997, w1=13.409173241456228\n",
      "Gradient Descent(2655/9999): loss=2.230030389728051, w0=72.89999999999998, w1=13.629290826860862\n",
      "Gradient Descent(2656/9999): loss=1.982716516231494, w0=72.73124999999997, w1=13.730218223072438\n",
      "Gradient Descent(2657/9999): loss=2.035857241609034, w0=72.78749999999998, w1=13.819713660512576\n",
      "Gradient Descent(2658/9999): loss=2.3182381428110026, w0=72.78749999999998, w1=13.81046704541943\n",
      "Gradient Descent(2659/9999): loss=2.473718324764353, w0=73.18124999999998, w1=13.66612527126653\n",
      "Gradient Descent(2660/9999): loss=1.7635910328612026, w0=73.18124999999998, w1=13.947531582518293\n",
      "Gradient Descent(2661/9999): loss=2.713106732920494, w0=72.89999999999998, w1=13.937044012868478\n",
      "Gradient Descent(2662/9999): loss=2.1311335708278487, w0=72.78749999999998, w1=13.891198586411093\n",
      "Gradient Descent(2663/9999): loss=1.897860654182862, w0=72.78749999999998, w1=13.918471890648338\n",
      "Gradient Descent(2664/9999): loss=1.8354985593291788, w0=72.95624999999998, w1=13.892304230407753\n",
      "Gradient Descent(2665/9999): loss=2.2685957488336967, w0=72.95624999999998, w1=13.620362282632497\n",
      "Gradient Descent(2666/9999): loss=1.8852167247019236, w0=72.89999999999998, w1=13.691275850012163\n",
      "Gradient Descent(2667/9999): loss=2.0336485223742855, w0=73.12499999999997, w1=13.989073909238506\n",
      "Gradient Descent(2668/9999): loss=2.1739808035666357, w0=73.23749999999997, w1=13.91088827010182\n",
      "Gradient Descent(2669/9999): loss=2.1965884053527356, w0=73.34999999999997, w1=14.074405594691981\n",
      "Gradient Descent(2670/9999): loss=2.084739734935531, w0=73.06874999999997, w1=14.289054646066653\n",
      "Gradient Descent(2671/9999): loss=2.2591819867285294, w0=72.78749999999997, w1=13.934886993269123\n",
      "Gradient Descent(2672/9999): loss=1.963432424202106, w0=73.29374999999996, w1=13.826144691182131\n",
      "Gradient Descent(2673/9999): loss=2.2770073831926596, w0=73.12499999999996, w1=14.022819868494981\n",
      "Gradient Descent(2674/9999): loss=2.2452683725220393, w0=72.89999999999996, w1=13.741478387695821\n",
      "Gradient Descent(2675/9999): loss=2.6171984818091687, w0=73.06874999999997, w1=13.690875837350038\n",
      "Gradient Descent(2676/9999): loss=2.724773713964508, w0=73.29374999999996, w1=13.542549833764486\n",
      "Gradient Descent(2677/9999): loss=1.3952909034372059, w0=73.18124999999996, w1=13.320640489265152\n",
      "Gradient Descent(2678/9999): loss=2.665001477327053, w0=73.18124999999996, w1=13.533261556964188\n",
      "Gradient Descent(2679/9999): loss=2.685697174652731, w0=73.29374999999996, w1=13.488922084088712\n",
      "Gradient Descent(2680/9999): loss=2.3419642652942767, w0=73.23749999999995, w1=13.643245790639014\n",
      "Gradient Descent(2681/9999): loss=2.3278637550004606, w0=72.95624999999995, w1=13.59111748971242\n",
      "Gradient Descent(2682/9999): loss=2.2955953025000904, w0=72.89999999999995, w1=13.565785933447719\n",
      "Gradient Descent(2683/9999): loss=1.964122794468044, w0=73.01249999999995, w1=13.459632519729992\n",
      "Gradient Descent(2684/9999): loss=2.1682589771615564, w0=73.40624999999994, w1=13.496207931364667\n",
      "Gradient Descent(2685/9999): loss=1.9400111804880098, w0=73.34999999999994, w1=13.437699745602309\n",
      "Gradient Descent(2686/9999): loss=2.4507905107367374, w0=73.12499999999994, w1=13.463715786515937\n",
      "Gradient Descent(2687/9999): loss=2.183426571995907, w0=73.18124999999995, w1=13.555966946147626\n",
      "Gradient Descent(2688/9999): loss=2.3384864125609357, w0=73.40624999999994, w1=13.11433852277303\n",
      "Gradient Descent(2689/9999): loss=2.033905225118879, w0=73.23749999999994, w1=13.438712000122008\n",
      "Gradient Descent(2690/9999): loss=2.364272725163837, w0=73.51874999999994, w1=13.314228008615435\n",
      "Gradient Descent(2691/9999): loss=2.39745174255403, w0=73.29374999999995, w1=13.34056994481779\n",
      "Gradient Descent(2692/9999): loss=2.413927475553499, w0=73.29374999999995, w1=13.404119533710446\n",
      "Gradient Descent(2693/9999): loss=2.416879820958489, w0=73.23749999999994, w1=13.435758913993016\n",
      "Gradient Descent(2694/9999): loss=2.2679131911576174, w0=73.01249999999995, w1=13.53418847015843\n",
      "Gradient Descent(2695/9999): loss=1.844320046638002, w0=73.12499999999994, w1=13.56768206003268\n",
      "Gradient Descent(2696/9999): loss=1.934054195242973, w0=73.29374999999995, w1=13.614265707460264\n",
      "Gradient Descent(2697/9999): loss=2.002408505509881, w0=73.29374999999995, w1=13.481746247102814\n",
      "Gradient Descent(2698/9999): loss=2.487895973618982, w0=73.23749999999994, w1=13.426445610998572\n",
      "Gradient Descent(2699/9999): loss=2.3560665110238164, w0=73.40624999999994, w1=13.645136358627123\n",
      "Gradient Descent(2700/9999): loss=2.2942761849163844, w0=73.40624999999994, w1=13.460418531452964\n",
      "Gradient Descent(2701/9999): loss=2.164824759456264, w0=73.51874999999994, w1=13.492846395201145\n",
      "Gradient Descent(2702/9999): loss=2.8794463895107, w0=73.40624999999994, w1=13.390536798431802\n",
      "Gradient Descent(2703/9999): loss=2.368757569851798, w0=73.46249999999995, w1=13.306994416069758\n",
      "Gradient Descent(2704/9999): loss=2.39024185937426, w0=73.34999999999995, w1=13.218088999831796\n",
      "Gradient Descent(2705/9999): loss=2.4042921442306873, w0=73.40624999999996, w1=13.416525126375927\n",
      "Gradient Descent(2706/9999): loss=1.9020015052224148, w0=73.40624999999996, w1=13.150321425087652\n",
      "Gradient Descent(2707/9999): loss=2.4784770707105177, w0=73.51874999999995, w1=13.28801526579654\n",
      "Gradient Descent(2708/9999): loss=1.577822043266941, w0=73.29374999999996, w1=13.365123019098386\n",
      "Gradient Descent(2709/9999): loss=2.5704029478547215, w0=73.01249999999996, w1=13.268327229402345\n",
      "Gradient Descent(2710/9999): loss=2.204455633795041, w0=73.12499999999996, w1=13.455296438297964\n",
      "Gradient Descent(2711/9999): loss=2.281840915081543, w0=73.01249999999996, w1=13.25385655592186\n",
      "Gradient Descent(2712/9999): loss=1.4355792552528746, w0=72.95624999999995, w1=13.411969483608306\n",
      "Gradient Descent(2713/9999): loss=2.3261271799336, w0=72.95624999999995, w1=13.497694815914702\n",
      "Gradient Descent(2714/9999): loss=2.6694104814889767, w0=73.12499999999996, w1=13.472442669449453\n",
      "Gradient Descent(2715/9999): loss=2.2792149410524383, w0=73.29374999999996, w1=13.410512282637496\n",
      "Gradient Descent(2716/9999): loss=1.8711243351812537, w0=73.34999999999997, w1=13.725071192122265\n",
      "Gradient Descent(2717/9999): loss=2.1376495067463432, w0=73.57499999999996, w1=13.915259390422534\n",
      "Gradient Descent(2718/9999): loss=2.044916279218636, w0=73.23749999999995, w1=14.15216452823951\n",
      "Gradient Descent(2719/9999): loss=2.1591692168189773, w0=73.29374999999996, w1=14.119900250956494\n",
      "Gradient Descent(2720/9999): loss=2.054278596519956, w0=73.34999999999997, w1=13.989442717249911\n",
      "Gradient Descent(2721/9999): loss=2.6500779937621792, w0=73.57499999999996, w1=14.017379322064137\n",
      "Gradient Descent(2722/9999): loss=2.079567036570775, w0=73.85624999999996, w1=13.962153099398016\n",
      "Gradient Descent(2723/9999): loss=1.8547271360745785, w0=73.91249999999997, w1=13.673610075814688\n",
      "Gradient Descent(2724/9999): loss=1.789247375188624, w0=73.91249999999997, w1=13.710663887031911\n",
      "Gradient Descent(2725/9999): loss=2.401573087274147, w0=73.68749999999997, w1=13.994848794288325\n",
      "Gradient Descent(2726/9999): loss=2.0299171951619845, w0=73.74374999999998, w1=13.870570496554647\n",
      "Gradient Descent(2727/9999): loss=2.8380999327122147, w0=73.63124999999998, w1=13.84261619965247\n",
      "Gradient Descent(2728/9999): loss=2.3042952514588784, w0=73.51874999999998, w1=13.667756585398546\n",
      "Gradient Descent(2729/9999): loss=2.4098694594996424, w0=73.63124999999998, w1=13.593778157298972\n",
      "Gradient Descent(2730/9999): loss=1.9271633912050203, w0=73.63124999999998, w1=13.472165643602647\n",
      "Gradient Descent(2731/9999): loss=2.557205334862118, w0=73.51874999999998, w1=13.429587558268997\n",
      "Gradient Descent(2732/9999): loss=2.3083343946642145, w0=73.40624999999999, w1=13.627588754592614\n",
      "Gradient Descent(2733/9999): loss=2.1735325488703925, w0=73.46249999999999, w1=13.502790915588106\n",
      "Gradient Descent(2734/9999): loss=2.616920953274352, w0=73.35, w1=13.79421521140874\n",
      "Gradient Descent(2735/9999): loss=2.009242887815657, w0=73.2375, w1=13.749838555455526\n",
      "Gradient Descent(2736/9999): loss=2.1224722940869984, w0=72.84375, w1=13.814648813585087\n",
      "Gradient Descent(2737/9999): loss=1.9850669935151677, w0=72.7875, w1=13.716064556706174\n",
      "Gradient Descent(2738/9999): loss=2.400721823262214, w0=72.7875, w1=13.633297181093386\n",
      "Gradient Descent(2739/9999): loss=2.2185840618942914, w0=73.01249999999999, w1=13.331601522106755\n",
      "Gradient Descent(2740/9999): loss=1.9911785235469353, w0=72.84374999999999, w1=13.407452102389453\n",
      "Gradient Descent(2741/9999): loss=2.6217959251523855, w0=72.78749999999998, w1=13.339330145349406\n",
      "Gradient Descent(2742/9999): loss=2.6157304832702986, w0=72.89999999999998, w1=13.813169423169843\n",
      "Gradient Descent(2743/9999): loss=2.200197020955496, w0=72.84374999999997, w1=14.11356937035662\n",
      "Gradient Descent(2744/9999): loss=2.2956280659942365, w0=73.12499999999997, w1=13.833085415113962\n",
      "Gradient Descent(2745/9999): loss=2.13361276144672, w0=73.01249999999997, w1=13.873290882960292\n",
      "Gradient Descent(2746/9999): loss=2.8445273249829084, w0=73.18124999999998, w1=13.59816267231084\n",
      "Gradient Descent(2747/9999): loss=2.123864036587625, w0=73.01249999999997, w1=13.567093749751477\n",
      "Gradient Descent(2748/9999): loss=2.354971812764064, w0=73.12499999999997, w1=13.321967700939583\n",
      "Gradient Descent(2749/9999): loss=2.6244746509272634, w0=73.12499999999997, w1=13.033818285957816\n",
      "Gradient Descent(2750/9999): loss=2.3238558341655944, w0=73.18124999999998, w1=13.185944332094492\n",
      "Gradient Descent(2751/9999): loss=2.5699933017894616, w0=73.23749999999998, w1=12.846717952453417\n",
      "Gradient Descent(2752/9999): loss=2.6323308221494934, w0=73.29374999999999, w1=13.077636760210309\n",
      "Gradient Descent(2753/9999): loss=2.848212408665411, w0=73.35, w1=12.853346505592715\n",
      "Gradient Descent(2754/9999): loss=2.0732303991400958, w0=73.35, w1=13.15552913588245\n",
      "Gradient Descent(2755/9999): loss=2.4962812576889037, w0=73.57499999999999, w1=12.723501076440003\n",
      "Gradient Descent(2756/9999): loss=2.1211047153868012, w0=73.06875, w1=13.131442096367783\n",
      "Gradient Descent(2757/9999): loss=1.9543123103687114, w0=73.01249999999999, w1=13.168535745925944\n",
      "Gradient Descent(2758/9999): loss=1.941545429940994, w0=72.95624999999998, w1=13.315898160698959\n",
      "Gradient Descent(2759/9999): loss=1.970257869738647, w0=72.84374999999999, w1=13.525397909145362\n",
      "Gradient Descent(2760/9999): loss=2.3708840098014843, w0=72.73124999999999, w1=13.580424231754717\n",
      "Gradient Descent(2761/9999): loss=2.125811171191832, w0=72.7875, w1=13.32658047688923\n",
      "Gradient Descent(2762/9999): loss=2.7393259946265225, w0=73.01249999999999, w1=13.377744302592241\n",
      "Gradient Descent(2763/9999): loss=1.8276729342346811, w0=73.12499999999999, w1=13.328823460535808\n",
      "Gradient Descent(2764/9999): loss=2.040923321562129, w0=73.12499999999999, w1=13.281170635324093\n",
      "Gradient Descent(2765/9999): loss=2.151572831692898, w0=73.18124999999999, w1=13.119992929104043\n",
      "Gradient Descent(2766/9999): loss=2.7095767424639217, w0=72.89999999999999, w1=13.101352523510025\n",
      "Gradient Descent(2767/9999): loss=2.1051909653132217, w0=72.95625, w1=13.277513087479738\n",
      "Gradient Descent(2768/9999): loss=2.984823899689802, w0=72.84375, w1=13.408258160580846\n",
      "Gradient Descent(2769/9999): loss=2.376882519940515, w0=72.7875, w1=13.431460839807766\n",
      "Gradient Descent(2770/9999): loss=1.94393983111469, w0=72.7875, w1=13.363357327912821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2771/9999): loss=2.4232477366100262, w0=72.84375, w1=13.298737591664729\n",
      "Gradient Descent(2772/9999): loss=2.111499531473723, w0=73.06875, w1=13.270780809193546\n",
      "Gradient Descent(2773/9999): loss=1.8664337164626637, w0=73.18124999999999, w1=13.380340056385679\n",
      "Gradient Descent(2774/9999): loss=2.253234664092295, w0=73.01249999999999, w1=13.529803005180549\n",
      "Gradient Descent(2775/9999): loss=2.1855768612530295, w0=72.89999999999999, w1=13.608354577206452\n",
      "Gradient Descent(2776/9999): loss=2.4840260396197493, w0=72.84374999999999, w1=13.72909450222737\n",
      "Gradient Descent(2777/9999): loss=2.477028947412588, w0=73.01249999999999, w1=13.969947747288316\n",
      "Gradient Descent(2778/9999): loss=1.9307270729082278, w0=72.84374999999999, w1=13.94492376663958\n",
      "Gradient Descent(2779/9999): loss=2.5096630916281706, w0=72.78749999999998, w1=13.621915422541818\n",
      "Gradient Descent(2780/9999): loss=2.3669718797659787, w0=72.56249999999999, w1=13.539152990269821\n",
      "Gradient Descent(2781/9999): loss=2.756733909219572, w0=72.67499999999998, w1=13.582301961221004\n",
      "Gradient Descent(2782/9999): loss=1.971879216585727, w0=72.44999999999999, w1=13.708569078164462\n",
      "Gradient Descent(2783/9999): loss=1.7858726189366938, w0=72.39374999999998, w1=13.740398545622368\n",
      "Gradient Descent(2784/9999): loss=2.4742453590522056, w0=72.56249999999999, w1=13.245136333684824\n",
      "Gradient Descent(2785/9999): loss=2.125085603358907, w0=72.73124999999999, w1=13.086284567965404\n",
      "Gradient Descent(2786/9999): loss=2.222803410801535, w0=72.7875, w1=13.154068537873949\n",
      "Gradient Descent(2787/9999): loss=1.9786976222286656, w0=72.73124999999999, w1=13.3416529757226\n",
      "Gradient Descent(2788/9999): loss=2.259204063062716, w0=72.95624999999998, w1=13.575178657166674\n",
      "Gradient Descent(2789/9999): loss=2.6371916511809035, w0=73.12499999999999, w1=13.671106474019686\n",
      "Gradient Descent(2790/9999): loss=2.7038237094756137, w0=73.63124999999998, w1=13.67173235366453\n",
      "Gradient Descent(2791/9999): loss=2.3358027148813543, w0=73.57499999999997, w1=13.585925059567959\n",
      "Gradient Descent(2792/9999): loss=2.254193257906491, w0=73.46249999999998, w1=13.509226196520153\n",
      "Gradient Descent(2793/9999): loss=2.0560757338020323, w0=73.57499999999997, w1=13.472772010527576\n",
      "Gradient Descent(2794/9999): loss=2.458540699862314, w0=73.57499999999997, w1=13.693849083830484\n",
      "Gradient Descent(2795/9999): loss=1.977831612929965, w0=73.57499999999997, w1=13.974776186929622\n",
      "Gradient Descent(2796/9999): loss=2.422947582000824, w0=73.74374999999998, w1=13.890461201194093\n",
      "Gradient Descent(2797/9999): loss=2.657933536663799, w0=73.51874999999998, w1=13.920939797107511\n",
      "Gradient Descent(2798/9999): loss=1.9368363348932531, w0=73.34999999999998, w1=13.905966911620789\n",
      "Gradient Descent(2799/9999): loss=2.281319908047917, w0=73.23749999999998, w1=14.045001757158968\n",
      "Gradient Descent(2800/9999): loss=2.296634409408696, w0=73.29374999999999, w1=13.930791383637848\n",
      "Gradient Descent(2801/9999): loss=2.5026372548445295, w0=73.51874999999998, w1=13.807651188397749\n",
      "Gradient Descent(2802/9999): loss=1.9748279465337706, w0=73.68749999999999, w1=13.9071362891816\n",
      "Gradient Descent(2803/9999): loss=2.018519920197704, w0=73.68749999999999, w1=13.839379741702482\n",
      "Gradient Descent(2804/9999): loss=2.2435664787191696, w0=73.63124999999998, w1=13.443826813375496\n",
      "Gradient Descent(2805/9999): loss=2.424101943123679, w0=73.51874999999998, w1=13.512970238436191\n",
      "Gradient Descent(2806/9999): loss=2.310186377946819, w0=73.68749999999999, w1=13.533065826608194\n",
      "Gradient Descent(2807/9999): loss=2.1990405127204617, w0=73.57499999999999, w1=13.662075775753095\n",
      "Gradient Descent(2808/9999): loss=2.375336024324106, w0=73.57499999999999, w1=13.434380705258254\n",
      "Gradient Descent(2809/9999): loss=1.9900752406205129, w0=73.35, w1=13.495200620011357\n",
      "Gradient Descent(2810/9999): loss=2.3363802277393195, w0=73.40625, w1=13.404874801003016\n",
      "Gradient Descent(2811/9999): loss=1.754530770828869, w0=73.29375, w1=13.341000064344092\n",
      "Gradient Descent(2812/9999): loss=1.9466346874435863, w0=73.29375, w1=13.13172019040756\n",
      "Gradient Descent(2813/9999): loss=1.7862757026659337, w0=73.51875, w1=13.217053554039413\n",
      "Gradient Descent(2814/9999): loss=2.25943584437277, w0=73.85625, w1=13.253440128859243\n",
      "Gradient Descent(2815/9999): loss=2.595558767131392, w0=73.96875, w1=13.297656978566891\n",
      "Gradient Descent(2816/9999): loss=1.9755132463784495, w0=74.025, w1=13.44554780707939\n",
      "Gradient Descent(2817/9999): loss=2.08308226345196, w0=73.96875, w1=13.158307863617482\n",
      "Gradient Descent(2818/9999): loss=2.1694726097776478, w0=73.85625, w1=13.132792607223266\n",
      "Gradient Descent(2819/9999): loss=2.4172882431255664, w0=73.85625, w1=12.982534517447382\n",
      "Gradient Descent(2820/9999): loss=2.1304195266008605, w0=73.74375, w1=13.082017787968423\n",
      "Gradient Descent(2821/9999): loss=2.1981119422849775, w0=73.74375, w1=13.000887115871398\n",
      "Gradient Descent(2822/9999): loss=1.9518730796112223, w0=73.63125000000001, w1=13.319205206312882\n",
      "Gradient Descent(2823/9999): loss=2.2255850868727114, w0=73.91250000000001, w1=13.595203978630348\n",
      "Gradient Descent(2824/9999): loss=2.2959366908509002, w0=73.96875000000001, w1=13.614245445435866\n",
      "Gradient Descent(2825/9999): loss=2.3234445534908703, w0=73.63125000000001, w1=13.80269244961742\n",
      "Gradient Descent(2826/9999): loss=2.544620160173884, w0=73.18125, w1=13.74298579836033\n",
      "Gradient Descent(2827/9999): loss=2.0069479017073846, w0=73.29375, w1=13.838994840318062\n",
      "Gradient Descent(2828/9999): loss=2.4613499526830567, w0=73.4625, w1=13.680615301987286\n",
      "Gradient Descent(2829/9999): loss=2.118892021896068, w0=73.29375, w1=13.786008067260004\n",
      "Gradient Descent(2830/9999): loss=2.256458863446261, w0=73.35000000000001, w1=13.750439483777539\n",
      "Gradient Descent(2831/9999): loss=1.9415492197956112, w0=73.40625000000001, w1=13.838650496828631\n",
      "Gradient Descent(2832/9999): loss=2.0409431627614447, w0=73.57500000000002, w1=13.956229306697589\n",
      "Gradient Descent(2833/9999): loss=2.0113956353820672, w0=73.23750000000001, w1=14.216289490866695\n",
      "Gradient Descent(2834/9999): loss=2.650384722078308, w0=73.06875000000001, w1=14.154980683478833\n",
      "Gradient Descent(2835/9999): loss=2.6051930125447553, w0=73.18125, w1=13.931345798057684\n",
      "Gradient Descent(2836/9999): loss=1.7901219700558835, w0=72.95625000000001, w1=13.684051457868502\n",
      "Gradient Descent(2837/9999): loss=2.6319017276317473, w0=73.12500000000001, w1=13.951382977556275\n",
      "Gradient Descent(2838/9999): loss=2.507323302305268, w0=73.18125000000002, w1=13.834952806814572\n",
      "Gradient Descent(2839/9999): loss=2.44862527088472, w0=72.90000000000002, w1=14.047571285973723\n",
      "Gradient Descent(2840/9999): loss=1.8827685675722696, w0=72.73125000000002, w1=14.212200233261967\n",
      "Gradient Descent(2841/9999): loss=2.9444560095879693, w0=72.67500000000001, w1=14.050401732838816\n",
      "Gradient Descent(2842/9999): loss=1.875486098761603, w0=72.67500000000001, w1=13.959382468380264\n",
      "Gradient Descent(2843/9999): loss=1.8296142122431225, w0=72.73125000000002, w1=13.934506590892529\n",
      "Gradient Descent(2844/9999): loss=2.697407537319984, w0=72.78750000000002, w1=13.695153547945827\n",
      "Gradient Descent(2845/9999): loss=2.0258897621630814, w0=72.67500000000003, w1=13.813124986493623\n",
      "Gradient Descent(2846/9999): loss=1.628693457451282, w0=72.61875000000002, w1=13.969025994363115\n",
      "Gradient Descent(2847/9999): loss=1.8374024949151142, w0=72.73125000000002, w1=13.78605585004342\n",
      "Gradient Descent(2848/9999): loss=2.0832613893930096, w0=72.61875000000002, w1=13.506234597835059\n",
      "Gradient Descent(2849/9999): loss=2.6285156390618356, w0=72.84375000000001, w1=13.416100125205368\n",
      "Gradient Descent(2850/9999): loss=2.331379858584163, w0=72.67500000000001, w1=13.440629401997356\n",
      "Gradient Descent(2851/9999): loss=2.0290926872143094, w0=72.39375000000001, w1=13.50079361874803\n",
      "Gradient Descent(2852/9999): loss=2.759609407033874, w0=72.50625000000001, w1=13.48264656248954\n",
      "Gradient Descent(2853/9999): loss=1.9914886107268066, w0=72.73125, w1=13.43926098638864\n",
      "Gradient Descent(2854/9999): loss=1.9340220384703637, w0=72.73125, w1=13.139998240683589\n",
      "Gradient Descent(2855/9999): loss=2.556579264380816, w0=72.95625, w1=13.095307275973068\n",
      "Gradient Descent(2856/9999): loss=2.3582804289941697, w0=72.95625, w1=13.158621972800619\n",
      "Gradient Descent(2857/9999): loss=2.063778147583945, w0=73.06875, w1=13.190698677307449\n",
      "Gradient Descent(2858/9999): loss=2.149839429639181, w0=73.29374999999999, w1=13.090488044261944\n",
      "Gradient Descent(2859/9999): loss=2.661724148640363, w0=73.35, w1=13.180047946615948\n",
      "Gradient Descent(2860/9999): loss=2.352338749864593, w0=73.46249999999999, w1=12.976455304501199\n",
      "Gradient Descent(2861/9999): loss=2.666375201292987, w0=73.40624999999999, w1=13.085033973218174\n",
      "Gradient Descent(2862/9999): loss=2.2308282365292, w0=73.46249999999999, w1=13.130579799980953\n",
      "Gradient Descent(2863/9999): loss=2.0930249846495204, w0=73.51875, w1=13.136590359710828\n",
      "Gradient Descent(2864/9999): loss=2.0290973333173463, w0=73.2375, w1=13.463462275808581\n",
      "Gradient Descent(2865/9999): loss=2.093288923039352, w0=73.29375, w1=13.358459379898898\n",
      "Gradient Descent(2866/9999): loss=2.1830618653677063, w0=73.63125000000001, w1=13.217892878997594\n",
      "Gradient Descent(2867/9999): loss=1.9505854998441439, w0=73.68750000000001, w1=13.10730977955354\n",
      "Gradient Descent(2868/9999): loss=2.2828511427412908, w0=73.46250000000002, w1=13.291151900217596\n",
      "Gradient Descent(2869/9999): loss=2.6345504971850153, w0=73.63125000000002, w1=13.354729734547348\n",
      "Gradient Descent(2870/9999): loss=2.1785587416356442, w0=73.63125000000002, w1=13.584604282999617\n",
      "Gradient Descent(2871/9999): loss=2.533741690941173, w0=73.80000000000003, w1=13.530905518757718\n",
      "Gradient Descent(2872/9999): loss=2.2694213460057, w0=73.68750000000003, w1=13.300381468090958\n",
      "Gradient Descent(2873/9999): loss=2.882557264021169, w0=73.85625000000003, w1=13.434179891761401\n",
      "Gradient Descent(2874/9999): loss=2.3666542232318433, w0=73.68750000000003, w1=13.434701720253184\n",
      "Gradient Descent(2875/9999): loss=2.0536588132759963, w0=73.57500000000003, w1=13.482764011097466\n",
      "Gradient Descent(2876/9999): loss=2.481464057908159, w0=73.57500000000003, w1=13.429986871899581\n",
      "Gradient Descent(2877/9999): loss=2.2812271919427918, w0=73.68750000000003, w1=13.501484374254838\n",
      "Gradient Descent(2878/9999): loss=2.0532472379825117, w0=73.68750000000003, w1=13.435763238913228\n",
      "Gradient Descent(2879/9999): loss=1.7035089768215406, w0=73.46250000000003, w1=13.223672410192297\n",
      "Gradient Descent(2880/9999): loss=1.9104528563951175, w0=73.29375000000003, w1=13.078894686816234\n",
      "Gradient Descent(2881/9999): loss=2.509737117942934, w0=73.51875000000003, w1=13.127368185181208\n",
      "Gradient Descent(2882/9999): loss=2.417696527736771, w0=73.74375000000002, w1=13.076672519394071\n",
      "Gradient Descent(2883/9999): loss=1.805522580820966, w0=73.46250000000002, w1=12.967771355426843\n",
      "Gradient Descent(2884/9999): loss=2.16114771868358, w0=73.40625000000001, w1=12.952724327881503\n",
      "Gradient Descent(2885/9999): loss=1.8112350820040923, w0=73.40625000000001, w1=13.163387911703838\n",
      "Gradient Descent(2886/9999): loss=2.38527105436826, w0=73.63125000000001, w1=13.080474264624947\n",
      "Gradient Descent(2887/9999): loss=2.623294293115319, w0=73.63125000000001, w1=13.260882849592875\n",
      "Gradient Descent(2888/9999): loss=2.3419329182948467, w0=73.68750000000001, w1=13.458511207499685\n",
      "Gradient Descent(2889/9999): loss=2.475716930948269, w0=73.29375000000002, w1=13.551899781265538\n",
      "Gradient Descent(2890/9999): loss=2.267432118128732, w0=73.35000000000002, w1=13.513581464299975\n",
      "Gradient Descent(2891/9999): loss=2.2843840669111626, w0=73.68750000000003, w1=13.808283724577578\n",
      "Gradient Descent(2892/9999): loss=2.1963846220681127, w0=73.91250000000002, w1=13.669428939210947\n",
      "Gradient Descent(2893/9999): loss=2.2307782465102823, w0=74.08125000000003, w1=13.298278310636121\n",
      "Gradient Descent(2894/9999): loss=2.0645130587042226, w0=74.02500000000002, w1=13.365491453158121\n",
      "Gradient Descent(2895/9999): loss=1.5568329691814708, w0=73.74375000000002, w1=13.329242141314158\n",
      "Gradient Descent(2896/9999): loss=2.1387109916334, w0=73.63125000000002, w1=13.309831787491582\n",
      "Gradient Descent(2897/9999): loss=2.130548482343401, w0=73.35000000000002, w1=13.336931276198431\n",
      "Gradient Descent(2898/9999): loss=1.8400125632391908, w0=73.46250000000002, w1=13.369999043174856\n",
      "Gradient Descent(2899/9999): loss=1.8849276883180797, w0=73.51875000000003, w1=13.486193471765374\n",
      "Gradient Descent(2900/9999): loss=2.105317674786785, w0=73.46250000000002, w1=13.611419675111863\n",
      "Gradient Descent(2901/9999): loss=2.1006638029607334, w0=73.74375000000002, w1=13.645550007292716\n",
      "Gradient Descent(2902/9999): loss=2.4391555641567675, w0=73.46250000000002, w1=13.636642938930889\n",
      "Gradient Descent(2903/9999): loss=1.9375123114592103, w0=73.29375000000002, w1=13.395630394055898\n",
      "Gradient Descent(2904/9999): loss=2.010872910730186, w0=73.40625000000001, w1=13.241705345743195\n",
      "Gradient Descent(2905/9999): loss=2.061056303362543, w0=73.35000000000001, w1=13.278689096314107\n",
      "Gradient Descent(2906/9999): loss=1.8505555394983686, w0=73.35000000000001, w1=13.392882149403905\n",
      "Gradient Descent(2907/9999): loss=2.921999331753116, w0=73.35000000000001, w1=13.411712963516797\n",
      "Gradient Descent(2908/9999): loss=1.7594532268588032, w0=73.12500000000001, w1=13.33377787322383\n",
      "Gradient Descent(2909/9999): loss=2.132348450183896, w0=72.95625000000001, w1=13.241890075058455\n",
      "Gradient Descent(2910/9999): loss=2.2692321364978487, w0=73.06875000000001, w1=13.261217894842478\n",
      "Gradient Descent(2911/9999): loss=2.773427973969107, w0=73.35000000000001, w1=13.297941597613024\n",
      "Gradient Descent(2912/9999): loss=2.3464762790130473, w0=73.29375, w1=13.386968396725363\n",
      "Gradient Descent(2913/9999): loss=2.558323770693315, w0=73.2375, w1=13.406520868610471\n",
      "Gradient Descent(2914/9999): loss=2.3014730299122514, w0=73.18124999999999, w1=13.384457411266771\n",
      "Gradient Descent(2915/9999): loss=2.1847801572690373, w0=73.12499999999999, w1=13.213420529462411\n",
      "Gradient Descent(2916/9999): loss=1.4616191981625433, w0=73.06874999999998, w1=13.255755231367083\n",
      "Gradient Descent(2917/9999): loss=1.9957342068691049, w0=73.18124999999998, w1=13.463118377480745\n",
      "Gradient Descent(2918/9999): loss=2.141297839473948, w0=73.34999999999998, w1=13.51302027935447\n",
      "Gradient Descent(2919/9999): loss=1.9222440353399006, w0=73.12499999999999, w1=13.55825460254176\n",
      "Gradient Descent(2920/9999): loss=2.2230670932638397, w0=73.29374999999999, w1=13.78701861486553\n",
      "Gradient Descent(2921/9999): loss=1.8747552070606432, w0=73.23749999999998, w1=13.31964422719063\n",
      "Gradient Descent(2922/9999): loss=1.6037901179001994, w0=73.01249999999999, w1=13.13800518597047\n",
      "Gradient Descent(2923/9999): loss=1.989912225562151, w0=73.06875, w1=13.05926113638568\n",
      "Gradient Descent(2924/9999): loss=1.476834732565754, w0=72.89999999999999, w1=13.144422412198324\n",
      "Gradient Descent(2925/9999): loss=2.439312822521496, w0=72.7875, w1=13.007062015149796\n",
      "Gradient Descent(2926/9999): loss=2.666600407772889, w0=72.84375, w1=13.152789715656542\n",
      "Gradient Descent(2927/9999): loss=2.0481577751559366, w0=72.84375, w1=13.04760059438904\n",
      "Gradient Descent(2928/9999): loss=2.2886717103675824, w0=73.18125, w1=13.036414477264087\n",
      "Gradient Descent(2929/9999): loss=2.831032270595126, w0=73.23750000000001, w1=13.03432521412845\n",
      "Gradient Descent(2930/9999): loss=2.0741239904864432, w0=73.29375000000002, w1=12.817314730104371\n",
      "Gradient Descent(2931/9999): loss=1.9856050218961658, w0=73.35000000000002, w1=12.969748270073703\n",
      "Gradient Descent(2932/9999): loss=2.1253742304776253, w0=73.29375000000002, w1=12.980857988232927\n",
      "Gradient Descent(2933/9999): loss=2.48213814136023, w0=73.29375000000002, w1=13.189730255094831\n",
      "Gradient Descent(2934/9999): loss=2.6484078154849913, w0=73.18125000000002, w1=13.493528911550483\n",
      "Gradient Descent(2935/9999): loss=2.1121173883277944, w0=73.06875000000002, w1=13.592313455889487\n",
      "Gradient Descent(2936/9999): loss=2.1461538777818374, w0=72.95625000000003, w1=13.747910556450755\n",
      "Gradient Descent(2937/9999): loss=2.1828242499848116, w0=72.84375000000003, w1=14.015500025414656\n",
      "Gradient Descent(2938/9999): loss=2.4724792978100534, w0=72.90000000000003, w1=13.76369682242305\n",
      "Gradient Descent(2939/9999): loss=2.5571094688166696, w0=72.90000000000003, w1=13.729424143957488\n",
      "Gradient Descent(2940/9999): loss=2.0414448000273744, w0=72.90000000000003, w1=13.82344860336658\n",
      "Gradient Descent(2941/9999): loss=1.9094278525923571, w0=72.84375000000003, w1=13.82808510491687\n",
      "Gradient Descent(2942/9999): loss=2.0037064576407246, w0=72.95625000000003, w1=13.805224654456705\n",
      "Gradient Descent(2943/9999): loss=2.6464790872951225, w0=72.95625000000003, w1=13.718269883768308\n",
      "Gradient Descent(2944/9999): loss=2.5924052172297634, w0=72.95625000000003, w1=13.820824756564102\n",
      "Gradient Descent(2945/9999): loss=2.214271466774777, w0=73.01250000000003, w1=13.967505219202206\n",
      "Gradient Descent(2946/9999): loss=2.175334978195053, w0=73.01250000000003, w1=14.124303199673278\n",
      "Gradient Descent(2947/9999): loss=1.8901310337614678, w0=73.06875000000004, w1=14.06105048057607\n",
      "Gradient Descent(2948/9999): loss=2.690954210967913, w0=73.12500000000004, w1=13.920153094809827\n",
      "Gradient Descent(2949/9999): loss=2.307853529714892, w0=73.06875000000004, w1=13.66181568439349\n",
      "Gradient Descent(2950/9999): loss=2.398108709419115, w0=72.84375000000004, w1=13.331255384218071\n",
      "Gradient Descent(2951/9999): loss=2.167237188530618, w0=73.01250000000005, w1=13.2460879711992\n",
      "Gradient Descent(2952/9999): loss=2.3799294656911645, w0=73.06875000000005, w1=13.33446687093979\n",
      "Gradient Descent(2953/9999): loss=2.376784730094587, w0=73.12500000000006, w1=13.611954156824007\n",
      "Gradient Descent(2954/9999): loss=2.295793863059783, w0=73.23750000000005, w1=13.409044337708453\n",
      "Gradient Descent(2955/9999): loss=2.172777164143457, w0=73.57500000000006, w1=13.24040657952223\n",
      "Gradient Descent(2956/9999): loss=2.065554042396386, w0=73.68750000000006, w1=13.135263605850572\n",
      "Gradient Descent(2957/9999): loss=1.942120172959337, w0=73.85625000000006, w1=12.885516483950315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(2958/9999): loss=2.1702579735276397, w0=73.63125000000007, w1=13.055754284074885\n",
      "Gradient Descent(2959/9999): loss=2.1979353676468305, w0=73.63125000000007, w1=12.958337194920054\n",
      "Gradient Descent(2960/9999): loss=2.2119419964583003, w0=73.63125000000007, w1=13.14807728572649\n",
      "Gradient Descent(2961/9999): loss=2.246507696576452, w0=73.63125000000007, w1=13.366335551425003\n",
      "Gradient Descent(2962/9999): loss=2.7643595693875724, w0=73.46250000000006, w1=13.42753196029778\n",
      "Gradient Descent(2963/9999): loss=2.5631794758672433, w0=73.40625000000006, w1=13.229830320680948\n",
      "Gradient Descent(2964/9999): loss=2.4646246343180067, w0=73.23750000000005, w1=13.224640619531375\n",
      "Gradient Descent(2965/9999): loss=2.090106952736538, w0=73.29375000000006, w1=13.239872059374177\n",
      "Gradient Descent(2966/9999): loss=2.500265645628991, w0=73.35000000000007, w1=13.12788869600722\n",
      "Gradient Descent(2967/9999): loss=2.2474296540164183, w0=73.23750000000007, w1=13.623581009902706\n",
      "Gradient Descent(2968/9999): loss=2.611598495141934, w0=73.18125000000006, w1=13.72261319126363\n",
      "Gradient Descent(2969/9999): loss=2.2663380847871206, w0=73.12500000000006, w1=13.684350158865431\n",
      "Gradient Descent(2970/9999): loss=2.0794874645771273, w0=72.78750000000005, w1=13.773262987625593\n",
      "Gradient Descent(2971/9999): loss=1.7695108961616015, w0=72.84375000000006, w1=13.784227543703558\n",
      "Gradient Descent(2972/9999): loss=2.0444962113400633, w0=72.90000000000006, w1=13.77193350000683\n",
      "Gradient Descent(2973/9999): loss=2.409013083145049, w0=72.56250000000006, w1=13.990812839408715\n",
      "Gradient Descent(2974/9999): loss=2.226153436275278, w0=72.73125000000006, w1=14.051555586450007\n",
      "Gradient Descent(2975/9999): loss=2.1152513224063982, w0=72.67500000000005, w1=13.8245495497693\n",
      "Gradient Descent(2976/9999): loss=1.8835485953157427, w0=72.73125000000006, w1=13.434918395000551\n",
      "Gradient Descent(2977/9999): loss=2.4309184645473643, w0=72.73125000000006, w1=13.43770861258442\n",
      "Gradient Descent(2978/9999): loss=2.05086723769302, w0=72.73125000000006, w1=13.522003422373102\n",
      "Gradient Descent(2979/9999): loss=2.3601873105518716, w0=72.73125000000006, w1=13.663886151996207\n",
      "Gradient Descent(2980/9999): loss=2.5462509675540224, w0=72.90000000000006, w1=13.626181429206836\n",
      "Gradient Descent(2981/9999): loss=2.2292190345975036, w0=72.95625000000007, w1=13.440022057171966\n",
      "Gradient Descent(2982/9999): loss=2.3334211751149514, w0=72.90000000000006, w1=13.613745344299893\n",
      "Gradient Descent(2983/9999): loss=2.1354897105975956, w0=72.90000000000006, w1=13.439216216831499\n",
      "Gradient Descent(2984/9999): loss=2.3646905137402436, w0=72.73125000000006, w1=13.408296611868188\n",
      "Gradient Descent(2985/9999): loss=1.9618510486391072, w0=72.73125000000006, w1=13.503153995338208\n",
      "Gradient Descent(2986/9999): loss=2.119955976280507, w0=73.01250000000006, w1=13.662144272028005\n",
      "Gradient Descent(2987/9999): loss=2.202594092204605, w0=73.18125000000006, w1=13.808482063984306\n",
      "Gradient Descent(2988/9999): loss=1.964505309853262, w0=73.01250000000006, w1=14.056496279363799\n",
      "Gradient Descent(2989/9999): loss=2.67846376094057, w0=72.78750000000007, w1=13.60664326493619\n",
      "Gradient Descent(2990/9999): loss=2.1960188452525893, w0=72.84375000000007, w1=13.697720014553784\n",
      "Gradient Descent(2991/9999): loss=1.710619228105616, w0=73.06875000000007, w1=13.796494644165758\n",
      "Gradient Descent(2992/9999): loss=2.2282684734497136, w0=73.12500000000007, w1=13.850522439344331\n",
      "Gradient Descent(2993/9999): loss=2.1392943981491337, w0=73.01250000000007, w1=13.543921608050907\n",
      "Gradient Descent(2994/9999): loss=1.9633780245481531, w0=73.06875000000008, w1=13.749794251385174\n",
      "Gradient Descent(2995/9999): loss=2.6693681530449274, w0=73.06875000000008, w1=13.684783422423454\n",
      "Gradient Descent(2996/9999): loss=2.465657677526509, w0=73.12500000000009, w1=13.497532751960186\n",
      "Gradient Descent(2997/9999): loss=2.2474063320864666, w0=73.01250000000009, w1=13.524663191200933\n",
      "Gradient Descent(2998/9999): loss=2.072101588898196, w0=73.23750000000008, w1=13.53874483336232\n",
      "Gradient Descent(2999/9999): loss=2.5750168748634916, w0=73.18125000000008, w1=13.310134824718725\n",
      "Gradient Descent(3000/9999): loss=2.4869893111905634, w0=73.23750000000008, w1=13.357438886386133\n",
      "Gradient Descent(3001/9999): loss=2.048869891681167, w0=73.35000000000008, w1=13.36391439145937\n",
      "Gradient Descent(3002/9999): loss=2.1688958326730923, w0=73.18125000000008, w1=13.618124695305129\n",
      "Gradient Descent(3003/9999): loss=2.036885136588289, w0=73.06875000000008, w1=13.708544846318754\n",
      "Gradient Descent(3004/9999): loss=2.717970431781838, w0=73.06875000000008, w1=13.657273161351323\n",
      "Gradient Descent(3005/9999): loss=1.8371309182100366, w0=72.95625000000008, w1=13.881398582826407\n",
      "Gradient Descent(3006/9999): loss=2.3539867111599992, w0=73.12500000000009, w1=13.969459338824722\n",
      "Gradient Descent(3007/9999): loss=2.163991639109734, w0=72.84375000000009, w1=14.229075327111397\n",
      "Gradient Descent(3008/9999): loss=1.8778883877471562, w0=72.78750000000008, w1=14.214339530311792\n",
      "Gradient Descent(3009/9999): loss=2.073605911989005, w0=72.78750000000008, w1=14.053611498587527\n",
      "Gradient Descent(3010/9999): loss=2.337290766378081, w0=73.01250000000007, w1=14.147884211605758\n",
      "Gradient Descent(3011/9999): loss=2.1952147542939806, w0=73.18125000000008, w1=14.116405673589092\n",
      "Gradient Descent(3012/9999): loss=2.8047180596800794, w0=73.40625000000007, w1=14.33441708249849\n",
      "Gradient Descent(3013/9999): loss=2.04039609060698, w0=73.06875000000007, w1=13.960153121895399\n",
      "Gradient Descent(3014/9999): loss=2.4722765467106043, w0=73.06875000000007, w1=13.765462268417277\n",
      "Gradient Descent(3015/9999): loss=2.0696595988769477, w0=73.12500000000007, w1=13.691152615253145\n",
      "Gradient Descent(3016/9999): loss=1.944339814573448, w0=72.95625000000007, w1=13.687964778624973\n",
      "Gradient Descent(3017/9999): loss=1.9817846257411416, w0=72.90000000000006, w1=13.582576570062004\n",
      "Gradient Descent(3018/9999): loss=2.638882275237963, w0=73.01250000000006, w1=13.404120824283089\n",
      "Gradient Descent(3019/9999): loss=1.9100410217432262, w0=73.12500000000006, w1=13.44049107015468\n",
      "Gradient Descent(3020/9999): loss=1.7863361922924406, w0=73.06875000000005, w1=13.44001150737573\n",
      "Gradient Descent(3021/9999): loss=2.4162886311795475, w0=73.51875000000005, w1=13.392142347818295\n",
      "Gradient Descent(3022/9999): loss=2.166328461792302, w0=73.40625000000006, w1=13.229399323319376\n",
      "Gradient Descent(3023/9999): loss=2.5745096282584212, w0=73.35000000000005, w1=13.256472317202904\n",
      "Gradient Descent(3024/9999): loss=1.9073425307657461, w0=73.46250000000005, w1=13.159136683697925\n",
      "Gradient Descent(3025/9999): loss=2.2903274589070692, w0=73.40625000000004, w1=13.186611358886857\n",
      "Gradient Descent(3026/9999): loss=2.4782484133389073, w0=73.40625000000004, w1=12.785996279651288\n",
      "Gradient Descent(3027/9999): loss=2.575527590480083, w0=73.35000000000004, w1=12.939207776179458\n",
      "Gradient Descent(3028/9999): loss=2.2854663526251358, w0=73.29375000000003, w1=13.155695227582616\n",
      "Gradient Descent(3029/9999): loss=2.088233998243359, w0=73.06875000000004, w1=13.117765006820282\n",
      "Gradient Descent(3030/9999): loss=2.3299484576581677, w0=73.06875000000004, w1=13.052099971279919\n",
      "Gradient Descent(3031/9999): loss=2.0025646206835734, w0=72.84375000000004, w1=13.027346728282046\n",
      "Gradient Descent(3032/9999): loss=2.5841037875239072, w0=72.90000000000005, w1=13.09297216997641\n",
      "Gradient Descent(3033/9999): loss=2.779234861821816, w0=73.06875000000005, w1=13.387244618249605\n",
      "Gradient Descent(3034/9999): loss=2.165466097875849, w0=72.90000000000005, w1=13.649455289560253\n",
      "Gradient Descent(3035/9999): loss=2.1496071875421547, w0=72.84375000000004, w1=13.599530187985948\n",
      "Gradient Descent(3036/9999): loss=2.570903468317179, w0=73.06875000000004, w1=13.527463622240532\n",
      "Gradient Descent(3037/9999): loss=1.957205731302615, w0=73.06875000000004, w1=13.751125977007018\n",
      "Gradient Descent(3038/9999): loss=2.4618822244253415, w0=73.06875000000004, w1=13.593320805261605\n",
      "Gradient Descent(3039/9999): loss=2.4813644198542617, w0=72.78750000000004, w1=13.810865256539392\n",
      "Gradient Descent(3040/9999): loss=2.4098743345097144, w0=72.84375000000004, w1=13.660560571369404\n",
      "Gradient Descent(3041/9999): loss=1.4617853763936264, w0=72.67500000000004, w1=13.580168815592435\n",
      "Gradient Descent(3042/9999): loss=2.5615323407889914, w0=72.84375000000004, w1=13.902839379511526\n",
      "Gradient Descent(3043/9999): loss=1.9950587494692549, w0=73.18125000000005, w1=13.686396251538207\n",
      "Gradient Descent(3044/9999): loss=2.386872581368004, w0=73.12500000000004, w1=13.788382855389989\n",
      "Gradient Descent(3045/9999): loss=2.4934316580753624, w0=73.01250000000005, w1=13.666461910322313\n",
      "Gradient Descent(3046/9999): loss=2.589310774756569, w0=73.06875000000005, w1=13.723374598833468\n",
      "Gradient Descent(3047/9999): loss=1.9723644180093933, w0=73.12500000000006, w1=13.757738378492636\n",
      "Gradient Descent(3048/9999): loss=2.0216168437561284, w0=72.84375000000006, w1=13.616912022619315\n",
      "Gradient Descent(3049/9999): loss=2.053957086830992, w0=72.78750000000005, w1=13.608374627561792\n",
      "Gradient Descent(3050/9999): loss=2.7338336206407963, w0=72.67500000000005, w1=13.43242672718216\n",
      "Gradient Descent(3051/9999): loss=2.116246851003487, w0=72.61875000000005, w1=13.524791997612468\n",
      "Gradient Descent(3052/9999): loss=1.7591203746908777, w0=72.45000000000005, w1=13.6651491895453\n",
      "Gradient Descent(3053/9999): loss=2.6911813176429997, w0=72.56250000000004, w1=13.43190437037358\n",
      "Gradient Descent(3054/9999): loss=1.8745375653194358, w0=72.56250000000004, w1=13.563268096352589\n",
      "Gradient Descent(3055/9999): loss=2.499829919192905, w0=72.73125000000005, w1=13.505271408180118\n",
      "Gradient Descent(3056/9999): loss=2.0257622460327473, w0=72.73125000000005, w1=13.583653097518773\n",
      "Gradient Descent(3057/9999): loss=2.3940939002786243, w0=72.73125000000005, w1=13.483611991153097\n",
      "Gradient Descent(3058/9999): loss=1.6778955529796804, w0=72.67500000000004, w1=13.543531342862515\n",
      "Gradient Descent(3059/9999): loss=2.5330706630882927, w0=72.90000000000003, w1=13.633543612302926\n",
      "Gradient Descent(3060/9999): loss=2.3909100029729893, w0=72.78750000000004, w1=13.604079989574302\n",
      "Gradient Descent(3061/9999): loss=2.169951908037722, w0=72.95625000000004, w1=13.437392344977795\n",
      "Gradient Descent(3062/9999): loss=2.2269973666834533, w0=72.78750000000004, w1=13.211886870983873\n",
      "Gradient Descent(3063/9999): loss=1.9536562996238125, w0=72.95625000000004, w1=13.260055105055295\n",
      "Gradient Descent(3064/9999): loss=1.8899876903401267, w0=73.06875000000004, w1=13.2448885370445\n",
      "Gradient Descent(3065/9999): loss=1.8639521696670793, w0=72.90000000000003, w1=13.561213848700959\n",
      "Gradient Descent(3066/9999): loss=2.445835338067245, w0=72.84375000000003, w1=13.748503601822804\n",
      "Gradient Descent(3067/9999): loss=1.994520504378955, w0=72.84375000000003, w1=13.611705648596994\n",
      "Gradient Descent(3068/9999): loss=2.3183308876447493, w0=72.84375000000003, w1=13.479316613558046\n",
      "Gradient Descent(3069/9999): loss=2.182291404401849, w0=72.84375000000003, w1=13.601792384825833\n",
      "Gradient Descent(3070/9999): loss=2.2521827604892506, w0=73.01250000000003, w1=13.783699302615581\n",
      "Gradient Descent(3071/9999): loss=2.087496179639092, w0=73.06875000000004, w1=13.581011389266774\n",
      "Gradient Descent(3072/9999): loss=2.300637820896796, w0=73.18125000000003, w1=13.468089705378915\n",
      "Gradient Descent(3073/9999): loss=1.8713890871186287, w0=73.23750000000004, w1=13.53180812124352\n",
      "Gradient Descent(3074/9999): loss=2.2044159762512754, w0=73.06875000000004, w1=13.445707533013803\n",
      "Gradient Descent(3075/9999): loss=2.164284251816762, w0=73.18125000000003, w1=13.574727544933104\n",
      "Gradient Descent(3076/9999): loss=2.3797418732083795, w0=73.01250000000003, w1=13.398781786482294\n",
      "Gradient Descent(3077/9999): loss=1.9884291028745702, w0=72.78750000000004, w1=13.170764107123395\n",
      "Gradient Descent(3078/9999): loss=2.277994284671036, w0=72.84375000000004, w1=13.132707930854055\n",
      "Gradient Descent(3079/9999): loss=2.2736315586614273, w0=73.29375000000005, w1=13.124385517098617\n",
      "Gradient Descent(3080/9999): loss=1.6270870241513156, w0=73.23750000000004, w1=13.039953698157214\n",
      "Gradient Descent(3081/9999): loss=2.246908826139844, w0=73.12500000000004, w1=13.133904959978809\n",
      "Gradient Descent(3082/9999): loss=1.9543405008413945, w0=73.29375000000005, w1=13.098988538926116\n",
      "Gradient Descent(3083/9999): loss=2.0130523799839093, w0=73.35000000000005, w1=12.734206749565018\n",
      "Gradient Descent(3084/9999): loss=2.6367992251058467, w0=72.95625000000005, w1=12.556385512912811\n",
      "Gradient Descent(3085/9999): loss=1.7442101266017707, w0=72.84375000000006, w1=12.632135835261174\n",
      "Gradient Descent(3086/9999): loss=2.1845977582243195, w0=73.12500000000006, w1=13.064451924535165\n",
      "Gradient Descent(3087/9999): loss=2.4026281621146306, w0=72.95625000000005, w1=13.128396932345863\n",
      "Gradient Descent(3088/9999): loss=2.110617277493869, w0=73.12500000000006, w1=13.238354088660147\n",
      "Gradient Descent(3089/9999): loss=2.104164226387658, w0=73.12500000000006, w1=13.353364967112434\n",
      "Gradient Descent(3090/9999): loss=2.1772794512570144, w0=73.18125000000006, w1=13.387570881333005\n",
      "Gradient Descent(3091/9999): loss=1.9930404833687039, w0=72.95625000000007, w1=13.251840980789957\n",
      "Gradient Descent(3092/9999): loss=1.9791140804745482, w0=73.01250000000007, w1=13.150599654319963\n",
      "Gradient Descent(3093/9999): loss=1.713399511574857, w0=73.29375000000007, w1=13.298725834879852\n",
      "Gradient Descent(3094/9999): loss=2.798026188937467, w0=73.46250000000008, w1=13.112618820477076\n",
      "Gradient Descent(3095/9999): loss=2.4236896091603466, w0=73.57500000000007, w1=13.100988363569261\n",
      "Gradient Descent(3096/9999): loss=2.5034964353595788, w0=73.57500000000007, w1=13.362590593834858\n",
      "Gradient Descent(3097/9999): loss=1.942426674072405, w0=73.51875000000007, w1=13.584684529475275\n",
      "Gradient Descent(3098/9999): loss=2.5036465823976566, w0=73.57500000000007, w1=13.722458501964459\n",
      "Gradient Descent(3099/9999): loss=2.019628793876179, w0=73.57500000000007, w1=13.398300722989926\n",
      "Gradient Descent(3100/9999): loss=2.073824349378718, w0=73.35000000000008, w1=13.468156990237361\n",
      "Gradient Descent(3101/9999): loss=1.955094240265044, w0=73.40625000000009, w1=13.206802283895142\n",
      "Gradient Descent(3102/9999): loss=1.7772929075481383, w0=73.35000000000008, w1=13.335794391645758\n",
      "Gradient Descent(3103/9999): loss=2.3274397995626472, w0=73.18125000000008, w1=13.101635374839104\n",
      "Gradient Descent(3104/9999): loss=2.3827529869147015, w0=73.12500000000007, w1=13.031736504267004\n",
      "Gradient Descent(3105/9999): loss=2.1016413920262726, w0=73.18125000000008, w1=13.153246182625981\n",
      "Gradient Descent(3106/9999): loss=1.8548885400497999, w0=73.23750000000008, w1=13.206735513829702\n",
      "Gradient Descent(3107/9999): loss=2.2219012360915418, w0=73.46250000000008, w1=13.020894722862629\n",
      "Gradient Descent(3108/9999): loss=2.916766392632945, w0=73.46250000000008, w1=13.000197657046927\n",
      "Gradient Descent(3109/9999): loss=1.9604453618568038, w0=73.57500000000007, w1=12.948008950761675\n",
      "Gradient Descent(3110/9999): loss=2.397309691005284, w0=73.51875000000007, w1=13.108634622341315\n",
      "Gradient Descent(3111/9999): loss=2.0425353789199177, w0=73.68750000000007, w1=13.326107320435828\n",
      "Gradient Descent(3112/9999): loss=2.286783260519984, w0=73.85625000000007, w1=13.049153916074802\n",
      "Gradient Descent(3113/9999): loss=1.8536365499945853, w0=73.68750000000007, w1=13.128046978493893\n",
      "Gradient Descent(3114/9999): loss=2.567142524253569, w0=73.57500000000007, w1=13.346257887771298\n",
      "Gradient Descent(3115/9999): loss=1.7118572202700952, w0=73.57500000000007, w1=13.535432742685895\n",
      "Gradient Descent(3116/9999): loss=2.2408181184666267, w0=73.63125000000008, w1=13.610279138173711\n",
      "Gradient Descent(3117/9999): loss=2.2856352159277584, w0=73.51875000000008, w1=13.506734292467499\n",
      "Gradient Descent(3118/9999): loss=2.369211461807207, w0=73.51875000000008, w1=13.545157073667898\n",
      "Gradient Descent(3119/9999): loss=2.279214792671448, w0=73.80000000000008, w1=13.675166078774414\n",
      "Gradient Descent(3120/9999): loss=2.245429248718917, w0=73.85625000000009, w1=13.683151332264602\n",
      "Gradient Descent(3121/9999): loss=2.4309983667523722, w0=73.85625000000009, w1=13.770188964602083\n",
      "Gradient Descent(3122/9999): loss=1.5906624985810405, w0=73.68750000000009, w1=13.737782106724937\n",
      "Gradient Descent(3123/9999): loss=2.033573999107941, w0=73.68750000000009, w1=13.986187900343474\n",
      "Gradient Descent(3124/9999): loss=2.36383040829663, w0=73.74375000000009, w1=13.886897851848513\n",
      "Gradient Descent(3125/9999): loss=2.4551265617913884, w0=73.8000000000001, w1=13.995063051864879\n",
      "Gradient Descent(3126/9999): loss=2.2613440160634366, w0=73.74375000000009, w1=14.001926641365417\n",
      "Gradient Descent(3127/9999): loss=2.5191598960745054, w0=73.57500000000009, w1=14.036577770357619\n",
      "Gradient Descent(3128/9999): loss=1.9586546611120055, w0=73.74375000000009, w1=14.071083920428345\n",
      "Gradient Descent(3129/9999): loss=2.0581487876507074, w0=73.5187500000001, w1=13.967140420408672\n",
      "Gradient Descent(3130/9999): loss=2.951990499857187, w0=73.4062500000001, w1=13.849301670068435\n",
      "Gradient Descent(3131/9999): loss=2.4509859719725107, w0=73.3500000000001, w1=13.598983781642538\n",
      "Gradient Descent(3132/9999): loss=1.6726109364144301, w0=73.3500000000001, w1=13.672730559937273\n",
      "Gradient Descent(3133/9999): loss=2.095468597709086, w0=73.29375000000009, w1=13.758931157941543\n",
      "Gradient Descent(3134/9999): loss=1.9354009179206266, w0=73.51875000000008, w1=13.59456311170749\n",
      "Gradient Descent(3135/9999): loss=2.864495001651986, w0=73.23750000000008, w1=13.71235403653342\n",
      "Gradient Descent(3136/9999): loss=2.3532232512629, w0=73.35000000000008, w1=13.77090208113679\n",
      "Gradient Descent(3137/9999): loss=2.551657585733028, w0=73.18125000000008, w1=13.654745416047879\n",
      "Gradient Descent(3138/9999): loss=1.9612355689803125, w0=73.12500000000007, w1=13.680925252528906\n",
      "Gradient Descent(3139/9999): loss=2.4067524927377164, w0=72.90000000000008, w1=13.55664076709889\n",
      "Gradient Descent(3140/9999): loss=2.6016927188750953, w0=73.01250000000007, w1=13.726308705526828\n",
      "Gradient Descent(3141/9999): loss=1.926991952683303, w0=73.23750000000007, w1=13.2956079353735\n",
      "Gradient Descent(3142/9999): loss=1.8733139340518399, w0=73.18125000000006, w1=13.245570883859873\n",
      "Gradient Descent(3143/9999): loss=1.9002308466017925, w0=72.90000000000006, w1=13.12010075390143\n",
      "Gradient Descent(3144/9999): loss=2.446298763912157, w0=73.12500000000006, w1=13.199212716250395\n",
      "Gradient Descent(3145/9999): loss=2.511955113540342, w0=73.35000000000005, w1=13.274566962116037\n",
      "Gradient Descent(3146/9999): loss=2.29263218883617, w0=73.18125000000005, w1=13.45517175683523\n",
      "Gradient Descent(3147/9999): loss=2.5319851521098453, w0=73.29375000000005, w1=13.536454391983163\n",
      "Gradient Descent(3148/9999): loss=2.4009750840628263, w0=73.23750000000004, w1=13.748729420759604\n",
      "Gradient Descent(3149/9999): loss=2.7563312879577753, w0=73.29375000000005, w1=13.56883018583892\n",
      "Gradient Descent(3150/9999): loss=2.134393640253836, w0=73.46250000000005, w1=13.5860839808363\n",
      "Gradient Descent(3151/9999): loss=2.5969958788288854, w0=73.35000000000005, w1=13.6506943786625\n",
      "Gradient Descent(3152/9999): loss=2.477761185891034, w0=73.23750000000005, w1=13.67858477062144\n",
      "Gradient Descent(3153/9999): loss=2.118732042407683, w0=73.35000000000005, w1=13.645350815969872\n",
      "Gradient Descent(3154/9999): loss=2.6446948020822565, w0=73.46250000000005, w1=13.504857118445532\n",
      "Gradient Descent(3155/9999): loss=2.132686712683017, w0=73.06875000000005, w1=13.610451133982027\n",
      "Gradient Descent(3156/9999): loss=2.2167619517378316, w0=73.23750000000005, w1=13.530415033622118\n",
      "Gradient Descent(3157/9999): loss=2.475511389011108, w0=72.84375000000006, w1=13.47730903551284\n",
      "Gradient Descent(3158/9999): loss=2.355159008863322, w0=72.61875000000006, w1=13.339784966630333\n",
      "Gradient Descent(3159/9999): loss=2.3913098990035864, w0=72.45000000000006, w1=13.04026120632931\n",
      "Gradient Descent(3160/9999): loss=1.9141975147931123, w0=72.56250000000006, w1=12.887089892896766\n",
      "Gradient Descent(3161/9999): loss=2.6712204298588516, w0=72.61875000000006, w1=12.987820094633856\n",
      "Gradient Descent(3162/9999): loss=2.3157550906549704, w0=72.90000000000006, w1=13.197361418716888\n",
      "Gradient Descent(3163/9999): loss=2.4371049226181016, w0=73.12500000000006, w1=13.066086460701124\n",
      "Gradient Descent(3164/9999): loss=2.538573356999816, w0=73.18125000000006, w1=13.21555194230673\n",
      "Gradient Descent(3165/9999): loss=2.274854603550076, w0=73.06875000000007, w1=13.09399449910575\n",
      "Gradient Descent(3166/9999): loss=2.0197541549794042, w0=73.40625000000007, w1=13.497801626959413\n",
      "Gradient Descent(3167/9999): loss=2.2750860021686847, w0=73.35000000000007, w1=13.68093656733248\n",
      "Gradient Descent(3168/9999): loss=2.1924482420395757, w0=73.35000000000007, w1=13.650487096707835\n",
      "Gradient Descent(3169/9999): loss=2.3897419618325433, w0=73.29375000000006, w1=13.668928465272103\n",
      "Gradient Descent(3170/9999): loss=2.5642102643688416, w0=73.29375000000006, w1=13.807660855472596\n",
      "Gradient Descent(3171/9999): loss=2.1647716527880014, w0=73.18125000000006, w1=13.945094291507084\n",
      "Gradient Descent(3172/9999): loss=2.565253093717967, w0=73.18125000000006, w1=13.959514934891823\n",
      "Gradient Descent(3173/9999): loss=2.651901781266691, w0=72.95625000000007, w1=13.749524008959188\n",
      "Gradient Descent(3174/9999): loss=2.575576855349185, w0=73.23750000000007, w1=13.495653531483647\n",
      "Gradient Descent(3175/9999): loss=1.8460985666072027, w0=72.95625000000007, w1=13.471691697422948\n",
      "Gradient Descent(3176/9999): loss=2.0712621141521104, w0=73.06875000000007, w1=13.221853202403308\n",
      "Gradient Descent(3177/9999): loss=2.337577855768372, w0=73.12500000000007, w1=13.1651656596437\n",
      "Gradient Descent(3178/9999): loss=2.772057971535972, w0=72.84375000000007, w1=13.237231502237329\n",
      "Gradient Descent(3179/9999): loss=2.2316012060449073, w0=72.67500000000007, w1=13.21879419511586\n",
      "Gradient Descent(3180/9999): loss=2.372927537150515, w0=72.50625000000007, w1=13.277201606476197\n",
      "Gradient Descent(3181/9999): loss=2.100354564694463, w0=72.73125000000006, w1=13.545329049272816\n",
      "Gradient Descent(3182/9999): loss=2.6464406579392366, w0=72.95625000000005, w1=13.415303884911056\n",
      "Gradient Descent(3183/9999): loss=1.8606401868304892, w0=72.90000000000005, w1=13.467467672025847\n",
      "Gradient Descent(3184/9999): loss=2.271580631670627, w0=72.56250000000004, w1=13.949127398115824\n",
      "Gradient Descent(3185/9999): loss=2.0011868504550576, w0=72.61875000000005, w1=13.932932687624003\n",
      "Gradient Descent(3186/9999): loss=1.5730052678183792, w0=72.90000000000005, w1=13.547242693122065\n",
      "Gradient Descent(3187/9999): loss=2.5719117039352435, w0=72.95625000000005, w1=13.69617925821835\n",
      "Gradient Descent(3188/9999): loss=2.5424294565739074, w0=73.23750000000005, w1=13.986851195942611\n",
      "Gradient Descent(3189/9999): loss=2.3511770172788014, w0=73.18125000000005, w1=14.085064312954733\n",
      "Gradient Descent(3190/9999): loss=2.425482799809374, w0=72.95625000000005, w1=13.742238179726005\n",
      "Gradient Descent(3191/9999): loss=2.111594750027365, w0=72.78750000000005, w1=13.780168293978953\n",
      "Gradient Descent(3192/9999): loss=1.7315675741660428, w0=72.73125000000005, w1=13.715476447434588\n",
      "Gradient Descent(3193/9999): loss=2.167262828450306, w0=72.67500000000004, w1=13.680488675389292\n",
      "Gradient Descent(3194/9999): loss=2.0288842361423995, w0=72.73125000000005, w1=13.775995511375704\n",
      "Gradient Descent(3195/9999): loss=2.499894070529325, w0=72.84375000000004, w1=13.526638590152428\n",
      "Gradient Descent(3196/9999): loss=1.9296451410174102, w0=73.01250000000005, w1=13.378688823595555\n",
      "Gradient Descent(3197/9999): loss=2.795886381591295, w0=73.01250000000005, w1=13.587702686866036\n",
      "Gradient Descent(3198/9999): loss=2.034249676509397, w0=73.29375000000005, w1=13.487962357935\n",
      "Gradient Descent(3199/9999): loss=2.105103342364819, w0=73.57500000000005, w1=13.51826474195763\n",
      "Gradient Descent(3200/9999): loss=2.3057193180219397, w0=73.51875000000004, w1=13.29337571077899\n",
      "Gradient Descent(3201/9999): loss=1.6168599488753512, w0=73.40625000000004, w1=13.35820494258514\n",
      "Gradient Descent(3202/9999): loss=2.0981377854585013, w0=73.06875000000004, w1=13.30385062540313\n",
      "Gradient Descent(3203/9999): loss=2.0858756575878, w0=72.73125000000003, w1=13.205564059194204\n",
      "Gradient Descent(3204/9999): loss=2.2721406330964236, w0=72.78750000000004, w1=13.205993064202822\n",
      "Gradient Descent(3205/9999): loss=2.250148283667684, w0=72.73125000000003, w1=13.399922227170842\n",
      "Gradient Descent(3206/9999): loss=2.5035645193691125, w0=73.01250000000003, w1=13.378295856624401\n",
      "Gradient Descent(3207/9999): loss=2.0188194898649185, w0=72.78750000000004, w1=13.130809820068757\n",
      "Gradient Descent(3208/9999): loss=1.8938840415741065, w0=72.84375000000004, w1=13.049018534645956\n",
      "Gradient Descent(3209/9999): loss=2.105953705002288, w0=72.61875000000005, w1=13.22874795441048\n",
      "Gradient Descent(3210/9999): loss=1.9750875746685297, w0=72.56250000000004, w1=13.219762101411808\n",
      "Gradient Descent(3211/9999): loss=2.192931171338612, w0=72.61875000000005, w1=13.329781353939675\n",
      "Gradient Descent(3212/9999): loss=2.1328174694414956, w0=72.84375000000004, w1=13.39642328887184\n",
      "Gradient Descent(3213/9999): loss=2.439036275448828, w0=72.95625000000004, w1=13.521919451221933\n",
      "Gradient Descent(3214/9999): loss=1.8795021569429875, w0=72.95625000000004, w1=13.631192792764047\n",
      "Gradient Descent(3215/9999): loss=2.245662822059613, w0=72.90000000000003, w1=13.833864391303651\n",
      "Gradient Descent(3216/9999): loss=2.578945031638244, w0=72.73125000000003, w1=13.593019636719918\n",
      "Gradient Descent(3217/9999): loss=2.03469283064328, w0=72.73125000000003, w1=13.638305144331257\n",
      "Gradient Descent(3218/9999): loss=2.6524002344824886, w0=72.67500000000003, w1=13.584380088327324\n",
      "Gradient Descent(3219/9999): loss=2.369395896694474, w0=72.67500000000003, w1=13.7455372098814\n",
      "Gradient Descent(3220/9999): loss=1.6957752694255843, w0=72.45000000000003, w1=13.60750362806082\n",
      "Gradient Descent(3221/9999): loss=2.2604465528132693, w0=72.78750000000004, w1=13.327147639362991\n",
      "Gradient Descent(3222/9999): loss=2.631430540816859, w0=73.06875000000004, w1=13.43155210449087\n",
      "Gradient Descent(3223/9999): loss=2.019786062615924, w0=73.06875000000004, w1=13.475261478232936\n",
      "Gradient Descent(3224/9999): loss=2.2743559136158806, w0=73.23750000000004, w1=13.652399640196673\n",
      "Gradient Descent(3225/9999): loss=2.246825637824068, w0=73.23750000000004, w1=13.5801998398298\n",
      "Gradient Descent(3226/9999): loss=2.099479705736814, w0=73.23750000000004, w1=13.420638021580658\n",
      "Gradient Descent(3227/9999): loss=2.751244893046787, w0=73.35000000000004, w1=13.195773059149376\n",
      "Gradient Descent(3228/9999): loss=2.481411670461145, w0=73.46250000000003, w1=13.115300373371923\n",
      "Gradient Descent(3229/9999): loss=2.1666756725406677, w0=73.12500000000003, w1=13.337360397584666\n",
      "Gradient Descent(3230/9999): loss=1.5579296597598529, w0=73.46250000000003, w1=13.577370577963363\n",
      "Gradient Descent(3231/9999): loss=2.4257213667385735, w0=73.35000000000004, w1=13.51182203433718\n",
      "Gradient Descent(3232/9999): loss=2.0464145024964107, w0=73.51875000000004, w1=13.364529029389333\n",
      "Gradient Descent(3233/9999): loss=2.8704959555689262, w0=73.40625000000004, w1=13.523266867967708\n",
      "Gradient Descent(3234/9999): loss=2.3482437346997607, w0=73.35000000000004, w1=13.647445695662215\n",
      "Gradient Descent(3235/9999): loss=2.509681304340421, w0=73.35000000000004, w1=13.717039535226451\n",
      "Gradient Descent(3236/9999): loss=2.6210335669114535, w0=73.68750000000004, w1=13.923383359934087\n",
      "Gradient Descent(3237/9999): loss=2.6238900418784548, w0=73.57500000000005, w1=14.102894907921801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3238/9999): loss=2.019182956288253, w0=73.57500000000005, w1=14.351934475674312\n",
      "Gradient Descent(3239/9999): loss=2.3687155591035847, w0=73.63125000000005, w1=14.05213981970132\n",
      "Gradient Descent(3240/9999): loss=2.4506845832565416, w0=73.46250000000005, w1=14.040922715276626\n",
      "Gradient Descent(3241/9999): loss=2.0926204727105278, w0=73.46250000000005, w1=14.115623684388433\n",
      "Gradient Descent(3242/9999): loss=2.3919277773147565, w0=73.29375000000005, w1=14.316656872954203\n",
      "Gradient Descent(3243/9999): loss=2.171803051502153, w0=73.29375000000005, w1=14.188153372704658\n",
      "Gradient Descent(3244/9999): loss=2.5253893256066444, w0=73.46250000000005, w1=13.913584334068506\n",
      "Gradient Descent(3245/9999): loss=2.049618001266557, w0=73.40625000000004, w1=13.732693308934074\n",
      "Gradient Descent(3246/9999): loss=2.884919672560838, w0=73.35000000000004, w1=13.748058727001718\n",
      "Gradient Descent(3247/9999): loss=1.9250525563986494, w0=73.23750000000004, w1=13.949618505991488\n",
      "Gradient Descent(3248/9999): loss=2.34826162050301, w0=72.84375000000004, w1=13.416267949871136\n",
      "Gradient Descent(3249/9999): loss=2.050564220460207, w0=72.78750000000004, w1=13.345871089336574\n",
      "Gradient Descent(3250/9999): loss=1.935437940218699, w0=72.90000000000003, w1=13.520860467550602\n",
      "Gradient Descent(3251/9999): loss=1.9919738703661125, w0=72.90000000000003, w1=13.631416819166253\n",
      "Gradient Descent(3252/9999): loss=1.9042110179566323, w0=73.06875000000004, w1=13.722878735965304\n",
      "Gradient Descent(3253/9999): loss=2.370777537258263, w0=73.35000000000004, w1=13.731495742428693\n",
      "Gradient Descent(3254/9999): loss=2.582942687776244, w0=73.51875000000004, w1=14.010320989006662\n",
      "Gradient Descent(3255/9999): loss=2.658675974898822, w0=73.51875000000004, w1=13.867590183899717\n",
      "Gradient Descent(3256/9999): loss=2.020368151854873, w0=73.40625000000004, w1=13.757290767188488\n",
      "Gradient Descent(3257/9999): loss=2.336710295112743, w0=73.29375000000005, w1=13.529539860756396\n",
      "Gradient Descent(3258/9999): loss=1.9892335011769426, w0=73.35000000000005, w1=13.29929181877077\n",
      "Gradient Descent(3259/9999): loss=1.7459166569004951, w0=73.35000000000005, w1=13.30308352506012\n",
      "Gradient Descent(3260/9999): loss=2.4012846718903758, w0=73.12500000000006, w1=13.21217441396741\n",
      "Gradient Descent(3261/9999): loss=2.6152116897978033, w0=73.35000000000005, w1=13.105901900245176\n",
      "Gradient Descent(3262/9999): loss=2.370532645193933, w0=72.95625000000005, w1=13.209066737906623\n",
      "Gradient Descent(3263/9999): loss=2.5580215388595136, w0=72.78750000000005, w1=13.176480937610403\n",
      "Gradient Descent(3264/9999): loss=1.9864860318266269, w0=72.84375000000006, w1=13.2589761060323\n",
      "Gradient Descent(3265/9999): loss=2.621986357124621, w0=73.06875000000005, w1=13.245464900408777\n",
      "Gradient Descent(3266/9999): loss=1.8893582439063017, w0=73.01250000000005, w1=13.311255483753119\n",
      "Gradient Descent(3267/9999): loss=1.9465871832130404, w0=73.06875000000005, w1=13.392351937639216\n",
      "Gradient Descent(3268/9999): loss=2.550881747087139, w0=73.01250000000005, w1=13.633087229649844\n",
      "Gradient Descent(3269/9999): loss=2.1217938714398414, w0=72.90000000000005, w1=13.67940437875507\n",
      "Gradient Descent(3270/9999): loss=2.086479209176891, w0=72.84375000000004, w1=13.589294464442428\n",
      "Gradient Descent(3271/9999): loss=2.428719366278625, w0=73.01250000000005, w1=13.82070174425587\n",
      "Gradient Descent(3272/9999): loss=2.165586047067663, w0=73.06875000000005, w1=13.898993211705145\n",
      "Gradient Descent(3273/9999): loss=2.335770159290745, w0=72.84375000000006, w1=13.99233942252098\n",
      "Gradient Descent(3274/9999): loss=2.180484703439053, w0=72.84375000000006, w1=13.774597635119127\n",
      "Gradient Descent(3275/9999): loss=1.6774245768693645, w0=72.84375000000006, w1=13.661423359961667\n",
      "Gradient Descent(3276/9999): loss=1.9928466340571935, w0=72.67500000000005, w1=13.660454309954268\n",
      "Gradient Descent(3277/9999): loss=2.431729036656726, w0=72.56250000000006, w1=13.406128946413457\n",
      "Gradient Descent(3278/9999): loss=2.7058786136376005, w0=72.73125000000006, w1=13.45683896540678\n",
      "Gradient Descent(3279/9999): loss=2.220197647120058, w0=72.90000000000006, w1=13.411750310664116\n",
      "Gradient Descent(3280/9999): loss=2.4973286632837683, w0=72.95625000000007, w1=13.77731973211267\n",
      "Gradient Descent(3281/9999): loss=2.398282507575016, w0=73.18125000000006, w1=13.732008199119397\n",
      "Gradient Descent(3282/9999): loss=2.154876557849626, w0=72.95625000000007, w1=13.611084948063773\n",
      "Gradient Descent(3283/9999): loss=2.358126318744427, w0=72.95625000000007, w1=13.324818234889698\n",
      "Gradient Descent(3284/9999): loss=2.63320417531806, w0=72.90000000000006, w1=13.414127905115828\n",
      "Gradient Descent(3285/9999): loss=2.1500819130051987, w0=72.95625000000007, w1=13.60619346374245\n",
      "Gradient Descent(3286/9999): loss=2.237868814647521, w0=73.18125000000006, w1=13.510517321051026\n",
      "Gradient Descent(3287/9999): loss=2.130476925896959, w0=73.06875000000007, w1=13.314712440470288\n",
      "Gradient Descent(3288/9999): loss=2.4616228295288405, w0=73.18125000000006, w1=13.591823087355387\n",
      "Gradient Descent(3289/9999): loss=3.238994377849486, w0=73.29375000000006, w1=13.753324873280407\n",
      "Gradient Descent(3290/9999): loss=2.4109141591254923, w0=73.06875000000007, w1=13.739940975735324\n",
      "Gradient Descent(3291/9999): loss=2.561398789067072, w0=72.78750000000007, w1=13.917170224525586\n",
      "Gradient Descent(3292/9999): loss=1.804879443675804, w0=73.01250000000006, w1=13.776341924887877\n",
      "Gradient Descent(3293/9999): loss=2.448312574939557, w0=73.23750000000005, w1=13.575823886870639\n",
      "Gradient Descent(3294/9999): loss=2.148458396009045, w0=73.35000000000005, w1=13.428627981524748\n",
      "Gradient Descent(3295/9999): loss=2.617135834456559, w0=73.12500000000006, w1=13.135201625799375\n",
      "Gradient Descent(3296/9999): loss=1.774484695115612, w0=73.01250000000006, w1=13.467790016561024\n",
      "Gradient Descent(3297/9999): loss=2.2540212194864324, w0=73.18125000000006, w1=13.438996610402542\n",
      "Gradient Descent(3298/9999): loss=2.827350205790255, w0=73.12500000000006, w1=13.457491984210721\n",
      "Gradient Descent(3299/9999): loss=2.438167801134896, w0=73.29375000000006, w1=13.482112527028109\n",
      "Gradient Descent(3300/9999): loss=2.191481362659869, w0=73.18125000000006, w1=13.65104083423611\n",
      "Gradient Descent(3301/9999): loss=1.975148222200596, w0=73.51875000000007, w1=13.474364307331129\n",
      "Gradient Descent(3302/9999): loss=2.274502711958644, w0=73.57500000000007, w1=13.476964935065448\n",
      "Gradient Descent(3303/9999): loss=2.5845435849081575, w0=73.40625000000007, w1=13.673261154188388\n",
      "Gradient Descent(3304/9999): loss=2.3551217246087504, w0=73.35000000000007, w1=13.827816380572628\n",
      "Gradient Descent(3305/9999): loss=2.15860746922918, w0=73.35000000000007, w1=13.4830668331194\n",
      "Gradient Descent(3306/9999): loss=2.2158176130600484, w0=73.40625000000007, w1=13.53021612642331\n",
      "Gradient Descent(3307/9999): loss=2.3066141952380397, w0=73.46250000000008, w1=13.519501291689046\n",
      "Gradient Descent(3308/9999): loss=1.5775118621574096, w0=73.40625000000007, w1=13.71933705392855\n",
      "Gradient Descent(3309/9999): loss=2.3898950062095334, w0=73.40625000000007, w1=13.708344772252307\n",
      "Gradient Descent(3310/9999): loss=2.333458703711787, w0=73.51875000000007, w1=13.68149232369959\n",
      "Gradient Descent(3311/9999): loss=2.3571468376305007, w0=73.63125000000007, w1=13.508783446914931\n",
      "Gradient Descent(3312/9999): loss=2.745801568445412, w0=73.68750000000007, w1=13.578368523536822\n",
      "Gradient Descent(3313/9999): loss=2.2190485302424836, w0=73.51875000000007, w1=13.43045845830665\n",
      "Gradient Descent(3314/9999): loss=2.1326225436543065, w0=73.40625000000007, w1=13.339374589585907\n",
      "Gradient Descent(3315/9999): loss=1.8332773099867536, w0=73.40625000000007, w1=13.569163964324124\n",
      "Gradient Descent(3316/9999): loss=2.316393441459717, w0=73.40625000000007, w1=13.326834511108617\n",
      "Gradient Descent(3317/9999): loss=2.0736405559815245, w0=73.40625000000007, w1=13.351725244635999\n",
      "Gradient Descent(3318/9999): loss=2.4899985818505463, w0=73.18125000000008, w1=13.29297822821479\n",
      "Gradient Descent(3319/9999): loss=2.3819697686900643, w0=72.90000000000008, w1=13.083253650629327\n",
      "Gradient Descent(3320/9999): loss=2.145070119123572, w0=73.01250000000007, w1=13.087123451368456\n",
      "Gradient Descent(3321/9999): loss=2.517854527871198, w0=73.18125000000008, w1=13.138101150565992\n",
      "Gradient Descent(3322/9999): loss=2.0515788600045215, w0=73.29375000000007, w1=13.175672684887992\n",
      "Gradient Descent(3323/9999): loss=2.2979576010379166, w0=73.06875000000008, w1=13.234874010016581\n",
      "Gradient Descent(3324/9999): loss=2.4186324433036983, w0=73.12500000000009, w1=13.317859702762252\n",
      "Gradient Descent(3325/9999): loss=2.4935183564798127, w0=73.23750000000008, w1=13.304024949623132\n",
      "Gradient Descent(3326/9999): loss=1.600417671667142, w0=73.68750000000009, w1=12.935775194037232\n",
      "Gradient Descent(3327/9999): loss=2.600482391794382, w0=73.91250000000008, w1=12.856942369510257\n",
      "Gradient Descent(3328/9999): loss=2.432766210031857, w0=73.74375000000008, w1=13.105516912654734\n",
      "Gradient Descent(3329/9999): loss=2.1609700387229784, w0=73.63125000000008, w1=13.272265754534192\n",
      "Gradient Descent(3330/9999): loss=2.425682088489494, w0=73.68750000000009, w1=12.926641255769105\n",
      "Gradient Descent(3331/9999): loss=2.163281362502949, w0=73.57500000000009, w1=13.045294763109977\n",
      "Gradient Descent(3332/9999): loss=2.3238783435328862, w0=73.57500000000009, w1=13.302280814689372\n",
      "Gradient Descent(3333/9999): loss=1.9832125181957982, w0=73.46250000000009, w1=13.418453699976483\n",
      "Gradient Descent(3334/9999): loss=2.046822004878339, w0=73.46250000000009, w1=13.114067410993835\n",
      "Gradient Descent(3335/9999): loss=2.1052107428755047, w0=73.40625000000009, w1=13.175424649489367\n",
      "Gradient Descent(3336/9999): loss=2.249118502033104, w0=73.51875000000008, w1=12.981168839222795\n",
      "Gradient Descent(3337/9999): loss=2.0034148974552126, w0=73.74375000000008, w1=13.186225872506732\n",
      "Gradient Descent(3338/9999): loss=2.295549321278952, w0=73.91250000000008, w1=13.133633120168396\n",
      "Gradient Descent(3339/9999): loss=2.0790766843697397, w0=73.74375000000008, w1=13.232215764461321\n",
      "Gradient Descent(3340/9999): loss=2.727304760925125, w0=73.57500000000007, w1=13.420180686092289\n",
      "Gradient Descent(3341/9999): loss=2.2598337414788703, w0=73.40625000000007, w1=13.354212311194734\n",
      "Gradient Descent(3342/9999): loss=2.359596519854724, w0=73.29375000000007, w1=13.67453501041695\n",
      "Gradient Descent(3343/9999): loss=2.2710206978445098, w0=73.29375000000007, w1=13.478495916431681\n",
      "Gradient Descent(3344/9999): loss=2.326935198871328, w0=73.40625000000007, w1=13.325357063489834\n",
      "Gradient Descent(3345/9999): loss=2.4918010376848865, w0=73.46250000000008, w1=13.143595573389016\n",
      "Gradient Descent(3346/9999): loss=2.924760306462436, w0=73.23750000000008, w1=13.072806140437685\n",
      "Gradient Descent(3347/9999): loss=2.1005229362629256, w0=73.06875000000008, w1=12.98691084762774\n",
      "Gradient Descent(3348/9999): loss=1.6643302361908663, w0=73.12500000000009, w1=12.89044374019795\n",
      "Gradient Descent(3349/9999): loss=1.9664812885717036, w0=73.35000000000008, w1=13.070297521690364\n",
      "Gradient Descent(3350/9999): loss=2.398635829186329, w0=73.29375000000007, w1=12.905230716300084\n",
      "Gradient Descent(3351/9999): loss=2.293910540064464, w0=73.46250000000008, w1=12.90826041329743\n",
      "Gradient Descent(3352/9999): loss=2.4318419294646128, w0=73.57500000000007, w1=13.024987528351376\n",
      "Gradient Descent(3353/9999): loss=2.2009640311449807, w0=73.63125000000008, w1=12.913240050810822\n",
      "Gradient Descent(3354/9999): loss=2.110255393664655, w0=73.74375000000008, w1=12.611273774201372\n",
      "Gradient Descent(3355/9999): loss=2.059776529512849, w0=73.68750000000007, w1=12.761336937406671\n",
      "Gradient Descent(3356/9999): loss=2.646089663916487, w0=73.74375000000008, w1=13.026678534532959\n",
      "Gradient Descent(3357/9999): loss=2.178784510658849, w0=73.80000000000008, w1=12.855528006483187\n",
      "Gradient Descent(3358/9999): loss=2.309741175828899, w0=73.80000000000008, w1=12.89616252802783\n",
      "Gradient Descent(3359/9999): loss=2.043300949746594, w0=73.74375000000008, w1=13.197861416398325\n",
      "Gradient Descent(3360/9999): loss=2.6731134161566983, w0=73.68750000000007, w1=13.17947712297304\n",
      "Gradient Descent(3361/9999): loss=2.558538581688343, w0=73.40625000000007, w1=13.397069816900558\n",
      "Gradient Descent(3362/9999): loss=2.0949845458223115, w0=73.51875000000007, w1=13.377881741107954\n",
      "Gradient Descent(3363/9999): loss=2.1234186431241415, w0=73.63125000000007, w1=13.308466269744528\n",
      "Gradient Descent(3364/9999): loss=2.2291506688579417, w0=73.68750000000007, w1=13.141432003917524\n",
      "Gradient Descent(3365/9999): loss=2.210314049753052, w0=73.74375000000008, w1=13.087330638142818\n",
      "Gradient Descent(3366/9999): loss=2.4584363875165494, w0=73.51875000000008, w1=13.08455085542317\n",
      "Gradient Descent(3367/9999): loss=2.5492815627031353, w0=73.68750000000009, w1=12.97976718713205\n",
      "Gradient Descent(3368/9999): loss=2.3596893394942584, w0=73.57500000000009, w1=13.127410471639724\n",
      "Gradient Descent(3369/9999): loss=2.2364322136509003, w0=73.29375000000009, w1=13.191066569262734\n",
      "Gradient Descent(3370/9999): loss=2.028692239765977, w0=73.51875000000008, w1=13.417442062693844\n",
      "Gradient Descent(3371/9999): loss=2.593284463448784, w0=73.57500000000009, w1=13.613400287717655\n",
      "Gradient Descent(3372/9999): loss=1.9330663254069707, w0=73.40625000000009, w1=13.666093485228007\n",
      "Gradient Descent(3373/9999): loss=2.340370586577661, w0=73.35000000000008, w1=13.609761670423039\n",
      "Gradient Descent(3374/9999): loss=2.355529934835377, w0=73.12500000000009, w1=13.797629340930712\n",
      "Gradient Descent(3375/9999): loss=2.402310210637436, w0=73.18125000000009, w1=13.87963694016882\n",
      "Gradient Descent(3376/9999): loss=2.4368382264204436, w0=73.12500000000009, w1=13.580569799638244\n",
      "Gradient Descent(3377/9999): loss=2.1761973051702026, w0=73.23750000000008, w1=13.780295595231141\n",
      "Gradient Descent(3378/9999): loss=2.4831359641270865, w0=73.01250000000009, w1=13.623640635828291\n",
      "Gradient Descent(3379/9999): loss=2.4399967811169416, w0=73.23750000000008, w1=13.408988109421662\n",
      "Gradient Descent(3380/9999): loss=2.5467404109033422, w0=73.18125000000008, w1=13.405909173632917\n",
      "Gradient Descent(3381/9999): loss=2.6838392059083445, w0=73.23750000000008, w1=13.47146596039366\n",
      "Gradient Descent(3382/9999): loss=2.289951542751168, w0=73.23750000000008, w1=13.451062391784541\n",
      "Gradient Descent(3383/9999): loss=2.385396421183307, w0=72.95625000000008, w1=13.442882066198509\n",
      "Gradient Descent(3384/9999): loss=2.3530117406193747, w0=72.95625000000008, w1=13.613091330416275\n",
      "Gradient Descent(3385/9999): loss=2.3817978979735788, w0=72.84375000000009, w1=13.583728027968595\n",
      "Gradient Descent(3386/9999): loss=2.2422724085047694, w0=72.78750000000008, w1=13.053635420422468\n",
      "Gradient Descent(3387/9999): loss=2.2909358919651743, w0=72.95625000000008, w1=13.184254807558284\n",
      "Gradient Descent(3388/9999): loss=1.8235058783651612, w0=72.90000000000008, w1=13.387480928932815\n",
      "Gradient Descent(3389/9999): loss=2.389134300169389, w0=73.01250000000007, w1=13.453459336039181\n",
      "Gradient Descent(3390/9999): loss=1.9437188787110458, w0=73.01250000000007, w1=13.57273888466768\n",
      "Gradient Descent(3391/9999): loss=2.430146578081117, w0=72.84375000000007, w1=13.656344467497139\n",
      "Gradient Descent(3392/9999): loss=2.2355198631607682, w0=72.78750000000007, w1=13.808226263844034\n",
      "Gradient Descent(3393/9999): loss=1.7825661631432308, w0=72.67500000000007, w1=14.07743192582555\n",
      "Gradient Descent(3394/9999): loss=2.280618419453832, w0=72.56250000000007, w1=14.029579563539656\n",
      "Gradient Descent(3395/9999): loss=2.0962944708412015, w0=72.50625000000007, w1=14.092869793648093\n",
      "Gradient Descent(3396/9999): loss=1.8263784457308483, w0=72.56250000000007, w1=14.037650457122739\n",
      "Gradient Descent(3397/9999): loss=1.8196127312298405, w0=72.67500000000007, w1=13.910344228436475\n",
      "Gradient Descent(3398/9999): loss=2.4922360126547205, w0=72.61875000000006, w1=13.93470951064368\n",
      "Gradient Descent(3399/9999): loss=2.368799558995234, w0=72.67500000000007, w1=13.95952084721969\n",
      "Gradient Descent(3400/9999): loss=2.031402031866837, w0=72.56250000000007, w1=14.046952324959548\n",
      "Gradient Descent(3401/9999): loss=2.8715515117815906, w0=72.67500000000007, w1=13.992324089546159\n",
      "Gradient Descent(3402/9999): loss=2.5937786507414176, w0=72.84375000000007, w1=13.953691668400552\n",
      "Gradient Descent(3403/9999): loss=2.4326873264388507, w0=72.67500000000007, w1=13.821410852772532\n",
      "Gradient Descent(3404/9999): loss=1.6346078243475786, w0=72.90000000000006, w1=13.992794010566623\n",
      "Gradient Descent(3405/9999): loss=1.6096864142470446, w0=72.95625000000007, w1=14.218095588563322\n",
      "Gradient Descent(3406/9999): loss=2.2533083954453756, w0=73.06875000000007, w1=13.960886963892518\n",
      "Gradient Descent(3407/9999): loss=2.297884561660952, w0=72.90000000000006, w1=13.776718214493888\n",
      "Gradient Descent(3408/9999): loss=2.1529999382262743, w0=73.12500000000006, w1=13.702235722029346\n",
      "Gradient Descent(3409/9999): loss=2.6003783675692826, w0=73.23750000000005, w1=13.594433279299812\n",
      "Gradient Descent(3410/9999): loss=2.5319572474335534, w0=73.29375000000006, w1=13.347061918029423\n",
      "Gradient Descent(3411/9999): loss=2.285208920967824, w0=73.18125000000006, w1=13.565844576764691\n",
      "Gradient Descent(3412/9999): loss=1.9148921913924435, w0=73.46250000000006, w1=13.500058116676918\n",
      "Gradient Descent(3413/9999): loss=2.1836519453766523, w0=73.40625000000006, w1=13.202547801338126\n",
      "Gradient Descent(3414/9999): loss=1.9225133138555752, w0=73.29375000000006, w1=12.997003472619125\n",
      "Gradient Descent(3415/9999): loss=2.2841234714374696, w0=73.40625000000006, w1=13.163827304739371\n",
      "Gradient Descent(3416/9999): loss=1.9563210728991236, w0=73.40625000000006, w1=13.325883915784411\n",
      "Gradient Descent(3417/9999): loss=1.9735133807404919, w0=73.46250000000006, w1=13.365285530138614\n",
      "Gradient Descent(3418/9999): loss=3.0244489017062848, w0=73.46250000000006, w1=13.647559615604095\n",
      "Gradient Descent(3419/9999): loss=2.3506527378339666, w0=73.57500000000006, w1=13.754504545802066\n",
      "Gradient Descent(3420/9999): loss=2.0155583155290713, w0=73.80000000000005, w1=13.699347063502277\n",
      "Gradient Descent(3421/9999): loss=2.1289328962165612, w0=73.74375000000005, w1=13.714834709121941\n",
      "Gradient Descent(3422/9999): loss=2.271041952569651, w0=73.51875000000005, w1=13.547245588996082\n",
      "Gradient Descent(3423/9999): loss=2.2060103715946777, w0=73.46250000000005, w1=13.935377317612067\n",
      "Gradient Descent(3424/9999): loss=2.839646819604577, w0=73.35000000000005, w1=13.825140702008241\n",
      "Gradient Descent(3425/9999): loss=2.172855605666436, w0=73.40625000000006, w1=13.904505273788319\n",
      "Gradient Descent(3426/9999): loss=2.038017302344505, w0=73.12500000000006, w1=14.031204499609299\n",
      "Gradient Descent(3427/9999): loss=2.4147973015811024, w0=73.18125000000006, w1=13.906729058392408\n",
      "Gradient Descent(3428/9999): loss=1.6836609112494219, w0=73.01250000000006, w1=13.992619725732249\n",
      "Gradient Descent(3429/9999): loss=2.360203608031685, w0=72.90000000000006, w1=13.874306799696095\n",
      "Gradient Descent(3430/9999): loss=2.6871329288733348, w0=73.01250000000006, w1=14.0460713102029\n",
      "Gradient Descent(3431/9999): loss=1.875949441842598, w0=73.18125000000006, w1=13.958068208629763\n",
      "Gradient Descent(3432/9999): loss=2.703511716040449, w0=73.23750000000007, w1=13.896328580148204\n",
      "Gradient Descent(3433/9999): loss=2.1699971692645303, w0=73.23750000000007, w1=13.965967071729153\n",
      "Gradient Descent(3434/9999): loss=1.8814214927362354, w0=73.35000000000007, w1=13.66033607855569\n",
      "Gradient Descent(3435/9999): loss=2.1001299072144644, w0=73.35000000000007, w1=13.639838944565566\n",
      "Gradient Descent(3436/9999): loss=2.4139803153611874, w0=73.18125000000006, w1=13.701306355073134\n",
      "Gradient Descent(3437/9999): loss=2.3743442305037945, w0=73.12500000000006, w1=13.408637023009698\n",
      "Gradient Descent(3438/9999): loss=1.899314868107276, w0=72.90000000000006, w1=13.503438200396655\n",
      "Gradient Descent(3439/9999): loss=2.3348555709657255, w0=72.90000000000006, w1=13.2371695100864\n",
      "Gradient Descent(3440/9999): loss=2.2157556887013135, w0=72.67500000000007, w1=13.231973286186706\n",
      "Gradient Descent(3441/9999): loss=2.327320729602447, w0=72.67500000000007, w1=13.325825268300939\n",
      "Gradient Descent(3442/9999): loss=2.0147737063901303, w0=72.45000000000007, w1=13.160934630130065\n",
      "Gradient Descent(3443/9999): loss=2.2270606367734866, w0=72.39375000000007, w1=13.416799944270055\n",
      "Gradient Descent(3444/9999): loss=2.0024317503538045, w0=72.45000000000007, w1=13.178095166860993\n",
      "Gradient Descent(3445/9999): loss=2.1135935018386816, w0=72.90000000000008, w1=12.982259813702678\n",
      "Gradient Descent(3446/9999): loss=2.473110065626848, w0=73.01250000000007, w1=13.097485698630626\n",
      "Gradient Descent(3447/9999): loss=2.1216850111212375, w0=73.18125000000008, w1=13.126412474148314\n",
      "Gradient Descent(3448/9999): loss=1.7354099238427214, w0=73.29375000000007, w1=13.093227326777788\n",
      "Gradient Descent(3449/9999): loss=2.3453655019511075, w0=73.29375000000007, w1=13.4639580790249\n",
      "Gradient Descent(3450/9999): loss=2.042301068669193, w0=73.23750000000007, w1=13.94486901199559\n",
      "Gradient Descent(3451/9999): loss=2.3957780213312825, w0=73.51875000000007, w1=14.04932189731097\n",
      "Gradient Descent(3452/9999): loss=2.2244420022549307, w0=73.23750000000007, w1=13.925873243090303\n",
      "Gradient Descent(3453/9999): loss=2.170705422522026, w0=73.40625000000007, w1=13.820281234911828\n",
      "Gradient Descent(3454/9999): loss=2.1298373668864117, w0=73.29375000000007, w1=13.595412456446555\n",
      "Gradient Descent(3455/9999): loss=2.1582944802392836, w0=73.23750000000007, w1=13.368765257380467\n",
      "Gradient Descent(3456/9999): loss=2.4020336338053347, w0=73.23750000000007, w1=13.183636297847595\n",
      "Gradient Descent(3457/9999): loss=2.149336259382051, w0=73.23750000000007, w1=13.04289768327981\n",
      "Gradient Descent(3458/9999): loss=2.2316364060419245, w0=73.23750000000007, w1=13.108387300928678\n",
      "Gradient Descent(3459/9999): loss=2.369497947954378, w0=73.23750000000007, w1=13.225465478208045\n",
      "Gradient Descent(3460/9999): loss=2.0637395671393106, w0=73.01250000000007, w1=13.346901964075062\n",
      "Gradient Descent(3461/9999): loss=2.141581889877795, w0=72.95625000000007, w1=13.379046225583972\n",
      "Gradient Descent(3462/9999): loss=2.3355671083474334, w0=72.95625000000007, w1=13.574406948571118\n",
      "Gradient Descent(3463/9999): loss=2.093045984675701, w0=72.90000000000006, w1=13.270895848781086\n",
      "Gradient Descent(3464/9999): loss=1.6969664318448283, w0=73.06875000000007, w1=13.498042524488781\n",
      "Gradient Descent(3465/9999): loss=2.2933033213970084, w0=72.90000000000006, w1=13.497923682708509\n",
      "Gradient Descent(3466/9999): loss=1.9961178910506208, w0=72.84375000000006, w1=13.175518032150462\n",
      "Gradient Descent(3467/9999): loss=2.255824335024644, w0=72.84375000000006, w1=13.500017619939134\n",
      "Gradient Descent(3468/9999): loss=2.51878544376695, w0=72.90000000000006, w1=13.89344229272238\n",
      "Gradient Descent(3469/9999): loss=2.314372047544887, w0=72.78750000000007, w1=13.769038306881002\n",
      "Gradient Descent(3470/9999): loss=2.2001390210744, w0=72.61875000000006, w1=13.465900793199497\n",
      "Gradient Descent(3471/9999): loss=2.4407672417575674, w0=72.61875000000006, w1=13.622671397087345\n",
      "Gradient Descent(3472/9999): loss=1.805174666329759, w0=72.84375000000006, w1=13.53532700560708\n",
      "Gradient Descent(3473/9999): loss=2.2637409023452633, w0=72.78750000000005, w1=13.846999084993753\n",
      "Gradient Descent(3474/9999): loss=1.8877095943322826, w0=72.67500000000005, w1=13.663135953597196\n",
      "Gradient Descent(3475/9999): loss=2.1027858139289064, w0=72.73125000000006, w1=13.686900670472596\n",
      "Gradient Descent(3476/9999): loss=2.418448077060278, w0=72.78750000000007, w1=13.690270739421381\n",
      "Gradient Descent(3477/9999): loss=2.0628552600479466, w0=72.67500000000007, w1=13.79058489846628\n",
      "Gradient Descent(3478/9999): loss=1.8605790204281587, w0=73.12500000000007, w1=13.980586782839564\n",
      "Gradient Descent(3479/9999): loss=2.407450721964587, w0=73.23750000000007, w1=14.125922830859512\n",
      "Gradient Descent(3480/9999): loss=1.849838854834161, w0=73.23750000000007, w1=14.009444188086526\n",
      "Gradient Descent(3481/9999): loss=2.1610174948813343, w0=73.29375000000007, w1=13.96909094286997\n",
      "Gradient Descent(3482/9999): loss=2.613120122794286, w0=73.18125000000008, w1=14.04250543953304\n",
      "Gradient Descent(3483/9999): loss=2.2988724550331257, w0=73.12500000000007, w1=14.046205560162205\n",
      "Gradient Descent(3484/9999): loss=2.6226392266263536, w0=73.29375000000007, w1=14.103077105392247\n",
      "Gradient Descent(3485/9999): loss=2.8077526767630063, w0=73.01250000000007, w1=13.993128037137016\n",
      "Gradient Descent(3486/9999): loss=1.6730410843424142, w0=73.01250000000007, w1=13.88018431138301\n",
      "Gradient Descent(3487/9999): loss=2.487865486379131, w0=73.01250000000007, w1=13.934616835250017\n",
      "Gradient Descent(3488/9999): loss=1.9225545438119829, w0=73.06875000000008, w1=13.874655026003891\n",
      "Gradient Descent(3489/9999): loss=2.412359636492084, w0=73.46250000000008, w1=13.565812798712008\n",
      "Gradient Descent(3490/9999): loss=2.147876230182985, w0=73.51875000000008, w1=13.662783818653434\n",
      "Gradient Descent(3491/9999): loss=2.1108861401908916, w0=73.63125000000008, w1=13.781489689536\n",
      "Gradient Descent(3492/9999): loss=2.2519658075894404, w0=73.63125000000008, w1=13.63340747749856\n",
      "Gradient Descent(3493/9999): loss=1.8738618427131903, w0=73.57500000000007, w1=13.843630749271275\n",
      "Gradient Descent(3494/9999): loss=2.1299326106760024, w0=73.51875000000007, w1=13.768134321319287\n",
      "Gradient Descent(3495/9999): loss=2.10657221113319, w0=73.23750000000007, w1=13.628749686231753\n",
      "Gradient Descent(3496/9999): loss=2.3585257581665697, w0=73.35000000000007, w1=14.006301254161313\n",
      "Gradient Descent(3497/9999): loss=2.093682548138802, w0=73.35000000000007, w1=13.814345684018583\n",
      "Gradient Descent(3498/9999): loss=2.24032284949738, w0=73.35000000000007, w1=13.801539989260645\n",
      "Gradient Descent(3499/9999): loss=1.736945159758681, w0=73.23750000000007, w1=13.657843494879307\n",
      "Gradient Descent(3500/9999): loss=2.4826859065323807, w0=73.18125000000006, w1=13.607878300013587\n",
      "Gradient Descent(3501/9999): loss=2.289656854290418, w0=73.12500000000006, w1=13.333635885634182\n",
      "Gradient Descent(3502/9999): loss=2.215135407651167, w0=73.40625000000006, w1=13.29279726295047\n",
      "Gradient Descent(3503/9999): loss=2.196068989248327, w0=73.46250000000006, w1=13.312849150157026\n",
      "Gradient Descent(3504/9999): loss=2.2947625631655297, w0=73.46250000000006, w1=13.444550850120704\n",
      "Gradient Descent(3505/9999): loss=2.0953717785887114, w0=73.29375000000006, w1=13.138400881438285\n",
      "Gradient Descent(3506/9999): loss=2.127876554044862, w0=73.51875000000005, w1=13.02485297250616\n",
      "Gradient Descent(3507/9999): loss=1.4884112520034745, w0=73.57500000000006, w1=12.810350495853134\n",
      "Gradient Descent(3508/9999): loss=2.3982185025717815, w0=73.46250000000006, w1=12.98541859896248\n",
      "Gradient Descent(3509/9999): loss=2.12188061241799, w0=73.63125000000007, w1=13.324539635309886\n",
      "Gradient Descent(3510/9999): loss=1.9597469779268089, w0=73.80000000000007, w1=13.325738883260252\n",
      "Gradient Descent(3511/9999): loss=2.2031736209718114, w0=73.74375000000006, w1=13.323193845073815\n",
      "Gradient Descent(3512/9999): loss=2.5457326600696026, w0=73.80000000000007, w1=13.287050057533358\n",
      "Gradient Descent(3513/9999): loss=2.0742408970691892, w0=73.80000000000007, w1=13.251108641155252\n",
      "Gradient Descent(3514/9999): loss=1.7231609266538612, w0=73.80000000000007, w1=13.229975682189416\n",
      "Gradient Descent(3515/9999): loss=2.433991355958689, w0=73.68750000000007, w1=13.142783344992289\n",
      "Gradient Descent(3516/9999): loss=2.2882204209228156, w0=73.46250000000008, w1=13.03118403064204\n",
      "Gradient Descent(3517/9999): loss=2.459642033898867, w0=73.29375000000007, w1=13.246852028839097\n",
      "Gradient Descent(3518/9999): loss=2.674025211614818, w0=73.40625000000007, w1=13.300683735512388\n",
      "Gradient Descent(3519/9999): loss=2.614788484097388, w0=73.29375000000007, w1=13.390972808860536\n",
      "Gradient Descent(3520/9999): loss=2.5962138694049273, w0=73.12500000000007, w1=13.642386592556921\n",
      "Gradient Descent(3521/9999): loss=2.2109004545528417, w0=73.06875000000007, w1=13.435707298080922\n",
      "Gradient Descent(3522/9999): loss=2.35733485073219, w0=73.01250000000006, w1=13.541061790645578\n",
      "Gradient Descent(3523/9999): loss=2.3769351625880226, w0=73.01250000000006, w1=13.597625965670577\n",
      "Gradient Descent(3524/9999): loss=1.7616712871815468, w0=73.06875000000007, w1=13.579509623634971\n",
      "Gradient Descent(3525/9999): loss=2.3903778097665462, w0=73.01250000000006, w1=13.464330485142328\n",
      "Gradient Descent(3526/9999): loss=2.160668031586903, w0=73.18125000000006, w1=13.677606122743915\n",
      "Gradient Descent(3527/9999): loss=2.0623734445195057, w0=73.18125000000006, w1=13.622532356311646\n",
      "Gradient Descent(3528/9999): loss=1.8758532495033458, w0=73.23750000000007, w1=13.827310566193193\n",
      "Gradient Descent(3529/9999): loss=2.183640702411994, w0=73.35000000000007, w1=13.590018253867408\n",
      "Gradient Descent(3530/9999): loss=2.0689118006123426, w0=73.29375000000006, w1=13.575730643224212\n",
      "Gradient Descent(3531/9999): loss=2.35149492314838, w0=73.35000000000007, w1=13.332764134226784\n",
      "Gradient Descent(3532/9999): loss=2.2094351088645534, w0=73.63125000000007, w1=13.485883747450414\n",
      "Gradient Descent(3533/9999): loss=2.598210059388493, w0=73.68750000000007, w1=13.594457945826983\n",
      "Gradient Descent(3534/9999): loss=2.3896546913234165, w0=73.51875000000007, w1=13.429393312152346\n",
      "Gradient Descent(3535/9999): loss=1.9284598802231605, w0=73.29375000000007, w1=13.3298091982075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3536/9999): loss=1.9071971451852514, w0=73.40625000000007, w1=13.17540110314163\n",
      "Gradient Descent(3537/9999): loss=1.6328756720384314, w0=73.01250000000007, w1=13.04058648537013\n",
      "Gradient Descent(3538/9999): loss=2.219642666183635, w0=73.12500000000007, w1=13.467319431060787\n",
      "Gradient Descent(3539/9999): loss=1.9053424262544387, w0=73.06875000000007, w1=13.33907510922373\n",
      "Gradient Descent(3540/9999): loss=1.825236366085413, w0=73.06875000000007, w1=13.400055766982861\n",
      "Gradient Descent(3541/9999): loss=2.0411098154777916, w0=73.46250000000006, w1=13.742900754958306\n",
      "Gradient Descent(3542/9999): loss=1.97483521589516, w0=73.46250000000006, w1=13.389144012845836\n",
      "Gradient Descent(3543/9999): loss=2.6061574306718662, w0=73.57500000000006, w1=13.232791610537067\n",
      "Gradient Descent(3544/9999): loss=2.2880111552120272, w0=73.63125000000007, w1=13.12377084735844\n",
      "Gradient Descent(3545/9999): loss=2.472478899437219, w0=73.57500000000006, w1=13.100703582339758\n",
      "Gradient Descent(3546/9999): loss=2.0009724313279342, w0=73.51875000000005, w1=13.212439145548549\n",
      "Gradient Descent(3547/9999): loss=1.6879608949824827, w0=73.85625000000006, w1=13.304573221565414\n",
      "Gradient Descent(3548/9999): loss=2.2425157865483927, w0=73.63125000000007, w1=13.12710934502255\n",
      "Gradient Descent(3549/9999): loss=1.8664309402416912, w0=73.68750000000007, w1=13.603982245093787\n",
      "Gradient Descent(3550/9999): loss=2.466332798648353, w0=73.68750000000007, w1=13.557530370455199\n",
      "Gradient Descent(3551/9999): loss=2.062302457921269, w0=73.51875000000007, w1=13.495733845945828\n",
      "Gradient Descent(3552/9999): loss=2.300497093855887, w0=73.46250000000006, w1=13.417087223938607\n",
      "Gradient Descent(3553/9999): loss=2.511541376006367, w0=73.57500000000006, w1=13.377486233272036\n",
      "Gradient Descent(3554/9999): loss=2.229436143313164, w0=73.40625000000006, w1=13.511458839261806\n",
      "Gradient Descent(3555/9999): loss=2.467033454906497, w0=73.35000000000005, w1=13.57625826792445\n",
      "Gradient Descent(3556/9999): loss=1.9326848799602803, w0=73.29375000000005, w1=13.435580177939975\n",
      "Gradient Descent(3557/9999): loss=2.5800817259004702, w0=73.06875000000005, w1=13.576238038059335\n",
      "Gradient Descent(3558/9999): loss=2.169733410188444, w0=73.01250000000005, w1=13.24931939151857\n",
      "Gradient Descent(3559/9999): loss=2.4576665390538794, w0=73.12500000000004, w1=13.263183281866377\n",
      "Gradient Descent(3560/9999): loss=2.544934288313138, w0=73.06875000000004, w1=13.238481544794485\n",
      "Gradient Descent(3561/9999): loss=1.924322019074666, w0=73.01250000000003, w1=13.72197707631968\n",
      "Gradient Descent(3562/9999): loss=1.8891480123451758, w0=72.95625000000003, w1=13.641203709959132\n",
      "Gradient Descent(3563/9999): loss=2.0508168980799395, w0=72.84375000000003, w1=13.720949370582758\n",
      "Gradient Descent(3564/9999): loss=2.0143355630406115, w0=72.61875000000003, w1=13.536771854165421\n",
      "Gradient Descent(3565/9999): loss=2.0312055609696023, w0=72.73125000000003, w1=13.27689815977045\n",
      "Gradient Descent(3566/9999): loss=1.911938466161068, w0=72.78750000000004, w1=13.345755840404504\n",
      "Gradient Descent(3567/9999): loss=1.7135652292377772, w0=72.78750000000004, w1=13.62805332533753\n",
      "Gradient Descent(3568/9999): loss=2.0470976708518265, w0=72.84375000000004, w1=13.772230669552602\n",
      "Gradient Descent(3569/9999): loss=2.495658066194303, w0=72.90000000000005, w1=13.871472897464137\n",
      "Gradient Descent(3570/9999): loss=2.09325035031146, w0=72.95625000000005, w1=13.678847407070307\n",
      "Gradient Descent(3571/9999): loss=2.241512001008786, w0=72.90000000000005, w1=13.671639572183016\n",
      "Gradient Descent(3572/9999): loss=2.3071183187530604, w0=73.29375000000005, w1=13.674903184446183\n",
      "Gradient Descent(3573/9999): loss=2.26787099315288, w0=73.29375000000005, w1=13.669643494154476\n",
      "Gradient Descent(3574/9999): loss=2.924801074367034, w0=73.35000000000005, w1=13.855839670669242\n",
      "Gradient Descent(3575/9999): loss=2.1924977348263326, w0=73.46250000000005, w1=13.860843197079534\n",
      "Gradient Descent(3576/9999): loss=2.332124103803852, w0=73.63125000000005, w1=13.780557264075124\n",
      "Gradient Descent(3577/9999): loss=2.560853462699738, w0=73.40625000000006, w1=13.81480738073909\n",
      "Gradient Descent(3578/9999): loss=1.9067204093788375, w0=73.46250000000006, w1=13.455107948909019\n",
      "Gradient Descent(3579/9999): loss=2.238047621726091, w0=73.51875000000007, w1=13.639490472396743\n",
      "Gradient Descent(3580/9999): loss=2.267423228436465, w0=73.46250000000006, w1=13.670772099547381\n",
      "Gradient Descent(3581/9999): loss=2.1921460263688046, w0=73.80000000000007, w1=13.510369516782378\n",
      "Gradient Descent(3582/9999): loss=1.9509605965136383, w0=73.74375000000006, w1=13.510594467409675\n",
      "Gradient Descent(3583/9999): loss=1.9014490214743935, w0=73.80000000000007, w1=13.626344073818256\n",
      "Gradient Descent(3584/9999): loss=1.7661755230051004, w0=73.51875000000007, w1=13.613946769069958\n",
      "Gradient Descent(3585/9999): loss=2.0574506802159007, w0=73.46250000000006, w1=13.3837663697447\n",
      "Gradient Descent(3586/9999): loss=2.0729588176153095, w0=73.46250000000006, w1=13.362849778783932\n",
      "Gradient Descent(3587/9999): loss=2.5034034584845575, w0=73.35000000000007, w1=13.147967253120097\n",
      "Gradient Descent(3588/9999): loss=2.4181711741731604, w0=73.01250000000006, w1=13.06306028821714\n",
      "Gradient Descent(3589/9999): loss=1.968388218710698, w0=73.06875000000007, w1=13.240997885859878\n",
      "Gradient Descent(3590/9999): loss=1.7648140223650535, w0=72.78750000000007, w1=13.232266504218254\n",
      "Gradient Descent(3591/9999): loss=2.622005889909386, w0=72.84375000000007, w1=13.333817967232664\n",
      "Gradient Descent(3592/9999): loss=2.094197661299562, w0=72.95625000000007, w1=13.2466882422126\n",
      "Gradient Descent(3593/9999): loss=2.3431144353074793, w0=72.67500000000007, w1=13.40188287718472\n",
      "Gradient Descent(3594/9999): loss=2.198111479736963, w0=72.73125000000007, w1=13.598884436713044\n",
      "Gradient Descent(3595/9999): loss=2.0688700293188917, w0=72.73125000000007, w1=13.62973056322262\n",
      "Gradient Descent(3596/9999): loss=2.427329744733573, w0=72.61875000000008, w1=13.418259386693506\n",
      "Gradient Descent(3597/9999): loss=1.6704106577506699, w0=72.78750000000008, w1=13.442524059514703\n",
      "Gradient Descent(3598/9999): loss=2.8547080875471518, w0=73.18125000000008, w1=13.392829785525581\n",
      "Gradient Descent(3599/9999): loss=2.4884829871560377, w0=73.35000000000008, w1=13.363211933423614\n",
      "Gradient Descent(3600/9999): loss=2.5414094172778254, w0=73.40625000000009, w1=13.33728649404088\n",
      "Gradient Descent(3601/9999): loss=2.0063070098256803, w0=73.74375000000009, w1=13.521432315136654\n",
      "Gradient Descent(3602/9999): loss=2.4496716066867203, w0=73.5187500000001, w1=13.39675618853237\n",
      "Gradient Descent(3603/9999): loss=2.1008790960920134, w0=73.46250000000009, w1=13.521394316177311\n",
      "Gradient Descent(3604/9999): loss=2.0400435775268346, w0=73.18125000000009, w1=13.382538979575138\n",
      "Gradient Descent(3605/9999): loss=2.4821461555824653, w0=73.12500000000009, w1=13.693190347953475\n",
      "Gradient Descent(3606/9999): loss=2.1160186102656233, w0=72.90000000000009, w1=13.29071978403366\n",
      "Gradient Descent(3607/9999): loss=2.554761413066742, w0=73.12500000000009, w1=13.234934148784069\n",
      "Gradient Descent(3608/9999): loss=1.971420995599678, w0=73.01250000000009, w1=13.497194082534632\n",
      "Gradient Descent(3609/9999): loss=1.7412829607172715, w0=73.12500000000009, w1=13.482334506094162\n",
      "Gradient Descent(3610/9999): loss=1.8344632006558492, w0=73.12500000000009, w1=13.354048739982764\n",
      "Gradient Descent(3611/9999): loss=2.1799208176932527, w0=73.29375000000009, w1=13.403651737207154\n",
      "Gradient Descent(3612/9999): loss=1.9465532203960563, w0=73.3500000000001, w1=13.31932058369011\n",
      "Gradient Descent(3613/9999): loss=1.8865053814363966, w0=73.1250000000001, w1=13.211281260613086\n",
      "Gradient Descent(3614/9999): loss=2.036347182763186, w0=73.1812500000001, w1=13.23779361285393\n",
      "Gradient Descent(3615/9999): loss=1.99879253346688, w0=73.23750000000011, w1=13.235980660331593\n",
      "Gradient Descent(3616/9999): loss=2.3643675422404353, w0=73.1812500000001, w1=12.997856803300948\n",
      "Gradient Descent(3617/9999): loss=2.111017575897149, w0=73.0125000000001, w1=12.841316861639163\n",
      "Gradient Descent(3618/9999): loss=1.954086676558635, w0=72.9562500000001, w1=12.997654458573017\n",
      "Gradient Descent(3619/9999): loss=2.245751958716845, w0=72.9562500000001, w1=12.88197861825014\n",
      "Gradient Descent(3620/9999): loss=2.360738807136836, w0=73.18125000000009, w1=12.872596560383135\n",
      "Gradient Descent(3621/9999): loss=1.7159165775365206, w0=73.29375000000009, w1=12.901372675171137\n",
      "Gradient Descent(3622/9999): loss=2.201459564395547, w0=73.3500000000001, w1=13.025650459442565\n",
      "Gradient Descent(3623/9999): loss=2.957747433968693, w0=73.57500000000009, w1=13.13376876916865\n",
      "Gradient Descent(3624/9999): loss=2.5630147346918672, w0=73.6312500000001, w1=13.11065204124986\n",
      "Gradient Descent(3625/9999): loss=1.7836712297114574, w0=73.46250000000009, w1=13.535197579265075\n",
      "Gradient Descent(3626/9999): loss=2.53628273551114, w0=73.57500000000009, w1=13.547334310768028\n",
      "Gradient Descent(3627/9999): loss=1.906777138444278, w0=73.51875000000008, w1=13.512604587969147\n",
      "Gradient Descent(3628/9999): loss=2.4803314158988066, w0=73.74375000000008, w1=13.393236152942558\n",
      "Gradient Descent(3629/9999): loss=2.2293035724487655, w0=73.40625000000007, w1=13.400749959926044\n",
      "Gradient Descent(3630/9999): loss=1.642439899706763, w0=73.29375000000007, w1=13.313507872697654\n",
      "Gradient Descent(3631/9999): loss=2.5180085829990895, w0=73.29375000000007, w1=13.232825429757531\n",
      "Gradient Descent(3632/9999): loss=1.9252961840732703, w0=73.29375000000007, w1=13.398821703048021\n",
      "Gradient Descent(3633/9999): loss=2.621755015844066, w0=73.35000000000008, w1=13.733961734569721\n",
      "Gradient Descent(3634/9999): loss=2.2023459283923357, w0=73.23750000000008, w1=14.034856404183998\n",
      "Gradient Descent(3635/9999): loss=1.9201830118967234, w0=73.23750000000008, w1=13.839093953819043\n",
      "Gradient Descent(3636/9999): loss=1.76852105019792, w0=73.57500000000009, w1=13.866885285555599\n",
      "Gradient Descent(3637/9999): loss=1.9402158180268574, w0=73.57500000000009, w1=13.964289607081884\n",
      "Gradient Descent(3638/9999): loss=2.8221583457373236, w0=73.57500000000009, w1=13.941689349956166\n",
      "Gradient Descent(3639/9999): loss=2.198150403254876, w0=73.6312500000001, w1=13.849067307790348\n",
      "Gradient Descent(3640/9999): loss=1.8266659291181047, w0=73.6875000000001, w1=13.635757218999442\n",
      "Gradient Descent(3641/9999): loss=2.091530865349016, w0=73.8000000000001, w1=13.54303297371277\n",
      "Gradient Descent(3642/9999): loss=1.6694270618827354, w0=73.8000000000001, w1=13.633023713630507\n",
      "Gradient Descent(3643/9999): loss=1.965558705465885, w0=73.74375000000009, w1=13.314792667082585\n",
      "Gradient Descent(3644/9999): loss=1.8628253426477333, w0=73.85625000000009, w1=13.507764370922258\n",
      "Gradient Descent(3645/9999): loss=2.2727616350369546, w0=73.68750000000009, w1=13.175745543366586\n",
      "Gradient Descent(3646/9999): loss=2.3096632807907382, w0=73.57500000000009, w1=12.861578671797583\n",
      "Gradient Descent(3647/9999): loss=2.0279019164919125, w0=73.46250000000009, w1=13.226107146102136\n",
      "Gradient Descent(3648/9999): loss=2.5919831734179564, w0=73.6312500000001, w1=13.469237316571194\n",
      "Gradient Descent(3649/9999): loss=2.083621955020884, w0=73.57500000000009, w1=13.746578007982135\n",
      "Gradient Descent(3650/9999): loss=2.5417154472745915, w0=73.9125000000001, w1=13.775329484118243\n",
      "Gradient Descent(3651/9999): loss=2.0573073324263316, w0=73.9125000000001, w1=14.056006581982647\n",
      "Gradient Descent(3652/9999): loss=2.304542320540072, w0=74.02500000000009, w1=14.009051750815127\n",
      "Gradient Descent(3653/9999): loss=2.6501669632180613, w0=73.96875000000009, w1=14.3543438629872\n",
      "Gradient Descent(3654/9999): loss=2.187201681408492, w0=74.08125000000008, w1=14.523774888415744\n",
      "Gradient Descent(3655/9999): loss=2.292240047485347, w0=73.80000000000008, w1=14.41341910546937\n",
      "Gradient Descent(3656/9999): loss=2.178710979884185, w0=73.63125000000008, w1=14.082214717575814\n",
      "Gradient Descent(3657/9999): loss=2.978781334356432, w0=73.74375000000008, w1=13.745596442669157\n",
      "Gradient Descent(3658/9999): loss=2.234868304227556, w0=73.57500000000007, w1=13.73324961798692\n",
      "Gradient Descent(3659/9999): loss=2.6351118294146025, w0=73.46250000000008, w1=13.88602350547851\n",
      "Gradient Descent(3660/9999): loss=2.1534621812226193, w0=73.40625000000007, w1=13.897445752069967\n",
      "Gradient Descent(3661/9999): loss=2.2878731625958615, w0=73.51875000000007, w1=13.80493312030478\n",
      "Gradient Descent(3662/9999): loss=2.013026373742763, w0=73.40625000000007, w1=13.985426096191732\n",
      "Gradient Descent(3663/9999): loss=2.0839909720398175, w0=73.63125000000007, w1=13.913078207453033\n",
      "Gradient Descent(3664/9999): loss=2.024100190512276, w0=73.57500000000006, w1=13.885038762542099\n",
      "Gradient Descent(3665/9999): loss=2.5930379203572924, w0=73.51875000000005, w1=13.972387401999235\n",
      "Gradient Descent(3666/9999): loss=2.21119990820039, w0=73.63125000000005, w1=14.151601345688245\n",
      "Gradient Descent(3667/9999): loss=1.9551970238148042, w0=73.51875000000005, w1=14.35529077414925\n",
      "Gradient Descent(3668/9999): loss=2.827922142479346, w0=73.57500000000006, w1=14.052222752078311\n",
      "Gradient Descent(3669/9999): loss=2.7142893540190465, w0=73.57500000000006, w1=13.792427527508563\n",
      "Gradient Descent(3670/9999): loss=2.3258665327681927, w0=73.91250000000007, w1=13.431212434224761\n",
      "Gradient Descent(3671/9999): loss=2.2513365386112802, w0=74.02500000000006, w1=13.153701664719835\n",
      "Gradient Descent(3672/9999): loss=2.375101811434278, w0=73.85625000000006, w1=13.596588952711036\n",
      "Gradient Descent(3673/9999): loss=1.9132941006381037, w0=73.85625000000006, w1=13.68543594703596\n",
      "Gradient Descent(3674/9999): loss=2.110020932475892, w0=73.57500000000006, w1=13.660477716032393\n",
      "Gradient Descent(3675/9999): loss=2.1173069965802296, w0=73.46250000000006, w1=13.594673794122635\n",
      "Gradient Descent(3676/9999): loss=2.6189259259782287, w0=73.51875000000007, w1=13.514887238181984\n",
      "Gradient Descent(3677/9999): loss=2.058035958003835, w0=73.40625000000007, w1=13.445876090531543\n",
      "Gradient Descent(3678/9999): loss=2.382750400018885, w0=73.40625000000007, w1=13.688151539006169\n",
      "Gradient Descent(3679/9999): loss=2.2702446756022865, w0=73.63125000000007, w1=13.59751850792114\n",
      "Gradient Descent(3680/9999): loss=2.1920223009397333, w0=73.80000000000007, w1=13.483623298735896\n",
      "Gradient Descent(3681/9999): loss=2.2591997208141907, w0=73.57500000000007, w1=13.347336176468586\n",
      "Gradient Descent(3682/9999): loss=2.367171844099355, w0=73.74375000000008, w1=13.405928804708811\n",
      "Gradient Descent(3683/9999): loss=2.112421020898653, w0=73.80000000000008, w1=13.71764345455701\n",
      "Gradient Descent(3684/9999): loss=2.0697699210379423, w0=73.57500000000009, w1=13.684254717423812\n",
      "Gradient Descent(3685/9999): loss=2.4646838404279263, w0=73.80000000000008, w1=13.818851365815895\n",
      "Gradient Descent(3686/9999): loss=2.13142641786229, w0=73.63125000000008, w1=13.918584891917844\n",
      "Gradient Descent(3687/9999): loss=1.7975903926727501, w0=73.51875000000008, w1=13.558917974967702\n",
      "Gradient Descent(3688/9999): loss=1.948824189668971, w0=73.40625000000009, w1=13.528434676253381\n",
      "Gradient Descent(3689/9999): loss=2.700018113046455, w0=73.40625000000009, w1=13.505633749889068\n",
      "Gradient Descent(3690/9999): loss=2.064632288991717, w0=73.18125000000009, w1=13.594789694266396\n",
      "Gradient Descent(3691/9999): loss=1.8410107042977135, w0=73.18125000000009, w1=13.509181530441879\n",
      "Gradient Descent(3692/9999): loss=2.3965950587944302, w0=73.18125000000009, w1=13.650796652426704\n",
      "Gradient Descent(3693/9999): loss=1.9549860010393483, w0=73.2375000000001, w1=13.825707960409398\n",
      "Gradient Descent(3694/9999): loss=2.308796025999574, w0=73.5187500000001, w1=13.789468620386407\n",
      "Gradient Descent(3695/9999): loss=2.091615931505843, w0=73.6312500000001, w1=13.991254456052019\n",
      "Gradient Descent(3696/9999): loss=2.13260443527592, w0=73.74375000000009, w1=13.796283290219197\n",
      "Gradient Descent(3697/9999): loss=2.2394274222091632, w0=73.57500000000009, w1=13.803313495659616\n",
      "Gradient Descent(3698/9999): loss=2.110968556105269, w0=73.40625000000009, w1=13.794004857993105\n",
      "Gradient Descent(3699/9999): loss=2.0667199283436437, w0=73.57500000000009, w1=13.581871306673776\n",
      "Gradient Descent(3700/9999): loss=2.4313206442224935, w0=73.6312500000001, w1=13.517535471851453\n",
      "Gradient Descent(3701/9999): loss=1.9219942709194613, w0=73.29375000000009, w1=13.621887308747398\n",
      "Gradient Descent(3702/9999): loss=2.371703384486672, w0=73.23750000000008, w1=13.67953176348022\n",
      "Gradient Descent(3703/9999): loss=1.9961801971553264, w0=73.12500000000009, w1=13.471116586022386\n",
      "Gradient Descent(3704/9999): loss=1.972961340537117, w0=72.95625000000008, w1=13.67757304439251\n",
      "Gradient Descent(3705/9999): loss=2.326003500488264, w0=73.01250000000009, w1=13.461069422400818\n",
      "Gradient Descent(3706/9999): loss=2.4431398710397416, w0=73.12500000000009, w1=13.459965612530192\n",
      "Gradient Descent(3707/9999): loss=2.2657706926740633, w0=73.01250000000009, w1=13.726658642832941\n",
      "Gradient Descent(3708/9999): loss=2.6009774433500215, w0=73.12500000000009, w1=13.865247059409327\n",
      "Gradient Descent(3709/9999): loss=1.8709456812091507, w0=72.67500000000008, w1=13.990374146159365\n",
      "Gradient Descent(3710/9999): loss=2.0903536279054746, w0=72.84375000000009, w1=14.080170211944267\n",
      "Gradient Descent(3711/9999): loss=2.104881425947851, w0=72.73125000000009, w1=14.088754476635232\n",
      "Gradient Descent(3712/9999): loss=2.5403273653757994, w0=72.56250000000009, w1=13.767985556409924\n",
      "Gradient Descent(3713/9999): loss=1.959659936157308, w0=72.50625000000008, w1=13.945582444771228\n",
      "Gradient Descent(3714/9999): loss=2.659194495874006, w0=72.61875000000008, w1=14.018585564758165\n",
      "Gradient Descent(3715/9999): loss=1.8512159508460089, w0=72.50625000000008, w1=14.067175713708664\n",
      "Gradient Descent(3716/9999): loss=2.3239411413187474, w0=72.73125000000007, w1=14.031923090414223\n",
      "Gradient Descent(3717/9999): loss=1.8588731747555154, w0=72.95625000000007, w1=13.9045877077202\n",
      "Gradient Descent(3718/9999): loss=2.2784127461161567, w0=73.06875000000007, w1=13.7105159819921\n",
      "Gradient Descent(3719/9999): loss=2.163189090124356, w0=72.78750000000007, w1=13.802826894425872\n",
      "Gradient Descent(3720/9999): loss=1.805669314928568, w0=72.78750000000007, w1=13.953458140768422\n",
      "Gradient Descent(3721/9999): loss=1.891698522123446, w0=72.90000000000006, w1=13.98899567604424\n",
      "Gradient Descent(3722/9999): loss=2.437532746645609, w0=73.01250000000006, w1=13.916489935343755\n",
      "Gradient Descent(3723/9999): loss=2.15886062038743, w0=72.78750000000007, w1=13.524162033310901\n",
      "Gradient Descent(3724/9999): loss=2.515833871493154, w0=72.61875000000006, w1=13.967057386057478\n",
      "Gradient Descent(3725/9999): loss=2.3315565316828204, w0=72.33750000000006, w1=13.799696074275058\n",
      "Gradient Descent(3726/9999): loss=1.3387227543202418, w0=72.50625000000007, w1=13.569501396195419\n",
      "Gradient Descent(3727/9999): loss=2.5771572570906014, w0=72.73125000000006, w1=13.385899996989187\n",
      "Gradient Descent(3728/9999): loss=2.540498538078758, w0=72.90000000000006, w1=13.432825605337849\n",
      "Gradient Descent(3729/9999): loss=1.917186591575786, w0=72.78750000000007, w1=13.540620007176319\n",
      "Gradient Descent(3730/9999): loss=1.918150361597818, w0=72.84375000000007, w1=13.453487977676923\n",
      "Gradient Descent(3731/9999): loss=1.8391492119975317, w0=72.84375000000007, w1=13.576286538005114\n",
      "Gradient Descent(3732/9999): loss=2.5486553745709424, w0=72.61875000000008, w1=13.572219645519265\n",
      "Gradient Descent(3733/9999): loss=1.7805319920416578, w0=72.61875000000008, w1=13.665301313712973\n",
      "Gradient Descent(3734/9999): loss=1.7062489366711056, w0=72.56250000000007, w1=13.717300524578889\n",
      "Gradient Descent(3735/9999): loss=2.2004989887323836, w0=72.50625000000007, w1=13.707906866908335\n",
      "Gradient Descent(3736/9999): loss=2.320794323371632, w0=72.33750000000006, w1=13.400731238456034\n",
      "Gradient Descent(3737/9999): loss=2.3493291684693745, w0=72.61875000000006, w1=13.724362004924744\n",
      "Gradient Descent(3738/9999): loss=1.8660728376097815, w0=72.56250000000006, w1=13.821995386329917\n",
      "Gradient Descent(3739/9999): loss=2.6952301372997636, w0=72.67500000000005, w1=13.73106468917854\n",
      "Gradient Descent(3740/9999): loss=1.836570014286111, w0=72.56250000000006, w1=13.912294579247067\n",
      "Gradient Descent(3741/9999): loss=1.8920524124148836, w0=72.67500000000005, w1=13.555155437132543\n",
      "Gradient Descent(3742/9999): loss=2.4283935069226246, w0=72.90000000000005, w1=13.754052371020753\n",
      "Gradient Descent(3743/9999): loss=3.2301816481826764, w0=72.95625000000005, w1=13.670073525412056\n",
      "Gradient Descent(3744/9999): loss=2.079064682879787, w0=73.01250000000006, w1=13.701918818204554\n",
      "Gradient Descent(3745/9999): loss=2.406017991535622, w0=73.06875000000007, w1=13.676763314498652\n",
      "Gradient Descent(3746/9999): loss=2.2550931558933685, w0=73.18125000000006, w1=13.592355501188688\n",
      "Gradient Descent(3747/9999): loss=1.9460772874596761, w0=73.23750000000007, w1=13.751914719173543\n",
      "Gradient Descent(3748/9999): loss=1.8894694938318797, w0=73.12500000000007, w1=13.496407281750233\n",
      "Gradient Descent(3749/9999): loss=2.401083472849964, w0=72.90000000000008, w1=13.38149425929698\n",
      "Gradient Descent(3750/9999): loss=2.26049605363307, w0=72.90000000000008, w1=13.461853090291964\n",
      "Gradient Descent(3751/9999): loss=2.4523433177050844, w0=73.23750000000008, w1=13.48567953628984\n",
      "Gradient Descent(3752/9999): loss=2.7323773734713592, w0=73.06875000000008, w1=13.553810427332948\n",
      "Gradient Descent(3753/9999): loss=2.309081699954726, w0=72.84375000000009, w1=13.542746009899618\n",
      "Gradient Descent(3754/9999): loss=2.6755720086813852, w0=73.01250000000009, w1=13.55683716520971\n",
      "Gradient Descent(3755/9999): loss=2.3924464207262774, w0=73.12500000000009, w1=13.701717519065035\n",
      "Gradient Descent(3756/9999): loss=1.7803913739950814, w0=73.18125000000009, w1=13.697509834305537\n",
      "Gradient Descent(3757/9999): loss=2.267193651850244, w0=73.12500000000009, w1=13.766778693184044\n",
      "Gradient Descent(3758/9999): loss=2.2292630549946306, w0=73.12500000000009, w1=13.600080429425512\n",
      "Gradient Descent(3759/9999): loss=1.9874209360862627, w0=73.29375000000009, w1=13.821783446021607\n",
      "Gradient Descent(3760/9999): loss=2.247845054107734, w0=73.40625000000009, w1=13.737423557024607\n",
      "Gradient Descent(3761/9999): loss=2.1746532578940543, w0=73.46250000000009, w1=13.798519599227081\n",
      "Gradient Descent(3762/9999): loss=2.2187940625695175, w0=73.6312500000001, w1=13.977705847441412\n",
      "Gradient Descent(3763/9999): loss=2.258078731787624, w0=73.5187500000001, w1=13.899156760220952\n",
      "Gradient Descent(3764/9999): loss=2.1808039249560376, w0=73.6875000000001, w1=13.759628028764794\n",
      "Gradient Descent(3765/9999): loss=1.869031504303855, w0=73.5750000000001, w1=13.546762981277963\n",
      "Gradient Descent(3766/9999): loss=2.4216808918449337, w0=73.2375000000001, w1=13.412591204767102\n",
      "Gradient Descent(3767/9999): loss=2.6138388275746682, w0=73.2375000000001, w1=13.401295530815604\n",
      "Gradient Descent(3768/9999): loss=2.310192576829092, w0=73.46250000000009, w1=13.574690334225542\n",
      "Gradient Descent(3769/9999): loss=2.4091145538976497, w0=73.6312500000001, w1=13.609388160868786\n",
      "Gradient Descent(3770/9999): loss=2.3914889176848275, w0=73.85625000000009, w1=13.625666134092464\n",
      "Gradient Descent(3771/9999): loss=2.6492686531812284, w0=73.9125000000001, w1=13.626712867683507\n",
      "Gradient Descent(3772/9999): loss=1.7113540560183154, w0=73.74375000000009, w1=13.804715517499606\n",
      "Gradient Descent(3773/9999): loss=2.1877681740634314, w0=73.57500000000009, w1=13.741504342448508\n",
      "Gradient Descent(3774/9999): loss=2.1221470513835046, w0=73.57500000000009, w1=13.746477607414068\n",
      "Gradient Descent(3775/9999): loss=2.3152299483581373, w0=73.3500000000001, w1=13.822086283632304\n",
      "Gradient Descent(3776/9999): loss=2.365964122907902, w0=73.46250000000009, w1=13.925423127149443\n",
      "Gradient Descent(3777/9999): loss=2.1099473964273905, w0=73.46250000000009, w1=13.640234777310027\n",
      "Gradient Descent(3778/9999): loss=2.123998266820584, w0=73.3500000000001, w1=13.964547738943269\n",
      "Gradient Descent(3779/9999): loss=1.640177479087833, w0=73.2375000000001, w1=13.898060406676139\n",
      "Gradient Descent(3780/9999): loss=2.402197994752487, w0=72.9562500000001, w1=13.745302088473547\n",
      "Gradient Descent(3781/9999): loss=1.9769419733995668, w0=72.7312500000001, w1=13.752531118432863\n",
      "Gradient Descent(3782/9999): loss=1.9734375830660678, w0=72.78750000000011, w1=13.747565467384115\n",
      "Gradient Descent(3783/9999): loss=2.1288159567002998, w0=72.6187500000001, w1=13.962432163606005\n",
      "Gradient Descent(3784/9999): loss=2.7000894450012254, w0=72.6187500000001, w1=14.116006176024555\n",
      "Gradient Descent(3785/9999): loss=2.326374342181563, w0=72.50625000000011, w1=13.80529771050187\n",
      "Gradient Descent(3786/9999): loss=2.0079074720491423, w0=72.4500000000001, w1=13.467808953136924\n",
      "Gradient Descent(3787/9999): loss=1.8924804551201238, w0=72.6187500000001, w1=13.609725368751832\n",
      "Gradient Descent(3788/9999): loss=2.9047552267935006, w0=73.0125000000001, w1=13.57315675343393\n",
      "Gradient Descent(3789/9999): loss=2.4046786590900866, w0=73.1250000000001, w1=13.651225241219628\n",
      "Gradient Descent(3790/9999): loss=1.8344581229943657, w0=73.3500000000001, w1=13.768890823248656\n",
      "Gradient Descent(3791/9999): loss=1.835971987902096, w0=73.29375000000009, w1=13.901204237753817\n",
      "Gradient Descent(3792/9999): loss=2.401594406943397, w0=73.18125000000009, w1=13.866288764128731\n",
      "Gradient Descent(3793/9999): loss=2.346791584104519, w0=73.29375000000009, w1=13.815582734663769\n",
      "Gradient Descent(3794/9999): loss=1.9488019209876295, w0=73.3500000000001, w1=14.064698375559672\n",
      "Gradient Descent(3795/9999): loss=2.0819744260646544, w0=73.29375000000009, w1=14.139369319562435\n",
      "Gradient Descent(3796/9999): loss=2.2057423778365766, w0=73.12500000000009, w1=14.063801960982882\n",
      "Gradient Descent(3797/9999): loss=2.2657478985942947, w0=73.51875000000008, w1=13.996552628652859\n",
      "Gradient Descent(3798/9999): loss=1.953682154992017, w0=73.35000000000008, w1=13.89474480438647\n",
      "Gradient Descent(3799/9999): loss=2.0921740101188187, w0=73.29375000000007, w1=13.913187790798984\n",
      "Gradient Descent(3800/9999): loss=2.125111904898062, w0=73.40625000000007, w1=13.615312934500848\n",
      "Gradient Descent(3801/9999): loss=2.1200075877453655, w0=73.29375000000007, w1=13.454893224215327\n",
      "Gradient Descent(3802/9999): loss=1.9619301505767235, w0=73.23750000000007, w1=13.375300567610797\n",
      "Gradient Descent(3803/9999): loss=2.0580842020280583, w0=73.29375000000007, w1=13.292433980345026\n",
      "Gradient Descent(3804/9999): loss=2.6696147209144234, w0=73.23750000000007, w1=13.464341360097643\n",
      "Gradient Descent(3805/9999): loss=2.4921364117298914, w0=73.12500000000007, w1=13.291171744584137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(3806/9999): loss=1.850391180518413, w0=73.12500000000007, w1=13.053570850668448\n",
      "Gradient Descent(3807/9999): loss=2.304729232042234, w0=72.95625000000007, w1=13.3051357554663\n",
      "Gradient Descent(3808/9999): loss=1.8090595638974798, w0=72.95625000000007, w1=13.002663520774004\n",
      "Gradient Descent(3809/9999): loss=2.5097126629646467, w0=73.12500000000007, w1=13.386080431415682\n",
      "Gradient Descent(3810/9999): loss=2.399594003727925, w0=73.12500000000007, w1=13.326401316326852\n",
      "Gradient Descent(3811/9999): loss=2.1363161185071187, w0=73.29375000000007, w1=13.353068974272926\n",
      "Gradient Descent(3812/9999): loss=2.3070502084869604, w0=73.40625000000007, w1=13.531409433066878\n",
      "Gradient Descent(3813/9999): loss=2.0238218623762116, w0=73.46250000000008, w1=13.792301575422757\n",
      "Gradient Descent(3814/9999): loss=1.9829477421959778, w0=73.46250000000008, w1=13.532194170323095\n",
      "Gradient Descent(3815/9999): loss=2.3271965455704535, w0=73.46250000000008, w1=13.600886883259767\n",
      "Gradient Descent(3816/9999): loss=2.2599906901117115, w0=73.40625000000007, w1=13.48540339355219\n",
      "Gradient Descent(3817/9999): loss=2.4152526536882464, w0=73.63125000000007, w1=13.504365262191099\n",
      "Gradient Descent(3818/9999): loss=2.2888765685625696, w0=73.57500000000006, w1=13.569929358471537\n",
      "Gradient Descent(3819/9999): loss=2.2467733053686594, w0=73.46250000000006, w1=13.579402929803441\n",
      "Gradient Descent(3820/9999): loss=2.0406368679459623, w0=73.46250000000006, w1=13.325446308076545\n",
      "Gradient Descent(3821/9999): loss=2.2819194672695393, w0=73.63125000000007, w1=13.44580369589067\n",
      "Gradient Descent(3822/9999): loss=2.2317222781240074, w0=73.57500000000006, w1=13.242920746063211\n",
      "Gradient Descent(3823/9999): loss=2.255088349362147, w0=73.46250000000006, w1=13.405722011902293\n",
      "Gradient Descent(3824/9999): loss=2.7986411758231045, w0=73.51875000000007, w1=13.37863850525195\n",
      "Gradient Descent(3825/9999): loss=2.05365417584846, w0=73.63125000000007, w1=13.38414411230715\n",
      "Gradient Descent(3826/9999): loss=1.8139934991710092, w0=73.51875000000007, w1=13.347148000794611\n",
      "Gradient Descent(3827/9999): loss=2.311631084483342, w0=73.35000000000007, w1=13.561393363210556\n",
      "Gradient Descent(3828/9999): loss=2.432801754649547, w0=73.40625000000007, w1=13.340857518294705\n",
      "Gradient Descent(3829/9999): loss=2.436550090090214, w0=73.40625000000007, w1=13.506133597425235\n",
      "Gradient Descent(3830/9999): loss=2.268064575531074, w0=73.57500000000007, w1=13.680659904099832\n",
      "Gradient Descent(3831/9999): loss=2.463180161704354, w0=73.63125000000008, w1=13.579674035516664\n",
      "Gradient Descent(3832/9999): loss=1.9079220293059898, w0=73.35000000000008, w1=13.512231189705817\n",
      "Gradient Descent(3833/9999): loss=1.9323376642096912, w0=73.51875000000008, w1=13.50196590528684\n",
      "Gradient Descent(3834/9999): loss=2.90679358963862, w0=73.18125000000008, w1=13.569591478363153\n",
      "Gradient Descent(3835/9999): loss=2.3351097901969196, w0=73.46250000000008, w1=13.494914658332291\n",
      "Gradient Descent(3836/9999): loss=2.607212693490989, w0=73.40625000000007, w1=13.456912291745407\n",
      "Gradient Descent(3837/9999): loss=2.661759792740931, w0=73.12500000000007, w1=13.47344195245026\n",
      "Gradient Descent(3838/9999): loss=2.347616640636633, w0=73.12500000000007, w1=13.74141134605121\n",
      "Gradient Descent(3839/9999): loss=2.695649745607149, w0=73.29375000000007, w1=14.02236087258699\n",
      "Gradient Descent(3840/9999): loss=2.132905581797605, w0=73.18125000000008, w1=13.826456112532068\n",
      "Gradient Descent(3841/9999): loss=2.385063460812174, w0=73.06875000000008, w1=13.872454394061323\n",
      "Gradient Descent(3842/9999): loss=2.5710599644638057, w0=73.06875000000008, w1=14.081177039397453\n",
      "Gradient Descent(3843/9999): loss=1.9480250835615038, w0=72.90000000000008, w1=14.217515968348076\n",
      "Gradient Descent(3844/9999): loss=1.770125900590679, w0=73.18125000000008, w1=13.683406424484257\n",
      "Gradient Descent(3845/9999): loss=2.1496137359364784, w0=73.18125000000008, w1=13.567955910734328\n",
      "Gradient Descent(3846/9999): loss=2.0780988469421477, w0=73.51875000000008, w1=13.332053104468711\n",
      "Gradient Descent(3847/9999): loss=1.8817542091575112, w0=73.40625000000009, w1=13.35578306617273\n",
      "Gradient Descent(3848/9999): loss=1.9135812634122404, w0=73.46250000000009, w1=13.474689144553716\n",
      "Gradient Descent(3849/9999): loss=2.3314385178301555, w0=73.40625000000009, w1=13.632830080988848\n",
      "Gradient Descent(3850/9999): loss=2.0834306432470546, w0=73.23750000000008, w1=13.563036432306799\n",
      "Gradient Descent(3851/9999): loss=2.444549300716686, w0=73.51875000000008, w1=13.591589460879339\n",
      "Gradient Descent(3852/9999): loss=2.150427189118301, w0=73.63125000000008, w1=13.477551417420088\n",
      "Gradient Descent(3853/9999): loss=2.2827339877753428, w0=73.51875000000008, w1=13.3843378108628\n",
      "Gradient Descent(3854/9999): loss=2.335865114242387, w0=73.63125000000008, w1=13.264712040509233\n",
      "Gradient Descent(3855/9999): loss=2.505017160653708, w0=73.63125000000008, w1=13.238291037941568\n",
      "Gradient Descent(3856/9999): loss=1.732646345534566, w0=73.80000000000008, w1=13.526622666307407\n",
      "Gradient Descent(3857/9999): loss=2.310341400357537, w0=73.85625000000009, w1=13.595974590777539\n",
      "Gradient Descent(3858/9999): loss=2.443010473414754, w0=73.85625000000009, w1=13.971538291884004\n",
      "Gradient Descent(3859/9999): loss=2.7979406942485365, w0=73.6312500000001, w1=13.7148193856586\n",
      "Gradient Descent(3860/9999): loss=1.814921168786272, w0=73.74375000000009, w1=13.666241584814795\n",
      "Gradient Descent(3861/9999): loss=2.111737034650461, w0=73.57500000000009, w1=13.571042643861727\n",
      "Gradient Descent(3862/9999): loss=2.119941828106911, w0=73.68750000000009, w1=13.263063187115925\n",
      "Gradient Descent(3863/9999): loss=2.4388576676823015, w0=73.57500000000009, w1=13.306263045933804\n",
      "Gradient Descent(3864/9999): loss=1.9358304074981825, w0=73.3500000000001, w1=13.251959917281965\n",
      "Gradient Descent(3865/9999): loss=2.527489965302646, w0=73.4062500000001, w1=13.334401981880852\n",
      "Gradient Descent(3866/9999): loss=2.07493583395721, w0=73.5750000000001, w1=13.45633407725608\n",
      "Gradient Descent(3867/9999): loss=1.9082259429915347, w0=73.7437500000001, w1=13.51555845999009\n",
      "Gradient Descent(3868/9999): loss=2.288757966203904, w0=73.35000000000011, w1=13.374056759038702\n",
      "Gradient Descent(3869/9999): loss=1.9277387274060303, w0=73.2937500000001, w1=13.436438886762406\n",
      "Gradient Descent(3870/9999): loss=2.087848303710815, w0=73.2375000000001, w1=13.300640670227876\n",
      "Gradient Descent(3871/9999): loss=1.962198894750775, w0=73.0687500000001, w1=13.223456976592571\n",
      "Gradient Descent(3872/9999): loss=2.5575055670175004, w0=72.90000000000009, w1=13.63179573534662\n",
      "Gradient Descent(3873/9999): loss=2.4397247407344773, w0=73.18125000000009, w1=13.555684730810784\n",
      "Gradient Descent(3874/9999): loss=2.4640756579277756, w0=73.01250000000009, w1=13.26778461828879\n",
      "Gradient Descent(3875/9999): loss=2.107328242797676, w0=73.01250000000009, w1=13.206266820322552\n",
      "Gradient Descent(3876/9999): loss=2.1373587042189612, w0=72.90000000000009, w1=13.30218339220591\n",
      "Gradient Descent(3877/9999): loss=2.5620180724298316, w0=72.9562500000001, w1=13.457483807127012\n",
      "Gradient Descent(3878/9999): loss=2.1478480024931876, w0=72.9562500000001, w1=13.597148858231842\n",
      "Gradient Descent(3879/9999): loss=2.962455513344587, w0=73.2375000000001, w1=13.218707643660068\n",
      "Gradient Descent(3880/9999): loss=2.1339725075602063, w0=73.4062500000001, w1=12.929458608208467\n",
      "Gradient Descent(3881/9999): loss=2.0139515828016705, w0=73.4625000000001, w1=12.830017976800836\n",
      "Gradient Descent(3882/9999): loss=2.036903197927024, w0=73.4625000000001, w1=12.967768519517259\n",
      "Gradient Descent(3883/9999): loss=2.3838710279218542, w0=73.4625000000001, w1=12.880293916443893\n",
      "Gradient Descent(3884/9999): loss=2.0803812323312005, w0=73.4625000000001, w1=13.167686190115708\n",
      "Gradient Descent(3885/9999): loss=2.51282257668674, w0=73.51875000000011, w1=13.153040804052084\n",
      "Gradient Descent(3886/9999): loss=2.096723335641751, w0=73.40625000000011, w1=13.210263355450948\n",
      "Gradient Descent(3887/9999): loss=2.183986766850281, w0=73.40625000000011, w1=13.29462100390481\n",
      "Gradient Descent(3888/9999): loss=2.1532273844067413, w0=73.29375000000012, w1=13.506607983333298\n",
      "Gradient Descent(3889/9999): loss=2.4034256413810193, w0=73.35000000000012, w1=13.465743409177858\n",
      "Gradient Descent(3890/9999): loss=1.9670433730945969, w0=73.35000000000012, w1=13.708886394455197\n",
      "Gradient Descent(3891/9999): loss=1.8962732026966478, w0=73.35000000000012, w1=13.762004719739979\n",
      "Gradient Descent(3892/9999): loss=1.9653452904379278, w0=73.35000000000012, w1=13.727780238501497\n",
      "Gradient Descent(3893/9999): loss=2.2522017622683252, w0=73.51875000000013, w1=13.755999385859354\n",
      "Gradient Descent(3894/9999): loss=2.1491025309480585, w0=73.40625000000013, w1=13.81383991189366\n",
      "Gradient Descent(3895/9999): loss=2.8144948091506117, w0=73.35000000000012, w1=13.685255550303612\n",
      "Gradient Descent(3896/9999): loss=2.444623676157142, w0=73.35000000000012, w1=13.781100674087536\n",
      "Gradient Descent(3897/9999): loss=2.316483733881329, w0=73.29375000000012, w1=13.358338629967903\n",
      "Gradient Descent(3898/9999): loss=2.1172972943894717, w0=73.35000000000012, w1=13.268178155468327\n",
      "Gradient Descent(3899/9999): loss=2.712076787738047, w0=73.18125000000012, w1=13.32428225782697\n",
      "Gradient Descent(3900/9999): loss=2.1431279458944017, w0=73.06875000000012, w1=13.677229385242235\n",
      "Gradient Descent(3901/9999): loss=2.6943558328302464, w0=72.95625000000013, w1=13.454555842140826\n",
      "Gradient Descent(3902/9999): loss=2.1738653468908398, w0=72.95625000000013, w1=13.457670346111346\n",
      "Gradient Descent(3903/9999): loss=2.0519493936668414, w0=72.90000000000012, w1=13.498188186251745\n",
      "Gradient Descent(3904/9999): loss=2.3587379361293586, w0=73.06875000000012, w1=13.488665313550358\n",
      "Gradient Descent(3905/9999): loss=2.2106266804027728, w0=73.23750000000013, w1=13.576930219567082\n",
      "Gradient Descent(3906/9999): loss=2.099949274956117, w0=73.46250000000012, w1=13.735136659778554\n",
      "Gradient Descent(3907/9999): loss=2.1923386959911646, w0=73.40625000000011, w1=13.76568209530908\n",
      "Gradient Descent(3908/9999): loss=1.9038035936784938, w0=73.40625000000011, w1=13.859785377875937\n",
      "Gradient Descent(3909/9999): loss=1.9489673403369152, w0=73.29375000000012, w1=13.983957359970349\n",
      "Gradient Descent(3910/9999): loss=2.673093685884571, w0=73.46250000000012, w1=13.789218549075866\n",
      "Gradient Descent(3911/9999): loss=2.296427738722247, w0=73.57500000000012, w1=14.131577963569253\n",
      "Gradient Descent(3912/9999): loss=2.544722951004375, w0=73.51875000000011, w1=14.091586097071387\n",
      "Gradient Descent(3913/9999): loss=2.6737599176831486, w0=73.29375000000012, w1=14.063382750926714\n",
      "Gradient Descent(3914/9999): loss=2.228357619163657, w0=73.18125000000012, w1=13.967234636472227\n",
      "Gradient Descent(3915/9999): loss=2.463542987835547, w0=73.01250000000012, w1=14.168490964057096\n",
      "Gradient Descent(3916/9999): loss=2.002083568995614, w0=73.01250000000012, w1=14.032320124517188\n",
      "Gradient Descent(3917/9999): loss=3.0698984913876504, w0=72.95625000000011, w1=14.079195946644456\n",
      "Gradient Descent(3918/9999): loss=2.0128370511670877, w0=72.73125000000012, w1=13.918040206210108\n",
      "Gradient Descent(3919/9999): loss=2.4220614030752707, w0=72.78750000000012, w1=13.763839932309734\n",
      "Gradient Descent(3920/9999): loss=1.7075188358476652, w0=72.84375000000013, w1=13.965919872843136\n",
      "Gradient Descent(3921/9999): loss=1.5338468908922753, w0=73.01250000000013, w1=13.800517559146583\n",
      "Gradient Descent(3922/9999): loss=1.9929970459080797, w0=73.35000000000014, w1=13.866191462325183\n",
      "Gradient Descent(3923/9999): loss=2.11787336919929, w0=73.29375000000013, w1=13.65668060474013\n",
      "Gradient Descent(3924/9999): loss=2.3433753379374123, w0=73.51875000000013, w1=13.243640756491137\n",
      "Gradient Descent(3925/9999): loss=2.408048796377842, w0=73.63125000000012, w1=13.146788690832594\n",
      "Gradient Descent(3926/9999): loss=1.4039709844425907, w0=73.63125000000012, w1=13.46532471599801\n",
      "Gradient Descent(3927/9999): loss=2.506413242757758, w0=73.63125000000012, w1=13.70825511808004\n",
      "Gradient Descent(3928/9999): loss=2.1056235221620527, w0=73.74375000000012, w1=13.777507669627592\n",
      "Gradient Descent(3929/9999): loss=2.1733593253563948, w0=73.74375000000012, w1=13.71657446002111\n",
      "Gradient Descent(3930/9999): loss=2.2121569055340036, w0=73.40625000000011, w1=13.52323808741969\n",
      "Gradient Descent(3931/9999): loss=2.1585469056333864, w0=73.68750000000011, w1=13.498586636792245\n",
      "Gradient Descent(3932/9999): loss=2.077890436706145, w0=73.74375000000012, w1=13.362385471533493\n",
      "Gradient Descent(3933/9999): loss=2.2957308141805455, w0=73.85625000000012, w1=13.64166300550537\n",
      "Gradient Descent(3934/9999): loss=1.9465852331983755, w0=73.74375000000012, w1=13.709203304945197\n",
      "Gradient Descent(3935/9999): loss=2.1888288796516004, w0=73.51875000000013, w1=13.961076280696506\n",
      "Gradient Descent(3936/9999): loss=2.19600617772638, w0=73.40625000000013, w1=13.617338796121913\n",
      "Gradient Descent(3937/9999): loss=1.839997320366877, w0=73.46250000000013, w1=13.736356058717321\n",
      "Gradient Descent(3938/9999): loss=2.150642970478521, w0=73.23750000000014, w1=13.684007932524228\n",
      "Gradient Descent(3939/9999): loss=1.6056161403252236, w0=73.12500000000014, w1=13.814979778636772\n",
      "Gradient Descent(3940/9999): loss=2.1992028695954584, w0=72.84375000000014, w1=13.760893830907058\n",
      "Gradient Descent(3941/9999): loss=2.774358732631531, w0=72.73125000000014, w1=13.608992307791993\n",
      "Gradient Descent(3942/9999): loss=2.320360450760572, w0=72.50625000000015, w1=13.94828105318698\n",
      "Gradient Descent(3943/9999): loss=2.481857923107497, w0=72.39375000000015, w1=13.913768267125043\n",
      "Gradient Descent(3944/9999): loss=2.70020735365451, w0=72.33750000000015, w1=14.062891214984909\n",
      "Gradient Descent(3945/9999): loss=2.6924465162393876, w0=72.50625000000015, w1=13.785344921304974\n",
      "Gradient Descent(3946/9999): loss=1.915841378501872, w0=72.73125000000014, w1=13.587811482233539\n",
      "Gradient Descent(3947/9999): loss=2.312844145797402, w0=72.84375000000014, w1=13.641622700872398\n",
      "Gradient Descent(3948/9999): loss=1.685457579236164, w0=72.73125000000014, w1=13.869860798884062\n",
      "Gradient Descent(3949/9999): loss=2.078306928034169, w0=72.95625000000014, w1=13.494879017324696\n",
      "Gradient Descent(3950/9999): loss=2.4988359085491996, w0=73.01250000000014, w1=13.414783763646444\n",
      "Gradient Descent(3951/9999): loss=1.9469592762059937, w0=73.12500000000014, w1=13.416187495458232\n",
      "Gradient Descent(3952/9999): loss=2.8476687457461867, w0=73.29375000000014, w1=13.712546890437645\n",
      "Gradient Descent(3953/9999): loss=3.0108541208564463, w0=73.06875000000015, w1=13.676147257087138\n",
      "Gradient Descent(3954/9999): loss=1.7906398972614945, w0=73.01250000000014, w1=13.32507975570418\n",
      "Gradient Descent(3955/9999): loss=2.1867545042227094, w0=72.90000000000015, w1=13.207323874305791\n",
      "Gradient Descent(3956/9999): loss=2.2616004006944133, w0=72.78750000000015, w1=13.230411940031734\n",
      "Gradient Descent(3957/9999): loss=2.3746504769145895, w0=73.18125000000015, w1=13.299046942542915\n",
      "Gradient Descent(3958/9999): loss=2.0341819694388086, w0=72.95625000000015, w1=13.101087717619077\n",
      "Gradient Descent(3959/9999): loss=2.0957874590355963, w0=73.12500000000016, w1=13.317260403204324\n",
      "Gradient Descent(3960/9999): loss=1.8903823947068028, w0=73.18125000000016, w1=13.353143654200869\n",
      "Gradient Descent(3961/9999): loss=2.495845339168585, w0=73.23750000000017, w1=13.641531679908976\n",
      "Gradient Descent(3962/9999): loss=1.7385212774793746, w0=73.18125000000016, w1=13.255083769723857\n",
      "Gradient Descent(3963/9999): loss=2.3557716561318696, w0=73.23750000000017, w1=13.440846307788421\n",
      "Gradient Descent(3964/9999): loss=2.127464773337614, w0=73.12500000000017, w1=13.524912498808995\n",
      "Gradient Descent(3965/9999): loss=2.2151320727758286, w0=73.29375000000017, w1=13.783722574120134\n",
      "Gradient Descent(3966/9999): loss=2.0793031419192927, w0=73.46250000000018, w1=13.85196961656924\n",
      "Gradient Descent(3967/9999): loss=1.68006223512786, w0=73.63125000000018, w1=13.879158344853101\n",
      "Gradient Descent(3968/9999): loss=2.3611700967143285, w0=73.57500000000017, w1=13.704451446110152\n",
      "Gradient Descent(3969/9999): loss=2.399727799943938, w0=73.57500000000017, w1=13.607498498770223\n",
      "Gradient Descent(3970/9999): loss=2.8538333737665815, w0=73.51875000000017, w1=13.330693932356446\n",
      "Gradient Descent(3971/9999): loss=2.5341351279841953, w0=73.29375000000017, w1=13.260318255327027\n",
      "Gradient Descent(3972/9999): loss=2.7135398634454133, w0=73.29375000000017, w1=13.246952166616303\n",
      "Gradient Descent(3973/9999): loss=2.0523206032915087, w0=73.35000000000018, w1=13.248146025488806\n",
      "Gradient Descent(3974/9999): loss=2.5981441597380748, w0=73.29375000000017, w1=13.217279408889326\n",
      "Gradient Descent(3975/9999): loss=2.0834202027137705, w0=73.29375000000017, w1=13.477626520362094\n",
      "Gradient Descent(3976/9999): loss=2.403751353549066, w0=73.40625000000017, w1=13.461375603415776\n",
      "Gradient Descent(3977/9999): loss=1.9169979681489184, w0=73.46250000000018, w1=13.703280228472813\n",
      "Gradient Descent(3978/9999): loss=2.6444411651430437, w0=73.23750000000018, w1=13.58783455773606\n",
      "Gradient Descent(3979/9999): loss=2.510826730131222, w0=73.46250000000018, w1=13.695995421844456\n",
      "Gradient Descent(3980/9999): loss=2.2492509179832587, w0=73.40625000000017, w1=13.929215630688114\n",
      "Gradient Descent(3981/9999): loss=2.7588553746529243, w0=73.23750000000017, w1=13.903531910837609\n",
      "Gradient Descent(3982/9999): loss=2.4973126168868554, w0=73.35000000000016, w1=13.886920165773871\n",
      "Gradient Descent(3983/9999): loss=2.3607497044737227, w0=73.51875000000017, w1=13.701017206023682\n",
      "Gradient Descent(3984/9999): loss=2.431390848241867, w0=73.51875000000017, w1=13.90730247466604\n",
      "Gradient Descent(3985/9999): loss=2.1328253944120825, w0=73.74375000000016, w1=13.769034765062237\n",
      "Gradient Descent(3986/9999): loss=2.117525311978076, w0=73.46250000000016, w1=13.855544105025134\n",
      "Gradient Descent(3987/9999): loss=2.4224417633389344, w0=73.46250000000016, w1=13.70765396103013\n",
      "Gradient Descent(3988/9999): loss=2.0421864963926293, w0=73.63125000000016, w1=13.79500837891964\n",
      "Gradient Descent(3989/9999): loss=1.901937502357936, w0=73.63125000000016, w1=13.67622707860248\n",
      "Gradient Descent(3990/9999): loss=1.8486800315896967, w0=73.63125000000016, w1=13.60738991019022\n",
      "Gradient Descent(3991/9999): loss=2.389709074626351, w0=73.57500000000016, w1=13.524870155235902\n",
      "Gradient Descent(3992/9999): loss=1.9665601453492185, w0=73.40625000000016, w1=13.486590492413768\n",
      "Gradient Descent(3993/9999): loss=2.1270094657395897, w0=73.40625000000016, w1=13.735449828370633\n",
      "Gradient Descent(3994/9999): loss=1.7718247063705372, w0=73.57500000000016, w1=13.761026092922252\n",
      "Gradient Descent(3995/9999): loss=2.0745408603399027, w0=73.35000000000016, w1=13.476462443374388\n",
      "Gradient Descent(3996/9999): loss=1.8693561742389613, w0=73.18125000000016, w1=13.601962240510014\n",
      "Gradient Descent(3997/9999): loss=2.1052313041138473, w0=73.29375000000016, w1=13.650803707856733\n",
      "Gradient Descent(3998/9999): loss=1.8195186405785708, w0=73.18125000000016, w1=13.659046859409843\n",
      "Gradient Descent(3999/9999): loss=2.170490376916229, w0=73.40625000000016, w1=13.73029796696953\n",
      "Gradient Descent(4000/9999): loss=2.6986620029004738, w0=73.40625000000016, w1=13.74956055288679\n",
      "Gradient Descent(4001/9999): loss=2.1931441966283347, w0=73.06875000000015, w1=13.667686559023185\n",
      "Gradient Descent(4002/9999): loss=2.686314875057032, w0=73.23750000000015, w1=13.688179553087549\n",
      "Gradient Descent(4003/9999): loss=1.9820496393482452, w0=73.63125000000015, w1=13.703586496283803\n",
      "Gradient Descent(4004/9999): loss=1.9916828943008595, w0=73.68750000000016, w1=13.817894071672741\n",
      "Gradient Descent(4005/9999): loss=2.4939948829422782, w0=73.68750000000016, w1=13.71760816981883\n",
      "Gradient Descent(4006/9999): loss=2.3809666886660574, w0=73.68750000000016, w1=13.76259275253774\n",
      "Gradient Descent(4007/9999): loss=2.4993903721083086, w0=73.57500000000016, w1=13.894096835015358\n",
      "Gradient Descent(4008/9999): loss=1.9327922189697415, w0=73.68750000000016, w1=13.801243532428593\n",
      "Gradient Descent(4009/9999): loss=2.2885274119601835, w0=73.46250000000016, w1=13.79518854003422\n",
      "Gradient Descent(4010/9999): loss=2.0858586312316456, w0=73.23750000000017, w1=13.994880395574118\n",
      "Gradient Descent(4011/9999): loss=2.191439748159967, w0=73.01250000000017, w1=13.949815996197747\n",
      "Gradient Descent(4012/9999): loss=2.0414237240280304, w0=73.12500000000017, w1=13.74626365327519\n",
      "Gradient Descent(4013/9999): loss=2.6598855677866453, w0=73.35000000000016, w1=13.864085574300823\n",
      "Gradient Descent(4014/9999): loss=2.233272434982122, w0=73.18125000000016, w1=13.6025874516531\n",
      "Gradient Descent(4015/9999): loss=2.9699618140447623, w0=73.35000000000016, w1=13.791989008969177\n",
      "Gradient Descent(4016/9999): loss=2.139981832788286, w0=73.46250000000016, w1=13.863319971633999\n",
      "Gradient Descent(4017/9999): loss=1.9826033423757967, w0=73.57500000000016, w1=13.991240614808147\n",
      "Gradient Descent(4018/9999): loss=1.8367840067127772, w0=73.46250000000016, w1=13.964894356129943\n",
      "Gradient Descent(4019/9999): loss=2.2486162416429902, w0=73.40625000000016, w1=13.73272908459821\n",
      "Gradient Descent(4020/9999): loss=2.703432467322105, w0=73.57500000000016, w1=13.637533481090369\n",
      "Gradient Descent(4021/9999): loss=2.125833721087957, w0=73.40625000000016, w1=13.476555467978928\n",
      "Gradient Descent(4022/9999): loss=2.008073830202338, w0=73.40625000000016, w1=13.844567713833662\n",
      "Gradient Descent(4023/9999): loss=2.079012177412742, w0=73.18125000000016, w1=13.627537010764069\n",
      "Gradient Descent(4024/9999): loss=2.1587703576379855, w0=73.12500000000016, w1=13.785253518544756\n",
      "Gradient Descent(4025/9999): loss=2.4398648430987975, w0=73.18125000000016, w1=13.696648512353603\n",
      "Gradient Descent(4026/9999): loss=2.094782015717404, w0=73.01250000000016, w1=13.247346766454259\n",
      "Gradient Descent(4027/9999): loss=2.173077051350594, w0=73.12500000000016, w1=13.488537341139327\n",
      "Gradient Descent(4028/9999): loss=2.0775602958229635, w0=73.35000000000015, w1=13.508589168806397\n",
      "Gradient Descent(4029/9999): loss=1.5090968648425556, w0=73.40625000000016, w1=13.268861034350879\n",
      "Gradient Descent(4030/9999): loss=2.2111329864446727, w0=73.63125000000015, w1=13.344458520640174\n",
      "Gradient Descent(4031/9999): loss=1.5634133391865916, w0=73.51875000000015, w1=13.128159264006818\n",
      "Gradient Descent(4032/9999): loss=2.511424117351702, w0=73.35000000000015, w1=13.043589705128417\n",
      "Gradient Descent(4033/9999): loss=2.2217195341722853, w0=73.35000000000015, w1=13.324622277875207\n",
      "Gradient Descent(4034/9999): loss=1.9719923842793734, w0=73.29375000000014, w1=13.284697318816695\n",
      "Gradient Descent(4035/9999): loss=1.8318361030997803, w0=73.35000000000015, w1=13.537817160358237\n",
      "Gradient Descent(4036/9999): loss=1.7797860988297143, w0=73.23750000000015, w1=13.436272095111226\n",
      "Gradient Descent(4037/9999): loss=2.0372189996052312, w0=73.01250000000016, w1=13.220021780026235\n",
      "Gradient Descent(4038/9999): loss=1.8798338254658855, w0=73.12500000000016, w1=13.115647327717655\n",
      "Gradient Descent(4039/9999): loss=2.266078614889536, w0=73.35000000000015, w1=13.200703974300394\n",
      "Gradient Descent(4040/9999): loss=1.8391532477018306, w0=73.18125000000015, w1=13.197601269685538\n",
      "Gradient Descent(4041/9999): loss=1.8814425642956516, w0=73.01250000000014, w1=13.441126614748999\n",
      "Gradient Descent(4042/9999): loss=2.702530794596554, w0=73.01250000000014, w1=13.423977583247842\n",
      "Gradient Descent(4043/9999): loss=1.9031800490765138, w0=73.23750000000014, w1=13.426255958605799\n",
      "Gradient Descent(4044/9999): loss=1.5820744100836062, w0=73.23750000000014, w1=13.1189278024227\n",
      "Gradient Descent(4045/9999): loss=1.9921033544874858, w0=73.46250000000013, w1=13.209853256741328\n",
      "Gradient Descent(4046/9999): loss=2.3746503583178953, w0=73.74375000000013, w1=13.42138936117648\n",
      "Gradient Descent(4047/9999): loss=2.3846015903891344, w0=73.80000000000014, w1=13.337453962987759\n",
      "Gradient Descent(4048/9999): loss=2.28795218878573, w0=73.63125000000014, w1=13.486743217435118\n",
      "Gradient Descent(4049/9999): loss=2.4907114884531776, w0=73.63125000000014, w1=13.68307097627793\n",
      "Gradient Descent(4050/9999): loss=2.35443414626977, w0=73.23750000000014, w1=13.833459499159044\n",
      "Gradient Descent(4051/9999): loss=1.649948774247328, w0=73.01250000000014, w1=13.963939961684959\n",
      "Gradient Descent(4052/9999): loss=2.510781897561405, w0=73.18125000000015, w1=13.561024922876545\n",
      "Gradient Descent(4053/9999): loss=1.9577886029762939, w0=72.95625000000015, w1=13.418796722189477\n",
      "Gradient Descent(4054/9999): loss=2.0375401997667106, w0=73.18125000000015, w1=13.283528361804908\n",
      "Gradient Descent(4055/9999): loss=2.3273208569254917, w0=73.23750000000015, w1=13.204494411551048\n",
      "Gradient Descent(4056/9999): loss=2.5282572218880914, w0=73.29375000000016, w1=13.13214932449643\n",
      "Gradient Descent(4057/9999): loss=2.8004614350154333, w0=73.40625000000016, w1=13.036329420389354\n",
      "Gradient Descent(4058/9999): loss=2.4095665068617382, w0=73.46250000000016, w1=13.070965118705825\n",
      "Gradient Descent(4059/9999): loss=2.069961795678375, w0=73.12500000000016, w1=13.031870129714267\n",
      "Gradient Descent(4060/9999): loss=1.9322650969792454, w0=72.95625000000015, w1=13.064664795017634\n",
      "Gradient Descent(4061/9999): loss=2.555555064795231, w0=73.29375000000016, w1=13.275717872700778\n",
      "Gradient Descent(4062/9999): loss=2.1196782193827275, w0=73.06875000000016, w1=13.140899996014905\n",
      "Gradient Descent(4063/9999): loss=2.340614830584242, w0=72.95625000000017, w1=13.313047454863701\n",
      "Gradient Descent(4064/9999): loss=1.4495252893182538, w0=73.23750000000017, w1=13.552682839162783\n",
      "Gradient Descent(4065/9999): loss=2.2954883297706448, w0=73.12500000000017, w1=13.644371778123292\n",
      "Gradient Descent(4066/9999): loss=2.253151334045345, w0=73.51875000000017, w1=13.753962062089188\n",
      "Gradient Descent(4067/9999): loss=2.1783340850882627, w0=73.57500000000017, w1=13.719306559103812\n",
      "Gradient Descent(4068/9999): loss=2.12092824112896, w0=73.68750000000017, w1=14.02582694963527\n",
      "Gradient Descent(4069/9999): loss=2.421501490305076, w0=73.63125000000016, w1=13.899220470360866\n",
      "Gradient Descent(4070/9999): loss=2.1808699063082644, w0=73.68750000000017, w1=13.895758928228375\n",
      "Gradient Descent(4071/9999): loss=2.570246249863678, w0=73.51875000000017, w1=13.74393604727962\n",
      "Gradient Descent(4072/9999): loss=2.1325900717044077, w0=73.40625000000017, w1=13.563634887382884\n",
      "Gradient Descent(4073/9999): loss=2.6129133898946213, w0=73.63125000000016, w1=13.680141252962455\n",
      "Gradient Descent(4074/9999): loss=1.7212992809114889, w0=73.46250000000016, w1=13.595822945584512\n",
      "Gradient Descent(4075/9999): loss=2.0608283398046523, w0=73.29375000000016, w1=13.308178175175351\n",
      "Gradient Descent(4076/9999): loss=2.494160359035996, w0=73.23750000000015, w1=13.535659765152802\n",
      "Gradient Descent(4077/9999): loss=1.8398610129786475, w0=72.90000000000015, w1=13.590927446003121\n",
      "Gradient Descent(4078/9999): loss=2.239578863979721, w0=73.01250000000014, w1=13.621135402722441\n",
      "Gradient Descent(4079/9999): loss=2.1877087635180708, w0=72.95625000000014, w1=13.729749879686402\n",
      "Gradient Descent(4080/9999): loss=2.4493138363268, w0=72.61875000000013, w1=13.276209479173762\n",
      "Gradient Descent(4081/9999): loss=2.6819176331066674, w0=72.84375000000013, w1=13.289608945206975\n",
      "Gradient Descent(4082/9999): loss=2.8519981130376646, w0=72.61875000000013, w1=13.318159761151689\n",
      "Gradient Descent(4083/9999): loss=2.4946579439600853, w0=72.61875000000013, w1=13.235321547662794\n",
      "Gradient Descent(4084/9999): loss=2.631179239688314, w0=72.67500000000014, w1=13.345993487232292\n",
      "Gradient Descent(4085/9999): loss=1.995991846483004, w0=72.84375000000014, w1=13.41403052853694\n",
      "Gradient Descent(4086/9999): loss=2.7016206087800834, w0=72.67500000000014, w1=13.428006764640573\n",
      "Gradient Descent(4087/9999): loss=2.1104471508484774, w0=72.56250000000014, w1=13.456689899113071\n",
      "Gradient Descent(4088/9999): loss=2.077700090999561, w0=72.73125000000014, w1=13.464806121850426\n",
      "Gradient Descent(4089/9999): loss=1.8758027056704285, w0=72.90000000000015, w1=13.438944632645594\n",
      "Gradient Descent(4090/9999): loss=2.5637540383523083, w0=72.78750000000015, w1=13.284032747399475\n",
      "Gradient Descent(4091/9999): loss=2.415751751128891, w0=72.84375000000016, w1=13.153074149226246\n",
      "Gradient Descent(4092/9999): loss=2.196713146556638, w0=72.90000000000016, w1=12.866742419120802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4093/9999): loss=2.2471762965798012, w0=73.29375000000016, w1=12.98535403541551\n",
      "Gradient Descent(4094/9999): loss=2.3573064064178304, w0=73.29375000000016, w1=12.962953642211938\n",
      "Gradient Descent(4095/9999): loss=2.42876102199213, w0=73.51875000000015, w1=13.041518359194173\n",
      "Gradient Descent(4096/9999): loss=2.5702937847681664, w0=73.35000000000015, w1=13.190129724113696\n",
      "Gradient Descent(4097/9999): loss=2.3267565201144835, w0=73.29375000000014, w1=13.399370221899582\n",
      "Gradient Descent(4098/9999): loss=2.219486173417727, w0=73.40625000000014, w1=13.35588437781961\n",
      "Gradient Descent(4099/9999): loss=3.0011418497165554, w0=73.29375000000014, w1=13.216157558374606\n",
      "Gradient Descent(4100/9999): loss=2.1061910848258294, w0=72.95625000000014, w1=13.181383894198007\n",
      "Gradient Descent(4101/9999): loss=1.893680769286445, w0=72.95625000000014, w1=13.075024327844293\n",
      "Gradient Descent(4102/9999): loss=2.2169898208448195, w0=72.84375000000014, w1=12.85404536742991\n",
      "Gradient Descent(4103/9999): loss=2.1756942925794642, w0=72.95625000000014, w1=12.816009477528812\n",
      "Gradient Descent(4104/9999): loss=2.6490433468616414, w0=73.23750000000014, w1=13.23351506852927\n",
      "Gradient Descent(4105/9999): loss=2.3422220016875643, w0=73.23750000000014, w1=13.308335707539477\n",
      "Gradient Descent(4106/9999): loss=2.5719953310234027, w0=73.06875000000014, w1=13.354967355280726\n",
      "Gradient Descent(4107/9999): loss=2.1845775934545926, w0=73.23750000000014, w1=13.415243186315756\n",
      "Gradient Descent(4108/9999): loss=2.3446418280584354, w0=73.57500000000014, w1=13.537788067786025\n",
      "Gradient Descent(4109/9999): loss=2.2772017771041755, w0=73.63125000000015, w1=13.76478606487643\n",
      "Gradient Descent(4110/9999): loss=1.8484508309735943, w0=73.40625000000016, w1=13.662395856417358\n",
      "Gradient Descent(4111/9999): loss=2.218290091174487, w0=73.46250000000016, w1=13.551647914334836\n",
      "Gradient Descent(4112/9999): loss=2.1116187133048823, w0=73.46250000000016, w1=13.550207882764669\n",
      "Gradient Descent(4113/9999): loss=1.5546111779732976, w0=73.35000000000016, w1=13.527944402453166\n",
      "Gradient Descent(4114/9999): loss=2.493673135963318, w0=73.35000000000016, w1=13.498058427597178\n",
      "Gradient Descent(4115/9999): loss=2.0898453776127806, w0=73.40625000000017, w1=13.413237023082193\n",
      "Gradient Descent(4116/9999): loss=2.8249992599361344, w0=73.23750000000017, w1=13.53570459913461\n",
      "Gradient Descent(4117/9999): loss=2.3063473591887833, w0=73.06875000000016, w1=13.530379525295594\n",
      "Gradient Descent(4118/9999): loss=2.657694319126042, w0=73.35000000000016, w1=13.561216148183565\n",
      "Gradient Descent(4119/9999): loss=2.138876969682827, w0=73.23750000000017, w1=13.553473340094056\n",
      "Gradient Descent(4120/9999): loss=1.883065795272622, w0=73.12500000000017, w1=13.495833184744601\n",
      "Gradient Descent(4121/9999): loss=2.5690939424783203, w0=72.95625000000017, w1=13.730127863409987\n",
      "Gradient Descent(4122/9999): loss=2.286013792865658, w0=72.90000000000016, w1=13.614420347314233\n",
      "Gradient Descent(4123/9999): loss=2.1227512809305917, w0=72.73125000000016, w1=13.469251071129277\n",
      "Gradient Descent(4124/9999): loss=2.2639812904076244, w0=72.61875000000016, w1=13.313828305490391\n",
      "Gradient Descent(4125/9999): loss=2.3143548492212327, w0=72.73125000000016, w1=13.182968698372507\n",
      "Gradient Descent(4126/9999): loss=2.1979646007247995, w0=72.56250000000016, w1=13.05864331264161\n",
      "Gradient Descent(4127/9999): loss=2.706695243642219, w0=73.12500000000016, w1=13.271929213224652\n",
      "Gradient Descent(4128/9999): loss=2.255445032378031, w0=73.35000000000015, w1=13.679431937293396\n",
      "Gradient Descent(4129/9999): loss=2.1979815978171855, w0=73.35000000000015, w1=13.711832716478087\n",
      "Gradient Descent(4130/9999): loss=1.9797452257210544, w0=73.63125000000015, w1=13.9274828955234\n",
      "Gradient Descent(4131/9999): loss=2.11729381309531, w0=73.74375000000015, w1=13.975354217522526\n",
      "Gradient Descent(4132/9999): loss=2.415549884514087, w0=73.68750000000014, w1=13.810617226533157\n",
      "Gradient Descent(4133/9999): loss=2.3623114859737937, w0=73.46250000000015, w1=13.724650281279025\n",
      "Gradient Descent(4134/9999): loss=1.9127600611600397, w0=73.29375000000014, w1=13.508345003985728\n",
      "Gradient Descent(4135/9999): loss=1.9202497830274887, w0=73.29375000000014, w1=13.662333575493673\n",
      "Gradient Descent(4136/9999): loss=1.7158353652159801, w0=73.23750000000014, w1=13.703763605759567\n",
      "Gradient Descent(4137/9999): loss=2.4209601324512757, w0=73.51875000000014, w1=13.839322907846736\n",
      "Gradient Descent(4138/9999): loss=2.4881294685759485, w0=73.35000000000014, w1=13.750204014451596\n",
      "Gradient Descent(4139/9999): loss=2.360074225189164, w0=73.35000000000014, w1=13.800016083210576\n",
      "Gradient Descent(4140/9999): loss=1.8051591054519864, w0=73.29375000000013, w1=13.875277804295843\n",
      "Gradient Descent(4141/9999): loss=2.1919990694447233, w0=73.29375000000013, w1=13.573197965841565\n",
      "Gradient Descent(4142/9999): loss=1.9614389532590484, w0=73.12500000000013, w1=13.585318187974613\n",
      "Gradient Descent(4143/9999): loss=1.835836794916387, w0=73.01250000000013, w1=13.752275174668817\n",
      "Gradient Descent(4144/9999): loss=1.6748666825489154, w0=73.06875000000014, w1=13.808786033523091\n",
      "Gradient Descent(4145/9999): loss=2.419626940872152, w0=73.18125000000013, w1=13.320017904014522\n",
      "Gradient Descent(4146/9999): loss=1.965356136515418, w0=73.12500000000013, w1=13.573820145222978\n",
      "Gradient Descent(4147/9999): loss=2.219871132362025, w0=73.06875000000012, w1=13.769488452451478\n",
      "Gradient Descent(4148/9999): loss=2.408249706885811, w0=73.18125000000012, w1=13.737147073356304\n",
      "Gradient Descent(4149/9999): loss=2.0201907146697895, w0=73.18125000000012, w1=13.623657329319483\n",
      "Gradient Descent(4150/9999): loss=2.421648541701773, w0=73.18125000000012, w1=13.692537439112938\n",
      "Gradient Descent(4151/9999): loss=1.851534387973844, w0=73.63125000000012, w1=13.437832262596466\n",
      "Gradient Descent(4152/9999): loss=1.6457862932380398, w0=73.74375000000012, w1=13.563455649223805\n",
      "Gradient Descent(4153/9999): loss=2.5947931348309567, w0=73.63125000000012, w1=13.735101644606514\n",
      "Gradient Descent(4154/9999): loss=2.2047181450222073, w0=73.46250000000012, w1=13.817294079673845\n",
      "Gradient Descent(4155/9999): loss=1.9624833763863665, w0=73.06875000000012, w1=14.048487279853775\n",
      "Gradient Descent(4156/9999): loss=2.439639395891602, w0=73.01250000000012, w1=14.048120547113301\n",
      "Gradient Descent(4157/9999): loss=2.2675020361582607, w0=73.06875000000012, w1=13.90032957527947\n",
      "Gradient Descent(4158/9999): loss=2.3596209337434453, w0=72.95625000000013, w1=13.964686079146457\n",
      "Gradient Descent(4159/9999): loss=2.4414005112075285, w0=72.84375000000013, w1=13.95495109704436\n",
      "Gradient Descent(4160/9999): loss=1.8542922943563402, w0=72.67500000000013, w1=13.694199106484538\n",
      "Gradient Descent(4161/9999): loss=2.086608329426586, w0=72.78750000000012, w1=13.701384459073317\n",
      "Gradient Descent(4162/9999): loss=2.176311552385515, w0=72.90000000000012, w1=13.727145933214793\n",
      "Gradient Descent(4163/9999): loss=2.3139635347548637, w0=73.01250000000012, w1=13.830824168487542\n",
      "Gradient Descent(4164/9999): loss=2.13243458235638, w0=72.90000000000012, w1=13.865803275533663\n",
      "Gradient Descent(4165/9999): loss=2.1706863940393033, w0=73.01250000000012, w1=14.040230189099105\n",
      "Gradient Descent(4166/9999): loss=2.4955809164808924, w0=72.73125000000012, w1=13.962998729482878\n",
      "Gradient Descent(4167/9999): loss=2.03142003062397, w0=72.73125000000012, w1=13.833913194097871\n",
      "Gradient Descent(4168/9999): loss=2.5704729947039997, w0=72.56250000000011, w1=13.557463785761598\n",
      "Gradient Descent(4169/9999): loss=2.1524498051436955, w0=72.78750000000011, w1=13.333487026807079\n",
      "Gradient Descent(4170/9999): loss=1.9097062801152713, w0=72.95625000000011, w1=13.480907497869392\n",
      "Gradient Descent(4171/9999): loss=2.0621415869052013, w0=72.84375000000011, w1=13.33838899692524\n",
      "Gradient Descent(4172/9999): loss=1.892405269328348, w0=72.73125000000012, w1=13.364215855331615\n",
      "Gradient Descent(4173/9999): loss=2.1932702313089734, w0=72.61875000000012, w1=13.731864827482392\n",
      "Gradient Descent(4174/9999): loss=2.214528263624015, w0=72.78750000000012, w1=13.528247325103441\n",
      "Gradient Descent(4175/9999): loss=2.7416194568804455, w0=73.01250000000012, w1=13.629284441602348\n",
      "Gradient Descent(4176/9999): loss=2.5519242861216394, w0=73.23750000000011, w1=13.675451898172037\n",
      "Gradient Descent(4177/9999): loss=2.25078062134377, w0=73.23750000000011, w1=13.75206429662075\n",
      "Gradient Descent(4178/9999): loss=2.795752162974412, w0=73.12500000000011, w1=13.508029024554899\n",
      "Gradient Descent(4179/9999): loss=2.1073394464626682, w0=73.18125000000012, w1=13.719129181893065\n",
      "Gradient Descent(4180/9999): loss=2.623593692373727, w0=73.40625000000011, w1=13.884210434530129\n",
      "Gradient Descent(4181/9999): loss=2.469817287225645, w0=73.51875000000011, w1=14.078808805075088\n",
      "Gradient Descent(4182/9999): loss=2.051771214239179, w0=73.51875000000011, w1=13.881207912927904\n",
      "Gradient Descent(4183/9999): loss=2.473390109365786, w0=73.51875000000011, w1=13.90089070924986\n",
      "Gradient Descent(4184/9999): loss=2.6946019378150377, w0=73.51875000000011, w1=13.835965384661458\n",
      "Gradient Descent(4185/9999): loss=2.650223315014333, w0=73.35000000000011, w1=13.879575671426231\n",
      "Gradient Descent(4186/9999): loss=2.7931200378689454, w0=73.12500000000011, w1=13.620713253779611\n",
      "Gradient Descent(4187/9999): loss=1.9872048336638228, w0=73.35000000000011, w1=13.61759004871729\n",
      "Gradient Descent(4188/9999): loss=2.4579538278556337, w0=73.35000000000011, w1=13.641279751281848\n",
      "Gradient Descent(4189/9999): loss=2.068728178903092, w0=73.2937500000001, w1=13.431372225868053\n",
      "Gradient Descent(4190/9999): loss=1.8407805432979294, w0=73.35000000000011, w1=13.592879816125354\n",
      "Gradient Descent(4191/9999): loss=2.693438002907416, w0=73.12500000000011, w1=13.562121962186724\n",
      "Gradient Descent(4192/9999): loss=2.267091685445777, w0=73.06875000000011, w1=13.593861608438692\n",
      "Gradient Descent(4193/9999): loss=2.2777297651416113, w0=73.23750000000011, w1=13.298908148857313\n",
      "Gradient Descent(4194/9999): loss=1.6715191275563805, w0=73.12500000000011, w1=13.34083054860376\n",
      "Gradient Descent(4195/9999): loss=2.182403686791794, w0=73.06875000000011, w1=13.380141586485017\n",
      "Gradient Descent(4196/9999): loss=2.1825268555317425, w0=73.0125000000001, w1=13.449647671310482\n",
      "Gradient Descent(4197/9999): loss=2.3081873454812696, w0=72.6187500000001, w1=13.348403684596551\n",
      "Gradient Descent(4198/9999): loss=1.9912813912892713, w0=72.7312500000001, w1=13.337288940996954\n",
      "Gradient Descent(4199/9999): loss=2.2104504292137896, w0=72.50625000000011, w1=13.204461502049423\n",
      "Gradient Descent(4200/9999): loss=1.6107513572047079, w0=72.50625000000011, w1=13.171471544355802\n",
      "Gradient Descent(4201/9999): loss=2.55084120976315, w0=72.6187500000001, w1=13.100954320095349\n",
      "Gradient Descent(4202/9999): loss=2.8826076885565572, w0=72.67500000000011, w1=13.083206736987277\n",
      "Gradient Descent(4203/9999): loss=1.998241224615377, w0=72.50625000000011, w1=13.24542666731971\n",
      "Gradient Descent(4204/9999): loss=2.0142136646472886, w0=72.67500000000011, w1=13.163938204401694\n",
      "Gradient Descent(4205/9999): loss=2.3060786132125672, w0=72.67500000000011, w1=13.2827767884218\n",
      "Gradient Descent(4206/9999): loss=2.230493813196994, w0=73.12500000000011, w1=13.352504325141558\n",
      "Gradient Descent(4207/9999): loss=2.48701393736601, w0=73.12500000000011, w1=13.23911944742765\n",
      "Gradient Descent(4208/9999): loss=1.9653223212362754, w0=73.23750000000011, w1=13.324616669807288\n",
      "Gradient Descent(4209/9999): loss=2.110757914338381, w0=73.35000000000011, w1=13.49920441917542\n",
      "Gradient Descent(4210/9999): loss=2.123782814410011, w0=73.2937500000001, w1=13.662695197693406\n",
      "Gradient Descent(4211/9999): loss=2.2847480851183537, w0=73.35000000000011, w1=13.79591477996752\n",
      "Gradient Descent(4212/9999): loss=1.9904373291164017, w0=72.95625000000011, w1=13.870048717332663\n",
      "Gradient Descent(4213/9999): loss=2.294277924386673, w0=72.9000000000001, w1=13.876517218591864\n",
      "Gradient Descent(4214/9999): loss=1.832138761986686, w0=72.9000000000001, w1=13.83445033202277\n",
      "Gradient Descent(4215/9999): loss=2.0735695222888197, w0=72.8437500000001, w1=13.833154432004038\n",
      "Gradient Descent(4216/9999): loss=2.434369643683875, w0=72.6750000000001, w1=13.912062666475135\n",
      "Gradient Descent(4217/9999): loss=1.7142457316333717, w0=72.8437500000001, w1=13.98577581448564\n",
      "Gradient Descent(4218/9999): loss=1.9350389501340426, w0=72.7312500000001, w1=14.071171106400495\n",
      "Gradient Descent(4219/9999): loss=1.9958113692319335, w0=73.06875000000011, w1=14.02382001385498\n",
      "Gradient Descent(4220/9999): loss=2.3651394356473547, w0=73.35000000000011, w1=14.048616893504573\n",
      "Gradient Descent(4221/9999): loss=2.4554873193158895, w0=73.40625000000011, w1=14.131772831823236\n",
      "Gradient Descent(4222/9999): loss=2.494827211666072, w0=73.46250000000012, w1=14.151140941269817\n",
      "Gradient Descent(4223/9999): loss=2.604910919510442, w0=73.40625000000011, w1=13.962452999606198\n",
      "Gradient Descent(4224/9999): loss=2.2869919546886317, w0=73.63125000000011, w1=13.644419723842534\n",
      "Gradient Descent(4225/9999): loss=2.658841932690949, w0=73.7437500000001, w1=13.461904028741762\n",
      "Gradient Descent(4226/9999): loss=1.603566079834481, w0=73.7437500000001, w1=13.607860718962682\n",
      "Gradient Descent(4227/9999): loss=2.0929334720999058, w0=73.80000000000011, w1=13.380851846014284\n",
      "Gradient Descent(4228/9999): loss=2.1525123921560096, w0=73.68750000000011, w1=13.098273853703688\n",
      "Gradient Descent(4229/9999): loss=2.667089254914175, w0=73.18125000000012, w1=13.490254450918895\n",
      "Gradient Descent(4230/9999): loss=1.8285202526587454, w0=73.12500000000011, w1=13.48926477832936\n",
      "Gradient Descent(4231/9999): loss=1.7346245942348868, w0=73.01250000000012, w1=13.577425892143706\n",
      "Gradient Descent(4232/9999): loss=2.370308326856488, w0=73.29375000000012, w1=13.55837306584521\n",
      "Gradient Descent(4233/9999): loss=2.1629637067047485, w0=73.57500000000012, w1=13.566378323683118\n",
      "Gradient Descent(4234/9999): loss=2.1645207307598295, w0=73.68750000000011, w1=13.545865607352939\n",
      "Gradient Descent(4235/9999): loss=1.8828526280011748, w0=73.51875000000011, w1=13.659087091643954\n",
      "Gradient Descent(4236/9999): loss=2.101241504605847, w0=73.4625000000001, w1=13.82700930797005\n",
      "Gradient Descent(4237/9999): loss=2.0075310646969062, w0=73.51875000000011, w1=14.014492566094582\n",
      "Gradient Descent(4238/9999): loss=2.72675674294622, w0=73.35000000000011, w1=13.938642742436498\n",
      "Gradient Descent(4239/9999): loss=2.0221667102389174, w0=73.1812500000001, w1=13.872022491349377\n",
      "Gradient Descent(4240/9999): loss=2.157829651129301, w0=73.1812500000001, w1=13.96364314116753\n",
      "Gradient Descent(4241/9999): loss=1.8221445764278834, w0=73.23750000000011, w1=14.063606176192335\n",
      "Gradient Descent(4242/9999): loss=2.6474001843420205, w0=73.01250000000012, w1=13.762699817303451\n",
      "Gradient Descent(4243/9999): loss=1.8888669979873243, w0=72.84375000000011, w1=13.713436724864001\n",
      "Gradient Descent(4244/9999): loss=2.2711496082519034, w0=72.90000000000012, w1=13.77581770640205\n",
      "Gradient Descent(4245/9999): loss=2.2453467722225775, w0=72.78750000000012, w1=13.545890419757079\n",
      "Gradient Descent(4246/9999): loss=2.8188896547914006, w0=72.84375000000013, w1=13.369227039577229\n",
      "Gradient Descent(4247/9999): loss=2.3159261273331646, w0=72.95625000000013, w1=13.232834056428793\n",
      "Gradient Descent(4248/9999): loss=2.8111470929757028, w0=73.06875000000012, w1=13.325716820015197\n",
      "Gradient Descent(4249/9999): loss=1.6569814781281655, w0=73.23750000000013, w1=13.316556583062214\n",
      "Gradient Descent(4250/9999): loss=2.243227352830517, w0=73.51875000000013, w1=13.53245746771987\n",
      "Gradient Descent(4251/9999): loss=3.082155277984099, w0=73.68750000000013, w1=13.530021183700363\n",
      "Gradient Descent(4252/9999): loss=2.2235987023873705, w0=73.57500000000013, w1=13.3937780546323\n",
      "Gradient Descent(4253/9999): loss=1.9237964695692904, w0=73.35000000000014, w1=13.579661607990147\n",
      "Gradient Descent(4254/9999): loss=2.140438052356544, w0=73.23750000000014, w1=13.805349379995569\n",
      "Gradient Descent(4255/9999): loss=2.0543921547152224, w0=73.51875000000014, w1=13.86051209989435\n",
      "Gradient Descent(4256/9999): loss=2.213394733402315, w0=73.40625000000014, w1=13.89738206438462\n",
      "Gradient Descent(4257/9999): loss=2.571920883619223, w0=73.23750000000014, w1=13.9042241892766\n",
      "Gradient Descent(4258/9999): loss=2.331751863107539, w0=73.35000000000014, w1=13.741447917861331\n",
      "Gradient Descent(4259/9999): loss=2.4802215777482344, w0=73.40625000000014, w1=13.765332746704647\n",
      "Gradient Descent(4260/9999): loss=2.011648127849727, w0=73.51875000000014, w1=13.585687380626176\n",
      "Gradient Descent(4261/9999): loss=1.955228614709001, w0=73.23750000000014, w1=13.507507515869422\n",
      "Gradient Descent(4262/9999): loss=1.7475209760760348, w0=73.12500000000014, w1=13.643883106029087\n",
      "Gradient Descent(4263/9999): loss=1.8811273427565565, w0=73.18125000000015, w1=13.77474311117138\n",
      "Gradient Descent(4264/9999): loss=2.264632094923388, w0=73.06875000000015, w1=13.801086738421699\n",
      "Gradient Descent(4265/9999): loss=2.228230087528318, w0=73.01250000000014, w1=13.814846987448758\n",
      "Gradient Descent(4266/9999): loss=2.5047650860686206, w0=72.90000000000015, w1=13.747607521303431\n",
      "Gradient Descent(4267/9999): loss=2.033085394490647, w0=72.73125000000014, w1=13.613402204279708\n",
      "Gradient Descent(4268/9999): loss=2.2806718554530176, w0=72.78750000000015, w1=13.659136025382974\n",
      "Gradient Descent(4269/9999): loss=2.0918654509286574, w0=72.67500000000015, w1=13.678350115719281\n",
      "Gradient Descent(4270/9999): loss=1.8963464455136123, w0=72.39375000000015, w1=13.699084266880698\n",
      "Gradient Descent(4271/9999): loss=2.445653146867748, w0=72.50625000000015, w1=13.610962818371382\n",
      "Gradient Descent(4272/9999): loss=2.2697654076842255, w0=72.50625000000015, w1=13.670496145613784\n",
      "Gradient Descent(4273/9999): loss=2.199183201572132, w0=72.45000000000014, w1=13.505852981669811\n",
      "Gradient Descent(4274/9999): loss=2.4879933293118563, w0=72.61875000000015, w1=13.332662109815303\n",
      "Gradient Descent(4275/9999): loss=1.801489572584797, w0=72.56250000000014, w1=13.21374641831177\n",
      "Gradient Descent(4276/9999): loss=1.6445579850944563, w0=72.50625000000014, w1=13.297039753313342\n",
      "Gradient Descent(4277/9999): loss=2.1003429652000434, w0=72.67500000000014, w1=13.117317080685261\n",
      "Gradient Descent(4278/9999): loss=1.9153102118647438, w0=72.45000000000014, w1=13.48146553888665\n",
      "Gradient Descent(4279/9999): loss=1.7480414558314934, w0=72.50625000000015, w1=13.251019924447903\n",
      "Gradient Descent(4280/9999): loss=2.2561513362686174, w0=72.61875000000015, w1=13.246723823146908\n",
      "Gradient Descent(4281/9999): loss=2.1982511120212105, w0=72.78750000000015, w1=13.300959271384613\n",
      "Gradient Descent(4282/9999): loss=1.7039134818163335, w0=72.84375000000016, w1=13.171391785649275\n",
      "Gradient Descent(4283/9999): loss=2.703195741177007, w0=72.78750000000015, w1=13.260638236943809\n",
      "Gradient Descent(4284/9999): loss=2.754626513999028, w0=72.90000000000015, w1=13.119532539314111\n",
      "Gradient Descent(4285/9999): loss=2.0637947326080823, w0=73.35000000000015, w1=13.069209608460879\n",
      "Gradient Descent(4286/9999): loss=2.430595308159356, w0=73.23750000000015, w1=13.164859711211253\n",
      "Gradient Descent(4287/9999): loss=2.581603178984115, w0=73.23750000000015, w1=13.178868456822993\n",
      "Gradient Descent(4288/9999): loss=2.1646181313847737, w0=73.40625000000016, w1=13.437398772823107\n",
      "Gradient Descent(4289/9999): loss=2.000186975203886, w0=73.74375000000016, w1=13.163164368133192\n",
      "Gradient Descent(4290/9999): loss=2.3054860922409963, w0=73.68750000000016, w1=13.092663743102333\n",
      "Gradient Descent(4291/9999): loss=2.5690216562939723, w0=73.63125000000015, w1=13.200668451214646\n",
      "Gradient Descent(4292/9999): loss=1.758938282657295, w0=73.63125000000015, w1=13.249183973959635\n",
      "Gradient Descent(4293/9999): loss=2.19557671759038, w0=73.46250000000015, w1=13.323375836708912\n",
      "Gradient Descent(4294/9999): loss=1.941847392875005, w0=73.63125000000015, w1=13.26190358579887\n",
      "Gradient Descent(4295/9999): loss=2.087372646545554, w0=73.63125000000015, w1=13.387369042717138\n",
      "Gradient Descent(4296/9999): loss=2.8057706227450345, w0=73.51875000000015, w1=13.657000691422475\n",
      "Gradient Descent(4297/9999): loss=2.5757769424953016, w0=73.51875000000015, w1=13.481629311816404\n",
      "Gradient Descent(4298/9999): loss=2.569396684902403, w0=73.46250000000015, w1=13.356857019617355\n",
      "Gradient Descent(4299/9999): loss=2.283434446765305, w0=73.23750000000015, w1=13.503945823746786\n",
      "Gradient Descent(4300/9999): loss=1.5684163475479804, w0=73.18125000000015, w1=13.654323054775968\n",
      "Gradient Descent(4301/9999): loss=2.5354790273710996, w0=73.18125000000015, w1=13.902477120569086\n",
      "Gradient Descent(4302/9999): loss=2.082827490378591, w0=73.23750000000015, w1=13.826056323934557\n",
      "Gradient Descent(4303/9999): loss=2.3042923026623634, w0=73.01250000000016, w1=13.990620354335906\n",
      "Gradient Descent(4304/9999): loss=2.4310705933529557, w0=72.95625000000015, w1=14.003208969585744\n",
      "Gradient Descent(4305/9999): loss=2.085050746206194, w0=72.61875000000015, w1=14.024566187512322\n",
      "Gradient Descent(4306/9999): loss=2.258613168657505, w0=72.78750000000015, w1=14.017690196749752\n",
      "Gradient Descent(4307/9999): loss=2.0038680522339636, w0=73.01250000000014, w1=14.15357710284093\n",
      "Gradient Descent(4308/9999): loss=2.462205133875566, w0=73.12500000000014, w1=14.274663489604205\n",
      "Gradient Descent(4309/9999): loss=1.980114383691194, w0=73.06875000000014, w1=14.21254858497229\n",
      "Gradient Descent(4310/9999): loss=2.532541529100615, w0=73.23750000000014, w1=14.004298060835403\n",
      "Gradient Descent(4311/9999): loss=2.005678202903842, w0=73.35000000000014, w1=14.094951048139688\n",
      "Gradient Descent(4312/9999): loss=2.2703777667099683, w0=73.40625000000014, w1=14.115471790895917\n",
      "Gradient Descent(4313/9999): loss=2.129342105620754, w0=73.51875000000014, w1=13.97305828310418\n",
      "Gradient Descent(4314/9999): loss=2.241883278394007, w0=73.51875000000014, w1=13.83318627861837\n",
      "Gradient Descent(4315/9999): loss=2.070012647971026, w0=73.51875000000014, w1=13.908295780945751\n",
      "Gradient Descent(4316/9999): loss=2.0068555956780623, w0=73.51875000000014, w1=13.795232854879245\n",
      "Gradient Descent(4317/9999): loss=2.090141083721022, w0=73.68750000000014, w1=13.882796164563812\n",
      "Gradient Descent(4318/9999): loss=2.0071013425830175, w0=73.63125000000014, w1=14.13282637238855\n",
      "Gradient Descent(4319/9999): loss=2.1944186099775775, w0=73.68750000000014, w1=14.187586212378472\n",
      "Gradient Descent(4320/9999): loss=2.4450132287299615, w0=73.63125000000014, w1=14.084913155261507\n",
      "Gradient Descent(4321/9999): loss=2.6702235145351834, w0=73.29375000000013, w1=13.86517498743101\n",
      "Gradient Descent(4322/9999): loss=2.5413190937536987, w0=73.18125000000013, w1=13.70683594060723\n",
      "Gradient Descent(4323/9999): loss=1.706109969749716, w0=73.23750000000014, w1=13.836151802636083\n",
      "Gradient Descent(4324/9999): loss=2.1486391523548085, w0=73.12500000000014, w1=14.056166590476627\n",
      "Gradient Descent(4325/9999): loss=2.1424602776230994, w0=72.95625000000014, w1=13.93716656647047\n",
      "Gradient Descent(4326/9999): loss=2.630916245357024, w0=73.01250000000014, w1=14.199332672623584\n",
      "Gradient Descent(4327/9999): loss=1.8590308256152956, w0=73.06875000000015, w1=14.231831339589224\n",
      "Gradient Descent(4328/9999): loss=2.282952930254453, w0=73.18125000000015, w1=13.822608373030045\n",
      "Gradient Descent(4329/9999): loss=1.9152551913489928, w0=72.95625000000015, w1=13.850602201362635\n",
      "Gradient Descent(4330/9999): loss=1.8586428729323212, w0=72.90000000000015, w1=13.812324215210309\n",
      "Gradient Descent(4331/9999): loss=2.166333640126747, w0=72.95625000000015, w1=13.750844435469798\n",
      "Gradient Descent(4332/9999): loss=2.191579612940206, w0=73.06875000000015, w1=13.659011384808474\n",
      "Gradient Descent(4333/9999): loss=2.1686677571793735, w0=73.18125000000015, w1=13.82733203883927\n",
      "Gradient Descent(4334/9999): loss=2.1919769835307257, w0=73.01250000000014, w1=13.628445394179026\n",
      "Gradient Descent(4335/9999): loss=2.213497347527044, w0=72.78750000000015, w1=13.789207741528006\n",
      "Gradient Descent(4336/9999): loss=2.239594196368916, w0=72.90000000000015, w1=13.738377631709579\n",
      "Gradient Descent(4337/9999): loss=2.3369150194281327, w0=73.12500000000014, w1=13.783712633219508\n",
      "Gradient Descent(4338/9999): loss=2.129823918677765, w0=73.23750000000014, w1=13.482207332846771\n",
      "Gradient Descent(4339/9999): loss=2.4219853191488268, w0=73.12500000000014, w1=13.574258488383022\n",
      "Gradient Descent(4340/9999): loss=1.6397422100018573, w0=72.84375000000014, w1=13.404614732648929\n",
      "Gradient Descent(4341/9999): loss=1.9766529647900761, w0=72.84375000000014, w1=13.523389813082458\n",
      "Gradient Descent(4342/9999): loss=1.985771065738561, w0=72.61875000000015, w1=13.489369534715474\n",
      "Gradient Descent(4343/9999): loss=2.731383784063187, w0=73.12500000000014, w1=13.314318681636792\n",
      "Gradient Descent(4344/9999): loss=2.270229490297673, w0=72.90000000000015, w1=13.736537394103692\n",
      "Gradient Descent(4345/9999): loss=1.9683314702515018, w0=73.12500000000014, w1=13.592944268159538\n",
      "Gradient Descent(4346/9999): loss=2.2790112175758264, w0=73.06875000000014, w1=13.416741662294669\n",
      "Gradient Descent(4347/9999): loss=2.4615792536089507, w0=72.90000000000013, w1=13.48786140868678\n",
      "Gradient Descent(4348/9999): loss=1.9388998049996906, w0=73.18125000000013, w1=13.5438140764364\n",
      "Gradient Descent(4349/9999): loss=2.310067730814197, w0=73.06875000000014, w1=13.619099219942358\n",
      "Gradient Descent(4350/9999): loss=1.8901481815296104, w0=73.01250000000013, w1=13.565682799900832\n",
      "Gradient Descent(4351/9999): loss=1.8497018027534982, w0=73.23750000000013, w1=13.626325001505727\n",
      "Gradient Descent(4352/9999): loss=2.452536851802021, w0=73.12500000000013, w1=13.333310732191816\n",
      "Gradient Descent(4353/9999): loss=1.9605485285724569, w0=73.35000000000012, w1=13.53970560781936\n",
      "Gradient Descent(4354/9999): loss=2.2288951982630296, w0=73.18125000000012, w1=13.45433349365904\n",
      "Gradient Descent(4355/9999): loss=1.9339597488694014, w0=73.01250000000012, w1=13.358158428695985\n",
      "Gradient Descent(4356/9999): loss=2.752660835975948, w0=72.95625000000011, w1=13.652846534728228\n",
      "Gradient Descent(4357/9999): loss=2.2850376138055664, w0=73.01250000000012, w1=13.703817628901026\n",
      "Gradient Descent(4358/9999): loss=2.5721586261754856, w0=73.12500000000011, w1=13.660904803187622\n",
      "Gradient Descent(4359/9999): loss=2.3066357450257495, w0=73.12500000000011, w1=13.725443366973952\n",
      "Gradient Descent(4360/9999): loss=2.5195511888105857, w0=73.23750000000011, w1=13.597006893920074\n",
      "Gradient Descent(4361/9999): loss=2.06915564947957, w0=73.35000000000011, w1=13.725933425252334\n",
      "Gradient Descent(4362/9999): loss=1.9712834688272793, w0=73.51875000000011, w1=13.667239220260393\n",
      "Gradient Descent(4363/9999): loss=2.6129370913721353, w0=73.85625000000012, w1=13.655709918426824\n",
      "Gradient Descent(4364/9999): loss=2.5177172384778554, w0=73.57500000000012, w1=13.816272296036296\n",
      "Gradient Descent(4365/9999): loss=1.7764470705417708, w0=73.74375000000012, w1=13.71103498799039\n",
      "Gradient Descent(4366/9999): loss=1.9657719989707771, w0=73.85625000000012, w1=13.813355871800555\n",
      "Gradient Descent(4367/9999): loss=2.6685169463519047, w0=73.46250000000012, w1=13.726878934883882\n",
      "Gradient Descent(4368/9999): loss=2.3902146521567094, w0=73.40625000000011, w1=13.652052113591594\n",
      "Gradient Descent(4369/9999): loss=2.477060074850886, w0=73.46250000000012, w1=13.536532539353782\n",
      "Gradient Descent(4370/9999): loss=2.406681048560669, w0=73.57500000000012, w1=13.399464860032573\n",
      "Gradient Descent(4371/9999): loss=2.0259473289648025, w0=73.68750000000011, w1=13.41388541480195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4372/9999): loss=2.359287807766677, w0=73.80000000000011, w1=13.04864675004373\n",
      "Gradient Descent(4373/9999): loss=2.121262338250014, w0=73.51875000000011, w1=13.22818451565492\n",
      "Gradient Descent(4374/9999): loss=2.288558643041854, w0=73.29375000000012, w1=13.364432778957331\n",
      "Gradient Descent(4375/9999): loss=2.355458455527719, w0=73.18125000000012, w1=13.317327840744355\n",
      "Gradient Descent(4376/9999): loss=2.412105971306733, w0=72.84375000000011, w1=13.44965413649926\n",
      "Gradient Descent(4377/9999): loss=2.582762482301454, w0=73.06875000000011, w1=13.443654721049796\n",
      "Gradient Descent(4378/9999): loss=1.9512662202058109, w0=73.12500000000011, w1=13.538899164741364\n",
      "Gradient Descent(4379/9999): loss=1.9034191626873895, w0=73.46250000000012, w1=13.417261038777422\n",
      "Gradient Descent(4380/9999): loss=1.861504850712557, w0=73.51875000000013, w1=13.358997109778036\n",
      "Gradient Descent(4381/9999): loss=1.907791197849249, w0=73.63125000000012, w1=13.369413755146775\n",
      "Gradient Descent(4382/9999): loss=1.8494161545384555, w0=73.35000000000012, w1=13.608974820453714\n",
      "Gradient Descent(4383/9999): loss=1.523289805414179, w0=73.35000000000012, w1=13.585953552329519\n",
      "Gradient Descent(4384/9999): loss=2.2700313998108648, w0=73.23750000000013, w1=13.463380953559275\n",
      "Gradient Descent(4385/9999): loss=1.9922777652096353, w0=72.95625000000013, w1=13.567707513394263\n",
      "Gradient Descent(4386/9999): loss=2.1132196372559604, w0=73.29375000000013, w1=13.60903187803979\n",
      "Gradient Descent(4387/9999): loss=2.184602603386046, w0=73.35000000000014, w1=13.672676357648772\n",
      "Gradient Descent(4388/9999): loss=2.742827386788525, w0=73.18125000000013, w1=13.649296678898184\n",
      "Gradient Descent(4389/9999): loss=2.0327698866500947, w0=72.95625000000014, w1=13.70292074146882\n",
      "Gradient Descent(4390/9999): loss=2.682110558498038, w0=72.95625000000014, w1=13.649330230370715\n",
      "Gradient Descent(4391/9999): loss=2.4887905401795156, w0=73.23750000000014, w1=13.52514014117177\n",
      "Gradient Descent(4392/9999): loss=2.508643524601874, w0=73.23750000000014, w1=13.606034773638966\n",
      "Gradient Descent(4393/9999): loss=2.0431593621605764, w0=72.95625000000014, w1=13.656212490728675\n",
      "Gradient Descent(4394/9999): loss=2.204651756902046, w0=72.78750000000014, w1=13.957835953944763\n",
      "Gradient Descent(4395/9999): loss=2.2300111694798637, w0=72.78750000000014, w1=14.032102943565384\n",
      "Gradient Descent(4396/9999): loss=2.098614415631882, w0=72.78750000000014, w1=14.101041907757178\n",
      "Gradient Descent(4397/9999): loss=2.2224659149626516, w0=72.84375000000014, w1=13.871175242884213\n",
      "Gradient Descent(4398/9999): loss=2.1850661562986238, w0=72.84375000000014, w1=13.97861936779771\n",
      "Gradient Descent(4399/9999): loss=2.473368079234399, w0=72.84375000000014, w1=13.999669173376406\n",
      "Gradient Descent(4400/9999): loss=3.3132765028654587, w0=72.90000000000015, w1=13.945617708698839\n",
      "Gradient Descent(4401/9999): loss=2.1053417054259596, w0=72.78750000000015, w1=13.683294056136427\n",
      "Gradient Descent(4402/9999): loss=2.2893872386148884, w0=72.84375000000016, w1=13.561501776226935\n",
      "Gradient Descent(4403/9999): loss=2.6689608706063392, w0=72.67500000000015, w1=13.389590575083485\n",
      "Gradient Descent(4404/9999): loss=2.5107884584693063, w0=72.67500000000015, w1=13.24248930907245\n",
      "Gradient Descent(4405/9999): loss=2.5358450500510292, w0=72.84375000000016, w1=13.333134079725498\n",
      "Gradient Descent(4406/9999): loss=2.42734970411391, w0=73.01250000000016, w1=13.267109436189529\n",
      "Gradient Descent(4407/9999): loss=2.3339174430756766, w0=72.78750000000016, w1=13.407784835987448\n",
      "Gradient Descent(4408/9999): loss=2.0420397423137753, w0=72.90000000000016, w1=13.428981242011256\n",
      "Gradient Descent(4409/9999): loss=2.411197775730116, w0=72.73125000000016, w1=13.359186135279518\n",
      "Gradient Descent(4410/9999): loss=2.295018730054676, w0=72.90000000000016, w1=13.246121398792964\n",
      "Gradient Descent(4411/9999): loss=2.410302776262033, w0=73.18125000000016, w1=13.281336567391556\n",
      "Gradient Descent(4412/9999): loss=1.6544707488248314, w0=73.18125000000016, w1=13.132089621226807\n",
      "Gradient Descent(4413/9999): loss=2.281834819323831, w0=73.23750000000017, w1=13.018846992741317\n",
      "Gradient Descent(4414/9999): loss=2.2082618188405796, w0=73.12500000000017, w1=12.96744342686416\n",
      "Gradient Descent(4415/9999): loss=2.221597767525904, w0=73.06875000000016, w1=12.866577281755239\n",
      "Gradient Descent(4416/9999): loss=2.207088232380393, w0=72.95625000000017, w1=12.90243160929708\n",
      "Gradient Descent(4417/9999): loss=2.4459480798854365, w0=73.01250000000017, w1=13.075643302267121\n",
      "Gradient Descent(4418/9999): loss=2.1544551376170666, w0=72.95625000000017, w1=13.137868525362235\n",
      "Gradient Descent(4419/9999): loss=2.081390330522252, w0=73.06875000000016, w1=13.016420163490578\n",
      "Gradient Descent(4420/9999): loss=2.6749165064131253, w0=72.90000000000016, w1=12.933248324816265\n",
      "Gradient Descent(4421/9999): loss=2.4706535036083928, w0=72.84375000000016, w1=12.752863347181243\n",
      "Gradient Descent(4422/9999): loss=2.6971537504550884, w0=73.01250000000016, w1=12.812005099856762\n",
      "Gradient Descent(4423/9999): loss=2.3224775079550417, w0=73.01250000000016, w1=13.062983489489193\n",
      "Gradient Descent(4424/9999): loss=1.9403264232010053, w0=73.12500000000016, w1=12.92631965538889\n",
      "Gradient Descent(4425/9999): loss=2.280089123028043, w0=73.40625000000016, w1=13.294726898451051\n",
      "Gradient Descent(4426/9999): loss=2.2219464820991326, w0=73.57500000000016, w1=13.64026160741841\n",
      "Gradient Descent(4427/9999): loss=2.349454840545657, w0=73.46250000000016, w1=13.726650235215597\n",
      "Gradient Descent(4428/9999): loss=1.9881674680957713, w0=73.46250000000016, w1=13.722634371798602\n",
      "Gradient Descent(4429/9999): loss=2.1817217138566485, w0=73.63125000000016, w1=13.789683044972577\n",
      "Gradient Descent(4430/9999): loss=2.3517821580825538, w0=73.63125000000016, w1=13.563765741719322\n",
      "Gradient Descent(4431/9999): loss=1.9009145016905393, w0=73.57500000000016, w1=13.473090520726526\n",
      "Gradient Descent(4432/9999): loss=2.2264388654204907, w0=73.51875000000015, w1=13.435405758176401\n",
      "Gradient Descent(4433/9999): loss=2.3764596773684565, w0=73.51875000000015, w1=13.767084189196328\n",
      "Gradient Descent(4434/9999): loss=1.9262978165784854, w0=73.29375000000016, w1=13.904993449543644\n",
      "Gradient Descent(4435/9999): loss=1.535671294910535, w0=73.29375000000016, w1=13.849813695103242\n",
      "Gradient Descent(4436/9999): loss=2.3836251698939366, w0=73.06875000000016, w1=13.673546402739895\n",
      "Gradient Descent(4437/9999): loss=1.9702205481514452, w0=73.23750000000017, w1=13.70796984539207\n",
      "Gradient Descent(4438/9999): loss=1.52590739332716, w0=73.23750000000017, w1=13.586305723516627\n",
      "Gradient Descent(4439/9999): loss=2.0552703707827806, w0=73.18125000000016, w1=13.50881890217374\n",
      "Gradient Descent(4440/9999): loss=2.4088898274299266, w0=73.12500000000016, w1=13.481475808216722\n",
      "Gradient Descent(4441/9999): loss=1.7604849818248622, w0=72.95625000000015, w1=13.46161987408182\n",
      "Gradient Descent(4442/9999): loss=2.6355725627087887, w0=72.95625000000015, w1=13.57080111031717\n",
      "Gradient Descent(4443/9999): loss=1.9663096810484657, w0=72.95625000000015, w1=13.740450612679897\n",
      "Gradient Descent(4444/9999): loss=1.5704687446722005, w0=72.73125000000016, w1=13.794658953680795\n",
      "Gradient Descent(4445/9999): loss=1.8147907643146337, w0=72.84375000000016, w1=13.790868013228115\n",
      "Gradient Descent(4446/9999): loss=2.761082914176841, w0=73.01250000000016, w1=13.674030376238202\n",
      "Gradient Descent(4447/9999): loss=1.9129177592425688, w0=72.73125000000016, w1=13.651824457409646\n",
      "Gradient Descent(4448/9999): loss=2.3619397388743226, w0=72.84375000000016, w1=13.575716104728837\n",
      "Gradient Descent(4449/9999): loss=2.1255178886317188, w0=72.67500000000015, w1=13.453434744943868\n",
      "Gradient Descent(4450/9999): loss=2.7101867545941434, w0=72.67500000000015, w1=13.252945454887726\n",
      "Gradient Descent(4451/9999): loss=1.9206976045704784, w0=72.78750000000015, w1=13.176227049728249\n",
      "Gradient Descent(4452/9999): loss=2.7399029341234815, w0=72.56250000000016, w1=13.493861024847067\n",
      "Gradient Descent(4453/9999): loss=2.4230089940445128, w0=72.67500000000015, w1=13.633090711365806\n",
      "Gradient Descent(4454/9999): loss=2.574773013145344, w0=72.67500000000015, w1=13.426562462494255\n",
      "Gradient Descent(4455/9999): loss=2.6290251203861965, w0=72.84375000000016, w1=13.520266200122949\n",
      "Gradient Descent(4456/9999): loss=2.347621045394459, w0=72.61875000000016, w1=13.643112092735668\n",
      "Gradient Descent(4457/9999): loss=2.8298032560454858, w0=72.78750000000016, w1=13.572942267850845\n",
      "Gradient Descent(4458/9999): loss=2.3058225425076615, w0=72.84375000000017, w1=13.621978138025428\n",
      "Gradient Descent(4459/9999): loss=1.9234808740073783, w0=72.67500000000017, w1=13.732876249834872\n",
      "Gradient Descent(4460/9999): loss=1.8733232593821267, w0=72.61875000000016, w1=13.835819496386195\n",
      "Gradient Descent(4461/9999): loss=2.2423542581056113, w0=72.95625000000017, w1=13.811572700656173\n",
      "Gradient Descent(4462/9999): loss=2.1805107138585633, w0=72.90000000000016, w1=14.02905092806557\n",
      "Gradient Descent(4463/9999): loss=2.1980468732355085, w0=73.01250000000016, w1=14.04113130994299\n",
      "Gradient Descent(4464/9999): loss=2.7090983575913823, w0=73.40625000000016, w1=13.783664564327184\n",
      "Gradient Descent(4465/9999): loss=1.9092083793724282, w0=73.57500000000016, w1=13.865583731403957\n",
      "Gradient Descent(4466/9999): loss=2.346573830604849, w0=73.57500000000016, w1=13.84727957612652\n",
      "Gradient Descent(4467/9999): loss=1.554074117511984, w0=73.23750000000015, w1=13.828647368363876\n",
      "Gradient Descent(4468/9999): loss=2.2475610701148105, w0=73.23750000000015, w1=13.795347973672815\n",
      "Gradient Descent(4469/9999): loss=2.920499748719773, w0=73.35000000000015, w1=13.590318228167712\n",
      "Gradient Descent(4470/9999): loss=2.3882084075211383, w0=73.29375000000014, w1=13.560614799231834\n",
      "Gradient Descent(4471/9999): loss=2.137196601355935, w0=73.46250000000015, w1=13.723867289897814\n",
      "Gradient Descent(4472/9999): loss=2.36372695548661, w0=73.40625000000014, w1=13.454100126558124\n",
      "Gradient Descent(4473/9999): loss=2.6881004444217833, w0=73.29375000000014, w1=13.394871859231124\n",
      "Gradient Descent(4474/9999): loss=2.2651460473209437, w0=73.35000000000015, w1=13.61085280428057\n",
      "Gradient Descent(4475/9999): loss=1.8472232992620587, w0=73.35000000000015, w1=13.422764921565628\n",
      "Gradient Descent(4476/9999): loss=2.386317512583065, w0=73.63125000000015, w1=13.533780400362637\n",
      "Gradient Descent(4477/9999): loss=1.9302573294997527, w0=73.57500000000014, w1=13.499540305474033\n",
      "Gradient Descent(4478/9999): loss=2.193215893676543, w0=73.74375000000015, w1=13.539552303291206\n",
      "Gradient Descent(4479/9999): loss=2.5227118321094038, w0=73.57500000000014, w1=13.921814370368052\n",
      "Gradient Descent(4480/9999): loss=2.4311869934536245, w0=73.63125000000015, w1=13.748595900589454\n",
      "Gradient Descent(4481/9999): loss=2.6512494564637734, w0=73.46250000000015, w1=13.781829469364231\n",
      "Gradient Descent(4482/9999): loss=2.1722786765465463, w0=73.51875000000015, w1=13.693523706229227\n",
      "Gradient Descent(4483/9999): loss=2.305995328290159, w0=73.57500000000016, w1=13.764341558522522\n",
      "Gradient Descent(4484/9999): loss=1.9996325037369913, w0=73.68750000000016, w1=13.841697894269785\n",
      "Gradient Descent(4485/9999): loss=2.7155635606947603, w0=73.74375000000016, w1=13.60480989870755\n",
      "Gradient Descent(4486/9999): loss=2.3679086961444806, w0=73.46250000000016, w1=13.779282122618163\n",
      "Gradient Descent(4487/9999): loss=2.3263119114247575, w0=73.29375000000016, w1=13.774737305625628\n",
      "Gradient Descent(4488/9999): loss=2.481534330215075, w0=73.35000000000016, w1=13.570355021784893\n",
      "Gradient Descent(4489/9999): loss=1.7678271111743988, w0=73.57500000000016, w1=13.206121808543209\n",
      "Gradient Descent(4490/9999): loss=2.139036865137625, w0=73.74375000000016, w1=13.214364274262282\n",
      "Gradient Descent(4491/9999): loss=1.946602134620894, w0=73.46250000000016, w1=13.287008490371452\n",
      "Gradient Descent(4492/9999): loss=2.24134261551845, w0=73.29375000000016, w1=13.345309036811248\n",
      "Gradient Descent(4493/9999): loss=2.391219126323718, w0=73.29375000000016, w1=13.220865156638881\n",
      "Gradient Descent(4494/9999): loss=2.2549063323013576, w0=73.06875000000016, w1=13.116368177206775\n",
      "Gradient Descent(4495/9999): loss=2.0453464824089087, w0=73.18125000000016, w1=13.165879718052134\n",
      "Gradient Descent(4496/9999): loss=2.1343625839004137, w0=73.18125000000016, w1=13.027057372134275\n",
      "Gradient Descent(4497/9999): loss=2.7649994028167626, w0=73.35000000000016, w1=13.005926340653087\n",
      "Gradient Descent(4498/9999): loss=2.548909754899405, w0=73.35000000000016, w1=12.693899623662528\n",
      "Gradient Descent(4499/9999): loss=2.400604196794288, w0=73.01250000000016, w1=12.819786277175828\n",
      "Gradient Descent(4500/9999): loss=2.181747636380285, w0=73.06875000000016, w1=12.891656446631007\n",
      "Gradient Descent(4501/9999): loss=2.3566916520353303, w0=73.06875000000016, w1=12.918184167367011\n",
      "Gradient Descent(4502/9999): loss=2.0314172733386373, w0=73.23750000000017, w1=13.009127908606203\n",
      "Gradient Descent(4503/9999): loss=1.7874166035308865, w0=73.35000000000016, w1=12.859377918611083\n",
      "Gradient Descent(4504/9999): loss=2.212660477431048, w0=73.40625000000017, w1=13.014540491023979\n",
      "Gradient Descent(4505/9999): loss=2.1550102178926576, w0=73.63125000000016, w1=13.20286076473259\n",
      "Gradient Descent(4506/9999): loss=2.3168532202622556, w0=73.46250000000016, w1=13.572591912971369\n",
      "Gradient Descent(4507/9999): loss=2.2072886115804318, w0=73.46250000000016, w1=13.658199202099235\n",
      "Gradient Descent(4508/9999): loss=1.6894568555353375, w0=73.23750000000017, w1=13.747730615006274\n",
      "Gradient Descent(4509/9999): loss=2.0004865301386956, w0=73.29375000000017, w1=13.93478883411822\n",
      "Gradient Descent(4510/9999): loss=2.06907722167149, w0=73.18125000000018, w1=13.988057895130996\n",
      "Gradient Descent(4511/9999): loss=2.0515345009396593, w0=73.35000000000018, w1=13.672350919134173\n",
      "Gradient Descent(4512/9999): loss=1.97737044789495, w0=73.23750000000018, w1=13.57517102229753\n",
      "Gradient Descent(4513/9999): loss=2.5968607864883397, w0=73.35000000000018, w1=13.472270820420533\n",
      "Gradient Descent(4514/9999): loss=2.4536868167935855, w0=73.06875000000018, w1=13.351439168562315\n",
      "Gradient Descent(4515/9999): loss=2.4789946711500153, w0=73.06875000000018, w1=13.389662320127057\n",
      "Gradient Descent(4516/9999): loss=2.4549451068114307, w0=73.29375000000017, w1=13.50522883315061\n",
      "Gradient Descent(4517/9999): loss=2.238682325507302, w0=73.57500000000017, w1=13.630780603184387\n",
      "Gradient Descent(4518/9999): loss=2.9294694234402496, w0=73.46250000000018, w1=13.653792481898062\n",
      "Gradient Descent(4519/9999): loss=2.2933318868971, w0=73.23750000000018, w1=13.940814951582874\n",
      "Gradient Descent(4520/9999): loss=1.91311447999214, w0=73.18125000000018, w1=14.168748124620556\n",
      "Gradient Descent(4521/9999): loss=2.310838758899991, w0=73.40625000000017, w1=13.86527081669726\n",
      "Gradient Descent(4522/9999): loss=1.684295778230446, w0=73.80000000000017, w1=13.680609903263743\n",
      "Gradient Descent(4523/9999): loss=2.318305585742606, w0=73.74375000000016, w1=13.764625910512382\n",
      "Gradient Descent(4524/9999): loss=1.9918519221962303, w0=73.85625000000016, w1=13.60467655037409\n",
      "Gradient Descent(4525/9999): loss=2.471695813110439, w0=73.57500000000016, w1=13.448546185134393\n",
      "Gradient Descent(4526/9999): loss=2.7101250714391254, w0=73.35000000000016, w1=13.405060633138916\n",
      "Gradient Descent(4527/9999): loss=2.0774792652521774, w0=73.12500000000017, w1=13.487080187311012\n",
      "Gradient Descent(4528/9999): loss=2.507758428354625, w0=72.84375000000017, w1=13.327813013471914\n",
      "Gradient Descent(4529/9999): loss=2.013932358196157, w0=72.84375000000017, w1=13.714613348725457\n",
      "Gradient Descent(4530/9999): loss=2.1836251770760713, w0=72.78750000000016, w1=13.541168916626345\n",
      "Gradient Descent(4531/9999): loss=2.443924097941248, w0=72.78750000000016, w1=13.628504049708548\n",
      "Gradient Descent(4532/9999): loss=2.3291082799091702, w0=72.90000000000016, w1=13.419930570513282\n",
      "Gradient Descent(4533/9999): loss=2.2272635744451295, w0=72.84375000000016, w1=13.301556556295006\n",
      "Gradient Descent(4534/9999): loss=2.088654257012099, w0=72.84375000000016, w1=13.253604798531006\n",
      "Gradient Descent(4535/9999): loss=2.229022397075817, w0=72.95625000000015, w1=13.46427257260536\n",
      "Gradient Descent(4536/9999): loss=2.340557235854893, w0=73.01250000000016, w1=13.214096500652051\n",
      "Gradient Descent(4537/9999): loss=2.335941612926559, w0=72.95625000000015, w1=13.424580267268606\n",
      "Gradient Descent(4538/9999): loss=1.945598696920682, w0=72.73125000000016, w1=13.339288440307548\n",
      "Gradient Descent(4539/9999): loss=2.282353510350582, w0=72.61875000000016, w1=13.355569664824353\n",
      "Gradient Descent(4540/9999): loss=1.7625704065811016, w0=72.78750000000016, w1=13.189408221160647\n",
      "Gradient Descent(4541/9999): loss=1.917341171850504, w0=72.78750000000016, w1=13.071273649442828\n",
      "Gradient Descent(4542/9999): loss=2.1219374733668888, w0=72.90000000000016, w1=13.262816604664478\n",
      "Gradient Descent(4543/9999): loss=1.834886486516095, w0=72.78750000000016, w1=13.455039505385413\n",
      "Gradient Descent(4544/9999): loss=1.9509342159900986, w0=72.61875000000016, w1=13.563170238221806\n",
      "Gradient Descent(4545/9999): loss=2.2970245166930683, w0=72.67500000000017, w1=13.46043854112031\n",
      "Gradient Descent(4546/9999): loss=2.695825817178252, w0=72.73125000000017, w1=13.61624469463276\n",
      "Gradient Descent(4547/9999): loss=2.2603966078637723, w0=73.06875000000018, w1=13.660334923162948\n",
      "Gradient Descent(4548/9999): loss=2.0930562579684318, w0=73.46250000000018, w1=13.610148661148386\n",
      "Gradient Descent(4549/9999): loss=2.1830827303029845, w0=73.51875000000018, w1=13.478839854416414\n",
      "Gradient Descent(4550/9999): loss=2.432904199285036, w0=73.35000000000018, w1=13.571398660050043\n",
      "Gradient Descent(4551/9999): loss=2.005402722741656, w0=73.29375000000017, w1=13.479909608283586\n",
      "Gradient Descent(4552/9999): loss=2.325777300762507, w0=73.40625000000017, w1=13.51313338672089\n",
      "Gradient Descent(4553/9999): loss=2.066981297664851, w0=73.63125000000016, w1=13.159027326502246\n",
      "Gradient Descent(4554/9999): loss=2.114647496955232, w0=73.51875000000017, w1=13.350649512923662\n",
      "Gradient Descent(4555/9999): loss=2.109641673790401, w0=73.35000000000016, w1=13.321122597965354\n",
      "Gradient Descent(4556/9999): loss=2.5126695147813063, w0=73.06875000000016, w1=13.315103821995239\n",
      "Gradient Descent(4557/9999): loss=2.3272385092494616, w0=73.12500000000017, w1=13.29691824073596\n",
      "Gradient Descent(4558/9999): loss=1.9380064074056158, w0=73.29375000000017, w1=13.354710362704031\n",
      "Gradient Descent(4559/9999): loss=2.4039011040147304, w0=73.23750000000017, w1=13.272131767861175\n",
      "Gradient Descent(4560/9999): loss=2.051325739882163, w0=73.46250000000016, w1=13.289482932812339\n",
      "Gradient Descent(4561/9999): loss=2.3459593422208904, w0=73.57500000000016, w1=13.125419814658192\n",
      "Gradient Descent(4562/9999): loss=2.0767830494867976, w0=73.74375000000016, w1=13.097278643987858\n",
      "Gradient Descent(4563/9999): loss=1.8882551845041495, w0=73.74375000000016, w1=13.016367478797745\n",
      "Gradient Descent(4564/9999): loss=2.3235742757931863, w0=73.85625000000016, w1=13.445421073769307\n",
      "Gradient Descent(4565/9999): loss=2.0413146145933947, w0=74.02500000000016, w1=13.501628259263429\n",
      "Gradient Descent(4566/9999): loss=2.113170551102804, w0=73.68750000000016, w1=13.335606206234766\n",
      "Gradient Descent(4567/9999): loss=2.076458890328494, w0=73.57500000000016, w1=13.4468267653758\n",
      "Gradient Descent(4568/9999): loss=2.3561272083076474, w0=73.63125000000016, w1=13.351510485549982\n",
      "Gradient Descent(4569/9999): loss=2.179798265486615, w0=73.68750000000017, w1=13.364373334621154\n",
      "Gradient Descent(4570/9999): loss=2.4240284170391337, w0=73.57500000000017, w1=13.540938646818356\n",
      "Gradient Descent(4571/9999): loss=2.6408458448452814, w0=73.46250000000018, w1=13.5781024899682\n",
      "Gradient Descent(4572/9999): loss=2.5653474217351633, w0=73.40625000000017, w1=13.738432180737883\n",
      "Gradient Descent(4573/9999): loss=2.394056908583644, w0=73.29375000000017, w1=13.55583180174239\n",
      "Gradient Descent(4574/9999): loss=1.9857519840002487, w0=73.29375000000017, w1=13.650420007211318\n",
      "Gradient Descent(4575/9999): loss=2.346470611265637, w0=72.90000000000018, w1=13.767009945514907\n",
      "Gradient Descent(4576/9999): loss=2.549469306496564, w0=73.01250000000017, w1=13.561836972335996\n",
      "Gradient Descent(4577/9999): loss=2.123936354068652, w0=73.12500000000017, w1=13.808628216842566\n",
      "Gradient Descent(4578/9999): loss=2.179108299028376, w0=73.06875000000016, w1=13.607460674032675\n",
      "Gradient Descent(4579/9999): loss=2.2137566727805558, w0=72.90000000000016, w1=13.437062021027414\n",
      "Gradient Descent(4580/9999): loss=1.8024903458884958, w0=73.23750000000017, w1=13.410691469029357\n",
      "Gradient Descent(4581/9999): loss=1.65136105243452, w0=72.95625000000017, w1=13.56568179061353\n",
      "Gradient Descent(4582/9999): loss=1.9930636590462119, w0=73.12500000000017, w1=13.75764471396099\n",
      "Gradient Descent(4583/9999): loss=2.3595004908958566, w0=73.29375000000017, w1=13.902236207640975\n",
      "Gradient Descent(4584/9999): loss=2.5264947960015527, w0=73.18125000000018, w1=13.727904110820557\n",
      "Gradient Descent(4585/9999): loss=2.706993372195984, w0=73.01250000000017, w1=13.602414168998786\n",
      "Gradient Descent(4586/9999): loss=2.1958476782470955, w0=72.95625000000017, w1=13.59041218516134\n",
      "Gradient Descent(4587/9999): loss=2.383648585328083, w0=72.95625000000017, w1=13.596395635768099\n",
      "Gradient Descent(4588/9999): loss=1.9899224391393013, w0=73.06875000000016, w1=13.70947539497632\n",
      "Gradient Descent(4589/9999): loss=2.2773119259609533, w0=73.23750000000017, w1=13.52942082885061\n",
      "Gradient Descent(4590/9999): loss=2.9119925765987675, w0=73.18125000000016, w1=13.504721336918207\n",
      "Gradient Descent(4591/9999): loss=2.3141311496933934, w0=73.18125000000016, w1=13.65207113816886\n",
      "Gradient Descent(4592/9999): loss=2.1242424835874374, w0=73.23750000000017, w1=13.619155399721395\n",
      "Gradient Descent(4593/9999): loss=1.7703241741152995, w0=73.06875000000016, w1=13.330447623737525\n",
      "Gradient Descent(4594/9999): loss=2.1796272170041213, w0=73.01250000000016, w1=13.702804652166181\n",
      "Gradient Descent(4595/9999): loss=2.109258164198161, w0=72.84375000000016, w1=13.700244270994713\n",
      "Gradient Descent(4596/9999): loss=2.3110730102716928, w0=72.90000000000016, w1=13.812538959867661\n",
      "Gradient Descent(4597/9999): loss=2.8546739844545006, w0=72.84375000000016, w1=13.823161362782715\n",
      "Gradient Descent(4598/9999): loss=1.9881863877797379, w0=72.78750000000015, w1=13.750069689171022\n",
      "Gradient Descent(4599/9999): loss=2.2030988617379035, w0=72.73125000000014, w1=13.718901886894328\n",
      "Gradient Descent(4600/9999): loss=2.3835166373130257, w0=72.78750000000015, w1=13.795976133896506\n",
      "Gradient Descent(4601/9999): loss=2.0498891655275466, w0=72.67500000000015, w1=13.618610523193036\n",
      "Gradient Descent(4602/9999): loss=2.316526712421786, w0=72.90000000000015, w1=13.5854178798916\n",
      "Gradient Descent(4603/9999): loss=2.6114946495972413, w0=73.18125000000015, w1=13.57477446224406\n",
      "Gradient Descent(4604/9999): loss=2.3789842224471824, w0=73.18125000000015, w1=13.546672513745948\n",
      "Gradient Descent(4605/9999): loss=2.126598936895604, w0=73.29375000000014, w1=13.297959059360313\n",
      "Gradient Descent(4606/9999): loss=2.077493130893279, w0=73.23750000000014, w1=13.350461456534847\n",
      "Gradient Descent(4607/9999): loss=2.148483012788133, w0=73.23750000000014, w1=13.336820146464795\n",
      "Gradient Descent(4608/9999): loss=1.779149531679295, w0=73.23750000000014, w1=13.31324155738096\n",
      "Gradient Descent(4609/9999): loss=2.4335338751415208, w0=73.23750000000014, w1=13.225852648218769\n",
      "Gradient Descent(4610/9999): loss=1.9678046124166875, w0=73.18125000000013, w1=13.01673413446343\n",
      "Gradient Descent(4611/9999): loss=2.4686087885861125, w0=73.51875000000014, w1=13.02954331954545\n",
      "Gradient Descent(4612/9999): loss=2.1837752138377717, w0=73.29375000000014, w1=12.991838182779702\n",
      "Gradient Descent(4613/9999): loss=2.1761590478560526, w0=73.23750000000014, w1=13.032643763950425\n",
      "Gradient Descent(4614/9999): loss=2.3770790291527533, w0=73.23750000000014, w1=12.971233122787988\n",
      "Gradient Descent(4615/9999): loss=2.083738948927157, w0=73.35000000000014, w1=12.89984960256631\n",
      "Gradient Descent(4616/9999): loss=2.2279473891137482, w0=73.35000000000014, w1=13.011828088355902\n",
      "Gradient Descent(4617/9999): loss=2.627185817682257, w0=73.29375000000013, w1=13.41014564561213\n",
      "Gradient Descent(4618/9999): loss=2.1742361469349625, w0=73.18125000000013, w1=13.590084370833129\n",
      "Gradient Descent(4619/9999): loss=2.26147042977664, w0=73.23750000000014, w1=13.528539901051609\n",
      "Gradient Descent(4620/9999): loss=1.8060162246290739, w0=73.40625000000014, w1=13.587497365683726\n",
      "Gradient Descent(4621/9999): loss=2.2819029624297373, w0=73.46250000000015, w1=13.402612008691014\n",
      "Gradient Descent(4622/9999): loss=2.0038284109327025, w0=73.51875000000015, w1=13.58760654321589\n",
      "Gradient Descent(4623/9999): loss=2.727973185722837, w0=73.29375000000016, w1=13.715691418865557\n",
      "Gradient Descent(4624/9999): loss=2.2206447619805525, w0=73.23750000000015, w1=13.843523300713475\n",
      "Gradient Descent(4625/9999): loss=2.0031194360443676, w0=73.06875000000015, w1=13.939742101194605\n",
      "Gradient Descent(4626/9999): loss=2.1542672746599614, w0=73.23750000000015, w1=13.94713717980887\n",
      "Gradient Descent(4627/9999): loss=2.3687347946590025, w0=73.23750000000015, w1=13.917557982364086\n",
      "Gradient Descent(4628/9999): loss=2.4014720572942077, w0=73.29375000000016, w1=13.925700759992823\n",
      "Gradient Descent(4629/9999): loss=2.2349842014533428, w0=73.40625000000016, w1=13.686474988579407\n",
      "Gradient Descent(4630/9999): loss=2.3679168567059174, w0=73.23750000000015, w1=13.867346603406707\n",
      "Gradient Descent(4631/9999): loss=1.725282163289857, w0=73.12500000000016, w1=13.728308006678189\n",
      "Gradient Descent(4632/9999): loss=2.1617015717652968, w0=72.95625000000015, w1=13.721696736187976\n",
      "Gradient Descent(4633/9999): loss=2.2454292355305063, w0=72.78750000000015, w1=13.475332603383926\n",
      "Gradient Descent(4634/9999): loss=2.288398173134315, w0=72.67500000000015, w1=13.375689589745614\n",
      "Gradient Descent(4635/9999): loss=2.2958166016602144, w0=72.39375000000015, w1=13.510856082756794\n",
      "Gradient Descent(4636/9999): loss=2.064345262574636, w0=72.45000000000016, w1=13.358902284460942\n",
      "Gradient Descent(4637/9999): loss=1.9949301947696934, w0=72.56250000000016, w1=13.208591421711048\n",
      "Gradient Descent(4638/9999): loss=1.852067430050309, w0=72.67500000000015, w1=13.341363395539593\n",
      "Gradient Descent(4639/9999): loss=2.284017557738202, w0=72.78750000000015, w1=13.369192848014146\n",
      "Gradient Descent(4640/9999): loss=2.2035949902729204, w0=73.01250000000014, w1=13.538931470468095\n",
      "Gradient Descent(4641/9999): loss=2.5355638352308487, w0=73.12500000000014, w1=13.424396803585593\n",
      "Gradient Descent(4642/9999): loss=2.101259534498129, w0=73.18125000000015, w1=13.436868376086775\n",
      "Gradient Descent(4643/9999): loss=2.326409545811876, w0=72.90000000000015, w1=13.631267724246088\n",
      "Gradient Descent(4644/9999): loss=2.428912440372561, w0=73.12500000000014, w1=13.645849361278223\n",
      "Gradient Descent(4645/9999): loss=1.9466396096933376, w0=73.01250000000014, w1=13.792700067946246\n",
      "Gradient Descent(4646/9999): loss=2.080569581002538, w0=72.95625000000014, w1=13.744760399017288\n",
      "Gradient Descent(4647/9999): loss=1.8043136042151287, w0=72.95625000000014, w1=13.55645126434692\n",
      "Gradient Descent(4648/9999): loss=2.5439002156379047, w0=73.12500000000014, w1=13.409995772554048\n",
      "Gradient Descent(4649/9999): loss=2.299108801566608, w0=72.95625000000014, w1=13.417321151766757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4650/9999): loss=2.2477240945877863, w0=73.06875000000014, w1=13.488048323956196\n",
      "Gradient Descent(4651/9999): loss=2.0591188273218055, w0=73.18125000000013, w1=13.558698255792196\n",
      "Gradient Descent(4652/9999): loss=2.6565308253478026, w0=73.01250000000013, w1=13.786027353330846\n",
      "Gradient Descent(4653/9999): loss=2.1049359864536967, w0=73.18125000000013, w1=13.733910123946519\n",
      "Gradient Descent(4654/9999): loss=2.3318493944363206, w0=73.23750000000014, w1=13.784389990668055\n",
      "Gradient Descent(4655/9999): loss=2.226671012829467, w0=73.18125000000013, w1=13.653430143246347\n",
      "Gradient Descent(4656/9999): loss=2.1332064202717853, w0=72.95625000000014, w1=13.924025687856947\n",
      "Gradient Descent(4657/9999): loss=1.7284557983464661, w0=73.01250000000014, w1=13.961917837279561\n",
      "Gradient Descent(4658/9999): loss=2.042337466798875, w0=73.18125000000015, w1=14.169424900290648\n",
      "Gradient Descent(4659/9999): loss=2.4233493920473372, w0=73.18125000000015, w1=14.10762495201297\n",
      "Gradient Descent(4660/9999): loss=1.9973805274764151, w0=73.18125000000015, w1=14.19010815676199\n",
      "Gradient Descent(4661/9999): loss=1.8374378461100915, w0=73.01250000000014, w1=14.142506827872355\n",
      "Gradient Descent(4662/9999): loss=2.4723485342571574, w0=72.78750000000015, w1=13.937607731719368\n",
      "Gradient Descent(4663/9999): loss=2.5232347421294343, w0=72.73125000000014, w1=14.01496457997678\n",
      "Gradient Descent(4664/9999): loss=2.2473289537605075, w0=73.01250000000014, w1=13.76810089139563\n",
      "Gradient Descent(4665/9999): loss=2.284729225712452, w0=72.95625000000014, w1=13.92242469824057\n",
      "Gradient Descent(4666/9999): loss=2.6294213896030056, w0=73.18125000000013, w1=14.065177632145534\n",
      "Gradient Descent(4667/9999): loss=1.3457019272010657, w0=73.06875000000014, w1=13.976377212556951\n",
      "Gradient Descent(4668/9999): loss=1.919948809337637, w0=72.84375000000014, w1=13.69695988527244\n",
      "Gradient Descent(4669/9999): loss=1.7654246358404961, w0=73.06875000000014, w1=13.839671226113198\n",
      "Gradient Descent(4670/9999): loss=1.7405282544179193, w0=73.06875000000014, w1=13.690609821605683\n",
      "Gradient Descent(4671/9999): loss=2.393688367503885, w0=73.29375000000013, w1=13.710132833987416\n",
      "Gradient Descent(4672/9999): loss=1.8051051605518271, w0=73.06875000000014, w1=13.535503324244495\n",
      "Gradient Descent(4673/9999): loss=2.0807804251082676, w0=73.23750000000014, w1=13.572798169529756\n",
      "Gradient Descent(4674/9999): loss=1.5066398385392812, w0=73.35000000000014, w1=13.580313292199623\n",
      "Gradient Descent(4675/9999): loss=1.906604881889943, w0=73.18125000000013, w1=13.735651054778023\n",
      "Gradient Descent(4676/9999): loss=2.0965739323608528, w0=73.12500000000013, w1=13.962711333169139\n",
      "Gradient Descent(4677/9999): loss=1.9701184790774084, w0=72.90000000000013, w1=13.621601294878138\n",
      "Gradient Descent(4678/9999): loss=1.9526332981251524, w0=72.95625000000014, w1=13.784100764417374\n",
      "Gradient Descent(4679/9999): loss=1.6543647253583045, w0=73.12500000000014, w1=14.01887036946056\n",
      "Gradient Descent(4680/9999): loss=2.632344787657571, w0=73.29375000000014, w1=13.845097111836411\n",
      "Gradient Descent(4681/9999): loss=2.029192794324866, w0=73.01250000000014, w1=13.66206953221247\n",
      "Gradient Descent(4682/9999): loss=2.148990056531086, w0=73.23750000000014, w1=13.921697283198332\n",
      "Gradient Descent(4683/9999): loss=3.064547006681676, w0=73.23750000000014, w1=14.043180823244404\n",
      "Gradient Descent(4684/9999): loss=2.6169565496241174, w0=73.01250000000014, w1=13.900352444654247\n",
      "Gradient Descent(4685/9999): loss=1.7976663939483724, w0=72.90000000000015, w1=13.716299653871932\n",
      "Gradient Descent(4686/9999): loss=2.6757940939096025, w0=72.78750000000015, w1=13.819504365817616\n",
      "Gradient Descent(4687/9999): loss=1.8737221126142825, w0=72.73125000000014, w1=13.784487909359644\n",
      "Gradient Descent(4688/9999): loss=1.9879009160801557, w0=72.95625000000014, w1=13.641869998173936\n",
      "Gradient Descent(4689/9999): loss=2.4937485400871178, w0=72.95625000000014, w1=13.872138344965977\n",
      "Gradient Descent(4690/9999): loss=2.4689665172001267, w0=72.78750000000014, w1=13.791832854510051\n",
      "Gradient Descent(4691/9999): loss=2.445249652945545, w0=72.84375000000014, w1=13.566976206756502\n",
      "Gradient Descent(4692/9999): loss=2.5362303122120737, w0=73.18125000000015, w1=13.561117607418968\n",
      "Gradient Descent(4693/9999): loss=1.9192408750281131, w0=73.29375000000014, w1=13.29052194751862\n",
      "Gradient Descent(4694/9999): loss=2.402181922043182, w0=73.57500000000014, w1=13.212367602258205\n",
      "Gradient Descent(4695/9999): loss=2.1775584549981617, w0=73.51875000000014, w1=13.030360704749933\n",
      "Gradient Descent(4696/9999): loss=1.8888707377214429, w0=73.68750000000014, w1=13.01921891838964\n",
      "Gradient Descent(4697/9999): loss=1.9715060239481117, w0=73.51875000000014, w1=13.373764936348628\n",
      "Gradient Descent(4698/9999): loss=1.9795866522914403, w0=73.51875000000014, w1=13.445939610077355\n",
      "Gradient Descent(4699/9999): loss=2.079795972394513, w0=73.40625000000014, w1=13.289931644604978\n",
      "Gradient Descent(4700/9999): loss=2.4175738692754822, w0=73.51875000000014, w1=13.087653318191151\n",
      "Gradient Descent(4701/9999): loss=2.0003223841594266, w0=73.51875000000014, w1=13.123485820265861\n",
      "Gradient Descent(4702/9999): loss=2.1550969412549326, w0=73.40625000000014, w1=13.197584292898597\n",
      "Gradient Descent(4703/9999): loss=2.143764298433602, w0=73.12500000000014, w1=13.281108302658136\n",
      "Gradient Descent(4704/9999): loss=2.2350706271257685, w0=73.23750000000014, w1=13.269194938025809\n",
      "Gradient Descent(4705/9999): loss=2.1427107573615256, w0=73.23750000000014, w1=13.280180197267388\n",
      "Gradient Descent(4706/9999): loss=1.491896949784925, w0=73.23750000000014, w1=13.263622604748159\n",
      "Gradient Descent(4707/9999): loss=2.358982157003517, w0=73.29375000000014, w1=13.223145026163701\n",
      "Gradient Descent(4708/9999): loss=2.229820305451428, w0=73.40625000000014, w1=13.424350003751053\n",
      "Gradient Descent(4709/9999): loss=2.478098847732639, w0=73.23750000000014, w1=13.316322489240719\n",
      "Gradient Descent(4710/9999): loss=2.00051976759835, w0=73.12500000000014, w1=13.06986706653395\n",
      "Gradient Descent(4711/9999): loss=1.9191015118376042, w0=73.06875000000014, w1=13.332336502701667\n",
      "Gradient Descent(4712/9999): loss=2.6909415736912967, w0=73.12500000000014, w1=13.247352908783515\n",
      "Gradient Descent(4713/9999): loss=1.7643620529563993, w0=73.35000000000014, w1=13.355622541424363\n",
      "Gradient Descent(4714/9999): loss=1.7687394988160585, w0=73.01250000000013, w1=13.470705116875875\n",
      "Gradient Descent(4715/9999): loss=2.2772575352126467, w0=73.12500000000013, w1=13.736315260102064\n",
      "Gradient Descent(4716/9999): loss=2.1059974886388875, w0=73.35000000000012, w1=13.628803797899751\n",
      "Gradient Descent(4717/9999): loss=1.7361620090102416, w0=73.23750000000013, w1=13.487169889312998\n",
      "Gradient Descent(4718/9999): loss=2.1825883500264016, w0=73.01250000000013, w1=13.778323525609185\n",
      "Gradient Descent(4719/9999): loss=2.287484412315065, w0=73.06875000000014, w1=13.674120195553094\n",
      "Gradient Descent(4720/9999): loss=2.3530263552978425, w0=72.84375000000014, w1=13.858025536337019\n",
      "Gradient Descent(4721/9999): loss=2.7489603729917365, w0=72.95625000000014, w1=14.117321050406982\n",
      "Gradient Descent(4722/9999): loss=2.3305978852621942, w0=72.73125000000014, w1=13.875826815319591\n",
      "Gradient Descent(4723/9999): loss=2.359855882620381, w0=72.78750000000015, w1=13.707144579888787\n",
      "Gradient Descent(4724/9999): loss=1.8810900784850255, w0=72.84375000000016, w1=13.74847648342997\n",
      "Gradient Descent(4725/9999): loss=2.358023420293553, w0=72.78750000000015, w1=13.642562785692855\n",
      "Gradient Descent(4726/9999): loss=2.1646766505682224, w0=72.90000000000015, w1=13.473575379226373\n",
      "Gradient Descent(4727/9999): loss=1.8322002515995863, w0=72.61875000000015, w1=13.4403886468526\n",
      "Gradient Descent(4728/9999): loss=2.1846911315849766, w0=72.61875000000015, w1=13.550804751006131\n",
      "Gradient Descent(4729/9999): loss=2.004500644866167, w0=72.61875000000015, w1=13.489541366942754\n",
      "Gradient Descent(4730/9999): loss=2.676238132259752, w0=72.61875000000015, w1=13.398521128668111\n",
      "Gradient Descent(4731/9999): loss=2.6225800581946723, w0=72.84375000000014, w1=13.389096389726614\n",
      "Gradient Descent(4732/9999): loss=1.5892745959238828, w0=73.06875000000014, w1=13.36709334280915\n",
      "Gradient Descent(4733/9999): loss=2.158718317894736, w0=73.12500000000014, w1=13.357262251018282\n",
      "Gradient Descent(4734/9999): loss=1.847547196220611, w0=73.18125000000015, w1=13.283440989243728\n",
      "Gradient Descent(4735/9999): loss=2.455473215716996, w0=73.23750000000015, w1=13.125083115343715\n",
      "Gradient Descent(4736/9999): loss=2.916399136432916, w0=73.01250000000016, w1=13.281467314632566\n",
      "Gradient Descent(4737/9999): loss=2.048861293721384, w0=72.84375000000016, w1=13.391207414038032\n",
      "Gradient Descent(4738/9999): loss=2.2733244163316995, w0=72.95625000000015, w1=13.381245752316454\n",
      "Gradient Descent(4739/9999): loss=2.6371335053882303, w0=72.84375000000016, w1=12.946339877422055\n",
      "Gradient Descent(4740/9999): loss=2.8583916406366026, w0=72.67500000000015, w1=12.914555081958849\n",
      "Gradient Descent(4741/9999): loss=2.1099869709378405, w0=72.78750000000015, w1=13.085058075492402\n",
      "Gradient Descent(4742/9999): loss=2.425576859276348, w0=72.90000000000015, w1=13.148090221205269\n",
      "Gradient Descent(4743/9999): loss=1.8214489159613496, w0=72.95625000000015, w1=13.238770086931837\n",
      "Gradient Descent(4744/9999): loss=2.4823914218556933, w0=72.84375000000016, w1=13.204803828580012\n",
      "Gradient Descent(4745/9999): loss=2.727691643551519, w0=72.61875000000016, w1=13.364926226176998\n",
      "Gradient Descent(4746/9999): loss=2.1737348812283033, w0=72.61875000000016, w1=13.545325198993712\n",
      "Gradient Descent(4747/9999): loss=2.190453987493476, w0=72.39375000000017, w1=13.991726029121743\n",
      "Gradient Descent(4748/9999): loss=2.283600533454746, w0=72.28125000000017, w1=13.95039557416196\n",
      "Gradient Descent(4749/9999): loss=2.0407057976798155, w0=72.56250000000017, w1=14.07390693039945\n",
      "Gradient Descent(4750/9999): loss=2.1000056341366964, w0=72.28125000000017, w1=13.977008302784993\n",
      "Gradient Descent(4751/9999): loss=2.6514699776055215, w0=72.56250000000017, w1=14.03165173446043\n",
      "Gradient Descent(4752/9999): loss=2.2026977242985444, w0=72.56250000000017, w1=14.198334195486359\n",
      "Gradient Descent(4753/9999): loss=2.0976146180180084, w0=72.56250000000017, w1=14.049013872674013\n",
      "Gradient Descent(4754/9999): loss=2.741574716122983, w0=72.61875000000018, w1=13.880331646930264\n",
      "Gradient Descent(4755/9999): loss=2.028704046086384, w0=72.90000000000018, w1=13.97138370626433\n",
      "Gradient Descent(4756/9999): loss=2.063652480129526, w0=72.90000000000018, w1=13.934841229233633\n",
      "Gradient Descent(4757/9999): loss=2.624497810922395, w0=72.90000000000018, w1=13.955695754800434\n",
      "Gradient Descent(4758/9999): loss=2.2050346711689754, w0=72.95625000000018, w1=13.98866213021669\n",
      "Gradient Descent(4759/9999): loss=2.3744375135419284, w0=73.01250000000019, w1=13.829895137806407\n",
      "Gradient Descent(4760/9999): loss=2.3244448492919716, w0=73.18125000000019, w1=13.702854089437494\n",
      "Gradient Descent(4761/9999): loss=2.4985093715789564, w0=73.18125000000019, w1=13.76981009819689\n",
      "Gradient Descent(4762/9999): loss=2.2806678451772036, w0=73.12500000000018, w1=13.565515692151067\n",
      "Gradient Descent(4763/9999): loss=2.1061626784684284, w0=73.40625000000018, w1=13.502129557926732\n",
      "Gradient Descent(4764/9999): loss=2.0012923361347053, w0=73.51875000000018, w1=13.5025462046167\n",
      "Gradient Descent(4765/9999): loss=2.2138511676372943, w0=73.40625000000018, w1=13.309637930622321\n",
      "Gradient Descent(4766/9999): loss=2.294101495545434, w0=73.40625000000018, w1=13.328892951500448\n",
      "Gradient Descent(4767/9999): loss=1.5823299717480301, w0=73.18125000000019, w1=13.09105095980449\n",
      "Gradient Descent(4768/9999): loss=2.365470646581763, w0=73.18125000000019, w1=12.91335223458848\n",
      "Gradient Descent(4769/9999): loss=2.509030710527357, w0=73.2375000000002, w1=13.204631360213991\n",
      "Gradient Descent(4770/9999): loss=2.4923962934982513, w0=73.2937500000002, w1=13.074488745820767\n",
      "Gradient Descent(4771/9999): loss=1.5394607040999484, w0=73.06875000000021, w1=13.151095199577707\n",
      "Gradient Descent(4772/9999): loss=1.969872984471572, w0=73.2937500000002, w1=13.256893100710277\n",
      "Gradient Descent(4773/9999): loss=2.460172286600927, w0=72.9562500000002, w1=13.245571470962116\n",
      "Gradient Descent(4774/9999): loss=2.2892797773631797, w0=72.90000000000019, w1=13.099096188286351\n",
      "Gradient Descent(4775/9999): loss=2.1978021839633772, w0=73.0687500000002, w1=12.987724527487831\n",
      "Gradient Descent(4776/9999): loss=1.9582445421622052, w0=73.1250000000002, w1=12.988352831165727\n",
      "Gradient Descent(4777/9999): loss=2.7667137162355373, w0=73.1812500000002, w1=13.024629112594925\n",
      "Gradient Descent(4778/9999): loss=2.060503144326698, w0=73.0125000000002, w1=13.020021539061828\n",
      "Gradient Descent(4779/9999): loss=2.5661761792936737, w0=73.1812500000002, w1=13.099585038589103\n",
      "Gradient Descent(4780/9999): loss=2.0255577644837177, w0=73.0125000000002, w1=12.930929142275206\n",
      "Gradient Descent(4781/9999): loss=2.216442115920569, w0=73.0125000000002, w1=12.849264786968167\n",
      "Gradient Descent(4782/9999): loss=1.6141475266685628, w0=72.6187500000002, w1=12.758543709942932\n",
      "Gradient Descent(4783/9999): loss=1.8641088630833693, w0=72.3375000000002, w1=12.625463240874454\n",
      "Gradient Descent(4784/9999): loss=2.3562822172624767, w0=72.50625000000021, w1=12.908087100875255\n",
      "Gradient Descent(4785/9999): loss=2.567427986469108, w0=72.6187500000002, w1=13.086900489153782\n",
      "Gradient Descent(4786/9999): loss=1.4517770810278923, w0=72.8437500000002, w1=13.329282121626946\n",
      "Gradient Descent(4787/9999): loss=2.094785625474708, w0=72.9000000000002, w1=13.266432945204196\n",
      "Gradient Descent(4788/9999): loss=1.8980524798727398, w0=72.67500000000021, w1=13.1074996951431\n",
      "Gradient Descent(4789/9999): loss=2.038863891214862, w0=72.84375000000021, w1=13.22730864439385\n",
      "Gradient Descent(4790/9999): loss=1.9388358641657706, w0=72.67500000000021, w1=13.426410362146303\n",
      "Gradient Descent(4791/9999): loss=2.234601879225278, w0=72.67500000000021, w1=12.976208583741634\n",
      "Gradient Descent(4792/9999): loss=2.485223183334396, w0=72.67500000000021, w1=13.06574909062614\n",
      "Gradient Descent(4793/9999): loss=1.8999661935284395, w0=72.78750000000021, w1=13.157749597403564\n",
      "Gradient Descent(4794/9999): loss=1.9368492904673011, w0=73.06875000000021, w1=13.241006100507128\n",
      "Gradient Descent(4795/9999): loss=2.1270292280659495, w0=73.06875000000021, w1=13.513865470772544\n",
      "Gradient Descent(4796/9999): loss=1.9282849962924538, w0=72.9000000000002, w1=13.833614328366837\n",
      "Gradient Descent(4797/9999): loss=2.084704800696912, w0=72.8437500000002, w1=13.778395019203852\n",
      "Gradient Descent(4798/9999): loss=2.7124259088417153, w0=72.5625000000002, w1=13.404586017668258\n",
      "Gradient Descent(4799/9999): loss=2.105745969156203, w0=72.4500000000002, w1=13.647866426612344\n",
      "Gradient Descent(4800/9999): loss=2.9254000324544265, w0=72.4500000000002, w1=13.864193186229326\n",
      "Gradient Descent(4801/9999): loss=2.408165770388998, w0=72.78750000000021, w1=13.777374591272256\n",
      "Gradient Descent(4802/9999): loss=1.9839896980717335, w0=72.6187500000002, w1=13.817210753903579\n",
      "Gradient Descent(4803/9999): loss=1.9592901068378574, w0=72.5625000000002, w1=13.655379448420613\n",
      "Gradient Descent(4804/9999): loss=2.481073407982944, w0=73.0125000000002, w1=13.953331134860697\n",
      "Gradient Descent(4805/9999): loss=1.6997697816171449, w0=73.06875000000021, w1=14.047950522357795\n",
      "Gradient Descent(4806/9999): loss=2.559910180701285, w0=73.1812500000002, w1=14.011754121493995\n",
      "Gradient Descent(4807/9999): loss=1.7741560212174223, w0=73.1250000000002, w1=13.867056382950418\n",
      "Gradient Descent(4808/9999): loss=2.0868531396602124, w0=73.0687500000002, w1=13.743833972941756\n",
      "Gradient Descent(4809/9999): loss=1.966766055004563, w0=73.29375000000019, w1=13.390406817521207\n",
      "Gradient Descent(4810/9999): loss=2.695157327050673, w0=73.46250000000019, w1=13.515198577526832\n",
      "Gradient Descent(4811/9999): loss=2.147002270487447, w0=73.29375000000019, w1=13.333524643128623\n",
      "Gradient Descent(4812/9999): loss=2.3081730411765085, w0=73.40625000000018, w1=13.188914436488723\n",
      "Gradient Descent(4813/9999): loss=2.566668499748399, w0=73.46250000000019, w1=13.100537275644292\n",
      "Gradient Descent(4814/9999): loss=2.3134985259432885, w0=73.57500000000019, w1=13.396880590459535\n",
      "Gradient Descent(4815/9999): loss=1.8909455021958954, w0=73.57500000000019, w1=13.16363087806743\n",
      "Gradient Descent(4816/9999): loss=2.193803828492456, w0=73.46250000000019, w1=13.185078382998038\n",
      "Gradient Descent(4817/9999): loss=2.282834760375914, w0=73.57500000000019, w1=13.21571702729431\n",
      "Gradient Descent(4818/9999): loss=1.8505098893846426, w0=73.85625000000019, w1=12.982354917385724\n",
      "Gradient Descent(4819/9999): loss=2.555193706641096, w0=73.74375000000019, w1=13.086255025203773\n",
      "Gradient Descent(4820/9999): loss=1.4048795638416625, w0=73.5187500000002, w1=13.164184069304211\n",
      "Gradient Descent(4821/9999): loss=2.43994604999952, w0=73.74375000000019, w1=13.058289048640402\n",
      "Gradient Descent(4822/9999): loss=2.9102062892394835, w0=73.74375000000019, w1=13.061836318544117\n",
      "Gradient Descent(4823/9999): loss=2.3528535834012176, w0=73.6312500000002, w1=13.294749716786097\n",
      "Gradient Descent(4824/9999): loss=1.9790418767067943, w0=73.46250000000019, w1=13.558491245557713\n",
      "Gradient Descent(4825/9999): loss=2.579349691627975, w0=73.46250000000019, w1=13.484871125103986\n",
      "Gradient Descent(4826/9999): loss=2.1001918781953384, w0=73.46250000000019, w1=13.504558470650355\n",
      "Gradient Descent(4827/9999): loss=2.665092259323162, w0=73.46250000000019, w1=13.4852799913193\n",
      "Gradient Descent(4828/9999): loss=2.3820760500438607, w0=73.40625000000018, w1=13.34069901059598\n",
      "Gradient Descent(4829/9999): loss=2.431804401284711, w0=73.29375000000019, w1=13.564011647410304\n",
      "Gradient Descent(4830/9999): loss=2.486791263236057, w0=73.40625000000018, w1=13.60138499715911\n",
      "Gradient Descent(4831/9999): loss=2.3454839136433114, w0=73.57500000000019, w1=13.382325807685097\n",
      "Gradient Descent(4832/9999): loss=1.4327882235429616, w0=73.74375000000019, w1=13.452955994281487\n",
      "Gradient Descent(4833/9999): loss=2.240815373531372, w0=73.85625000000019, w1=13.742405221765088\n",
      "Gradient Descent(4834/9999): loss=2.2391331909915984, w0=73.85625000000019, w1=13.710387707552963\n",
      "Gradient Descent(4835/9999): loss=1.8087200849509866, w0=73.68750000000018, w1=13.584144390388289\n",
      "Gradient Descent(4836/9999): loss=1.8717835816964352, w0=73.51875000000018, w1=13.653643683457558\n",
      "Gradient Descent(4837/9999): loss=2.7006050409160127, w0=73.51875000000018, w1=13.726890621274299\n",
      "Gradient Descent(4838/9999): loss=1.812092010393567, w0=73.35000000000018, w1=13.851854048715067\n",
      "Gradient Descent(4839/9999): loss=2.0061227946859845, w0=73.29375000000017, w1=13.7780287077678\n",
      "Gradient Descent(4840/9999): loss=2.1619141358621423, w0=73.18125000000018, w1=13.651084058663159\n",
      "Gradient Descent(4841/9999): loss=1.5914851980068672, w0=73.18125000000018, w1=13.42422728595763\n",
      "Gradient Descent(4842/9999): loss=1.5846861922794955, w0=73.12500000000017, w1=13.483409018285435\n",
      "Gradient Descent(4843/9999): loss=2.2647985691284953, w0=73.06875000000016, w1=13.521066219361598\n",
      "Gradient Descent(4844/9999): loss=2.0683447097584713, w0=73.06875000000016, w1=13.580930047043715\n",
      "Gradient Descent(4845/9999): loss=2.097517058888119, w0=73.12500000000017, w1=13.532851267803437\n",
      "Gradient Descent(4846/9999): loss=2.0182344030936306, w0=73.12500000000017, w1=13.736820580650038\n",
      "Gradient Descent(4847/9999): loss=1.90992206929421, w0=72.73125000000017, w1=13.760981869207288\n",
      "Gradient Descent(4848/9999): loss=2.3086281610293966, w0=73.06875000000018, w1=13.382505973006465\n",
      "Gradient Descent(4849/9999): loss=1.9924052898876705, w0=72.95625000000018, w1=13.466104139471108\n",
      "Gradient Descent(4850/9999): loss=2.295243251793511, w0=73.06875000000018, w1=13.380925705366197\n",
      "Gradient Descent(4851/9999): loss=1.978983967782016, w0=72.95625000000018, w1=13.76120861441979\n",
      "Gradient Descent(4852/9999): loss=2.4789136569271286, w0=72.95625000000018, w1=13.585094523078556\n",
      "Gradient Descent(4853/9999): loss=2.962691922390709, w0=72.84375000000018, w1=13.796253966144306\n",
      "Gradient Descent(4854/9999): loss=2.044028247016473, w0=73.06875000000018, w1=13.872486013715417\n",
      "Gradient Descent(4855/9999): loss=2.1694529006228698, w0=73.12500000000018, w1=13.757087444297373\n",
      "Gradient Descent(4856/9999): loss=2.1989336457022426, w0=73.23750000000018, w1=13.661333943151092\n",
      "Gradient Descent(4857/9999): loss=2.0832931906457963, w0=73.35000000000018, w1=13.458617526681245\n",
      "Gradient Descent(4858/9999): loss=2.3531398039052425, w0=73.18125000000018, w1=13.38257381688012\n",
      "Gradient Descent(4859/9999): loss=2.431345972980197, w0=73.35000000000018, w1=13.562405064599089\n",
      "Gradient Descent(4860/9999): loss=2.4356592302794535, w0=73.06875000000018, w1=13.437830676207177\n",
      "Gradient Descent(4861/9999): loss=2.2834985238448384, w0=73.18125000000018, w1=13.11126478585488\n",
      "Gradient Descent(4862/9999): loss=2.3953366011442565, w0=73.23750000000018, w1=13.326192350046517\n",
      "Gradient Descent(4863/9999): loss=2.2179175579808637, w0=73.29375000000019, w1=13.289444382081763\n",
      "Gradient Descent(4864/9999): loss=2.113634864641611, w0=73.23750000000018, w1=13.158908561865733\n",
      "Gradient Descent(4865/9999): loss=2.02242922257775, w0=73.12500000000018, w1=13.305617031664191\n",
      "Gradient Descent(4866/9999): loss=2.1389690818647544, w0=73.01250000000019, w1=13.46406013126612\n",
      "Gradient Descent(4867/9999): loss=2.4274065920604144, w0=72.90000000000019, w1=13.404371346972532\n",
      "Gradient Descent(4868/9999): loss=2.0657421897938666, w0=72.84375000000018, w1=13.505853184081126\n",
      "Gradient Descent(4869/9999): loss=1.7991225645323299, w0=73.01250000000019, w1=13.86379058297048\n",
      "Gradient Descent(4870/9999): loss=2.1039822665863213, w0=73.18125000000019, w1=13.786393762537235\n",
      "Gradient Descent(4871/9999): loss=2.484764080904388, w0=73.18125000000019, w1=13.78783095431137\n",
      "Gradient Descent(4872/9999): loss=2.189169612125466, w0=73.3500000000002, w1=13.751334578212411\n",
      "Gradient Descent(4873/9999): loss=2.7684835917942285, w0=73.2375000000002, w1=13.674882799052035\n",
      "Gradient Descent(4874/9999): loss=2.3244959134099226, w0=73.2375000000002, w1=13.504901109885477\n",
      "Gradient Descent(4875/9999): loss=2.0094368523657593, w0=73.18125000000019, w1=13.657571132989753\n",
      "Gradient Descent(4876/9999): loss=1.7352257177456134, w0=73.3500000000002, w1=13.543827205677545\n",
      "Gradient Descent(4877/9999): loss=2.1387669325594203, w0=73.6312500000002, w1=13.621097952537498\n",
      "Gradient Descent(4878/9999): loss=2.0778264603715146, w0=73.5187500000002, w1=13.675610437515495\n",
      "Gradient Descent(4879/9999): loss=2.3188022543460347, w0=73.5750000000002, w1=13.663688258044928\n",
      "Gradient Descent(4880/9999): loss=2.453531511379055, w0=73.4625000000002, w1=13.319393952102075\n",
      "Gradient Descent(4881/9999): loss=2.138626153781246, w0=73.4625000000002, w1=13.548907989845945\n",
      "Gradient Descent(4882/9999): loss=2.081855269620696, w0=73.4062500000002, w1=13.202011254883034\n",
      "Gradient Descent(4883/9999): loss=2.0619262482301055, w0=73.4062500000002, w1=13.189097433773364\n",
      "Gradient Descent(4884/9999): loss=2.0755418998807267, w0=73.3500000000002, w1=13.11955835263558\n",
      "Gradient Descent(4885/9999): loss=2.6624044257563027, w0=73.29375000000019, w1=12.951212931980992\n",
      "Gradient Descent(4886/9999): loss=1.936016427234827, w0=73.18125000000019, w1=12.953246702580195\n",
      "Gradient Descent(4887/9999): loss=2.0106245389754696, w0=72.9562500000002, w1=13.33275818523379\n",
      "Gradient Descent(4888/9999): loss=2.214259201114075, w0=73.0687500000002, w1=13.225829841346414\n",
      "Gradient Descent(4889/9999): loss=2.737232401713603, w0=73.3500000000002, w1=13.558317042359725\n",
      "Gradient Descent(4890/9999): loss=2.566642655990375, w0=73.4062500000002, w1=13.478367234672453\n",
      "Gradient Descent(4891/9999): loss=2.513984431099483, w0=73.3500000000002, w1=13.458778560082767\n",
      "Gradient Descent(4892/9999): loss=2.547656614497792, w0=73.29375000000019, w1=13.28960379794397\n",
      "Gradient Descent(4893/9999): loss=2.0827950007912444, w0=73.23750000000018, w1=13.400403870693054\n",
      "Gradient Descent(4894/9999): loss=1.8893592411129287, w0=73.29375000000019, w1=13.42109567520247\n",
      "Gradient Descent(4895/9999): loss=2.054734305098685, w0=73.23750000000018, w1=13.611058011478873\n",
      "Gradient Descent(4896/9999): loss=1.9259113718296077, w0=73.06875000000018, w1=13.527536697730774\n",
      "Gradient Descent(4897/9999): loss=1.7512984221309795, w0=73.29375000000017, w1=13.483168382487179\n",
      "Gradient Descent(4898/9999): loss=2.779177414915844, w0=73.23750000000017, w1=13.33253632547589\n",
      "Gradient Descent(4899/9999): loss=1.890306146617666, w0=73.29375000000017, w1=13.561122283121508\n",
      "Gradient Descent(4900/9999): loss=2.235125012687833, w0=73.40625000000017, w1=13.812417982948443\n",
      "Gradient Descent(4901/9999): loss=2.029304446502593, w0=73.80000000000017, w1=13.534525292052374\n",
      "Gradient Descent(4902/9999): loss=2.324547739984408, w0=73.80000000000017, w1=13.709992740911826\n",
      "Gradient Descent(4903/9999): loss=2.5707322780409347, w0=73.85625000000017, w1=13.592440123052999\n",
      "Gradient Descent(4904/9999): loss=2.192500835132147, w0=73.96875000000017, w1=13.548270850749693\n",
      "Gradient Descent(4905/9999): loss=2.286236980121721, w0=73.46250000000018, w1=13.39898937106886\n",
      "Gradient Descent(4906/9999): loss=2.574857392063751, w0=73.40625000000017, w1=13.21585885085274\n",
      "Gradient Descent(4907/9999): loss=1.7216013884796997, w0=73.35000000000016, w1=13.192928339183554\n",
      "Gradient Descent(4908/9999): loss=2.461377337928761, w0=73.35000000000016, w1=13.112463292963996\n",
      "Gradient Descent(4909/9999): loss=2.457922750971134, w0=73.23750000000017, w1=13.273946223815582\n",
      "Gradient Descent(4910/9999): loss=1.9957861687165657, w0=73.06875000000016, w1=13.541896561317113\n",
      "Gradient Descent(4911/9999): loss=2.083807241427964, w0=73.12500000000017, w1=13.419685643362362\n",
      "Gradient Descent(4912/9999): loss=1.8060507598538427, w0=73.01250000000017, w1=13.527639625322465\n",
      "Gradient Descent(4913/9999): loss=2.643749360666428, w0=72.95625000000017, w1=13.615092780471203\n",
      "Gradient Descent(4914/9999): loss=1.9262090066471886, w0=72.67500000000017, w1=13.42691088733583\n",
      "Gradient Descent(4915/9999): loss=2.28016089484803, w0=72.73125000000017, w1=13.247317222949592\n",
      "Gradient Descent(4916/9999): loss=2.3537162337292674, w0=72.84375000000017, w1=13.265879375682589\n",
      "Gradient Descent(4917/9999): loss=2.7290206667126875, w0=73.01250000000017, w1=13.3298043452216\n",
      "Gradient Descent(4918/9999): loss=1.9712989859644563, w0=73.01250000000017, w1=13.55726603909258\n",
      "Gradient Descent(4919/9999): loss=2.2672720569941576, w0=72.90000000000018, w1=13.660551077341275\n",
      "Gradient Descent(4920/9999): loss=2.2682418738736203, w0=73.01250000000017, w1=13.593715817433035\n",
      "Gradient Descent(4921/9999): loss=2.74278263686769, w0=73.01250000000017, w1=13.485601756389553\n",
      "Gradient Descent(4922/9999): loss=1.8334917833230229, w0=73.23750000000017, w1=13.503859296991273\n",
      "Gradient Descent(4923/9999): loss=2.0542451620816022, w0=73.35000000000016, w1=13.470810573299469\n",
      "Gradient Descent(4924/9999): loss=2.377908575093932, w0=73.35000000000016, w1=13.476274056469542\n",
      "Gradient Descent(4925/9999): loss=2.232903406438311, w0=73.51875000000017, w1=13.48884958373489\n",
      "Gradient Descent(4926/9999): loss=2.2406148578389766, w0=73.35000000000016, w1=13.717462881025549\n",
      "Gradient Descent(4927/9999): loss=2.175771711469184, w0=73.18125000000016, w1=13.654271777461538\n",
      "Gradient Descent(4928/9999): loss=2.2926410968316264, w0=73.40625000000016, w1=13.621226434959778\n",
      "Gradient Descent(4929/9999): loss=2.167396216205706, w0=73.29375000000016, w1=13.332050965543017\n",
      "Gradient Descent(4930/9999): loss=2.466403155242111, w0=73.35000000000016, w1=13.406301930903803\n",
      "Gradient Descent(4931/9999): loss=2.170107606729644, w0=73.23750000000017, w1=13.530572513176164\n",
      "Gradient Descent(4932/9999): loss=2.328189858200779, w0=73.40625000000017, w1=13.382264515272244\n",
      "Gradient Descent(4933/9999): loss=1.9566186231884086, w0=73.51875000000017, w1=13.600894859937195\n",
      "Gradient Descent(4934/9999): loss=2.2429321527922634, w0=73.51875000000017, w1=13.507578839634043\n",
      "Gradient Descent(4935/9999): loss=2.199218992304419, w0=73.51875000000017, w1=13.678148574061893\n",
      "Gradient Descent(4936/9999): loss=2.3318707390668796, w0=73.63125000000016, w1=13.690250823869844\n",
      "Gradient Descent(4937/9999): loss=2.847577911688111, w0=73.40625000000017, w1=13.832905670808916\n",
      "Gradient Descent(4938/9999): loss=2.253614370390726, w0=73.51875000000017, w1=13.723620841377075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(4939/9999): loss=2.3825822409308017, w0=73.51875000000017, w1=13.708595761517453\n",
      "Gradient Descent(4940/9999): loss=1.6720493950159894, w0=73.51875000000017, w1=13.743447366344078\n",
      "Gradient Descent(4941/9999): loss=1.966510573702436, w0=73.51875000000017, w1=13.858580275766265\n",
      "Gradient Descent(4942/9999): loss=2.0942001267984525, w0=73.51875000000017, w1=13.969879894854115\n",
      "Gradient Descent(4943/9999): loss=1.7938719716670644, w0=73.46250000000016, w1=13.949956022684514\n",
      "Gradient Descent(4944/9999): loss=2.6263634773169575, w0=73.68750000000016, w1=14.070608631267632\n",
      "Gradient Descent(4945/9999): loss=1.9805454922727268, w0=73.74375000000016, w1=14.267672425155228\n",
      "Gradient Descent(4946/9999): loss=2.3449170135803437, w0=73.74375000000016, w1=14.264001656655255\n",
      "Gradient Descent(4947/9999): loss=2.2852796786092346, w0=73.51875000000017, w1=14.003528090367046\n",
      "Gradient Descent(4948/9999): loss=1.9236455241016164, w0=73.46250000000016, w1=14.127685674802265\n",
      "Gradient Descent(4949/9999): loss=2.1248536423300237, w0=73.46250000000016, w1=13.941972373494059\n",
      "Gradient Descent(4950/9999): loss=2.2729880445820805, w0=73.51875000000017, w1=13.931162976742186\n",
      "Gradient Descent(4951/9999): loss=2.224535496681602, w0=73.18125000000016, w1=14.017190628946272\n",
      "Gradient Descent(4952/9999): loss=2.020354945141513, w0=73.29375000000016, w1=13.85917596117641\n",
      "Gradient Descent(4953/9999): loss=1.5484558746408121, w0=73.06875000000016, w1=14.033215933775493\n",
      "Gradient Descent(4954/9999): loss=2.0793851607458578, w0=73.18125000000016, w1=13.95383098001165\n",
      "Gradient Descent(4955/9999): loss=2.321791173920947, w0=73.29375000000016, w1=13.959269113998468\n",
      "Gradient Descent(4956/9999): loss=2.8936889789305664, w0=73.51875000000015, w1=13.537693660378478\n",
      "Gradient Descent(4957/9999): loss=2.2821413482219115, w0=73.06875000000015, w1=13.653498085512147\n",
      "Gradient Descent(4958/9999): loss=2.4266999580118265, w0=73.12500000000016, w1=13.600311879141314\n",
      "Gradient Descent(4959/9999): loss=2.2588132564919254, w0=72.90000000000016, w1=13.705785011072393\n",
      "Gradient Descent(4960/9999): loss=2.4916773540924453, w0=72.90000000000016, w1=13.717542928868045\n",
      "Gradient Descent(4961/9999): loss=2.21753601792927, w0=73.01250000000016, w1=13.633423991165303\n",
      "Gradient Descent(4962/9999): loss=2.2527345382860404, w0=73.06875000000016, w1=13.707473845021765\n",
      "Gradient Descent(4963/9999): loss=2.3072394860829615, w0=73.29375000000016, w1=13.527396485070549\n",
      "Gradient Descent(4964/9999): loss=2.4086757960348693, w0=73.29375000000016, w1=13.357344170298772\n",
      "Gradient Descent(4965/9999): loss=2.0907789003050103, w0=73.23750000000015, w1=13.65793453516723\n",
      "Gradient Descent(4966/9999): loss=2.6629254004479175, w0=73.46250000000015, w1=13.822941105300528\n",
      "Gradient Descent(4967/9999): loss=2.214389496522111, w0=73.51875000000015, w1=13.843965012775772\n",
      "Gradient Descent(4968/9999): loss=2.326268228289954, w0=73.80000000000015, w1=13.968332365627994\n",
      "Gradient Descent(4969/9999): loss=2.405334123204672, w0=74.08125000000015, w1=13.875040984658536\n",
      "Gradient Descent(4970/9999): loss=2.5291865863098715, w0=73.96875000000016, w1=13.598643926304776\n",
      "Gradient Descent(4971/9999): loss=2.3647340362894638, w0=73.80000000000015, w1=13.345175578276281\n",
      "Gradient Descent(4972/9999): loss=2.3881533029092132, w0=73.35000000000015, w1=13.67568000339193\n",
      "Gradient Descent(4973/9999): loss=1.8833219276481967, w0=73.23750000000015, w1=13.834584014757224\n",
      "Gradient Descent(4974/9999): loss=2.1780475501286034, w0=73.46250000000015, w1=13.754557809475022\n",
      "Gradient Descent(4975/9999): loss=2.4232505020659256, w0=73.40625000000014, w1=13.770681881035776\n",
      "Gradient Descent(4976/9999): loss=2.398515375317211, w0=73.40625000000014, w1=13.507661919040752\n",
      "Gradient Descent(4977/9999): loss=2.210469061888319, w0=73.18125000000015, w1=13.409695280944947\n",
      "Gradient Descent(4978/9999): loss=2.4689551972736736, w0=73.12500000000014, w1=13.741004523262612\n",
      "Gradient Descent(4979/9999): loss=2.136719492677944, w0=73.29375000000014, w1=13.782942291870855\n",
      "Gradient Descent(4980/9999): loss=1.7195980701964955, w0=73.18125000000015, w1=13.779130912946071\n",
      "Gradient Descent(4981/9999): loss=2.1137552416134, w0=73.40625000000014, w1=13.741713799201834\n",
      "Gradient Descent(4982/9999): loss=2.3704690953075866, w0=73.29375000000014, w1=13.722067081227749\n",
      "Gradient Descent(4983/9999): loss=2.234856713586015, w0=73.40625000000014, w1=13.60282586677306\n",
      "Gradient Descent(4984/9999): loss=2.212377897955612, w0=73.51875000000014, w1=13.517516503627634\n",
      "Gradient Descent(4985/9999): loss=2.4090424867356512, w0=73.57500000000014, w1=13.682528133608473\n",
      "Gradient Descent(4986/9999): loss=2.6886036657122174, w0=73.80000000000014, w1=13.616193530704217\n",
      "Gradient Descent(4987/9999): loss=2.26317721395425, w0=73.80000000000014, w1=13.673865937721143\n",
      "Gradient Descent(4988/9999): loss=1.983668004342686, w0=73.63125000000014, w1=13.81715580452679\n",
      "Gradient Descent(4989/9999): loss=2.0856347487242317, w0=73.46250000000013, w1=13.755257850662614\n",
      "Gradient Descent(4990/9999): loss=2.344598593803113, w0=73.51875000000014, w1=13.46559667998649\n",
      "Gradient Descent(4991/9999): loss=2.0960720875863807, w0=73.80000000000014, w1=13.32277470666164\n",
      "Gradient Descent(4992/9999): loss=2.4500010884078662, w0=73.74375000000013, w1=13.399775545369835\n",
      "Gradient Descent(4993/9999): loss=2.291263114063378, w0=73.74375000000013, w1=13.348287211489868\n",
      "Gradient Descent(4994/9999): loss=2.547453104434113, w0=73.80000000000014, w1=13.661612391132483\n",
      "Gradient Descent(4995/9999): loss=2.1395307026783335, w0=73.80000000000014, w1=13.473683166283323\n",
      "Gradient Descent(4996/9999): loss=2.2752067055232237, w0=74.02500000000013, w1=13.370060724627447\n",
      "Gradient Descent(4997/9999): loss=2.296744861890131, w0=73.91250000000014, w1=13.201953728502307\n",
      "Gradient Descent(4998/9999): loss=2.338788340951631, w0=73.80000000000014, w1=13.181225751228064\n",
      "Gradient Descent(4999/9999): loss=2.09044657796977, w0=73.80000000000014, w1=13.49234995503663\n",
      "Gradient Descent(5000/9999): loss=2.534679500171567, w0=73.68750000000014, w1=13.142333671730567\n",
      "Gradient Descent(5001/9999): loss=1.9418659399894163, w0=73.80000000000014, w1=13.350628347071245\n",
      "Gradient Descent(5002/9999): loss=2.7752055274248657, w0=73.40625000000014, w1=13.154573600684367\n",
      "Gradient Descent(5003/9999): loss=2.5746979136694295, w0=73.29375000000014, w1=13.268083839122488\n",
      "Gradient Descent(5004/9999): loss=2.2577991999047913, w0=73.35000000000015, w1=13.317263413672602\n",
      "Gradient Descent(5005/9999): loss=2.0524730723403497, w0=73.29375000000014, w1=13.35933627232129\n",
      "Gradient Descent(5006/9999): loss=1.9717369416135122, w0=73.12500000000014, w1=13.334003378981434\n",
      "Gradient Descent(5007/9999): loss=2.401649470506582, w0=73.01250000000014, w1=13.290844040258913\n",
      "Gradient Descent(5008/9999): loss=2.588277989480649, w0=73.06875000000015, w1=13.227923922798384\n",
      "Gradient Descent(5009/9999): loss=2.1621868200273484, w0=73.06875000000015, w1=13.454778912888681\n",
      "Gradient Descent(5010/9999): loss=2.451974410186028, w0=73.01250000000014, w1=13.784212985013422\n",
      "Gradient Descent(5011/9999): loss=2.2786941184853093, w0=73.06875000000015, w1=13.854052288759316\n",
      "Gradient Descent(5012/9999): loss=2.5395822410013427, w0=73.23750000000015, w1=13.632055046152692\n",
      "Gradient Descent(5013/9999): loss=2.0999993668665766, w0=73.18125000000015, w1=13.828160211688678\n",
      "Gradient Descent(5014/9999): loss=2.7045023444240064, w0=73.23750000000015, w1=13.527476819860196\n",
      "Gradient Descent(5015/9999): loss=1.640237014302933, w0=73.12500000000016, w1=13.42175472444288\n",
      "Gradient Descent(5016/9999): loss=2.444530107016177, w0=73.18125000000016, w1=13.577582375173158\n",
      "Gradient Descent(5017/9999): loss=2.299512726733675, w0=73.29375000000016, w1=13.602964689734602\n",
      "Gradient Descent(5018/9999): loss=2.25827661707884, w0=73.29375000000016, w1=13.686466027714191\n",
      "Gradient Descent(5019/9999): loss=2.372669350321078, w0=73.46250000000016, w1=13.854636250776483\n",
      "Gradient Descent(5020/9999): loss=2.020498944038442, w0=73.57500000000016, w1=13.96937682775428\n",
      "Gradient Descent(5021/9999): loss=1.7656139931973698, w0=73.12500000000016, w1=13.88942800541134\n",
      "Gradient Descent(5022/9999): loss=1.8124369463642718, w0=73.12500000000016, w1=13.950041847691347\n",
      "Gradient Descent(5023/9999): loss=2.3714099481427393, w0=72.95625000000015, w1=13.881203850910792\n",
      "Gradient Descent(5024/9999): loss=2.2622797157547274, w0=73.18125000000015, w1=13.867981373821051\n",
      "Gradient Descent(5025/9999): loss=1.8131535745848633, w0=73.29375000000014, w1=13.946323525557903\n",
      "Gradient Descent(5026/9999): loss=2.2770402369248295, w0=73.40625000000014, w1=13.915302858335686\n",
      "Gradient Descent(5027/9999): loss=2.4410744716228923, w0=73.23750000000014, w1=13.914304151533663\n",
      "Gradient Descent(5028/9999): loss=2.091690008929575, w0=73.40625000000014, w1=13.96674573421559\n",
      "Gradient Descent(5029/9999): loss=2.837038480967273, w0=73.46250000000015, w1=13.912562447015203\n",
      "Gradient Descent(5030/9999): loss=2.223557584181393, w0=73.57500000000014, w1=13.853901324352142\n",
      "Gradient Descent(5031/9999): loss=2.2381528136193953, w0=73.06875000000015, w1=13.810608535024656\n",
      "Gradient Descent(5032/9999): loss=2.3605428705576665, w0=73.01250000000014, w1=13.33865528067052\n",
      "Gradient Descent(5033/9999): loss=2.013759166743614, w0=72.90000000000015, w1=13.254813036994976\n",
      "Gradient Descent(5034/9999): loss=1.9963328943578205, w0=72.84375000000014, w1=13.271904821477406\n",
      "Gradient Descent(5035/9999): loss=2.6556198817708596, w0=73.06875000000014, w1=13.272971197067728\n",
      "Gradient Descent(5036/9999): loss=2.4284424944497642, w0=73.29375000000013, w1=13.347361180581279\n",
      "Gradient Descent(5037/9999): loss=2.2750342359187687, w0=73.40625000000013, w1=13.757175204259193\n",
      "Gradient Descent(5038/9999): loss=2.2749129112250968, w0=73.06875000000012, w1=13.736732334842857\n",
      "Gradient Descent(5039/9999): loss=2.2570887204843366, w0=73.01250000000012, w1=13.603824464159066\n",
      "Gradient Descent(5040/9999): loss=2.283163633789381, w0=72.84375000000011, w1=13.358315223482906\n",
      "Gradient Descent(5041/9999): loss=2.2573264847235013, w0=72.90000000000012, w1=13.603507790831332\n",
      "Gradient Descent(5042/9999): loss=2.5605124709324873, w0=73.06875000000012, w1=13.544864854454229\n",
      "Gradient Descent(5043/9999): loss=2.402264179995038, w0=73.18125000000012, w1=13.375305215040205\n",
      "Gradient Descent(5044/9999): loss=1.876095933817964, w0=73.01250000000012, w1=13.570422835127232\n",
      "Gradient Descent(5045/9999): loss=2.25969180341895, w0=73.12500000000011, w1=13.316767784659577\n",
      "Gradient Descent(5046/9999): loss=2.130312658497075, w0=73.35000000000011, w1=13.214510045208794\n",
      "Gradient Descent(5047/9999): loss=1.955391475972519, w0=73.35000000000011, w1=13.2205604926439\n",
      "Gradient Descent(5048/9999): loss=2.7858662242316345, w0=73.2937500000001, w1=13.32545260859343\n",
      "Gradient Descent(5049/9999): loss=2.083817093090357, w0=73.35000000000011, w1=13.373267966902443\n",
      "Gradient Descent(5050/9999): loss=2.8469698216394654, w0=73.2937500000001, w1=13.392826329448685\n",
      "Gradient Descent(5051/9999): loss=2.2380755132725616, w0=73.1812500000001, w1=13.555526229895149\n",
      "Gradient Descent(5052/9999): loss=2.4104159554158304, w0=73.4625000000001, w1=13.544120676850962\n",
      "Gradient Descent(5053/9999): loss=2.2284526564261715, w0=73.7437500000001, w1=13.494905037467069\n",
      "Gradient Descent(5054/9999): loss=2.4208687702907956, w0=73.9687500000001, w1=13.415790766667092\n",
      "Gradient Descent(5055/9999): loss=2.3832036961507512, w0=73.7437500000001, w1=13.64186142730336\n",
      "Gradient Descent(5056/9999): loss=1.823778775963059, w0=73.51875000000011, w1=13.580961155921043\n",
      "Gradient Descent(5057/9999): loss=1.8837549490525454, w0=73.4625000000001, w1=13.683147542288433\n",
      "Gradient Descent(5058/9999): loss=2.6193644715672155, w0=73.4062500000001, w1=13.557918559806001\n",
      "Gradient Descent(5059/9999): loss=2.0197516984318016, w0=73.4625000000001, w1=13.753075781695808\n",
      "Gradient Descent(5060/9999): loss=2.931684736735147, w0=73.4062500000001, w1=13.75481841288741\n",
      "Gradient Descent(5061/9999): loss=2.240642290245846, w0=73.3500000000001, w1=13.773915909858129\n",
      "Gradient Descent(5062/9999): loss=1.9747743044186479, w0=73.3500000000001, w1=13.528636092382403\n",
      "Gradient Descent(5063/9999): loss=2.532300651201707, w0=73.5187500000001, w1=13.505168318782676\n",
      "Gradient Descent(5064/9999): loss=1.9734141934056904, w0=73.18125000000009, w1=13.52551314976777\n",
      "Gradient Descent(5065/9999): loss=2.13164007297138, w0=73.12500000000009, w1=13.741537519613797\n",
      "Gradient Descent(5066/9999): loss=2.2978299223326424, w0=73.18125000000009, w1=13.645152514877587\n",
      "Gradient Descent(5067/9999): loss=1.9455203666383978, w0=73.2375000000001, w1=13.569101620231596\n",
      "Gradient Descent(5068/9999): loss=2.6378857557973774, w0=73.18125000000009, w1=13.489425218900136\n",
      "Gradient Descent(5069/9999): loss=2.3364131380568627, w0=73.29375000000009, w1=13.268148779247927\n",
      "Gradient Descent(5070/9999): loss=2.223320392841978, w0=73.12500000000009, w1=13.473104437257355\n",
      "Gradient Descent(5071/9999): loss=2.771034339963127, w0=73.12500000000009, w1=13.40215340182835\n",
      "Gradient Descent(5072/9999): loss=1.7518536452365139, w0=73.12500000000009, w1=13.32814122210721\n",
      "Gradient Descent(5073/9999): loss=1.7287259717045533, w0=73.06875000000008, w1=13.087129776753738\n",
      "Gradient Descent(5074/9999): loss=1.9289354308770204, w0=72.90000000000008, w1=13.295498545001937\n",
      "Gradient Descent(5075/9999): loss=1.7198927126327277, w0=72.78750000000008, w1=13.232485656980215\n",
      "Gradient Descent(5076/9999): loss=2.30272586092056, w0=73.01250000000007, w1=13.280774748122347\n",
      "Gradient Descent(5077/9999): loss=2.1209161604505273, w0=73.23750000000007, w1=13.432933840260864\n",
      "Gradient Descent(5078/9999): loss=2.952820121857323, w0=73.51875000000007, w1=13.296684558727799\n",
      "Gradient Descent(5079/9999): loss=1.654992007229372, w0=73.46250000000006, w1=13.554664662128975\n",
      "Gradient Descent(5080/9999): loss=2.1780043367788195, w0=73.40625000000006, w1=13.481950899113157\n",
      "Gradient Descent(5081/9999): loss=1.978143362223064, w0=73.35000000000005, w1=13.69342931198992\n",
      "Gradient Descent(5082/9999): loss=2.5683826912096483, w0=73.74375000000005, w1=13.8400870844172\n",
      "Gradient Descent(5083/9999): loss=2.374922768855127, w0=73.35000000000005, w1=13.823302826104005\n",
      "Gradient Descent(5084/9999): loss=2.076737697185945, w0=73.46250000000005, w1=13.547910564516538\n",
      "Gradient Descent(5085/9999): loss=2.3321497994072993, w0=73.85625000000005, w1=13.563074826988133\n",
      "Gradient Descent(5086/9999): loss=2.0810511699571475, w0=73.85625000000005, w1=13.373368083511764\n",
      "Gradient Descent(5087/9999): loss=1.9727181877043392, w0=73.91250000000005, w1=13.308196977712223\n",
      "Gradient Descent(5088/9999): loss=2.36861349856442, w0=73.85625000000005, w1=13.325202137267874\n",
      "Gradient Descent(5089/9999): loss=1.772666771688077, w0=73.68750000000004, w1=13.496131294812555\n",
      "Gradient Descent(5090/9999): loss=2.3032459089562582, w0=73.57500000000005, w1=13.50613688981679\n",
      "Gradient Descent(5091/9999): loss=2.445259826299621, w0=73.40625000000004, w1=13.545360133429023\n",
      "Gradient Descent(5092/9999): loss=2.0672582510844113, w0=73.35000000000004, w1=13.43973584960531\n",
      "Gradient Descent(5093/9999): loss=2.401519799188526, w0=73.40625000000004, w1=13.434852879405748\n",
      "Gradient Descent(5094/9999): loss=2.109395015194767, w0=73.35000000000004, w1=13.46671963698127\n",
      "Gradient Descent(5095/9999): loss=1.8901009048951347, w0=73.29375000000003, w1=13.404150778525349\n",
      "Gradient Descent(5096/9999): loss=1.9805448104457055, w0=73.51875000000003, w1=13.404256000416886\n",
      "Gradient Descent(5097/9999): loss=1.8891305447990778, w0=73.51875000000003, w1=13.432687566477021\n",
      "Gradient Descent(5098/9999): loss=1.7769334895624114, w0=73.35000000000002, w1=13.513335455068892\n",
      "Gradient Descent(5099/9999): loss=2.0797608502653717, w0=73.35000000000002, w1=13.727815934991572\n",
      "Gradient Descent(5100/9999): loss=2.5785510722281506, w0=73.35000000000002, w1=13.457180322557093\n",
      "Gradient Descent(5101/9999): loss=1.9469160665437373, w0=73.29375000000002, w1=13.632888883945704\n",
      "Gradient Descent(5102/9999): loss=2.263252102860278, w0=73.01250000000002, w1=13.84530416530444\n",
      "Gradient Descent(5103/9999): loss=1.640864542184722, w0=73.29375000000002, w1=13.518220191311897\n",
      "Gradient Descent(5104/9999): loss=2.5872822770201775, w0=73.35000000000002, w1=13.49805394710095\n",
      "Gradient Descent(5105/9999): loss=2.0887737981746866, w0=73.57500000000002, w1=13.545670690308006\n",
      "Gradient Descent(5106/9999): loss=2.276767910390908, w0=73.68750000000001, w1=13.131347613621985\n",
      "Gradient Descent(5107/9999): loss=2.3197912490694863, w0=73.57500000000002, w1=13.391399229374137\n",
      "Gradient Descent(5108/9999): loss=2.2926401846966535, w0=73.57500000000002, w1=13.323095672909963\n",
      "Gradient Descent(5109/9999): loss=1.945276955391655, w0=73.18125000000002, w1=13.150836776391404\n",
      "Gradient Descent(5110/9999): loss=2.8198353196081607, w0=73.12500000000001, w1=13.064817424941713\n",
      "Gradient Descent(5111/9999): loss=2.5616626020256095, w0=73.23750000000001, w1=13.371079831622502\n",
      "Gradient Descent(5112/9999): loss=1.9513758526048706, w0=73.23750000000001, w1=13.450950561020504\n",
      "Gradient Descent(5113/9999): loss=2.3105851517730267, w0=73.35000000000001, w1=13.624137007437472\n",
      "Gradient Descent(5114/9999): loss=2.2011412979816534, w0=73.12500000000001, w1=13.51109202092975\n",
      "Gradient Descent(5115/9999): loss=2.716078103932426, w0=73.18125000000002, w1=13.177268219251264\n",
      "Gradient Descent(5116/9999): loss=1.3868992035966226, w0=73.23750000000003, w1=13.614129023146603\n",
      "Gradient Descent(5117/9999): loss=2.31177680179304, w0=73.40625000000003, w1=13.555210252773549\n",
      "Gradient Descent(5118/9999): loss=2.7839274892287302, w0=73.57500000000003, w1=13.607354362548309\n",
      "Gradient Descent(5119/9999): loss=1.8594991051299112, w0=73.29375000000003, w1=13.33607916615636\n",
      "Gradient Descent(5120/9999): loss=2.740398329432966, w0=73.51875000000003, w1=13.153188299873563\n",
      "Gradient Descent(5121/9999): loss=2.5693162303780808, w0=73.57500000000003, w1=13.11306384839006\n",
      "Gradient Descent(5122/9999): loss=2.442221662397939, w0=73.74375000000003, w1=13.257703763197247\n",
      "Gradient Descent(5123/9999): loss=1.8433799897702252, w0=73.51875000000004, w1=13.154272571961236\n",
      "Gradient Descent(5124/9999): loss=2.223590076459559, w0=73.23750000000004, w1=13.54031793428046\n",
      "Gradient Descent(5125/9999): loss=2.61758247486579, w0=73.12500000000004, w1=13.492401742701981\n",
      "Gradient Descent(5126/9999): loss=1.8841057528556533, w0=73.18125000000005, w1=13.445982762886695\n",
      "Gradient Descent(5127/9999): loss=2.0245435382371717, w0=73.06875000000005, w1=13.380505016271416\n",
      "Gradient Descent(5128/9999): loss=2.022027061215935, w0=73.01250000000005, w1=13.281844802329658\n",
      "Gradient Descent(5129/9999): loss=2.3395634515017436, w0=72.95625000000004, w1=13.387659049576163\n",
      "Gradient Descent(5130/9999): loss=2.1513728671256667, w0=72.84375000000004, w1=13.322980175209583\n",
      "Gradient Descent(5131/9999): loss=2.3022500401948305, w0=73.01250000000005, w1=13.320530942815138\n",
      "Gradient Descent(5132/9999): loss=1.9637553717436398, w0=73.35000000000005, w1=13.48347336191566\n",
      "Gradient Descent(5133/9999): loss=1.8443675912751285, w0=73.63125000000005, w1=13.548967722600286\n",
      "Gradient Descent(5134/9999): loss=2.4356137013391264, w0=73.63125000000005, w1=13.583296267627153\n",
      "Gradient Descent(5135/9999): loss=2.005389367818986, w0=73.57500000000005, w1=13.550373247529079\n",
      "Gradient Descent(5136/9999): loss=2.461028587171387, w0=73.51875000000004, w1=13.530727335564649\n",
      "Gradient Descent(5137/9999): loss=2.454850552649308, w0=73.51875000000004, w1=13.55511438919167\n",
      "Gradient Descent(5138/9999): loss=2.304327858553277, w0=73.29375000000005, w1=13.605790626334858\n",
      "Gradient Descent(5139/9999): loss=1.6819576802993983, w0=73.40625000000004, w1=13.404178947982327\n",
      "Gradient Descent(5140/9999): loss=2.8859707481470678, w0=73.68750000000004, w1=13.372879937772854\n",
      "Gradient Descent(5141/9999): loss=2.311097086476264, w0=73.51875000000004, w1=13.427116870234773\n",
      "Gradient Descent(5142/9999): loss=2.0684271183624214, w0=73.63125000000004, w1=13.559732061320378\n",
      "Gradient Descent(5143/9999): loss=2.3789187072525095, w0=73.51875000000004, w1=13.728870952949974\n",
      "Gradient Descent(5144/9999): loss=2.0620268215146558, w0=73.46250000000003, w1=13.791803706621062\n",
      "Gradient Descent(5145/9999): loss=2.4067103591556447, w0=73.35000000000004, w1=14.002830061468023\n",
      "Gradient Descent(5146/9999): loss=2.4164155999251924, w0=73.51875000000004, w1=13.91466049273984\n",
      "Gradient Descent(5147/9999): loss=2.0210984806087096, w0=73.40625000000004, w1=14.155118939745998\n",
      "Gradient Descent(5148/9999): loss=2.331968396997466, w0=73.29375000000005, w1=14.155883728948094\n",
      "Gradient Descent(5149/9999): loss=2.1367741289807767, w0=73.29375000000005, w1=13.780964430730497\n",
      "Gradient Descent(5150/9999): loss=2.3065436647439546, w0=73.40625000000004, w1=13.494637114445123\n",
      "Gradient Descent(5151/9999): loss=1.9474590925552497, w0=73.46250000000005, w1=13.494826506491526\n",
      "Gradient Descent(5152/9999): loss=2.5822009363648166, w0=73.63125000000005, w1=13.570519443158814\n",
      "Gradient Descent(5153/9999): loss=2.1904852557584236, w0=73.68750000000006, w1=13.377825630561746\n",
      "Gradient Descent(5154/9999): loss=1.9245472962923884, w0=73.40625000000006, w1=13.384620863861562\n",
      "Gradient Descent(5155/9999): loss=2.1374909534942894, w0=73.40625000000006, w1=13.389722896155655\n",
      "Gradient Descent(5156/9999): loss=2.34635492512203, w0=73.57500000000006, w1=13.52431739441079\n",
      "Gradient Descent(5157/9999): loss=2.7745840365647223, w0=73.23750000000005, w1=13.108261076063767\n",
      "Gradient Descent(5158/9999): loss=2.2139981581948405, w0=73.12500000000006, w1=13.001226812386312\n",
      "Gradient Descent(5159/9999): loss=2.283192877173968, w0=73.01250000000006, w1=13.257839136275125\n",
      "Gradient Descent(5160/9999): loss=2.2337482531081445, w0=73.23750000000005, w1=13.223780319293338\n",
      "Gradient Descent(5161/9999): loss=2.0858931516650356, w0=72.95625000000005, w1=13.014197695704825\n",
      "Gradient Descent(5162/9999): loss=2.06998198128673, w0=73.12500000000006, w1=13.202635996731805\n",
      "Gradient Descent(5163/9999): loss=1.9974830201808704, w0=73.29375000000006, w1=13.284088046265614\n",
      "Gradient Descent(5164/9999): loss=2.727997170006014, w0=73.12500000000006, w1=13.333018138074921\n",
      "Gradient Descent(5165/9999): loss=1.981277964025799, w0=72.67500000000005, w1=13.335445427026045\n",
      "Gradient Descent(5166/9999): loss=2.096633257335266, w0=72.45000000000006, w1=13.182748701496042\n",
      "Gradient Descent(5167/9999): loss=2.1814260988609786, w0=72.67500000000005, w1=12.899958609227902\n",
      "Gradient Descent(5168/9999): loss=2.8812927069526375, w0=72.56250000000006, w1=13.245048147133502\n",
      "Gradient Descent(5169/9999): loss=2.5415271936338097, w0=72.50625000000005, w1=13.386323300522575\n",
      "Gradient Descent(5170/9999): loss=2.5053781499538705, w0=72.67500000000005, w1=13.360452703546633\n",
      "Gradient Descent(5171/9999): loss=1.619622136484252, w0=72.95625000000005, w1=13.34702264640403\n",
      "Gradient Descent(5172/9999): loss=2.3623063894583605, w0=72.84375000000006, w1=13.455657162190295\n",
      "Gradient Descent(5173/9999): loss=1.8380210683745541, w0=73.18125000000006, w1=13.501976329051955\n",
      "Gradient Descent(5174/9999): loss=2.2267120783282284, w0=73.35000000000007, w1=13.643850862818956\n",
      "Gradient Descent(5175/9999): loss=2.6293388879082604, w0=73.40625000000007, w1=13.515503739330045\n",
      "Gradient Descent(5176/9999): loss=2.09767013381528, w0=73.23750000000007, w1=13.663266883864123\n",
      "Gradient Descent(5177/9999): loss=2.185931931533518, w0=73.29375000000007, w1=13.904977896426413\n",
      "Gradient Descent(5178/9999): loss=2.215401254483766, w0=73.29375000000007, w1=14.171417609509714\n",
      "Gradient Descent(5179/9999): loss=2.4105132281093873, w0=73.18125000000008, w1=14.31946837920735\n",
      "Gradient Descent(5180/9999): loss=2.148937891194887, w0=73.18125000000008, w1=14.376795210022673\n",
      "Gradient Descent(5181/9999): loss=2.102255910029636, w0=73.06875000000008, w1=14.240615955600129\n",
      "Gradient Descent(5182/9999): loss=2.867575907066781, w0=73.18125000000008, w1=14.049719001378852\n",
      "Gradient Descent(5183/9999): loss=2.466105618353546, w0=73.18125000000008, w1=13.651192618366732\n",
      "Gradient Descent(5184/9999): loss=2.3116706324056127, w0=73.23750000000008, w1=13.646503013633023\n",
      "Gradient Descent(5185/9999): loss=2.2355023582028855, w0=73.35000000000008, w1=13.707184059107652\n",
      "Gradient Descent(5186/9999): loss=2.3789758033026613, w0=73.23750000000008, w1=13.70865587965378\n",
      "Gradient Descent(5187/9999): loss=1.9480107658563974, w0=73.35000000000008, w1=13.694080727240792\n",
      "Gradient Descent(5188/9999): loss=1.6895129054086615, w0=73.35000000000008, w1=13.90720089096854\n",
      "Gradient Descent(5189/9999): loss=2.535960810951843, w0=73.35000000000008, w1=13.797955221155876\n",
      "Gradient Descent(5190/9999): loss=1.8284015247862428, w0=73.46250000000008, w1=13.81453594752469\n",
      "Gradient Descent(5191/9999): loss=1.9342378477817967, w0=73.57500000000007, w1=13.931692241342414\n",
      "Gradient Descent(5192/9999): loss=2.087914345251173, w0=73.63125000000008, w1=14.073470918642473\n",
      "Gradient Descent(5193/9999): loss=1.6612784553042674, w0=73.74375000000008, w1=14.187297523121368\n",
      "Gradient Descent(5194/9999): loss=2.369998928055085, w0=73.68750000000007, w1=13.916104687548923\n",
      "Gradient Descent(5195/9999): loss=2.111079569946285, w0=73.68750000000007, w1=13.888151352885929\n",
      "Gradient Descent(5196/9999): loss=1.9871208418657589, w0=73.57500000000007, w1=13.886157884809279\n",
      "Gradient Descent(5197/9999): loss=2.423350491012578, w0=73.51875000000007, w1=13.87961111856775\n",
      "Gradient Descent(5198/9999): loss=2.220676850560932, w0=73.51875000000007, w1=13.64674172890756\n",
      "Gradient Descent(5199/9999): loss=2.4014725958728755, w0=73.57500000000007, w1=13.757693014443213\n",
      "Gradient Descent(5200/9999): loss=2.078129802812196, w0=73.29375000000007, w1=13.871042605006851\n",
      "Gradient Descent(5201/9999): loss=2.1355459679580444, w0=73.46250000000008, w1=13.957451486406358\n",
      "Gradient Descent(5202/9999): loss=2.5687508687904703, w0=73.46250000000008, w1=13.929192836378359\n",
      "Gradient Descent(5203/9999): loss=2.1543624836301873, w0=73.57500000000007, w1=14.064998529440446\n",
      "Gradient Descent(5204/9999): loss=2.110038058496868, w0=73.40625000000007, w1=14.106353075242426\n",
      "Gradient Descent(5205/9999): loss=2.0278514284260396, w0=73.51875000000007, w1=13.834172291721911\n",
      "Gradient Descent(5206/9999): loss=2.19754574286267, w0=73.18125000000006, w1=13.892625877186248\n",
      "Gradient Descent(5207/9999): loss=2.0048663573399743, w0=73.01250000000006, w1=13.933181356397366\n",
      "Gradient Descent(5208/9999): loss=2.5345976575182636, w0=73.06875000000007, w1=14.126405731019464\n",
      "Gradient Descent(5209/9999): loss=2.0247092063670227, w0=73.23750000000007, w1=14.031526779621572\n",
      "Gradient Descent(5210/9999): loss=2.2289955712283502, w0=73.40625000000007, w1=14.122928024568578\n",
      "Gradient Descent(5211/9999): loss=2.3770391552625627, w0=73.35000000000007, w1=13.939999707928665\n",
      "Gradient Descent(5212/9999): loss=1.8622208383229368, w0=73.35000000000007, w1=14.02461947227518\n",
      "Gradient Descent(5213/9999): loss=2.0235430546999438, w0=73.29375000000006, w1=14.24384495586499\n",
      "Gradient Descent(5214/9999): loss=2.2160054796571607, w0=73.23750000000005, w1=14.285469469696967\n",
      "Gradient Descent(5215/9999): loss=1.6769871056360472, w0=73.01250000000006, w1=14.331312406305752\n",
      "Gradient Descent(5216/9999): loss=2.6795796969829393, w0=72.90000000000006, w1=14.268507046986176\n",
      "Gradient Descent(5217/9999): loss=2.1653189436203375, w0=73.01250000000006, w1=14.095302272492468\n",
      "Gradient Descent(5218/9999): loss=2.3208501956455745, w0=73.06875000000007, w1=13.870046415947911\n",
      "Gradient Descent(5219/9999): loss=2.3775611247946093, w0=73.12500000000007, w1=13.787420537183923\n",
      "Gradient Descent(5220/9999): loss=2.7316510624564607, w0=72.95625000000007, w1=13.798321251059084\n",
      "Gradient Descent(5221/9999): loss=2.6237456130046692, w0=73.18125000000006, w1=13.888378917992965\n",
      "Gradient Descent(5222/9999): loss=2.0670469271979415, w0=73.18125000000006, w1=13.868308840120946\n",
      "Gradient Descent(5223/9999): loss=2.8155193285773192, w0=73.35000000000007, w1=13.593928806885213\n",
      "Gradient Descent(5224/9999): loss=2.1197811850031942, w0=73.29375000000006, w1=13.588245270189443\n",
      "Gradient Descent(5225/9999): loss=2.359325627635937, w0=73.18125000000006, w1=13.734437248225568\n",
      "Gradient Descent(5226/9999): loss=2.0660421819276618, w0=73.01250000000006, w1=13.38108561194567\n",
      "Gradient Descent(5227/9999): loss=1.8930699817441903, w0=72.84375000000006, w1=13.233730199095902\n",
      "Gradient Descent(5228/9999): loss=2.316456965306197, w0=72.84375000000006, w1=13.052258790713205\n",
      "Gradient Descent(5229/9999): loss=2.263218024596198, w0=72.73125000000006, w1=12.962149285345934\n",
      "Gradient Descent(5230/9999): loss=2.5242093517588127, w0=72.84375000000006, w1=13.001245614272825\n",
      "Gradient Descent(5231/9999): loss=2.1967020391102965, w0=72.61875000000006, w1=12.815184079156692\n",
      "Gradient Descent(5232/9999): loss=2.219689862674789, w0=72.78750000000007, w1=12.869093763203091\n",
      "Gradient Descent(5233/9999): loss=2.0520289689730182, w0=72.78750000000007, w1=13.040759749165076\n",
      "Gradient Descent(5234/9999): loss=2.318007360720724, w0=72.61875000000006, w1=13.162381002670418\n",
      "Gradient Descent(5235/9999): loss=2.158510116975009, w0=72.39375000000007, w1=13.33899915846797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5236/9999): loss=1.9947651968330127, w0=72.50625000000007, w1=13.623305612104305\n",
      "Gradient Descent(5237/9999): loss=2.0245384809333453, w0=72.39375000000007, w1=13.865023806563114\n",
      "Gradient Descent(5238/9999): loss=1.810624502800841, w0=72.33750000000006, w1=13.895676075387948\n",
      "Gradient Descent(5239/9999): loss=2.797992846876844, w0=72.16875000000006, w1=13.954646745502316\n",
      "Gradient Descent(5240/9999): loss=2.222690198456727, w0=72.28125000000006, w1=13.651343826342275\n",
      "Gradient Descent(5241/9999): loss=2.215075706065835, w0=72.28125000000006, w1=13.502819660478439\n",
      "Gradient Descent(5242/9999): loss=2.7315609584292413, w0=72.05625000000006, w1=13.73092253311983\n",
      "Gradient Descent(5243/9999): loss=2.256145822031111, w0=72.11250000000007, w1=13.677361793203206\n",
      "Gradient Descent(5244/9999): loss=2.6763659573590397, w0=72.33750000000006, w1=13.63584598085627\n",
      "Gradient Descent(5245/9999): loss=2.6856515002281824, w0=72.28125000000006, w1=13.501061619397731\n",
      "Gradient Descent(5246/9999): loss=1.969965233195054, w0=72.56250000000006, w1=13.491407854296783\n",
      "Gradient Descent(5247/9999): loss=1.8528323604561276, w0=72.73125000000006, w1=13.735022199373192\n",
      "Gradient Descent(5248/9999): loss=2.439713740687115, w0=72.84375000000006, w1=13.649384269366653\n",
      "Gradient Descent(5249/9999): loss=2.3123577864419866, w0=73.01250000000006, w1=13.712199926552433\n",
      "Gradient Descent(5250/9999): loss=2.033724077444069, w0=72.73125000000006, w1=13.77155181583274\n",
      "Gradient Descent(5251/9999): loss=2.4503057768492487, w0=72.73125000000006, w1=13.678668931262576\n",
      "Gradient Descent(5252/9999): loss=2.1246453223754895, w0=72.95625000000005, w1=14.01233173564033\n",
      "Gradient Descent(5253/9999): loss=2.3468325797369936, w0=73.01250000000006, w1=13.749273905264067\n",
      "Gradient Descent(5254/9999): loss=2.65097928882904, w0=72.90000000000006, w1=13.812605186168112\n",
      "Gradient Descent(5255/9999): loss=2.4948892072389137, w0=72.78750000000007, w1=13.70215414592162\n",
      "Gradient Descent(5256/9999): loss=2.109075035186494, w0=72.67500000000007, w1=13.678250388916771\n",
      "Gradient Descent(5257/9999): loss=1.5518212170568118, w0=72.84375000000007, w1=13.945959224347673\n",
      "Gradient Descent(5258/9999): loss=2.0392316789502045, w0=72.61875000000008, w1=13.987114789735605\n",
      "Gradient Descent(5259/9999): loss=2.112525070955553, w0=72.61875000000008, w1=13.782144705732772\n",
      "Gradient Descent(5260/9999): loss=2.3722264226412397, w0=72.78750000000008, w1=13.995507556357508\n",
      "Gradient Descent(5261/9999): loss=2.439425680603635, w0=73.12500000000009, w1=13.749043958283481\n",
      "Gradient Descent(5262/9999): loss=1.9971885542445837, w0=73.57500000000009, w1=13.582840558896557\n",
      "Gradient Descent(5263/9999): loss=2.770131516704752, w0=73.29375000000009, w1=13.330421569407278\n",
      "Gradient Descent(5264/9999): loss=2.77411656039072, w0=73.3500000000001, w1=13.326260395812712\n",
      "Gradient Descent(5265/9999): loss=2.5794230553334754, w0=73.3500000000001, w1=13.396528136379162\n",
      "Gradient Descent(5266/9999): loss=2.078829662636038, w0=73.2375000000001, w1=13.188218838746911\n",
      "Gradient Descent(5267/9999): loss=2.574919929461674, w0=73.1250000000001, w1=13.187624521328388\n",
      "Gradient Descent(5268/9999): loss=2.0723605855245575, w0=73.3500000000001, w1=13.202404291280647\n",
      "Gradient Descent(5269/9999): loss=2.434707135054084, w0=73.5187500000001, w1=13.373570555816526\n",
      "Gradient Descent(5270/9999): loss=2.08620720498957, w0=73.6875000000001, w1=13.37957557901526\n",
      "Gradient Descent(5271/9999): loss=2.66231916328221, w0=73.4062500000001, w1=13.461588793837631\n",
      "Gradient Descent(5272/9999): loss=2.2735040033886347, w0=73.1250000000001, w1=13.397302302153072\n",
      "Gradient Descent(5273/9999): loss=2.054957495239012, w0=73.3500000000001, w1=13.413027671493019\n",
      "Gradient Descent(5274/9999): loss=2.133071502705169, w0=73.29375000000009, w1=13.423549753026263\n",
      "Gradient Descent(5275/9999): loss=2.1529608629533916, w0=73.01250000000009, w1=13.75471201611962\n",
      "Gradient Descent(5276/9999): loss=2.1277195075913276, w0=73.12500000000009, w1=13.964023041083477\n",
      "Gradient Descent(5277/9999): loss=2.085256269441671, w0=73.35000000000008, w1=14.027482819232427\n",
      "Gradient Descent(5278/9999): loss=2.736994895173651, w0=73.46250000000008, w1=14.115682042207418\n",
      "Gradient Descent(5279/9999): loss=2.006399599988384, w0=73.51875000000008, w1=13.977023838901044\n",
      "Gradient Descent(5280/9999): loss=2.2635100903598078, w0=73.63125000000008, w1=14.057651226486604\n",
      "Gradient Descent(5281/9999): loss=2.2934962779822765, w0=73.68750000000009, w1=14.174904371066328\n",
      "Gradient Descent(5282/9999): loss=2.2744050273494243, w0=73.63125000000008, w1=14.254500002530673\n",
      "Gradient Descent(5283/9999): loss=1.9310439718304213, w0=73.85625000000007, w1=14.065710743862914\n",
      "Gradient Descent(5284/9999): loss=2.4746244676339257, w0=73.85625000000007, w1=14.065043760327566\n",
      "Gradient Descent(5285/9999): loss=2.637008812770483, w0=73.68750000000007, w1=14.11253143623493\n",
      "Gradient Descent(5286/9999): loss=2.3798761222185747, w0=73.57500000000007, w1=13.772850383014644\n",
      "Gradient Descent(5287/9999): loss=2.5535791861144874, w0=73.63125000000008, w1=13.834870226398923\n",
      "Gradient Descent(5288/9999): loss=2.0787409645172374, w0=73.46250000000008, w1=13.704384420409227\n",
      "Gradient Descent(5289/9999): loss=2.235856289385362, w0=73.12500000000007, w1=13.77323532113095\n",
      "Gradient Descent(5290/9999): loss=2.3667694587649035, w0=73.23750000000007, w1=13.863187074772924\n",
      "Gradient Descent(5291/9999): loss=2.0895433861217185, w0=73.23750000000007, w1=13.623790267665777\n",
      "Gradient Descent(5292/9999): loss=2.440212784423381, w0=73.18125000000006, w1=13.58455480462649\n",
      "Gradient Descent(5293/9999): loss=2.1755828314172794, w0=73.51875000000007, w1=13.546416564454532\n",
      "Gradient Descent(5294/9999): loss=1.8429635025449775, w0=73.68750000000007, w1=13.301403846666567\n",
      "Gradient Descent(5295/9999): loss=1.966030917011343, w0=73.63125000000007, w1=13.72522324099127\n",
      "Gradient Descent(5296/9999): loss=2.1625063298689664, w0=73.80000000000007, w1=13.578934642101755\n",
      "Gradient Descent(5297/9999): loss=2.3024294549346713, w0=73.35000000000007, w1=13.476187639173302\n",
      "Gradient Descent(5298/9999): loss=1.6396305920576904, w0=73.29375000000006, w1=13.73407748636391\n",
      "Gradient Descent(5299/9999): loss=2.3081088058771915, w0=73.40625000000006, w1=13.779594782781565\n",
      "Gradient Descent(5300/9999): loss=1.8916696955081647, w0=73.63125000000005, w1=13.53903165711692\n",
      "Gradient Descent(5301/9999): loss=1.827013878450435, w0=73.46250000000005, w1=13.224768600247138\n",
      "Gradient Descent(5302/9999): loss=1.7930390769494, w0=73.35000000000005, w1=13.258409761858394\n",
      "Gradient Descent(5303/9999): loss=2.011449435859365, w0=73.35000000000005, w1=13.136406714721183\n",
      "Gradient Descent(5304/9999): loss=2.3523751519359033, w0=73.35000000000005, w1=13.340788710560497\n",
      "Gradient Descent(5305/9999): loss=2.088084993499617, w0=73.51875000000005, w1=13.448630359443081\n",
      "Gradient Descent(5306/9999): loss=2.014686417753555, w0=73.63125000000005, w1=13.644461378476969\n",
      "Gradient Descent(5307/9999): loss=2.360673088427845, w0=73.46250000000005, w1=13.948167478919283\n",
      "Gradient Descent(5308/9999): loss=2.2487356142989268, w0=73.40625000000004, w1=14.024111954573273\n",
      "Gradient Descent(5309/9999): loss=2.4602624770760793, w0=73.40625000000004, w1=13.889110650176438\n",
      "Gradient Descent(5310/9999): loss=2.3000330316034057, w0=73.29375000000005, w1=13.929957971504347\n",
      "Gradient Descent(5311/9999): loss=2.147411285986095, w0=73.18125000000005, w1=13.911663800829382\n",
      "Gradient Descent(5312/9999): loss=1.982848054752476, w0=73.29375000000005, w1=13.860096536327072\n",
      "Gradient Descent(5313/9999): loss=1.9319363728970769, w0=73.12500000000004, w1=13.67502051024322\n",
      "Gradient Descent(5314/9999): loss=1.9552712658996403, w0=73.23750000000004, w1=13.749061022459118\n",
      "Gradient Descent(5315/9999): loss=1.8901259785040592, w0=73.23750000000004, w1=13.7660388300242\n",
      "Gradient Descent(5316/9999): loss=2.318021946005479, w0=73.35000000000004, w1=13.888237856204812\n",
      "Gradient Descent(5317/9999): loss=2.130288468148396, w0=73.06875000000004, w1=13.942063505711788\n",
      "Gradient Descent(5318/9999): loss=2.574833695843467, w0=73.01250000000003, w1=13.931165911253045\n",
      "Gradient Descent(5319/9999): loss=1.818802241701173, w0=73.01250000000003, w1=13.540890868775334\n",
      "Gradient Descent(5320/9999): loss=2.6446759709776364, w0=72.95625000000003, w1=13.346400268232037\n",
      "Gradient Descent(5321/9999): loss=2.365416922297654, w0=73.18125000000002, w1=13.259165198540856\n",
      "Gradient Descent(5322/9999): loss=2.0988149990574483, w0=73.23750000000003, w1=13.513647238491938\n",
      "Gradient Descent(5323/9999): loss=1.9585628567402136, w0=73.23750000000003, w1=13.344145460074566\n",
      "Gradient Descent(5324/9999): loss=2.957782401159232, w0=72.90000000000002, w1=13.44890643625423\n",
      "Gradient Descent(5325/9999): loss=2.389066142242974, w0=73.01250000000002, w1=13.247229940901477\n",
      "Gradient Descent(5326/9999): loss=2.3699681419460994, w0=72.67500000000001, w1=13.176107064989713\n",
      "Gradient Descent(5327/9999): loss=2.1688392510283, w0=72.45000000000002, w1=13.140136852525682\n",
      "Gradient Descent(5328/9999): loss=2.1675289855759012, w0=72.61875000000002, w1=13.33306296366527\n",
      "Gradient Descent(5329/9999): loss=1.6830487911004433, w0=72.78750000000002, w1=13.347201673785134\n",
      "Gradient Descent(5330/9999): loss=1.9673241627271034, w0=72.95625000000003, w1=13.28300593219228\n",
      "Gradient Descent(5331/9999): loss=2.2793757754033095, w0=73.06875000000002, w1=13.31085752734155\n",
      "Gradient Descent(5332/9999): loss=1.9586652899529196, w0=73.01250000000002, w1=13.27953754415623\n",
      "Gradient Descent(5333/9999): loss=2.125164204777885, w0=72.95625000000001, w1=13.728496824817187\n",
      "Gradient Descent(5334/9999): loss=2.27573386675516, w0=73.06875000000001, w1=13.703666017886539\n",
      "Gradient Descent(5335/9999): loss=2.423616875870618, w0=72.9, w1=13.815110683389916\n",
      "Gradient Descent(5336/9999): loss=2.221702743777417, w0=73.06875000000001, w1=13.779360644617732\n",
      "Gradient Descent(5337/9999): loss=2.3085789284776848, w0=73.12500000000001, w1=13.658309633448814\n",
      "Gradient Descent(5338/9999): loss=2.312355534582133, w0=73.40625000000001, w1=13.789182052671984\n",
      "Gradient Descent(5339/9999): loss=2.0471098095057627, w0=73.40625000000001, w1=13.444668361155225\n",
      "Gradient Descent(5340/9999): loss=2.110132902288113, w0=73.57500000000002, w1=13.488577848879657\n",
      "Gradient Descent(5341/9999): loss=2.5167357586487857, w0=73.51875000000001, w1=13.5700547267107\n",
      "Gradient Descent(5342/9999): loss=2.1892410943498914, w0=73.40625000000001, w1=13.35073726474767\n",
      "Gradient Descent(5343/9999): loss=1.9149753636037972, w0=73.40625000000001, w1=13.526509447205578\n",
      "Gradient Descent(5344/9999): loss=2.542824133231261, w0=73.35000000000001, w1=13.662470437188071\n",
      "Gradient Descent(5345/9999): loss=1.910009145289351, w0=73.23750000000001, w1=13.82878272770186\n",
      "Gradient Descent(5346/9999): loss=2.6666777002693642, w0=73.12500000000001, w1=13.794381527807404\n",
      "Gradient Descent(5347/9999): loss=2.349192859975609, w0=72.95625000000001, w1=13.935722637006473\n",
      "Gradient Descent(5348/9999): loss=2.386026127956711, w0=73.06875000000001, w1=14.070660642039512\n",
      "Gradient Descent(5349/9999): loss=2.2417954326418323, w0=73.12500000000001, w1=14.072752116892609\n",
      "Gradient Descent(5350/9999): loss=1.8972140487261853, w0=72.84375000000001, w1=14.163943614460981\n",
      "Gradient Descent(5351/9999): loss=2.1907871133451295, w0=72.73125000000002, w1=14.221595764934563\n",
      "Gradient Descent(5352/9999): loss=2.1735201653477447, w0=72.78750000000002, w1=14.317033273403238\n",
      "Gradient Descent(5353/9999): loss=2.503438995661931, w0=73.06875000000002, w1=14.175558117698447\n",
      "Gradient Descent(5354/9999): loss=2.3079766673518423, w0=73.06875000000002, w1=14.183745633386065\n",
      "Gradient Descent(5355/9999): loss=1.8273292509469354, w0=73.29375000000002, w1=14.450410043431782\n",
      "Gradient Descent(5356/9999): loss=2.083677838288035, w0=73.40625000000001, w1=14.23772432310682\n",
      "Gradient Descent(5357/9999): loss=1.8839599060108136, w0=73.35000000000001, w1=14.314579222966795\n",
      "Gradient Descent(5358/9999): loss=2.3195535297595895, w0=73.575, w1=14.498304037792082\n",
      "Gradient Descent(5359/9999): loss=2.5189636538382087, w0=73.51875, w1=14.440762834252155\n",
      "Gradient Descent(5360/9999): loss=2.2488283334634254, w0=73.2375, w1=14.411899709870042\n",
      "Gradient Descent(5361/9999): loss=2.1796768640364004, w0=73.35, w1=14.2855259608751\n",
      "Gradient Descent(5362/9999): loss=2.145553412492938, w0=72.84375, w1=14.025412193587918\n",
      "Gradient Descent(5363/9999): loss=1.9541658225787621, w0=72.675, w1=14.013419132482275\n",
      "Gradient Descent(5364/9999): loss=2.0872782861402563, w0=72.95625, w1=13.752597014301209\n",
      "Gradient Descent(5365/9999): loss=1.8928124478961137, w0=73.06875, w1=13.689586113288946\n",
      "Gradient Descent(5366/9999): loss=2.177761408005707, w0=73.18124999999999, w1=13.531384057771985\n",
      "Gradient Descent(5367/9999): loss=2.2777636862443034, w0=73.29374999999999, w1=13.572625140410747\n",
      "Gradient Descent(5368/9999): loss=2.5767100059620676, w0=73.29374999999999, w1=13.303362105537534\n",
      "Gradient Descent(5369/9999): loss=2.263892526518629, w0=73.57499999999999, w1=13.64123986701462\n",
      "Gradient Descent(5370/9999): loss=2.7965985335880363, w0=73.68749999999999, w1=13.41905119823409\n",
      "Gradient Descent(5371/9999): loss=1.8101936967616024, w0=73.51874999999998, w1=13.218215897328692\n",
      "Gradient Descent(5372/9999): loss=2.5936101589210976, w0=73.34999999999998, w1=12.937285475655267\n",
      "Gradient Descent(5373/9999): loss=2.751357722606575, w0=73.40624999999999, w1=13.144562465373031\n",
      "Gradient Descent(5374/9999): loss=2.729137735775835, w0=73.40624999999999, w1=13.068609533464512\n",
      "Gradient Descent(5375/9999): loss=2.281344600348578, w0=73.29374999999999, w1=13.267755079460752\n",
      "Gradient Descent(5376/9999): loss=2.3249000087954994, w0=73.46249999999999, w1=13.214825665878235\n",
      "Gradient Descent(5377/9999): loss=1.8543692829356644, w0=73.40624999999999, w1=13.204732102661282\n",
      "Gradient Descent(5378/9999): loss=2.3605917100408043, w0=73.57499999999999, w1=13.283910863193528\n",
      "Gradient Descent(5379/9999): loss=1.905173026084464, w0=73.51874999999998, w1=13.256263552351461\n",
      "Gradient Descent(5380/9999): loss=2.260809044646743, w0=73.57499999999999, w1=13.090992612742776\n",
      "Gradient Descent(5381/9999): loss=2.2106825760464415, w0=73.51874999999998, w1=12.87018947723497\n",
      "Gradient Descent(5382/9999): loss=2.4505011177756497, w0=73.57499999999999, w1=12.943246442400712\n",
      "Gradient Descent(5383/9999): loss=2.253501953644582, w0=73.35, w1=13.029839708339715\n",
      "Gradient Descent(5384/9999): loss=2.245220683466572, w0=73.40625, w1=13.108731855652628\n",
      "Gradient Descent(5385/9999): loss=1.9834366902451, w0=73.51875, w1=13.361768591529898\n",
      "Gradient Descent(5386/9999): loss=2.0717129067955433, w0=73.575, w1=13.400378472332674\n",
      "Gradient Descent(5387/9999): loss=1.9491330860078622, w0=73.63125000000001, w1=13.498386030154256\n",
      "Gradient Descent(5388/9999): loss=1.7017853233160432, w0=73.575, w1=13.361612133897717\n",
      "Gradient Descent(5389/9999): loss=2.143805526661324, w0=73.575, w1=13.260784317222965\n",
      "Gradient Descent(5390/9999): loss=2.0245925178418136, w0=73.51875, w1=13.292913206156728\n",
      "Gradient Descent(5391/9999): loss=2.3270918880080687, w0=73.63125, w1=13.179469378920803\n",
      "Gradient Descent(5392/9999): loss=2.121272720283301, w0=73.6875, w1=13.348592453770895\n",
      "Gradient Descent(5393/9999): loss=2.566200174223626, w0=73.63125, w1=13.434101263700763\n",
      "Gradient Descent(5394/9999): loss=2.4854651423654968, w0=73.51875, w1=13.254783098123157\n",
      "Gradient Descent(5395/9999): loss=2.3262400056730996, w0=73.40625, w1=13.575194680468435\n",
      "Gradient Descent(5396/9999): loss=1.8462881250938787, w0=73.18125, w1=13.704586641703633\n",
      "Gradient Descent(5397/9999): loss=2.306335024547133, w0=73.29375, w1=13.628519717208633\n",
      "Gradient Descent(5398/9999): loss=2.1643545800043675, w0=73.29375, w1=13.335456466755172\n",
      "Gradient Descent(5399/9999): loss=2.066983616203856, w0=73.29375, w1=13.362864747497916\n",
      "Gradient Descent(5400/9999): loss=2.1860733647027706, w0=73.29375, w1=13.506137223594509\n",
      "Gradient Descent(5401/9999): loss=2.6003565245252007, w0=73.18125, w1=13.688065167999262\n",
      "Gradient Descent(5402/9999): loss=2.097488035056789, w0=73.23750000000001, w1=13.549735373003456\n",
      "Gradient Descent(5403/9999): loss=2.085367391522905, w0=73.12500000000001, w1=13.582505220825146\n",
      "Gradient Descent(5404/9999): loss=2.361884650032021, w0=72.90000000000002, w1=13.928462643786881\n",
      "Gradient Descent(5405/9999): loss=2.4016639643814184, w0=72.90000000000002, w1=14.122780296033325\n",
      "Gradient Descent(5406/9999): loss=2.1529656295458928, w0=72.90000000000002, w1=14.21898465138686\n",
      "Gradient Descent(5407/9999): loss=2.3442108876900685, w0=72.67500000000003, w1=14.17351693381489\n",
      "Gradient Descent(5408/9999): loss=2.521975799943438, w0=72.90000000000002, w1=14.294527873225972\n",
      "Gradient Descent(5409/9999): loss=1.6360642046512932, w0=72.56250000000001, w1=14.291542326167852\n",
      "Gradient Descent(5410/9999): loss=1.9904221820902301, w0=72.67500000000001, w1=14.017568436073756\n",
      "Gradient Descent(5411/9999): loss=2.147166355129616, w0=72.67500000000001, w1=13.58702808069825\n",
      "Gradient Descent(5412/9999): loss=2.5823020682318027, w0=72.3375, w1=13.584915178380784\n",
      "Gradient Descent(5413/9999): loss=2.4201988139724517, w0=72.45, w1=13.688264589889675\n",
      "Gradient Descent(5414/9999): loss=2.266471689375937, w0=72.73125, w1=13.484211253362863\n",
      "Gradient Descent(5415/9999): loss=2.1330797327457045, w0=72.675, w1=13.540772333690862\n",
      "Gradient Descent(5416/9999): loss=1.5033300044687712, w0=72.61874999999999, w1=13.374956197636461\n",
      "Gradient Descent(5417/9999): loss=2.4207898129260403, w0=72.84374999999999, w1=13.64692625085967\n",
      "Gradient Descent(5418/9999): loss=2.2420678920083823, w0=72.78749999999998, w1=13.772457846791543\n",
      "Gradient Descent(5419/9999): loss=2.342655390654201, w0=72.84374999999999, w1=13.828906137946769\n",
      "Gradient Descent(5420/9999): loss=1.970457218381033, w0=72.67499999999998, w1=13.871683164820803\n",
      "Gradient Descent(5421/9999): loss=2.1581883005366773, w0=72.73124999999999, w1=13.78999907258378\n",
      "Gradient Descent(5422/9999): loss=2.0236387038164745, w0=72.84374999999999, w1=13.571736705595791\n",
      "Gradient Descent(5423/9999): loss=2.2024330244943453, w0=73.18124999999999, w1=13.577197198320958\n",
      "Gradient Descent(5424/9999): loss=2.096417745683703, w0=73.2375, w1=13.838334453394841\n",
      "Gradient Descent(5425/9999): loss=1.9708678677780256, w0=73.06875, w1=13.906604283047633\n",
      "Gradient Descent(5426/9999): loss=2.323039234727524, w0=73.06875, w1=13.660908249465834\n",
      "Gradient Descent(5427/9999): loss=1.9770909305014894, w0=73.18124999999999, w1=13.707605163456957\n",
      "Gradient Descent(5428/9999): loss=2.5643890360553603, w0=73.35, w1=13.505770010809954\n",
      "Gradient Descent(5429/9999): loss=2.8236990598965095, w0=73.29374999999999, w1=13.421601263213214\n",
      "Gradient Descent(5430/9999): loss=1.8492618298338044, w0=73.12499999999999, w1=13.306447977917635\n",
      "Gradient Descent(5431/9999): loss=2.0668374490063472, w0=72.89999999999999, w1=13.469978203731337\n",
      "Gradient Descent(5432/9999): loss=2.2605071265650016, w0=73.18124999999999, w1=13.070053567471955\n",
      "Gradient Descent(5433/9999): loss=2.3511055687205946, w0=73.29374999999999, w1=13.090558246052419\n",
      "Gradient Descent(5434/9999): loss=2.6836988442901393, w0=73.63125, w1=13.120851478651455\n",
      "Gradient Descent(5435/9999): loss=2.2090808836795146, w0=73.63125, w1=12.99317842158349\n",
      "Gradient Descent(5436/9999): loss=2.345812270538961, w0=73.85624999999999, w1=12.991497853046296\n",
      "Gradient Descent(5437/9999): loss=2.1160229971096736, w0=73.85624999999999, w1=12.935120529153554\n",
      "Gradient Descent(5438/9999): loss=2.0495152131784176, w0=73.63125, w1=13.075305433144193\n",
      "Gradient Descent(5439/9999): loss=2.3910481808630015, w0=73.63125, w1=13.086427654290224\n",
      "Gradient Descent(5440/9999): loss=2.4249336254541376, w0=73.63125, w1=13.267230493543845\n",
      "Gradient Descent(5441/9999): loss=2.097840076484138, w0=73.57499999999999, w1=13.061479382044972\n",
      "Gradient Descent(5442/9999): loss=1.8157496667945405, w0=73.29374999999999, w1=13.087406950517481\n",
      "Gradient Descent(5443/9999): loss=1.3813181305370614, w0=73.23749999999998, w1=13.475217982910745\n",
      "Gradient Descent(5444/9999): loss=2.6619587933638127, w0=73.34999999999998, w1=13.376701491576178\n",
      "Gradient Descent(5445/9999): loss=2.2726215230450832, w0=73.46249999999998, w1=13.497792274300595\n",
      "Gradient Descent(5446/9999): loss=2.333160225995929, w0=73.74374999999998, w1=13.696974034756453\n",
      "Gradient Descent(5447/9999): loss=2.3240115751056676, w0=73.91249999999998, w1=13.836188112787065\n",
      "Gradient Descent(5448/9999): loss=2.372895776444712, w0=73.74374999999998, w1=13.675771555734286\n",
      "Gradient Descent(5449/9999): loss=1.9301171349567587, w0=73.74374999999998, w1=13.856193416234694\n",
      "Gradient Descent(5450/9999): loss=1.784757046922875, w0=73.74374999999998, w1=13.983992098370127\n",
      "Gradient Descent(5451/9999): loss=1.9461760678862519, w0=73.79999999999998, w1=14.124625035254809\n",
      "Gradient Descent(5452/9999): loss=1.9743729517355257, w0=73.51874999999998, w1=13.99664490303123\n",
      "Gradient Descent(5453/9999): loss=2.0272386320062674, w0=73.51874999999998, w1=13.964067579269416\n",
      "Gradient Descent(5454/9999): loss=2.608990033180632, w0=73.63124999999998, w1=13.87936477758754\n",
      "Gradient Descent(5455/9999): loss=2.4029517713992816, w0=73.51874999999998, w1=13.815767126697365\n",
      "Gradient Descent(5456/9999): loss=1.752575669027026, w0=73.46249999999998, w1=13.910621421704123\n",
      "Gradient Descent(5457/9999): loss=2.7417324251538755, w0=73.57499999999997, w1=13.669147716024856\n",
      "Gradient Descent(5458/9999): loss=2.3971106894047196, w0=73.51874999999997, w1=13.846325935042639\n",
      "Gradient Descent(5459/9999): loss=2.829048648960515, w0=73.51874999999997, w1=13.636876107215\n",
      "Gradient Descent(5460/9999): loss=2.240625986723287, w0=73.79999999999997, w1=13.558455394075528\n",
      "Gradient Descent(5461/9999): loss=2.2821481247301634, w0=73.63124999999997, w1=13.566447631826225\n",
      "Gradient Descent(5462/9999): loss=2.078010475537294, w0=73.40624999999997, w1=13.602202110870948\n",
      "Gradient Descent(5463/9999): loss=2.2805137723720783, w0=73.40624999999997, w1=13.536402957575262\n",
      "Gradient Descent(5464/9999): loss=2.398828184728136, w0=73.34999999999997, w1=13.719473405229683\n",
      "Gradient Descent(5465/9999): loss=2.2025122346714063, w0=73.29374999999996, w1=13.690008836867252\n",
      "Gradient Descent(5466/9999): loss=1.894420824307846, w0=73.40624999999996, w1=13.934341301075904\n",
      "Gradient Descent(5467/9999): loss=2.415384620840044, w0=73.34999999999995, w1=13.785120583268121\n",
      "Gradient Descent(5468/9999): loss=2.515381430391866, w0=73.29374999999995, w1=13.809997029881112\n",
      "Gradient Descent(5469/9999): loss=1.6703847422416462, w0=73.23749999999994, w1=13.838591810451618\n",
      "Gradient Descent(5470/9999): loss=2.2801495298782193, w0=73.40624999999994, w1=14.037956722140162\n",
      "Gradient Descent(5471/9999): loss=2.2256479481856584, w0=73.74374999999995, w1=13.992740983300516\n",
      "Gradient Descent(5472/9999): loss=2.243880027866488, w0=73.63124999999995, w1=14.196981811582326\n",
      "Gradient Descent(5473/9999): loss=2.2953355211586404, w0=73.40624999999996, w1=14.190118198767978\n",
      "Gradient Descent(5474/9999): loss=2.363326781927946, w0=73.51874999999995, w1=14.371356707225182\n",
      "Gradient Descent(5475/9999): loss=2.2627110521948284, w0=73.46249999999995, w1=13.998723169297982\n",
      "Gradient Descent(5476/9999): loss=2.1725634730303853, w0=73.34999999999995, w1=13.968330484813746\n",
      "Gradient Descent(5477/9999): loss=2.079328682930481, w0=73.34999999999995, w1=14.003768841492402\n",
      "Gradient Descent(5478/9999): loss=2.408782566084285, w0=73.46249999999995, w1=13.705726567329933\n",
      "Gradient Descent(5479/9999): loss=2.518869872341651, w0=73.40624999999994, w1=13.729021600719966\n",
      "Gradient Descent(5480/9999): loss=2.30178798132135, w0=73.29374999999995, w1=13.733849943359235\n",
      "Gradient Descent(5481/9999): loss=2.2394900839055176, w0=73.29374999999995, w1=13.694095033162505\n",
      "Gradient Descent(5482/9999): loss=2.201470510775448, w0=73.23749999999994, w1=13.7852862157005\n",
      "Gradient Descent(5483/9999): loss=2.06080370187368, w0=73.34999999999994, w1=14.020307514667962\n",
      "Gradient Descent(5484/9999): loss=2.404474779821106, w0=73.68749999999994, w1=14.181035273523744\n",
      "Gradient Descent(5485/9999): loss=2.708781937373848, w0=73.51874999999994, w1=14.384546955675487\n",
      "Gradient Descent(5486/9999): loss=1.765456817724832, w0=73.79999999999994, w1=14.322680833654244\n",
      "Gradient Descent(5487/9999): loss=2.319632034329078, w0=73.63124999999994, w1=14.179584329907287\n",
      "Gradient Descent(5488/9999): loss=1.5076113757775695, w0=73.74374999999993, w1=14.142497958783979\n",
      "Gradient Descent(5489/9999): loss=2.5128454337520605, w0=73.91249999999994, w1=14.127358695080869\n",
      "Gradient Descent(5490/9999): loss=2.436514984796179, w0=73.51874999999994, w1=14.040960471769388\n",
      "Gradient Descent(5491/9999): loss=2.0463965910539326, w0=73.51874999999994, w1=13.881310386705042\n",
      "Gradient Descent(5492/9999): loss=2.131293087090502, w0=73.40624999999994, w1=13.736577019114167\n",
      "Gradient Descent(5493/9999): loss=2.78885614751983, w0=73.46249999999995, w1=13.627240112256604\n",
      "Gradient Descent(5494/9999): loss=2.6265203170262508, w0=73.40624999999994, w1=13.516716339339904\n",
      "Gradient Descent(5495/9999): loss=2.157973442492205, w0=73.40624999999994, w1=13.5577276408105\n",
      "Gradient Descent(5496/9999): loss=2.0126727446744757, w0=73.40624999999994, w1=13.451404281147637\n",
      "Gradient Descent(5497/9999): loss=1.8436661060476203, w0=73.34999999999994, w1=13.336234880257557\n",
      "Gradient Descent(5498/9999): loss=1.4647724847371424, w0=73.29374999999993, w1=13.384988042059836\n",
      "Gradient Descent(5499/9999): loss=1.9310076443378148, w0=73.40624999999993, w1=13.786795933756697\n",
      "Gradient Descent(5500/9999): loss=1.9652930503170405, w0=73.18124999999993, w1=13.850408122423193\n",
      "Gradient Descent(5501/9999): loss=2.164849296066987, w0=73.01249999999993, w1=13.943242570860352\n",
      "Gradient Descent(5502/9999): loss=2.3121194286906936, w0=73.12499999999993, w1=14.052421268679236\n",
      "Gradient Descent(5503/9999): loss=1.9989858149086346, w0=73.23749999999993, w1=13.960909668322923\n",
      "Gradient Descent(5504/9999): loss=2.113927601212148, w0=73.12499999999993, w1=13.927196677945508\n",
      "Gradient Descent(5505/9999): loss=2.4269427066977025, w0=73.23749999999993, w1=14.06569295617102\n",
      "Gradient Descent(5506/9999): loss=2.0691312219702773, w0=73.29374999999993, w1=14.138104143450747\n",
      "Gradient Descent(5507/9999): loss=2.1703335442963363, w0=73.40624999999993, w1=13.97985038651956\n",
      "Gradient Descent(5508/9999): loss=2.7113168278859616, w0=73.40624999999993, w1=13.772701477518728\n",
      "Gradient Descent(5509/9999): loss=2.300513280122148, w0=73.34999999999992, w1=13.736452061191509\n",
      "Gradient Descent(5510/9999): loss=2.3313178856243297, w0=73.40624999999993, w1=13.672983856451332\n",
      "Gradient Descent(5511/9999): loss=1.7940534285794183, w0=73.40624999999993, w1=13.64088607517431\n",
      "Gradient Descent(5512/9999): loss=1.9673978173962867, w0=73.34999999999992, w1=13.516677487776393\n",
      "Gradient Descent(5513/9999): loss=1.727040457514408, w0=73.46249999999992, w1=13.386777737830728\n",
      "Gradient Descent(5514/9999): loss=2.307945288256592, w0=73.74374999999992, w1=13.530893460865583\n",
      "Gradient Descent(5515/9999): loss=2.1647661807822343, w0=73.74374999999992, w1=13.647748602365047\n",
      "Gradient Descent(5516/9999): loss=2.412567494232577, w0=73.91249999999992, w1=13.42910766728501\n",
      "Gradient Descent(5517/9999): loss=2.310028179369298, w0=73.63124999999992, w1=13.423443655725224\n",
      "Gradient Descent(5518/9999): loss=2.274914386593121, w0=73.57499999999992, w1=13.448932768322663\n",
      "Gradient Descent(5519/9999): loss=1.9535084030443723, w0=73.63124999999992, w1=13.444583403294505\n",
      "Gradient Descent(5520/9999): loss=2.0815064739329068, w0=73.96874999999993, w1=13.429609150278518\n",
      "Gradient Descent(5521/9999): loss=1.8600881658998278, w0=73.91249999999992, w1=13.240964835627382\n",
      "Gradient Descent(5522/9999): loss=2.0460655336859084, w0=73.85624999999992, w1=13.379702160425731\n",
      "Gradient Descent(5523/9999): loss=2.7022599965594996, w0=73.74374999999992, w1=13.189530735254928\n",
      "Gradient Descent(5524/9999): loss=1.3441853387140958, w0=73.63124999999992, w1=13.378445779111958\n",
      "Gradient Descent(5525/9999): loss=1.7071916783226933, w0=73.57499999999992, w1=13.401248694854548\n",
      "Gradient Descent(5526/9999): loss=2.3086804215248495, w0=73.51874999999991, w1=13.304756688357585\n",
      "Gradient Descent(5527/9999): loss=1.7830318802727623, w0=73.63124999999991, w1=13.47468371153966\n",
      "Gradient Descent(5528/9999): loss=2.003835275959632, w0=73.5749999999999, w1=13.43643645205892\n",
      "Gradient Descent(5529/9999): loss=2.2526226661929005, w0=73.4062499999999, w1=13.394214232362373\n",
      "Gradient Descent(5530/9999): loss=2.607967928064293, w0=73.4624999999999, w1=13.198643092221218\n",
      "Gradient Descent(5531/9999): loss=1.8623970448411766, w0=73.2937499999999, w1=12.977187438090391\n",
      "Gradient Descent(5532/9999): loss=2.112553401894443, w0=73.4062499999999, w1=13.22433577830714\n",
      "Gradient Descent(5533/9999): loss=2.14981602757286, w0=73.4624999999999, w1=13.403961944472698\n",
      "Gradient Descent(5534/9999): loss=1.9487538326626113, w0=73.63124999999991, w1=13.429237971045021\n",
      "Gradient Descent(5535/9999): loss=1.9199004985414396, w0=73.40624999999991, w1=13.354554709273758\n",
      "Gradient Descent(5536/9999): loss=2.530192539609114, w0=73.63124999999991, w1=13.462052305734751\n",
      "Gradient Descent(5537/9999): loss=2.8238797261821915, w0=73.51874999999991, w1=13.391985470626798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5538/9999): loss=1.750230709882211, w0=73.68749999999991, w1=13.675320305453042\n",
      "Gradient Descent(5539/9999): loss=2.0095543967820997, w0=73.79999999999991, w1=13.854556197574613\n",
      "Gradient Descent(5540/9999): loss=2.0432913958223073, w0=73.79999999999991, w1=14.00849733901849\n",
      "Gradient Descent(5541/9999): loss=2.0144978483967027, w0=73.85624999999992, w1=13.713344847139707\n",
      "Gradient Descent(5542/9999): loss=2.261019065635207, w0=73.79999999999991, w1=13.963683824622303\n",
      "Gradient Descent(5543/9999): loss=2.2364302528051674, w0=73.79999999999991, w1=13.94857867855674\n",
      "Gradient Descent(5544/9999): loss=2.498543834720693, w0=73.91249999999991, w1=13.99720860397145\n",
      "Gradient Descent(5545/9999): loss=2.075984413102705, w0=74.0249999999999, w1=13.882684290688971\n",
      "Gradient Descent(5546/9999): loss=1.9468690801601087, w0=74.0249999999999, w1=13.784267138425788\n",
      "Gradient Descent(5547/9999): loss=2.3528416604628015, w0=73.9687499999999, w1=13.461997226074155\n",
      "Gradient Descent(5548/9999): loss=2.1134819310157598, w0=73.9124999999999, w1=13.530156182644154\n",
      "Gradient Descent(5549/9999): loss=2.085890867280531, w0=73.85624999999989, w1=13.611383904396412\n",
      "Gradient Descent(5550/9999): loss=2.3033671584333706, w0=73.57499999999989, w1=13.649756900479305\n",
      "Gradient Descent(5551/9999): loss=2.391364837530542, w0=73.46249999999989, w1=13.795122346528688\n",
      "Gradient Descent(5552/9999): loss=1.8471259382731415, w0=73.5187499999999, w1=13.886504192453549\n",
      "Gradient Descent(5553/9999): loss=2.4284963926131216, w0=73.2937499999999, w1=13.988519289952887\n",
      "Gradient Descent(5554/9999): loss=2.2607793741452875, w0=73.1812499999999, w1=13.842704588536442\n",
      "Gradient Descent(5555/9999): loss=2.199961270742681, w0=73.1249999999999, w1=13.908455689278338\n",
      "Gradient Descent(5556/9999): loss=2.2376406210645507, w0=73.2374999999999, w1=14.126644734120873\n",
      "Gradient Descent(5557/9999): loss=2.3942889288129585, w0=73.3499999999999, w1=14.006333648921027\n",
      "Gradient Descent(5558/9999): loss=2.296346338630963, w0=73.4062499999999, w1=14.121291458880053\n",
      "Gradient Descent(5559/9999): loss=2.0176006177756785, w0=73.2937499999999, w1=14.102817859839828\n",
      "Gradient Descent(5560/9999): loss=2.928223435523904, w0=73.2937499999999, w1=14.016977011471361\n",
      "Gradient Descent(5561/9999): loss=1.981759743519738, w0=73.1249999999999, w1=14.099702367091535\n",
      "Gradient Descent(5562/9999): loss=1.851229828638516, w0=73.2374999999999, w1=13.926475616078289\n",
      "Gradient Descent(5563/9999): loss=2.510082433924895, w0=73.74374999999989, w1=13.831011342407665\n",
      "Gradient Descent(5564/9999): loss=2.7867140151303182, w0=73.85624999999989, w1=13.880544621447717\n",
      "Gradient Descent(5565/9999): loss=1.9907137909407522, w0=73.96874999999989, w1=13.837279478364172\n",
      "Gradient Descent(5566/9999): loss=2.4335735488984316, w0=73.96874999999989, w1=13.86831056923231\n",
      "Gradient Descent(5567/9999): loss=2.0383140634938552, w0=73.74374999999989, w1=13.73195266130584\n",
      "Gradient Descent(5568/9999): loss=2.1094346776971937, w0=73.9124999999999, w1=13.84746769000878\n",
      "Gradient Descent(5569/9999): loss=2.5544243212900155, w0=73.85624999999989, w1=13.744304557247508\n",
      "Gradient Descent(5570/9999): loss=1.713760695093751, w0=73.96874999999989, w1=13.75808660274089\n",
      "Gradient Descent(5571/9999): loss=2.5457403972326382, w0=73.68749999999989, w1=13.588365759336801\n",
      "Gradient Descent(5572/9999): loss=2.400746794633667, w0=73.34999999999988, w1=13.721859443230903\n",
      "Gradient Descent(5573/9999): loss=1.881654506382844, w0=73.57499999999987, w1=13.922631256994324\n",
      "Gradient Descent(5574/9999): loss=1.7177962683577723, w0=73.46249999999988, w1=13.771752768896247\n",
      "Gradient Descent(5575/9999): loss=2.1981637191188304, w0=73.57499999999987, w1=13.528650979824697\n",
      "Gradient Descent(5576/9999): loss=2.1555461240594354, w0=73.51874999999987, w1=13.724406450504722\n",
      "Gradient Descent(5577/9999): loss=2.265201452560895, w0=73.57499999999987, w1=13.71775558187763\n",
      "Gradient Descent(5578/9999): loss=2.618337544885599, w0=73.57499999999987, w1=13.63574170683348\n",
      "Gradient Descent(5579/9999): loss=2.979468824939371, w0=73.34999999999988, w1=13.706843137523647\n",
      "Gradient Descent(5580/9999): loss=1.8663832590139782, w0=73.18124999999988, w1=13.994545756065367\n",
      "Gradient Descent(5581/9999): loss=2.2637060686998876, w0=72.78749999999988, w1=13.940648317133514\n",
      "Gradient Descent(5582/9999): loss=2.011390208500377, w0=72.84374999999989, w1=13.91876821422148\n",
      "Gradient Descent(5583/9999): loss=1.9446578668861338, w0=72.84374999999989, w1=13.709219349152692\n",
      "Gradient Descent(5584/9999): loss=2.1300722062568833, w0=72.84374999999989, w1=13.877498222115555\n",
      "Gradient Descent(5585/9999): loss=1.5592831806036052, w0=72.67499999999988, w1=14.036908076969663\n",
      "Gradient Descent(5586/9999): loss=1.8255163302112043, w0=72.84374999999989, w1=14.058733338415207\n",
      "Gradient Descent(5587/9999): loss=2.143922904886286, w0=72.67499999999988, w1=14.123150853268395\n",
      "Gradient Descent(5588/9999): loss=2.642547190537636, w0=72.84374999999989, w1=14.061602307424357\n",
      "Gradient Descent(5589/9999): loss=2.358318983633299, w0=73.29374999999989, w1=14.105202912702527\n",
      "Gradient Descent(5590/9999): loss=2.449879449392893, w0=73.3499999999999, w1=13.85521437064543\n",
      "Gradient Descent(5591/9999): loss=2.2823029169144275, w0=73.57499999999989, w1=13.734890019059149\n",
      "Gradient Descent(5592/9999): loss=1.9932369223062245, w0=73.6312499999999, w1=13.653439259707312\n",
      "Gradient Descent(5593/9999): loss=1.9412644035050073, w0=73.4062499999999, w1=13.612585004429857\n",
      "Gradient Descent(5594/9999): loss=1.6208370349644299, w0=73.8562499999999, w1=13.377279462758437\n",
      "Gradient Descent(5595/9999): loss=2.5502765586195677, w0=73.7999999999999, w1=13.627388743938177\n",
      "Gradient Descent(5596/9999): loss=2.415122730308936, w0=73.9124999999999, w1=13.660498758078058\n",
      "Gradient Descent(5597/9999): loss=2.5247284567546653, w0=73.85624999999989, w1=13.664719545978576\n",
      "Gradient Descent(5598/9999): loss=2.2077598096518134, w0=73.96874999999989, w1=13.686570154184349\n",
      "Gradient Descent(5599/9999): loss=2.3166995794160936, w0=74.13749999999989, w1=13.756102319921723\n",
      "Gradient Descent(5600/9999): loss=1.995200426096707, w0=73.9124999999999, w1=14.036532204398041\n",
      "Gradient Descent(5601/9999): loss=1.9894963333599183, w0=73.74374999999989, w1=14.07064891931045\n",
      "Gradient Descent(5602/9999): loss=2.1348866157653896, w0=73.46249999999989, w1=14.06929338093745\n",
      "Gradient Descent(5603/9999): loss=2.5912428414771775, w0=73.5187499999999, w1=13.879125219443978\n",
      "Gradient Descent(5604/9999): loss=2.709445126170013, w0=73.74374999999989, w1=13.807856387768222\n",
      "Gradient Descent(5605/9999): loss=2.236158388701371, w0=73.7999999999999, w1=13.68314811352876\n",
      "Gradient Descent(5606/9999): loss=2.9283399176068787, w0=73.9124999999999, w1=13.906467686236235\n",
      "Gradient Descent(5607/9999): loss=1.85392327364386, w0=73.74374999999989, w1=14.071822599206198\n",
      "Gradient Descent(5608/9999): loss=2.2321359086675217, w0=73.68749999999989, w1=13.7281817265355\n",
      "Gradient Descent(5609/9999): loss=2.2242872251129695, w0=73.57499999999989, w1=13.672515175229272\n",
      "Gradient Descent(5610/9999): loss=2.237343503421081, w0=73.57499999999989, w1=13.550232568542556\n",
      "Gradient Descent(5611/9999): loss=1.9772514645572576, w0=73.29374999999989, w1=13.622144618632735\n",
      "Gradient Descent(5612/9999): loss=2.3989967717361003, w0=73.23749999999988, w1=13.325849452138144\n",
      "Gradient Descent(5613/9999): loss=1.9354408910025218, w0=73.29374999999989, w1=13.344781519700922\n",
      "Gradient Descent(5614/9999): loss=2.087665554476845, w0=73.18124999999989, w1=13.346286385709373\n",
      "Gradient Descent(5615/9999): loss=1.950393704111345, w0=73.3499999999999, w1=13.420596911898478\n",
      "Gradient Descent(5616/9999): loss=2.089487965845113, w0=73.18124999999989, w1=13.21454114603598\n",
      "Gradient Descent(5617/9999): loss=2.1239060091658715, w0=73.57499999999989, w1=13.373782130964802\n",
      "Gradient Descent(5618/9999): loss=1.8876760051859405, w0=73.6312499999999, w1=13.276836480445079\n",
      "Gradient Descent(5619/9999): loss=2.61184336321656, w0=73.7999999999999, w1=13.151214826619693\n",
      "Gradient Descent(5620/9999): loss=2.40945101439298, w0=73.7999999999999, w1=13.134608800407355\n",
      "Gradient Descent(5621/9999): loss=2.047804009287696, w0=73.9124999999999, w1=13.580512451281441\n",
      "Gradient Descent(5622/9999): loss=2.5797885516855903, w0=73.6874999999999, w1=13.849848499464697\n",
      "Gradient Descent(5623/9999): loss=2.197648165985076, w0=73.8562499999999, w1=13.973364442646028\n",
      "Gradient Descent(5624/9999): loss=2.224382306949413, w0=73.5187499999999, w1=13.862091511605424\n",
      "Gradient Descent(5625/9999): loss=2.139747914141318, w0=73.5749999999999, w1=13.917370368472197\n",
      "Gradient Descent(5626/9999): loss=2.7503851202713574, w0=73.91249999999991, w1=13.764834226104655\n",
      "Gradient Descent(5627/9999): loss=1.8604273311728345, w0=73.68749999999991, w1=13.888698159698896\n",
      "Gradient Descent(5628/9999): loss=1.9342569814298598, w0=73.68749999999991, w1=13.779071798874455\n",
      "Gradient Descent(5629/9999): loss=2.363713446718756, w0=73.68749999999991, w1=13.909309426836556\n",
      "Gradient Descent(5630/9999): loss=2.064955961154789, w0=73.74374999999992, w1=13.748816607639068\n",
      "Gradient Descent(5631/9999): loss=2.2114329531468426, w0=73.46249999999992, w1=13.856030611524382\n",
      "Gradient Descent(5632/9999): loss=1.945542323582525, w0=73.57499999999992, w1=13.729718199368252\n",
      "Gradient Descent(5633/9999): loss=2.3774355241342704, w0=73.06874999999992, w1=13.868513098698072\n",
      "Gradient Descent(5634/9999): loss=1.9145224422154867, w0=72.73124999999992, w1=13.885762807367172\n",
      "Gradient Descent(5635/9999): loss=2.213671636926944, w0=72.67499999999991, w1=13.915863588934142\n",
      "Gradient Descent(5636/9999): loss=1.811993131399335, w0=72.6187499999999, w1=13.641662798852037\n",
      "Gradient Descent(5637/9999): loss=2.728918979527824, w0=72.7312499999999, w1=13.671960825159479\n",
      "Gradient Descent(5638/9999): loss=2.0366863642783795, w0=72.9562499999999, w1=13.866590909742703\n",
      "Gradient Descent(5639/9999): loss=1.8152565263839444, w0=72.7874999999999, w1=13.682640730129664\n",
      "Gradient Descent(5640/9999): loss=1.856059743232879, w0=72.8437499999999, w1=13.575359274093055\n",
      "Gradient Descent(5641/9999): loss=1.6681705817695671, w0=72.7874999999999, w1=13.394162828896885\n",
      "Gradient Descent(5642/9999): loss=2.0771529551937267, w0=72.7874999999999, w1=13.628174318042817\n",
      "Gradient Descent(5643/9999): loss=2.772724793346872, w0=72.7874999999999, w1=13.688717544666769\n",
      "Gradient Descent(5644/9999): loss=1.648373659748962, w0=72.9562499999999, w1=13.650354226054931\n",
      "Gradient Descent(5645/9999): loss=2.3743353406742833, w0=73.1249999999999, w1=13.511121114871953\n",
      "Gradient Descent(5646/9999): loss=1.9889856809219575, w0=73.5187499999999, w1=13.857557124188366\n",
      "Gradient Descent(5647/9999): loss=2.0769993475861224, w0=73.46249999999989, w1=13.729157072571931\n",
      "Gradient Descent(5648/9999): loss=1.7607223245076402, w0=73.40624999999989, w1=13.63161473537671\n",
      "Gradient Descent(5649/9999): loss=2.558085621340406, w0=73.46249999999989, w1=13.693772516844673\n",
      "Gradient Descent(5650/9999): loss=2.165514825748156, w0=73.2374999999999, w1=13.75455843239982\n",
      "Gradient Descent(5651/9999): loss=2.1500884212816294, w0=73.3499999999999, w1=13.676195038673091\n",
      "Gradient Descent(5652/9999): loss=1.8609738960591315, w0=73.2374999999999, w1=13.64329328607239\n",
      "Gradient Descent(5653/9999): loss=2.586736108882521, w0=73.0124999999999, w1=13.923910791587337\n",
      "Gradient Descent(5654/9999): loss=2.1728970800915626, w0=73.06874999999991, w1=13.813254089299235\n",
      "Gradient Descent(5655/9999): loss=2.2628153786213017, w0=72.8999999999999, w1=13.411894566135878\n",
      "Gradient Descent(5656/9999): loss=2.6659274738146257, w0=73.0124999999999, w1=13.379243665407161\n",
      "Gradient Descent(5657/9999): loss=2.340486078738266, w0=73.2374999999999, w1=13.459133896246115\n",
      "Gradient Descent(5658/9999): loss=1.7202356482844023, w0=73.2937499999999, w1=13.572201522854874\n",
      "Gradient Descent(5659/9999): loss=2.0916286846201237, w0=73.2374999999999, w1=13.528248893417725\n",
      "Gradient Descent(5660/9999): loss=2.1164937463183886, w0=73.0124999999999, w1=13.560184251875551\n",
      "Gradient Descent(5661/9999): loss=1.9133979886831038, w0=72.9562499999999, w1=13.4896324298798\n",
      "Gradient Descent(5662/9999): loss=2.369319716480245, w0=73.0124999999999, w1=13.457246543447495\n",
      "Gradient Descent(5663/9999): loss=2.69814309770482, w0=72.8999999999999, w1=13.631109421940632\n",
      "Gradient Descent(5664/9999): loss=2.4076300818395824, w0=73.0124999999999, w1=13.466299260310779\n",
      "Gradient Descent(5665/9999): loss=2.143238153674627, w0=73.0124999999999, w1=13.199383341402363\n",
      "Gradient Descent(5666/9999): loss=2.337507481115557, w0=73.1249999999999, w1=13.049988031252393\n",
      "Gradient Descent(5667/9999): loss=2.4605529761693554, w0=73.0124999999999, w1=13.06785635976135\n",
      "Gradient Descent(5668/9999): loss=2.5113062529794083, w0=73.2374999999999, w1=13.193123838881997\n",
      "Gradient Descent(5669/9999): loss=2.3277379074400635, w0=73.2937499999999, w1=13.17210183487705\n",
      "Gradient Descent(5670/9999): loss=2.3331467748503454, w0=73.4624999999999, w1=13.111412337935588\n",
      "Gradient Descent(5671/9999): loss=2.254372236545266, w0=73.5749999999999, w1=13.067674598620911\n",
      "Gradient Descent(5672/9999): loss=2.1206111672474117, w0=73.34999999999991, w1=13.611868980720864\n",
      "Gradient Descent(5673/9999): loss=2.088529072329285, w0=73.34999999999991, w1=13.549241951579768\n",
      "Gradient Descent(5674/9999): loss=2.323570231310506, w0=73.06874999999991, w1=13.43288704729736\n",
      "Gradient Descent(5675/9999): loss=1.9582060914448955, w0=73.06874999999991, w1=13.274713611100841\n",
      "Gradient Descent(5676/9999): loss=2.0814857569712775, w0=73.0124999999999, w1=13.383822797648477\n",
      "Gradient Descent(5677/9999): loss=2.6732215042067575, w0=73.0124999999999, w1=13.278908751975083\n",
      "Gradient Descent(5678/9999): loss=1.9877767773653698, w0=72.9562499999999, w1=13.332830849303212\n",
      "Gradient Descent(5679/9999): loss=2.1804621091923697, w0=73.0124999999999, w1=13.518390707182256\n",
      "Gradient Descent(5680/9999): loss=3.053023232320668, w0=73.1812499999999, w1=13.310115273020354\n",
      "Gradient Descent(5681/9999): loss=2.6398133299795337, w0=73.2937499999999, w1=13.53444391577387\n",
      "Gradient Descent(5682/9999): loss=1.5766595783882438, w0=73.2937499999999, w1=13.593511062207464\n",
      "Gradient Descent(5683/9999): loss=2.2650763216438827, w0=73.4624999999999, w1=13.68804073755045\n",
      "Gradient Descent(5684/9999): loss=2.1089080891651086, w0=73.4062499999999, w1=13.551380776402494\n",
      "Gradient Descent(5685/9999): loss=2.2425017851595106, w0=73.5187499999999, w1=13.544914212627672\n",
      "Gradient Descent(5686/9999): loss=2.0872120007043806, w0=73.6312499999999, w1=13.674368862227933\n",
      "Gradient Descent(5687/9999): loss=2.1389851703856966, w0=73.74374999999989, w1=13.876753943081633\n",
      "Gradient Descent(5688/9999): loss=2.637363979341819, w0=73.40624999999989, w1=13.721286043724966\n",
      "Gradient Descent(5689/9999): loss=1.71862004359586, w0=73.12499999999989, w1=13.785188213127329\n",
      "Gradient Descent(5690/9999): loss=2.1295121921489057, w0=73.23749999999988, w1=13.939143764655016\n",
      "Gradient Descent(5691/9999): loss=2.306882931391221, w0=73.40624999999989, w1=13.637204735462568\n",
      "Gradient Descent(5692/9999): loss=2.592808820610437, w0=73.34999999999988, w1=13.604240414333034\n",
      "Gradient Descent(5693/9999): loss=2.108920340786418, w0=73.29374999999987, w1=13.601759206525836\n",
      "Gradient Descent(5694/9999): loss=1.9152852119175232, w0=73.06874999999988, w1=13.400884856471285\n",
      "Gradient Descent(5695/9999): loss=2.1636532266657458, w0=73.23749999999988, w1=13.289505178564252\n",
      "Gradient Descent(5696/9999): loss=2.1278667055499465, w0=73.34999999999988, w1=13.418628450534014\n",
      "Gradient Descent(5697/9999): loss=2.2837754277103537, w0=73.46249999999988, w1=13.236204007921677\n",
      "Gradient Descent(5698/9999): loss=1.7711382345114148, w0=73.57499999999987, w1=13.349474824925899\n",
      "Gradient Descent(5699/9999): loss=2.3958848036476184, w0=73.51874999999987, w1=13.300536277798203\n",
      "Gradient Descent(5700/9999): loss=2.5147951058513947, w0=73.68749999999987, w1=13.117223050090038\n",
      "Gradient Descent(5701/9999): loss=2.583748906966889, w0=73.57499999999987, w1=12.928703632070071\n",
      "Gradient Descent(5702/9999): loss=2.330651496997574, w0=73.57499999999987, w1=13.230756157635897\n",
      "Gradient Descent(5703/9999): loss=1.8030188035642, w0=73.51874999999987, w1=13.482278805603757\n",
      "Gradient Descent(5704/9999): loss=2.139147620806, w0=73.68749999999987, w1=13.506083494411628\n",
      "Gradient Descent(5705/9999): loss=2.0505737029912128, w0=73.51874999999987, w1=13.363511726085976\n",
      "Gradient Descent(5706/9999): loss=2.209047624080833, w0=73.40624999999987, w1=13.083907182719077\n",
      "Gradient Descent(5707/9999): loss=2.2132543625998866, w0=73.12499999999987, w1=13.191491715751033\n",
      "Gradient Descent(5708/9999): loss=2.1614110976293883, w0=72.84374999999987, w1=13.303992133794871\n",
      "Gradient Descent(5709/9999): loss=1.83269390539594, w0=72.95624999999987, w1=13.463051326462804\n",
      "Gradient Descent(5710/9999): loss=2.380186741429813, w0=72.89999999999986, w1=13.403175661323814\n",
      "Gradient Descent(5711/9999): loss=2.59941287389195, w0=72.89999999999986, w1=13.397402515981263\n",
      "Gradient Descent(5712/9999): loss=2.295735262326811, w0=72.67499999999987, w1=13.25477812049178\n",
      "Gradient Descent(5713/9999): loss=1.7464032642529355, w0=72.73124999999987, w1=13.533143556389453\n",
      "Gradient Descent(5714/9999): loss=2.4064996003925936, w0=72.84374999999987, w1=13.656636841639449\n",
      "Gradient Descent(5715/9999): loss=1.8905441232818911, w0=72.73124999999987, w1=13.502099745059985\n",
      "Gradient Descent(5716/9999): loss=1.9045782148318802, w0=73.06874999999988, w1=13.340376486918181\n",
      "Gradient Descent(5717/9999): loss=2.251920528699487, w0=72.89999999999988, w1=13.327827582233752\n",
      "Gradient Descent(5718/9999): loss=1.9637931439371217, w0=72.78749999999988, w1=13.334180064439062\n",
      "Gradient Descent(5719/9999): loss=2.220860912999225, w0=72.78749999999988, w1=13.033317568718768\n",
      "Gradient Descent(5720/9999): loss=1.7082249806714753, w0=73.06874999999988, w1=13.143435194386207\n",
      "Gradient Descent(5721/9999): loss=2.5308598534957683, w0=73.06874999999988, w1=13.24652813279197\n",
      "Gradient Descent(5722/9999): loss=2.3192995068273223, w0=73.29374999999987, w1=13.421197320276695\n",
      "Gradient Descent(5723/9999): loss=2.1175701758750174, w0=73.23749999999987, w1=13.673324976864317\n",
      "Gradient Descent(5724/9999): loss=2.314411336802044, w0=73.34999999999987, w1=13.298047828573097\n",
      "Gradient Descent(5725/9999): loss=1.8000305715033404, w0=73.51874999999987, w1=13.471849339700734\n",
      "Gradient Descent(5726/9999): loss=2.1337338448704504, w0=73.34999999999987, w1=13.918580161926629\n",
      "Gradient Descent(5727/9999): loss=1.8569263179239095, w0=73.29374999999986, w1=14.23935977386297\n",
      "Gradient Descent(5728/9999): loss=2.119893744904986, w0=73.12499999999986, w1=14.30539428748989\n",
      "Gradient Descent(5729/9999): loss=2.2915246042793003, w0=73.18124999999986, w1=14.074615965176562\n",
      "Gradient Descent(5730/9999): loss=2.30485507275878, w0=73.18124999999986, w1=13.87908170298892\n",
      "Gradient Descent(5731/9999): loss=2.320688234256245, w0=73.40624999999986, w1=13.77319548152327\n",
      "Gradient Descent(5732/9999): loss=1.9525660571922812, w0=73.63124999999985, w1=13.943047416652398\n",
      "Gradient Descent(5733/9999): loss=2.0819697230092586, w0=73.57499999999985, w1=14.130739286771547\n",
      "Gradient Descent(5734/9999): loss=1.9895523253189749, w0=73.51874999999984, w1=14.259697138200188\n",
      "Gradient Descent(5735/9999): loss=1.5649767545981348, w0=73.23749999999984, w1=14.168235790562278\n",
      "Gradient Descent(5736/9999): loss=2.2974954730179475, w0=73.46249999999984, w1=14.232621245577752\n",
      "Gradient Descent(5737/9999): loss=2.741853342908858, w0=73.63124999999984, w1=14.244650521930113\n",
      "Gradient Descent(5738/9999): loss=1.8310497444964478, w0=73.46249999999984, w1=14.371141203569746\n",
      "Gradient Descent(5739/9999): loss=2.4230968822478225, w0=73.51874999999984, w1=13.94925186404511\n",
      "Gradient Descent(5740/9999): loss=1.8803265096736808, w0=73.68749999999984, w1=13.974118963764587\n",
      "Gradient Descent(5741/9999): loss=2.5541297098602347, w0=73.63124999999984, w1=13.697652931556624\n",
      "Gradient Descent(5742/9999): loss=2.4165408877175887, w0=73.57499999999983, w1=13.813147024492704\n",
      "Gradient Descent(5743/9999): loss=2.3603423444699754, w0=73.91249999999984, w1=13.915309775162426\n",
      "Gradient Descent(5744/9999): loss=2.183302465094636, w0=73.74374999999984, w1=13.767794504683907\n",
      "Gradient Descent(5745/9999): loss=2.39964302084081, w0=73.57499999999983, w1=13.660119759486964\n",
      "Gradient Descent(5746/9999): loss=2.1344796586894086, w0=73.57499999999983, w1=13.526649101099933\n",
      "Gradient Descent(5747/9999): loss=1.9432517111547885, w0=73.68749999999983, w1=13.506845359711377\n",
      "Gradient Descent(5748/9999): loss=1.9060108769611368, w0=73.85624999999983, w1=13.812884665833408\n",
      "Gradient Descent(5749/9999): loss=2.0908551143687593, w0=73.79999999999983, w1=13.629831438478647\n",
      "Gradient Descent(5750/9999): loss=1.89523617016492, w0=73.85624999999983, w1=13.486475131172593\n",
      "Gradient Descent(5751/9999): loss=1.5483941384597044, w0=74.02499999999984, w1=13.384436067095361\n",
      "Gradient Descent(5752/9999): loss=2.1457703478260033, w0=74.02499999999984, w1=13.221800938546668\n",
      "Gradient Descent(5753/9999): loss=1.6435006301632975, w0=73.85624999999983, w1=13.266222576558631\n",
      "Gradient Descent(5754/9999): loss=2.544465007008893, w0=74.24999999999983, w1=13.319208708197989\n",
      "Gradient Descent(5755/9999): loss=1.9636475476602575, w0=73.96874999999983, w1=13.314155773715173\n",
      "Gradient Descent(5756/9999): loss=2.523159544431003, w0=73.91249999999982, w1=13.33929010898785\n",
      "Gradient Descent(5757/9999): loss=2.5004028634654705, w0=73.91249999999982, w1=13.252790356322132\n",
      "Gradient Descent(5758/9999): loss=2.045498745156972, w0=73.91249999999982, w1=13.435333596804234\n",
      "Gradient Descent(5759/9999): loss=2.2979988095789277, w0=73.68749999999983, w1=13.62495302076067\n",
      "Gradient Descent(5760/9999): loss=1.673536583444532, w0=73.34999999999982, w1=13.98142601884422\n",
      "Gradient Descent(5761/9999): loss=2.5262600383173313, w0=73.29374999999982, w1=13.734843653381468\n",
      "Gradient Descent(5762/9999): loss=2.083267932767765, w0=73.46249999999982, w1=13.513222255387875\n",
      "Gradient Descent(5763/9999): loss=1.9868161702065346, w0=73.57499999999982, w1=13.593331428281822\n",
      "Gradient Descent(5764/9999): loss=2.3601096133773485, w0=73.57499999999982, w1=13.740700095366593\n",
      "Gradient Descent(5765/9999): loss=2.354135823009652, w0=73.57499999999982, w1=13.688894227080285\n",
      "Gradient Descent(5766/9999): loss=2.263519268985001, w0=73.40624999999982, w1=13.529333064701975\n",
      "Gradient Descent(5767/9999): loss=2.037332525947036, w0=73.23749999999981, w1=13.376253511651088\n",
      "Gradient Descent(5768/9999): loss=2.189590132252314, w0=73.12499999999982, w1=13.5071814879113\n",
      "Gradient Descent(5769/9999): loss=2.7593353736891144, w0=73.23749999999981, w1=13.47590102853104\n",
      "Gradient Descent(5770/9999): loss=2.4093840987850332, w0=73.34999999999981, w1=13.503071210621137\n",
      "Gradient Descent(5771/9999): loss=1.9815931986140114, w0=73.1812499999998, w1=13.705048106404373\n",
      "Gradient Descent(5772/9999): loss=1.8597089530281132, w0=73.1812499999998, w1=13.746691885459255\n",
      "Gradient Descent(5773/9999): loss=2.2362366986463864, w0=73.1812499999998, w1=13.830064692482573\n",
      "Gradient Descent(5774/9999): loss=1.9529474977686645, w0=73.06874999999981, w1=13.65528708454536\n",
      "Gradient Descent(5775/9999): loss=1.8467032921568078, w0=73.23749999999981, w1=13.53517022340987\n",
      "Gradient Descent(5776/9999): loss=2.138850579412808, w0=73.51874999999981, w1=13.323825935470676\n",
      "Gradient Descent(5777/9999): loss=2.7046088001060697, w0=73.34999999999981, w1=13.383695289916261\n",
      "Gradient Descent(5778/9999): loss=2.254440400352839, w0=73.0124999999998, w1=13.206611191541718\n",
      "Gradient Descent(5779/9999): loss=2.649245929610531, w0=73.2374999999998, w1=13.554829865617421\n",
      "Gradient Descent(5780/9999): loss=2.1926796216583835, w0=73.1249999999998, w1=13.37612781872055\n",
      "Gradient Descent(5781/9999): loss=2.4901901299660087, w0=73.2374999999998, w1=13.364461091862418\n",
      "Gradient Descent(5782/9999): loss=2.1671894688805926, w0=73.5187499999998, w1=13.280677236574407\n",
      "Gradient Descent(5783/9999): loss=2.24727086369667, w0=73.3499999999998, w1=13.19606215481635\n",
      "Gradient Descent(5784/9999): loss=2.2756069164419985, w0=73.29374999999979, w1=12.914428974823126\n",
      "Gradient Descent(5785/9999): loss=1.7204171645710984, w0=73.40624999999979, w1=12.992682269865202\n",
      "Gradient Descent(5786/9999): loss=1.9895617728792185, w0=73.46249999999979, w1=12.903791885665948\n",
      "Gradient Descent(5787/9999): loss=2.2866415196444465, w0=73.57499999999979, w1=13.07149545538954\n",
      "Gradient Descent(5788/9999): loss=2.1232864220919625, w0=73.46249999999979, w1=13.023948274550987\n",
      "Gradient Descent(5789/9999): loss=2.112690731396188, w0=73.57499999999979, w1=13.078668978303345\n",
      "Gradient Descent(5790/9999): loss=2.685939715422987, w0=73.46249999999979, w1=13.13711285125836\n",
      "Gradient Descent(5791/9999): loss=2.368472042933706, w0=73.2374999999998, w1=13.163610396104641\n",
      "Gradient Descent(5792/9999): loss=2.6397134555534065, w0=73.0687499999998, w1=13.56581081903836\n",
      "Gradient Descent(5793/9999): loss=2.5699358305129403, w0=73.18124999999979, w1=14.044227158506205\n",
      "Gradient Descent(5794/9999): loss=2.6117777733653886, w0=73.2374999999998, w1=13.895390516086346\n",
      "Gradient Descent(5795/9999): loss=1.9245545089424976, w0=73.3499999999998, w1=13.88310533513149\n",
      "Gradient Descent(5796/9999): loss=2.435692415789325, w0=73.01249999999979, w1=13.949248859988192\n",
      "Gradient Descent(5797/9999): loss=1.6706969132550706, w0=72.95624999999978, w1=13.927144659697829\n",
      "Gradient Descent(5798/9999): loss=2.165469927319312, w0=73.06874999999978, w1=13.941968444860779\n",
      "Gradient Descent(5799/9999): loss=2.2587509100373095, w0=73.29374999999978, w1=13.72421125944127\n",
      "Gradient Descent(5800/9999): loss=2.55068384087199, w0=73.23749999999977, w1=13.74558046177862\n",
      "Gradient Descent(5801/9999): loss=2.399053032086917, w0=73.40624999999977, w1=13.764474150826768\n",
      "Gradient Descent(5802/9999): loss=1.9308437419092312, w0=73.29374999999978, w1=13.739118391215257\n",
      "Gradient Descent(5803/9999): loss=2.3172665496663107, w0=73.46249999999978, w1=13.351992001974399\n",
      "Gradient Descent(5804/9999): loss=2.291785688600549, w0=73.40624999999977, w1=13.547140971539113\n",
      "Gradient Descent(5805/9999): loss=2.258532956634626, w0=73.18124999999978, w1=13.562587227942956\n",
      "Gradient Descent(5806/9999): loss=1.9015603249824846, w0=73.18124999999978, w1=13.489509121366584\n",
      "Gradient Descent(5807/9999): loss=2.4896374562099104, w0=73.29374999999978, w1=13.36911663685416\n",
      "Gradient Descent(5808/9999): loss=2.2093841474127176, w0=73.06874999999978, w1=13.528060948476604\n",
      "Gradient Descent(5809/9999): loss=2.0556890933630276, w0=73.23749999999978, w1=13.404969528158507\n",
      "Gradient Descent(5810/9999): loss=2.392184895341363, w0=73.29374999999979, w1=13.551442682728664\n",
      "Gradient Descent(5811/9999): loss=1.999850662435518, w0=73.40624999999979, w1=13.560374299438983\n",
      "Gradient Descent(5812/9999): loss=2.0018158770942525, w0=73.40624999999979, w1=13.631684229058735\n",
      "Gradient Descent(5813/9999): loss=2.2479844068401245, w0=73.40624999999979, w1=13.427488477039589\n",
      "Gradient Descent(5814/9999): loss=2.0506486636193384, w0=73.29374999999979, w1=13.210193210812244\n",
      "Gradient Descent(5815/9999): loss=2.444630046509352, w0=73.40624999999979, w1=13.449932427571033\n",
      "Gradient Descent(5816/9999): loss=1.7468651529907813, w0=73.40624999999979, w1=13.463651045647662\n",
      "Gradient Descent(5817/9999): loss=2.002450376190561, w0=73.29374999999979, w1=13.387164490094287\n",
      "Gradient Descent(5818/9999): loss=1.7579296638206554, w0=73.18124999999979, w1=13.276298668272174\n",
      "Gradient Descent(5819/9999): loss=2.4015392254031136, w0=73.40624999999979, w1=12.963478334113173\n",
      "Gradient Descent(5820/9999): loss=2.243052350996447, w0=73.63124999999978, w1=13.079104520842927\n",
      "Gradient Descent(5821/9999): loss=1.9855699523119328, w0=73.34999999999978, w1=13.482086802690201\n",
      "Gradient Descent(5822/9999): loss=2.24579050538787, w0=73.18124999999978, w1=13.482605630432712\n",
      "Gradient Descent(5823/9999): loss=1.6436385394196549, w0=73.01249999999978, w1=13.179436989212638\n",
      "Gradient Descent(5824/9999): loss=2.00570033440769, w0=73.29374999999978, w1=13.156878448799441\n",
      "Gradient Descent(5825/9999): loss=1.945643136990559, w0=73.23749999999977, w1=13.268183178610261\n",
      "Gradient Descent(5826/9999): loss=2.598333349519795, w0=73.18124999999976, w1=13.131547351321984\n",
      "Gradient Descent(5827/9999): loss=2.520221549323537, w0=73.12499999999976, w1=13.277488561518533\n",
      "Gradient Descent(5828/9999): loss=2.1726614116590905, w0=73.06874999999975, w1=13.266484804777406\n",
      "Gradient Descent(5829/9999): loss=1.9425017599574588, w0=72.89999999999975, w1=13.392005566927613\n",
      "Gradient Descent(5830/9999): loss=2.297202868647845, w0=72.84374999999974, w1=13.696788239505205\n",
      "Gradient Descent(5831/9999): loss=1.976389135729827, w0=72.73124999999975, w1=13.506908478136197\n",
      "Gradient Descent(5832/9999): loss=2.236505365236575, w0=72.89999999999975, w1=13.443027339004798\n",
      "Gradient Descent(5833/9999): loss=2.282068663690267, w0=72.89999999999975, w1=13.487448053920302\n",
      "Gradient Descent(5834/9999): loss=1.9060110368576009, w0=72.84374999999974, w1=13.61999233782711\n",
      "Gradient Descent(5835/9999): loss=2.943991823130939, w0=73.06874999999974, w1=13.346506135884466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(5836/9999): loss=2.2831004593172466, w0=72.95624999999974, w1=13.023562578055826\n",
      "Gradient Descent(5837/9999): loss=1.8554367538099514, w0=72.78749999999974, w1=13.225977581014831\n",
      "Gradient Descent(5838/9999): loss=2.654561817642287, w0=72.89999999999974, w1=13.626016521640569\n",
      "Gradient Descent(5839/9999): loss=2.864041242112475, w0=73.12499999999973, w1=13.518340592242469\n",
      "Gradient Descent(5840/9999): loss=2.0762930839059335, w0=73.06874999999972, w1=13.59654908352617\n",
      "Gradient Descent(5841/9999): loss=2.251288655914376, w0=72.95624999999973, w1=13.61860528729181\n",
      "Gradient Descent(5842/9999): loss=2.4461556317168984, w0=72.95624999999973, w1=13.717465041875947\n",
      "Gradient Descent(5843/9999): loss=2.0087911707042316, w0=73.06874999999972, w1=13.704805643431893\n",
      "Gradient Descent(5844/9999): loss=2.5098511654998443, w0=73.23749999999973, w1=13.825879929608096\n",
      "Gradient Descent(5845/9999): loss=2.560219847725979, w0=73.18124999999972, w1=14.091277398868684\n",
      "Gradient Descent(5846/9999): loss=2.440449310631029, w0=73.18124999999972, w1=14.027655251035272\n",
      "Gradient Descent(5847/9999): loss=2.4105938115798042, w0=73.06874999999972, w1=13.753472875151528\n",
      "Gradient Descent(5848/9999): loss=1.7583423418677127, w0=72.73124999999972, w1=13.844537740904219\n",
      "Gradient Descent(5849/9999): loss=2.4119358236729163, w0=73.01249999999972, w1=13.88063780559889\n",
      "Gradient Descent(5850/9999): loss=1.9868227149451085, w0=72.84374999999972, w1=13.948769556138297\n",
      "Gradient Descent(5851/9999): loss=2.2367520020525524, w0=72.56249999999972, w1=13.893275909073054\n",
      "Gradient Descent(5852/9999): loss=2.7545317289081814, w0=72.22499999999971, w1=13.901663267386365\n",
      "Gradient Descent(5853/9999): loss=2.3156902189759734, w0=72.28124999999972, w1=13.953684734150938\n",
      "Gradient Descent(5854/9999): loss=2.2547223797549183, w0=72.44999999999972, w1=13.978177796877489\n",
      "Gradient Descent(5855/9999): loss=1.956385895138681, w0=72.50624999999972, w1=13.773877579355316\n",
      "Gradient Descent(5856/9999): loss=2.3513366275596903, w0=72.89999999999972, w1=13.573723782785503\n",
      "Gradient Descent(5857/9999): loss=2.7271072179263154, w0=72.56249999999972, w1=13.62131254997829\n",
      "Gradient Descent(5858/9999): loss=1.9447110482094623, w0=72.78749999999971, w1=13.58055507295505\n",
      "Gradient Descent(5859/9999): loss=2.617215948473425, w0=72.95624999999971, w1=13.423220113046847\n",
      "Gradient Descent(5860/9999): loss=2.1568887987794354, w0=73.01249999999972, w1=13.489486275217187\n",
      "Gradient Descent(5861/9999): loss=1.6350842022561767, w0=73.12499999999972, w1=13.343755847277334\n",
      "Gradient Descent(5862/9999): loss=2.318979855992518, w0=73.01249999999972, w1=13.195360501179687\n",
      "Gradient Descent(5863/9999): loss=2.0396596456637615, w0=73.12499999999972, w1=12.919291975915137\n",
      "Gradient Descent(5864/9999): loss=2.394029928289906, w0=72.89999999999972, w1=12.96093404449027\n",
      "Gradient Descent(5865/9999): loss=2.2054813385735086, w0=73.06874999999972, w1=13.071439529604904\n",
      "Gradient Descent(5866/9999): loss=2.194336465796192, w0=73.12499999999973, w1=13.148418038804442\n",
      "Gradient Descent(5867/9999): loss=2.2130079791371378, w0=73.06874999999972, w1=13.243501584832913\n",
      "Gradient Descent(5868/9999): loss=2.1560006078178704, w0=73.23749999999973, w1=13.20958065149016\n",
      "Gradient Descent(5869/9999): loss=2.2259801454805226, w0=73.51874999999973, w1=13.303451936222432\n",
      "Gradient Descent(5870/9999): loss=2.0393853393673234, w0=73.23749999999973, w1=13.213844844657514\n",
      "Gradient Descent(5871/9999): loss=2.2972389097826484, w0=73.23749999999973, w1=13.10378082468747\n",
      "Gradient Descent(5872/9999): loss=2.853787037750176, w0=73.29374999999973, w1=13.061926793714346\n",
      "Gradient Descent(5873/9999): loss=2.131899460476482, w0=73.40624999999973, w1=13.114596848173308\n",
      "Gradient Descent(5874/9999): loss=2.2338762965196417, w0=73.46249999999974, w1=13.241625369444872\n",
      "Gradient Descent(5875/9999): loss=2.178523926516907, w0=73.85624999999973, w1=13.134354347837865\n",
      "Gradient Descent(5876/9999): loss=2.3760657806986245, w0=73.79999999999973, w1=13.120002037450607\n",
      "Gradient Descent(5877/9999): loss=2.5388888949564827, w0=73.68749999999973, w1=13.299991275878837\n",
      "Gradient Descent(5878/9999): loss=2.3387791710456876, w0=73.68749999999973, w1=13.347755216309226\n",
      "Gradient Descent(5879/9999): loss=2.00663131522968, w0=73.68749999999973, w1=13.257816331317091\n",
      "Gradient Descent(5880/9999): loss=2.070416499901323, w0=73.34999999999972, w1=13.187016536509672\n",
      "Gradient Descent(5881/9999): loss=2.625447334869038, w0=73.29374999999972, w1=13.095794861042783\n",
      "Gradient Descent(5882/9999): loss=2.287706903720601, w0=73.46249999999972, w1=13.190705925044778\n",
      "Gradient Descent(5883/9999): loss=2.03577773786268, w0=73.34999999999972, w1=13.178826244481138\n",
      "Gradient Descent(5884/9999): loss=2.018591157383957, w0=73.29374999999972, w1=13.268809305080467\n",
      "Gradient Descent(5885/9999): loss=2.8033619156097087, w0=73.12499999999972, w1=13.376738001454592\n",
      "Gradient Descent(5886/9999): loss=1.9673471311446882, w0=73.01249999999972, w1=13.031394921494499\n",
      "Gradient Descent(5887/9999): loss=2.680825406302901, w0=73.12499999999972, w1=13.178010340796172\n",
      "Gradient Descent(5888/9999): loss=2.4145480540787396, w0=72.95624999999971, w1=13.16060252549043\n",
      "Gradient Descent(5889/9999): loss=2.244496016435967, w0=72.89999999999971, w1=13.350176233204103\n",
      "Gradient Descent(5890/9999): loss=2.2678140085478207, w0=73.06874999999971, w1=13.067217623563087\n",
      "Gradient Descent(5891/9999): loss=1.60111282791138, w0=72.89999999999971, w1=13.289134152829108\n",
      "Gradient Descent(5892/9999): loss=2.2036780855139613, w0=73.0124999999997, w1=13.176607950588485\n",
      "Gradient Descent(5893/9999): loss=2.3550755901871576, w0=72.7312499999997, w1=13.137212236163549\n",
      "Gradient Descent(5894/9999): loss=2.362506347029964, w0=73.18124999999971, w1=13.150049043168462\n",
      "Gradient Descent(5895/9999): loss=2.4180031143361993, w0=73.23749999999971, w1=13.074546334572037\n",
      "Gradient Descent(5896/9999): loss=1.9480535964777164, w0=73.51874999999971, w1=13.027047247146852\n",
      "Gradient Descent(5897/9999): loss=2.6020703829029492, w0=73.63124999999971, w1=13.258905982866601\n",
      "Gradient Descent(5898/9999): loss=2.3666785185603714, w0=73.91249999999971, w1=13.333165599349663\n",
      "Gradient Descent(5899/9999): loss=2.3232317986981723, w0=73.91249999999971, w1=13.183659420123085\n",
      "Gradient Descent(5900/9999): loss=1.9067462957448158, w0=73.74374999999971, w1=13.256040018347159\n",
      "Gradient Descent(5901/9999): loss=2.581447976295788, w0=73.5749999999997, w1=13.474237807084922\n",
      "Gradient Descent(5902/9999): loss=2.2550497987337423, w0=73.7999999999997, w1=13.43568523498248\n",
      "Gradient Descent(5903/9999): loss=2.5890779432326827, w0=73.6874999999997, w1=13.551236602884064\n",
      "Gradient Descent(5904/9999): loss=2.2265960172194057, w0=73.7999999999997, w1=13.611464278391459\n",
      "Gradient Descent(5905/9999): loss=2.0531166771432696, w0=73.6874999999997, w1=13.589260266386418\n",
      "Gradient Descent(5906/9999): loss=1.9940586020432365, w0=73.9687499999997, w1=13.544332915043896\n",
      "Gradient Descent(5907/9999): loss=2.26404165004828, w0=73.9124999999997, w1=13.454485948145715\n",
      "Gradient Descent(5908/9999): loss=1.7356678198451996, w0=74.13749999999969, w1=13.44647210303032\n",
      "Gradient Descent(5909/9999): loss=2.4770854143276924, w0=73.85624999999969, w1=13.349210593024388\n",
      "Gradient Descent(5910/9999): loss=1.7618243416382717, w0=73.7437499999997, w1=13.485920175929175\n",
      "Gradient Descent(5911/9999): loss=2.2648993609970516, w0=73.68749999999969, w1=13.697509111787687\n",
      "Gradient Descent(5912/9999): loss=1.8705466955494858, w0=73.79999999999968, w1=13.574398295137925\n",
      "Gradient Descent(5913/9999): loss=2.1526592231659176, w0=73.91249999999968, w1=13.452565226720653\n",
      "Gradient Descent(5914/9999): loss=2.032018715910021, w0=73.74374999999968, w1=13.307949248937664\n",
      "Gradient Descent(5915/9999): loss=2.3895495409444933, w0=73.63124999999968, w1=13.175393988989246\n",
      "Gradient Descent(5916/9999): loss=2.2763226311968103, w0=73.68749999999969, w1=13.334196452056052\n",
      "Gradient Descent(5917/9999): loss=2.124928403043696, w0=73.57499999999969, w1=13.007037836654824\n",
      "Gradient Descent(5918/9999): loss=2.4719506125509665, w0=73.85624999999969, w1=13.084000709937333\n",
      "Gradient Descent(5919/9999): loss=1.7108220338855988, w0=73.51874999999968, w1=12.881857183851976\n",
      "Gradient Descent(5920/9999): loss=2.354687964194679, w0=73.29374999999969, w1=12.72660032200233\n",
      "Gradient Descent(5921/9999): loss=1.8096945234794466, w0=73.57499999999969, w1=12.737977096753097\n",
      "Gradient Descent(5922/9999): loss=2.437682167055798, w0=73.6312499999997, w1=12.980767369317414\n",
      "Gradient Descent(5923/9999): loss=2.134804972093977, w0=73.6874999999997, w1=13.074710278542938\n",
      "Gradient Descent(5924/9999): loss=2.5307294630367525, w0=73.5749999999997, w1=12.82758396718269\n",
      "Gradient Descent(5925/9999): loss=2.057529587629052, w0=73.34999999999971, w1=12.834029035685948\n",
      "Gradient Descent(5926/9999): loss=2.759215540707092, w0=73.46249999999971, w1=13.021715368932128\n",
      "Gradient Descent(5927/9999): loss=2.038371686857115, w0=73.4062499999997, w1=13.288446970556361\n",
      "Gradient Descent(5928/9999): loss=2.3735196798154474, w0=73.6874999999997, w1=13.115130114706062\n",
      "Gradient Descent(5929/9999): loss=1.9695111110077876, w0=73.8562499999997, w1=13.447986112807243\n",
      "Gradient Descent(5930/9999): loss=2.5246430461161897, w0=73.6874999999997, w1=13.414874650715312\n",
      "Gradient Descent(5931/9999): loss=2.593323942571929, w0=73.6312499999997, w1=13.644050963307569\n",
      "Gradient Descent(5932/9999): loss=2.0345788429645095, w0=73.85624999999969, w1=13.805290797209635\n",
      "Gradient Descent(5933/9999): loss=1.9258489443765363, w0=73.57499999999969, w1=13.687409596814588\n",
      "Gradient Descent(5934/9999): loss=2.8489755760249356, w0=73.29374999999969, w1=13.770439967347276\n",
      "Gradient Descent(5935/9999): loss=1.9726833435367714, w0=73.0687499999997, w1=13.6972298720723\n",
      "Gradient Descent(5936/9999): loss=1.772865882401288, w0=72.9562499999997, w1=13.677212636685418\n",
      "Gradient Descent(5937/9999): loss=2.2324764882203443, w0=72.9562499999997, w1=13.65688139209332\n",
      "Gradient Descent(5938/9999): loss=2.0191692276945776, w0=73.0687499999997, w1=13.805763236540363\n",
      "Gradient Descent(5939/9999): loss=2.021175367493718, w0=73.3499999999997, w1=13.961752388470012\n",
      "Gradient Descent(5940/9999): loss=2.5740572277279714, w0=73.4624999999997, w1=13.598778112722828\n",
      "Gradient Descent(5941/9999): loss=1.665493737155263, w0=73.1812499999997, w1=13.672346786444722\n",
      "Gradient Descent(5942/9999): loss=2.370996033226564, w0=73.2374999999997, w1=13.673095208743893\n",
      "Gradient Descent(5943/9999): loss=2.0562197018518167, w0=73.2374999999997, w1=13.505703236253616\n",
      "Gradient Descent(5944/9999): loss=2.4959580781573583, w0=73.0124999999997, w1=13.364298377073473\n",
      "Gradient Descent(5945/9999): loss=1.9495593240274816, w0=73.1249999999997, w1=13.300966812526049\n",
      "Gradient Descent(5946/9999): loss=2.1599783256296075, w0=73.0124999999997, w1=13.354912772072465\n",
      "Gradient Descent(5947/9999): loss=2.4458198302720873, w0=73.06874999999971, w1=13.668168660756834\n",
      "Gradient Descent(5948/9999): loss=2.1921742029165925, w0=73.23749999999971, w1=13.673275667663694\n",
      "Gradient Descent(5949/9999): loss=1.836729972431757, w0=73.06874999999971, w1=13.530888398531394\n",
      "Gradient Descent(5950/9999): loss=2.22888888817306, w0=72.89999999999971, w1=13.644648663194952\n",
      "Gradient Descent(5951/9999): loss=2.7599427609200506, w0=73.0124999999997, w1=13.54184361413868\n",
      "Gradient Descent(5952/9999): loss=1.7843654465203034, w0=73.1249999999997, w1=13.714960161563182\n",
      "Gradient Descent(5953/9999): loss=2.4677747472596003, w0=72.89999999999971, w1=13.582686675748162\n",
      "Gradient Descent(5954/9999): loss=2.1553851445563676, w0=73.06874999999971, w1=13.874279349339725\n",
      "Gradient Descent(5955/9999): loss=2.8506348050558, w0=73.34999999999971, w1=13.795005555103897\n",
      "Gradient Descent(5956/9999): loss=2.2260628827620286, w0=73.23749999999971, w1=13.626544878241196\n",
      "Gradient Descent(5957/9999): loss=2.4251022888523375, w0=73.34999999999971, w1=13.928372812730059\n",
      "Gradient Descent(5958/9999): loss=1.941233478779586, w0=73.34999999999971, w1=13.79320522055581\n",
      "Gradient Descent(5959/9999): loss=2.189922288945416, w0=73.46249999999971, w1=13.983558090850885\n",
      "Gradient Descent(5960/9999): loss=1.93709727289147, w0=73.4062499999997, w1=13.803848141310558\n",
      "Gradient Descent(5961/9999): loss=2.33545066517006, w0=73.6312499999997, w1=13.488857322294397\n",
      "Gradient Descent(5962/9999): loss=2.7126982328237497, w0=73.57499999999969, w1=13.362941678680583\n",
      "Gradient Descent(5963/9999): loss=2.4697431089640265, w0=73.0687499999997, w1=13.419418968311076\n",
      "Gradient Descent(5964/9999): loss=2.1428222865129696, w0=73.2374999999997, w1=13.634868956525185\n",
      "Gradient Descent(5965/9999): loss=2.581499066357618, w0=73.5187499999997, w1=13.50972742095017\n",
      "Gradient Descent(5966/9999): loss=2.3066020711383497, w0=73.6312499999997, w1=13.634870057665426\n",
      "Gradient Descent(5967/9999): loss=2.1457652893747747, w0=73.6312499999997, w1=13.603602645925154\n",
      "Gradient Descent(5968/9999): loss=2.2427708476256267, w0=73.5187499999997, w1=13.607749222133899\n",
      "Gradient Descent(5969/9999): loss=2.1398923475872227, w0=73.3499999999997, w1=13.6587030346631\n",
      "Gradient Descent(5970/9999): loss=2.2529819546789067, w0=73.1249999999997, w1=13.776999313959843\n",
      "Gradient Descent(5971/9999): loss=2.5865878141897056, w0=73.18124999999971, w1=13.824805239783903\n",
      "Gradient Descent(5972/9999): loss=2.1281516321605496, w0=72.89999999999971, w1=14.122631251815607\n",
      "Gradient Descent(5973/9999): loss=2.2368071182466363, w0=73.18124999999971, w1=13.782067431759257\n",
      "Gradient Descent(5974/9999): loss=1.9652286921705489, w0=73.1249999999997, w1=13.857501525569463\n",
      "Gradient Descent(5975/9999): loss=1.9995303442532615, w0=73.3499999999997, w1=13.889248551152546\n",
      "Gradient Descent(5976/9999): loss=2.1926087405129104, w0=73.1812499999997, w1=13.615084215688217\n",
      "Gradient Descent(5977/9999): loss=2.5824056236626562, w0=73.3499999999997, w1=13.689071455589339\n",
      "Gradient Descent(5978/9999): loss=2.1084004558878346, w0=73.3499999999997, w1=13.34301222898661\n",
      "Gradient Descent(5979/9999): loss=2.1662781547537495, w0=73.1812499999997, w1=13.374423451620457\n",
      "Gradient Descent(5980/9999): loss=2.0398334458970773, w0=73.0687499999997, w1=13.271440422013725\n",
      "Gradient Descent(5981/9999): loss=2.1569544249737187, w0=73.01249999999969, w1=13.494502391099639\n",
      "Gradient Descent(5982/9999): loss=1.9379746439786854, w0=73.0687499999997, w1=13.551299594611768\n",
      "Gradient Descent(5983/9999): loss=2.3881249835079394, w0=73.29374999999969, w1=13.674352174924433\n",
      "Gradient Descent(5984/9999): loss=2.1206789819884757, w0=73.12499999999969, w1=13.44292910972145\n",
      "Gradient Descent(5985/9999): loss=1.8594228231098926, w0=73.06874999999968, w1=13.595534568139957\n",
      "Gradient Descent(5986/9999): loss=2.1525681980416174, w0=73.18124999999968, w1=13.649335996388261\n",
      "Gradient Descent(5987/9999): loss=2.1485064599499886, w0=73.12499999999967, w1=13.326509562358709\n",
      "Gradient Descent(5988/9999): loss=2.5015069523532505, w0=72.89999999999968, w1=13.362287042847067\n",
      "Gradient Descent(5989/9999): loss=2.3536580235358944, w0=72.67499999999968, w1=13.278135381466457\n",
      "Gradient Descent(5990/9999): loss=2.4324600991363368, w0=72.73124999999969, w1=12.951052519844824\n",
      "Gradient Descent(5991/9999): loss=2.6290754697645022, w0=72.84374999999969, w1=12.971646514529484\n",
      "Gradient Descent(5992/9999): loss=1.8367733640599182, w0=72.8999999999997, w1=13.117080568882903\n",
      "Gradient Descent(5993/9999): loss=2.7940027689383973, w0=72.6749999999997, w1=13.337956931842063\n",
      "Gradient Descent(5994/9999): loss=2.0871456454305615, w0=72.6749999999997, w1=13.515162424369956\n",
      "Gradient Descent(5995/9999): loss=1.6304035062868, w0=72.8437499999997, w1=13.651514419269617\n",
      "Gradient Descent(5996/9999): loss=2.0800123649896123, w0=72.7874999999997, w1=13.731134952178241\n",
      "Gradient Descent(5997/9999): loss=2.2014824936973514, w0=72.8437499999997, w1=13.74494946486636\n",
      "Gradient Descent(5998/9999): loss=1.9878715881694204, w0=72.89999999999971, w1=13.828667259742017\n",
      "Gradient Descent(5999/9999): loss=2.718705604158429, w0=72.7312499999997, w1=13.910615761772007\n",
      "Gradient Descent(6000/9999): loss=1.9150445139884675, w0=72.8437499999997, w1=13.859794101136858\n",
      "Gradient Descent(6001/9999): loss=2.0174050156649184, w0=73.0687499999997, w1=13.789349569349234\n",
      "Gradient Descent(6002/9999): loss=2.5716401479182656, w0=73.1249999999997, w1=13.77131725161239\n",
      "Gradient Descent(6003/9999): loss=2.0778160887376362, w0=73.1249999999997, w1=13.987816483913845\n",
      "Gradient Descent(6004/9999): loss=2.29705021810274, w0=73.3499999999997, w1=13.94195891827682\n",
      "Gradient Descent(6005/9999): loss=1.5616366963633306, w0=73.2374999999997, w1=13.89750859900643\n",
      "Gradient Descent(6006/9999): loss=2.021555918461226, w0=73.0124999999997, w1=13.829881345276819\n",
      "Gradient Descent(6007/9999): loss=2.197331301731494, w0=72.89999999999971, w1=13.936436302930547\n",
      "Gradient Descent(6008/9999): loss=2.038658975986248, w0=72.7312499999997, w1=13.879078083096513\n",
      "Gradient Descent(6009/9999): loss=2.1820152967562327, w0=72.78749999999971, w1=13.86928618056881\n",
      "Gradient Descent(6010/9999): loss=2.1356392576438603, w0=72.67499999999971, w1=13.866249929804903\n",
      "Gradient Descent(6011/9999): loss=3.0424556664641207, w0=72.73124999999972, w1=13.925379597735976\n",
      "Gradient Descent(6012/9999): loss=2.292882251558588, w0=72.95624999999971, w1=13.60235744437349\n",
      "Gradient Descent(6013/9999): loss=2.2370510132486014, w0=72.84374999999972, w1=13.43218161980527\n",
      "Gradient Descent(6014/9999): loss=2.148503128706187, w0=72.95624999999971, w1=13.426239756042435\n",
      "Gradient Descent(6015/9999): loss=2.4953719724456587, w0=73.06874999999971, w1=13.364889939950208\n",
      "Gradient Descent(6016/9999): loss=2.4833520426793587, w0=72.95624999999971, w1=13.201375248912125\n",
      "Gradient Descent(6017/9999): loss=1.783736782390994, w0=72.95624999999971, w1=13.077039957653128\n",
      "Gradient Descent(6018/9999): loss=2.4458490483673616, w0=72.78749999999971, w1=13.242890072910939\n",
      "Gradient Descent(6019/9999): loss=1.8999924847796628, w0=72.84374999999972, w1=12.948799070532116\n",
      "Gradient Descent(6020/9999): loss=1.8377459546954287, w0=72.78749999999971, w1=13.035596983624236\n",
      "Gradient Descent(6021/9999): loss=2.1426446038859264, w0=72.95624999999971, w1=13.260089382683475\n",
      "Gradient Descent(6022/9999): loss=2.1810614310493626, w0=72.84374999999972, w1=13.469592532721625\n",
      "Gradient Descent(6023/9999): loss=2.4576663155318483, w0=73.23749999999971, w1=13.440046889146927\n",
      "Gradient Descent(6024/9999): loss=2.454390479442015, w0=73.63124999999971, w1=13.61943748533251\n",
      "Gradient Descent(6025/9999): loss=2.299171348091507, w0=73.23749999999971, w1=13.28098683565302\n",
      "Gradient Descent(6026/9999): loss=1.899430721689382, w0=73.40624999999972, w1=13.477247608107717\n",
      "Gradient Descent(6027/9999): loss=2.4380038720033537, w0=73.46249999999972, w1=13.387188254514253\n",
      "Gradient Descent(6028/9999): loss=1.966520747808323, w0=73.46249999999972, w1=13.874121616120554\n",
      "Gradient Descent(6029/9999): loss=2.1368647850562903, w0=73.51874999999973, w1=14.040407486213685\n",
      "Gradient Descent(6030/9999): loss=2.151325005829187, w0=73.40624999999973, w1=13.691596988907888\n",
      "Gradient Descent(6031/9999): loss=2.0005414067349094, w0=73.46249999999974, w1=13.46633757432456\n",
      "Gradient Descent(6032/9999): loss=2.4360010001558847, w0=73.51874999999974, w1=13.578027995632896\n",
      "Gradient Descent(6033/9999): loss=2.0730393060327064, w0=73.51874999999974, w1=13.869864175155493\n",
      "Gradient Descent(6034/9999): loss=2.152767956883153, w0=73.46249999999974, w1=13.51530689560311\n",
      "Gradient Descent(6035/9999): loss=2.41348926226061, w0=73.06874999999974, w1=13.56254358589661\n",
      "Gradient Descent(6036/9999): loss=2.5491683678104042, w0=73.01249999999973, w1=13.35634476427358\n",
      "Gradient Descent(6037/9999): loss=2.1463115196086857, w0=73.23749999999973, w1=13.634335597794461\n",
      "Gradient Descent(6038/9999): loss=2.0951877279134488, w0=73.29374999999973, w1=13.66293861638179\n",
      "Gradient Descent(6039/9999): loss=2.4101973304118323, w0=73.57499999999973, w1=13.718039969658017\n",
      "Gradient Descent(6040/9999): loss=2.0670981619130604, w0=73.63124999999974, w1=13.81370024458895\n",
      "Gradient Descent(6041/9999): loss=2.0330222625011123, w0=73.40624999999974, w1=14.104659307828152\n",
      "Gradient Descent(6042/9999): loss=2.5362035536039595, w0=73.40624999999974, w1=14.142952321753267\n",
      "Gradient Descent(6043/9999): loss=2.2894212184377087, w0=73.51874999999974, w1=13.878958233594654\n",
      "Gradient Descent(6044/9999): loss=2.9435221475789657, w0=73.63124999999974, w1=13.887628738123777\n",
      "Gradient Descent(6045/9999): loss=2.1897175036074907, w0=73.63124999999974, w1=13.785192796660978\n",
      "Gradient Descent(6046/9999): loss=2.067114171444494, w0=73.40624999999974, w1=13.679077035302415\n",
      "Gradient Descent(6047/9999): loss=2.807950215763322, w0=73.46249999999975, w1=13.528326696189426\n",
      "Gradient Descent(6048/9999): loss=1.7498900226253653, w0=73.68749999999974, w1=13.376478805157097\n",
      "Gradient Descent(6049/9999): loss=2.0061309771217593, w0=73.68749999999974, w1=13.410401124492749\n",
      "Gradient Descent(6050/9999): loss=2.3431253195905857, w0=73.46249999999975, w1=13.396266879787492\n",
      "Gradient Descent(6051/9999): loss=2.1378500868179073, w0=73.40624999999974, w1=13.180513755392605\n",
      "Gradient Descent(6052/9999): loss=2.1764239568545163, w0=73.34999999999974, w1=13.39251490224843\n",
      "Gradient Descent(6053/9999): loss=2.048974280325223, w0=73.18124999999974, w1=13.505896499400855\n",
      "Gradient Descent(6054/9999): loss=2.4819655249384764, w0=73.34999999999974, w1=13.78753730684173\n",
      "Gradient Descent(6055/9999): loss=1.8287434903786264, w0=73.46249999999974, w1=13.564176646093598\n",
      "Gradient Descent(6056/9999): loss=2.293440414576029, w0=73.12499999999973, w1=13.223545810341662\n",
      "Gradient Descent(6057/9999): loss=2.187043239540931, w0=73.34999999999972, w1=13.145788112361144\n",
      "Gradient Descent(6058/9999): loss=2.4418831517698667, w0=73.06874999999972, w1=13.20157515708961\n",
      "Gradient Descent(6059/9999): loss=2.5599451660451265, w0=72.95624999999973, w1=13.20590938063167\n",
      "Gradient Descent(6060/9999): loss=2.3897623908078427, w0=73.18124999999972, w1=13.329037035699404\n",
      "Gradient Descent(6061/9999): loss=2.379124362481287, w0=72.84374999999972, w1=13.525599025590259\n",
      "Gradient Descent(6062/9999): loss=2.205317263773976, w0=73.23749999999971, w1=13.457626613013945\n",
      "Gradient Descent(6063/9999): loss=2.254454134149852, w0=73.40624999999972, w1=13.357873836082913\n",
      "Gradient Descent(6064/9999): loss=1.9796674458570493, w0=73.57499999999972, w1=13.190934017936847\n",
      "Gradient Descent(6065/9999): loss=2.179051258023589, w0=73.74374999999972, w1=12.934244148354338\n",
      "Gradient Descent(6066/9999): loss=2.073392959723014, w0=73.63124999999972, w1=12.80899251520823\n",
      "Gradient Descent(6067/9999): loss=1.8347578241964708, w0=73.51874999999973, w1=12.895906142479202\n",
      "Gradient Descent(6068/9999): loss=1.7250409923607575, w0=73.34999999999972, w1=12.986766568441455\n",
      "Gradient Descent(6069/9999): loss=2.3589942568657376, w0=73.57499999999972, w1=12.859564691248805\n",
      "Gradient Descent(6070/9999): loss=2.1108246557157417, w0=73.46249999999972, w1=12.792678871515502\n",
      "Gradient Descent(6071/9999): loss=3.143916617802908, w0=73.34999999999972, w1=12.921975159356625\n",
      "Gradient Descent(6072/9999): loss=2.012377085956083, w0=73.51874999999973, w1=12.998560457183597\n",
      "Gradient Descent(6073/9999): loss=1.929150149134554, w0=73.46249999999972, w1=13.202295781816652\n",
      "Gradient Descent(6074/9999): loss=2.5401516420378814, w0=73.40624999999972, w1=13.117860815205153\n",
      "Gradient Descent(6075/9999): loss=2.408435224444154, w0=73.63124999999971, w1=13.050393589438105\n",
      "Gradient Descent(6076/9999): loss=1.8577916485410455, w0=73.8562499999997, w1=13.41767406092944\n",
      "Gradient Descent(6077/9999): loss=2.3720340405587783, w0=73.74374999999971, w1=13.307583515781321\n",
      "Gradient Descent(6078/9999): loss=2.039824918114835, w0=73.63124999999971, w1=13.215748209771025\n",
      "Gradient Descent(6079/9999): loss=2.4995377174957647, w0=73.5749999999997, w1=13.408994420943008\n",
      "Gradient Descent(6080/9999): loss=1.670692011501009, w0=73.2937499999997, w1=13.428586904771306\n",
      "Gradient Descent(6081/9999): loss=2.358598086299808, w0=72.78749999999971, w1=13.67715882893446\n",
      "Gradient Descent(6082/9999): loss=1.9532795047680043, w0=72.7312499999997, w1=13.652134020634454\n",
      "Gradient Descent(6083/9999): loss=2.402595082455401, w0=72.7312499999997, w1=13.934302931383353\n",
      "Gradient Descent(6084/9999): loss=2.21406508501288, w0=72.7312499999997, w1=13.781965674403224\n",
      "Gradient Descent(6085/9999): loss=2.3171791182680432, w0=72.5624999999997, w1=13.733559222170106\n",
      "Gradient Descent(6086/9999): loss=2.3730233712501025, w0=72.61874999999971, w1=13.500222627033036\n",
      "Gradient Descent(6087/9999): loss=1.995330233686962, w0=72.39374999999971, w1=13.638794357146487\n",
      "Gradient Descent(6088/9999): loss=2.6275663977206847, w0=72.28124999999972, w1=13.701629404480338\n",
      "Gradient Descent(6089/9999): loss=2.463512431814163, w0=72.67499999999971, w1=13.576754650711074\n",
      "Gradient Descent(6090/9999): loss=2.581290049122499, w0=72.73124999999972, w1=13.534993614969299\n",
      "Gradient Descent(6091/9999): loss=1.8433411481752366, w0=72.39374999999971, w1=13.575341253526672\n",
      "Gradient Descent(6092/9999): loss=2.155618509280998, w0=72.50624999999971, w1=13.606767900985139\n",
      "Gradient Descent(6093/9999): loss=2.631792725187574, w0=72.7312499999997, w1=13.684552077135047\n",
      "Gradient Descent(6094/9999): loss=2.0626253144871107, w0=73.0124999999997, w1=13.302966553064023\n",
      "Gradient Descent(6095/9999): loss=2.446318362212682, w0=73.0124999999997, w1=13.13918275355913\n",
      "Gradient Descent(6096/9999): loss=2.4430040897375873, w0=73.2374999999997, w1=13.229047919993702\n",
      "Gradient Descent(6097/9999): loss=2.219315850001387, w0=73.4062499999997, w1=13.297814123001142\n",
      "Gradient Descent(6098/9999): loss=2.4742216226387894, w0=73.46249999999971, w1=13.558341018132186\n",
      "Gradient Descent(6099/9999): loss=2.397747090573961, w0=73.2937499999997, w1=13.569764018450872\n",
      "Gradient Descent(6100/9999): loss=2.1720600731021076, w0=73.4062499999997, w1=13.70963542030998\n",
      "Gradient Descent(6101/9999): loss=1.9277632065998511, w0=73.5187499999997, w1=13.606257122584614\n",
      "Gradient Descent(6102/9999): loss=2.315126578724837, w0=73.4062499999997, w1=13.730680176066885\n",
      "Gradient Descent(6103/9999): loss=1.7288890317881158, w0=73.2374999999997, w1=13.742067054843703\n",
      "Gradient Descent(6104/9999): loss=1.8690631195224576, w0=73.2937499999997, w1=13.80458480685083\n",
      "Gradient Descent(6105/9999): loss=2.619648431004663, w0=73.5749999999997, w1=13.786482280773676\n",
      "Gradient Descent(6106/9999): loss=2.159165652154414, w0=73.6874999999997, w1=13.698595814756281\n",
      "Gradient Descent(6107/9999): loss=2.2448212441873467, w0=73.5749999999997, w1=13.511766260145235\n",
      "Gradient Descent(6108/9999): loss=1.8655809027399792, w0=73.34999999999971, w1=13.208434634797069\n",
      "Gradient Descent(6109/9999): loss=2.538868498370039, w0=73.23749999999971, w1=13.393973124988317\n",
      "Gradient Descent(6110/9999): loss=2.104036797445336, w0=73.06874999999971, w1=13.237794076990577\n",
      "Gradient Descent(6111/9999): loss=2.0243881290184778, w0=73.06874999999971, w1=13.275004595608149\n",
      "Gradient Descent(6112/9999): loss=2.4477262929811436, w0=72.89999999999971, w1=13.602217370883729\n",
      "Gradient Descent(6113/9999): loss=1.5440881711327963, w0=72.8437499999997, w1=13.367829362238034\n",
      "Gradient Descent(6114/9999): loss=1.917415579374332, w0=72.7874999999997, w1=13.595452559471894\n",
      "Gradient Descent(6115/9999): loss=2.4384616181535246, w0=72.8437499999997, w1=13.653814591253113\n",
      "Gradient Descent(6116/9999): loss=1.4581489638138572, w0=72.7874999999997, w1=13.684335238491942\n",
      "Gradient Descent(6117/9999): loss=2.1035774039023063, w0=72.8437499999997, w1=13.752115443397814\n",
      "Gradient Descent(6118/9999): loss=2.6369168949110016, w0=73.0124999999997, w1=13.597186781452201\n",
      "Gradient Descent(6119/9999): loss=1.9748869774207025, w0=72.6749999999997, w1=13.817302785750336\n",
      "Gradient Descent(6120/9999): loss=1.6973207402403012, w0=72.3937499999997, w1=13.832895831415374\n",
      "Gradient Descent(6121/9999): loss=2.420977486207343, w0=72.5624999999997, w1=13.932833304786397\n",
      "Gradient Descent(6122/9999): loss=2.7990924191374473, w0=72.6749999999997, w1=13.797521013385957\n",
      "Gradient Descent(6123/9999): loss=2.3474863847860337, w0=72.4499999999997, w1=13.55998380612826\n",
      "Gradient Descent(6124/9999): loss=2.3487510536597584, w0=72.4499999999997, w1=13.609554454952347\n",
      "Gradient Descent(6125/9999): loss=2.257821399562006, w0=72.3937499999997, w1=13.59738076228007\n",
      "Gradient Descent(6126/9999): loss=2.0241913278027597, w0=72.5062499999997, w1=13.566974920191052\n",
      "Gradient Descent(6127/9999): loss=1.9928117032708372, w0=72.5624999999997, w1=13.809053711166895\n",
      "Gradient Descent(6128/9999): loss=2.2558986499363005, w0=72.4499999999997, w1=13.360228647458689\n",
      "Gradient Descent(6129/9999): loss=2.248313257947078, w0=72.3937499999997, w1=13.303878386694752\n",
      "Gradient Descent(6130/9999): loss=2.595367446348875, w0=72.4499999999997, w1=13.499015485306765\n",
      "Gradient Descent(6131/9999): loss=2.4989147291631193, w0=72.50624999999971, w1=13.541048766917454\n",
      "Gradient Descent(6132/9999): loss=1.49071501457913, w0=72.61874999999971, w1=13.559284561874401\n",
      "Gradient Descent(6133/9999): loss=1.9997360810298004, w0=72.39374999999971, w1=13.566522579097883\n",
      "Gradient Descent(6134/9999): loss=2.6199784582572594, w0=72.56249999999972, w1=13.491396485832253\n",
      "Gradient Descent(6135/9999): loss=2.052605614666951, w0=72.56249999999972, w1=13.476736497595539\n",
      "Gradient Descent(6136/9999): loss=2.2565842183163745, w0=72.61874999999972, w1=13.539329191027047\n",
      "Gradient Descent(6137/9999): loss=2.2488109954381406, w0=72.67499999999973, w1=13.696111932948815\n",
      "Gradient Descent(6138/9999): loss=2.7810701825648128, w0=72.78749999999972, w1=13.859546242394673\n",
      "Gradient Descent(6139/9999): loss=2.778232509281941, w0=73.01249999999972, w1=13.78094306740372\n",
      "Gradient Descent(6140/9999): loss=2.3540288566727634, w0=73.18124999999972, w1=13.513642957001672\n",
      "Gradient Descent(6141/9999): loss=2.3274772716728687, w0=72.84374999999972, w1=13.512421362945233\n",
      "Gradient Descent(6142/9999): loss=2.0130257920481047, w0=72.73124999999972, w1=13.517836164885086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6143/9999): loss=1.8705167179697348, w0=72.95624999999971, w1=13.542979925320108\n",
      "Gradient Descent(6144/9999): loss=2.281937126972453, w0=73.01249999999972, w1=13.747668266842002\n",
      "Gradient Descent(6145/9999): loss=2.023976693571975, w0=72.78749999999972, w1=13.88253829315354\n",
      "Gradient Descent(6146/9999): loss=2.4226159876753792, w0=72.78749999999972, w1=13.592348346157557\n",
      "Gradient Descent(6147/9999): loss=2.5413275883962916, w0=72.95624999999973, w1=13.54038400313779\n",
      "Gradient Descent(6148/9999): loss=2.621878337236064, w0=73.06874999999972, w1=13.58716847787067\n",
      "Gradient Descent(6149/9999): loss=2.4813431127785694, w0=73.06874999999972, w1=13.309417996003312\n",
      "Gradient Descent(6150/9999): loss=2.8234288530807614, w0=72.89999999999972, w1=13.365033160280262\n",
      "Gradient Descent(6151/9999): loss=2.355548095551039, w0=72.84374999999972, w1=13.351150039402798\n",
      "Gradient Descent(6152/9999): loss=2.0481241471016824, w0=72.89999999999972, w1=13.267586632823186\n",
      "Gradient Descent(6153/9999): loss=2.3401279189724784, w0=73.12499999999972, w1=13.264662206558988\n",
      "Gradient Descent(6154/9999): loss=1.8229281526419916, w0=73.29374999999972, w1=13.324889048462245\n",
      "Gradient Descent(6155/9999): loss=2.4688418317987786, w0=73.29374999999972, w1=13.1498517952832\n",
      "Gradient Descent(6156/9999): loss=1.9346473016781467, w0=73.40624999999972, w1=13.193732997645132\n",
      "Gradient Descent(6157/9999): loss=2.2338988889500877, w0=73.46249999999972, w1=13.295007074071165\n",
      "Gradient Descent(6158/9999): loss=2.22876729515633, w0=73.40624999999972, w1=13.505915311206058\n",
      "Gradient Descent(6159/9999): loss=2.5861339477582908, w0=73.34999999999971, w1=13.19328916005268\n",
      "Gradient Descent(6160/9999): loss=2.4300801931158524, w0=73.18124999999971, w1=13.637908606953683\n",
      "Gradient Descent(6161/9999): loss=1.74882042736662, w0=72.89999999999971, w1=13.573533147652563\n",
      "Gradient Descent(6162/9999): loss=1.7478362166892023, w0=72.95624999999971, w1=13.900468357232937\n",
      "Gradient Descent(6163/9999): loss=2.709196897439135, w0=73.18124999999971, w1=13.797187399790046\n",
      "Gradient Descent(6164/9999): loss=2.1882139234616447, w0=73.2937499999997, w1=13.78868277993091\n",
      "Gradient Descent(6165/9999): loss=2.5421365198986363, w0=73.4062499999997, w1=13.72388362415632\n",
      "Gradient Descent(6166/9999): loss=2.0866108216377848, w0=73.1249999999997, w1=13.727927473654255\n",
      "Gradient Descent(6167/9999): loss=2.069870672058395, w0=73.46249999999971, w1=13.912782730903078\n",
      "Gradient Descent(6168/9999): loss=2.469415561032428, w0=73.6874999999997, w1=13.80339954373818\n",
      "Gradient Descent(6169/9999): loss=2.1496617748555007, w0=73.6874999999997, w1=14.003836000711711\n",
      "Gradient Descent(6170/9999): loss=1.6770078159575053, w0=73.2937499999997, w1=14.135295616034373\n",
      "Gradient Descent(6171/9999): loss=2.111491027309796, w0=73.2374999999997, w1=14.125676297607\n",
      "Gradient Descent(6172/9999): loss=1.495962260404035, w0=73.0687499999997, w1=14.179998316620669\n",
      "Gradient Descent(6173/9999): loss=2.2417843396633677, w0=73.01249999999969, w1=14.081920453911295\n",
      "Gradient Descent(6174/9999): loss=2.1070168822666915, w0=73.12499999999969, w1=14.106338287819428\n",
      "Gradient Descent(6175/9999): loss=2.5982017135857944, w0=72.95624999999968, w1=14.05797556687568\n",
      "Gradient Descent(6176/9999): loss=1.9382864993414444, w0=72.84374999999969, w1=13.837858377770358\n",
      "Gradient Descent(6177/9999): loss=1.887436846129629, w0=73.23749999999968, w1=13.847668736941737\n",
      "Gradient Descent(6178/9999): loss=2.2635744132333917, w0=73.34999999999968, w1=13.561122222661018\n",
      "Gradient Descent(6179/9999): loss=1.831354233931729, w0=73.23749999999968, w1=13.432388769486465\n",
      "Gradient Descent(6180/9999): loss=2.005378672157458, w0=73.40624999999969, w1=13.250506330284788\n",
      "Gradient Descent(6181/9999): loss=2.57701923264861, w0=73.23749999999968, w1=13.550101084878833\n",
      "Gradient Descent(6182/9999): loss=2.2211339093263387, w0=73.18124999999968, w1=13.61488715507984\n",
      "Gradient Descent(6183/9999): loss=2.3331431994012677, w0=72.95624999999968, w1=13.371097993944984\n",
      "Gradient Descent(6184/9999): loss=1.800175120813782, w0=72.67499999999968, w1=13.218604729918889\n",
      "Gradient Descent(6185/9999): loss=2.1711773514273083, w0=72.84374999999969, w1=13.403926416148563\n",
      "Gradient Descent(6186/9999): loss=1.7885656338550247, w0=73.01249999999969, w1=13.643095096397994\n",
      "Gradient Descent(6187/9999): loss=2.2226850680408266, w0=73.01249999999969, w1=13.529357725312153\n",
      "Gradient Descent(6188/9999): loss=2.2896096245148185, w0=73.0687499999997, w1=13.682076528134973\n",
      "Gradient Descent(6189/9999): loss=2.2033149508989105, w0=73.0687499999997, w1=13.766384372448346\n",
      "Gradient Descent(6190/9999): loss=2.1805127403184383, w0=73.1812499999997, w1=13.671973775767675\n",
      "Gradient Descent(6191/9999): loss=2.7228394412362773, w0=73.12499999999969, w1=13.52337466206972\n",
      "Gradient Descent(6192/9999): loss=2.506811609075843, w0=73.06874999999968, w1=13.38420152109717\n",
      "Gradient Descent(6193/9999): loss=2.349791351798814, w0=72.95624999999968, w1=13.300372493487968\n",
      "Gradient Descent(6194/9999): loss=2.0882591820511607, w0=72.78749999999968, w1=13.145832656690738\n",
      "Gradient Descent(6195/9999): loss=2.0776397993666, w0=72.67499999999968, w1=13.274904717569836\n",
      "Gradient Descent(6196/9999): loss=1.957114057225646, w0=72.95624999999968, w1=13.373246782163479\n",
      "Gradient Descent(6197/9999): loss=2.199964462497948, w0=73.06874999999968, w1=13.626761619440867\n",
      "Gradient Descent(6198/9999): loss=1.9437573825404946, w0=73.01249999999968, w1=13.453915900361437\n",
      "Gradient Descent(6199/9999): loss=2.3169404779227882, w0=72.84374999999967, w1=13.643119792689278\n",
      "Gradient Descent(6200/9999): loss=2.7118146374441867, w0=73.06874999999967, w1=13.493462223021501\n",
      "Gradient Descent(6201/9999): loss=1.792686590627296, w0=73.18124999999966, w1=13.33960263558747\n",
      "Gradient Descent(6202/9999): loss=2.4579932869070538, w0=73.34999999999967, w1=13.26587498752263\n",
      "Gradient Descent(6203/9999): loss=1.7964049052941742, w0=73.40624999999967, w1=13.217049659653352\n",
      "Gradient Descent(6204/9999): loss=2.1586853404386446, w0=73.18124999999968, w1=13.220401877869625\n",
      "Gradient Descent(6205/9999): loss=1.714883643557424, w0=72.95624999999968, w1=13.337331079485882\n",
      "Gradient Descent(6206/9999): loss=2.131040021897476, w0=73.12499999999969, w1=13.38046345154943\n",
      "Gradient Descent(6207/9999): loss=2.189426770529403, w0=73.40624999999969, w1=13.22251779744307\n",
      "Gradient Descent(6208/9999): loss=2.004915537223191, w0=73.1812499999997, w1=13.053421097661374\n",
      "Gradient Descent(6209/9999): loss=1.7503347702008916, w0=73.01249999999969, w1=13.162867813338364\n",
      "Gradient Descent(6210/9999): loss=2.1029109043111296, w0=72.84374999999969, w1=13.181671781381656\n",
      "Gradient Descent(6211/9999): loss=2.6000478954511825, w0=73.29374999999969, w1=13.543408628244993\n",
      "Gradient Descent(6212/9999): loss=1.9957205774491653, w0=73.1812499999997, w1=13.507615601223883\n",
      "Gradient Descent(6213/9999): loss=2.7872737881811833, w0=73.01249999999969, w1=13.756133311073452\n",
      "Gradient Descent(6214/9999): loss=2.004922157947612, w0=73.0687499999997, w1=13.563394758824922\n",
      "Gradient Descent(6215/9999): loss=1.9198085135805545, w0=73.0687499999997, w1=13.276205901351043\n",
      "Gradient Descent(6216/9999): loss=2.334322408627869, w0=73.0687499999997, w1=13.412455929000247\n",
      "Gradient Descent(6217/9999): loss=2.3327835997049036, w0=73.2374999999997, w1=13.37251699074809\n",
      "Gradient Descent(6218/9999): loss=2.2146448612943885, w0=73.2374999999997, w1=13.455261686823961\n",
      "Gradient Descent(6219/9999): loss=2.3202851064805916, w0=73.4062499999997, w1=13.23512224958615\n",
      "Gradient Descent(6220/9999): loss=2.9101726707980413, w0=73.46249999999971, w1=13.266221378041928\n",
      "Gradient Descent(6221/9999): loss=2.202747101914805, w0=73.18124999999971, w1=13.35991819164848\n",
      "Gradient Descent(6222/9999): loss=2.427186970925498, w0=73.1249999999997, w1=13.341096123006693\n",
      "Gradient Descent(6223/9999): loss=2.1640803049069324, w0=73.0687499999997, w1=13.42675129540613\n",
      "Gradient Descent(6224/9999): loss=2.606124390968029, w0=73.1249999999997, w1=13.288045865598617\n",
      "Gradient Descent(6225/9999): loss=1.836991017556666, w0=73.1249999999997, w1=12.962885269034246\n",
      "Gradient Descent(6226/9999): loss=2.5159081923432898, w0=73.0124999999997, w1=12.761851659826643\n",
      "Gradient Descent(6227/9999): loss=2.559955703880097, w0=72.78749999999971, w1=13.005539446229518\n",
      "Gradient Descent(6228/9999): loss=1.7839960463446844, w0=72.67499999999971, w1=12.922537610846486\n",
      "Gradient Descent(6229/9999): loss=2.4373153320576284, w0=72.73124999999972, w1=12.649338676414411\n",
      "Gradient Descent(6230/9999): loss=2.1237289313249974, w0=72.56249999999972, w1=12.881929917883992\n",
      "Gradient Descent(6231/9999): loss=1.9208214377692072, w0=72.84374999999972, w1=12.772229325629802\n",
      "Gradient Descent(6232/9999): loss=1.8350478061922173, w0=73.06874999999971, w1=12.997231804002912\n",
      "Gradient Descent(6233/9999): loss=2.4793057993696443, w0=72.95624999999971, w1=13.082442538826307\n",
      "Gradient Descent(6234/9999): loss=1.7948014584778622, w0=72.84374999999972, w1=13.146511628993125\n",
      "Gradient Descent(6235/9999): loss=2.2064987979923325, w0=72.89999999999972, w1=13.003197014498069\n",
      "Gradient Descent(6236/9999): loss=2.04991476509327, w0=73.06874999999972, w1=13.21726159503069\n",
      "Gradient Descent(6237/9999): loss=1.950136333286344, w0=72.84374999999973, w1=13.086287944873655\n",
      "Gradient Descent(6238/9999): loss=2.192320142906929, w0=72.67499999999973, w1=13.32283275246297\n",
      "Gradient Descent(6239/9999): loss=1.974521781328404, w0=72.73124999999973, w1=13.290471767530109\n",
      "Gradient Descent(6240/9999): loss=2.4035102714842678, w0=72.67499999999973, w1=13.52868402375958\n",
      "Gradient Descent(6241/9999): loss=2.5544602195179684, w0=72.73124999999973, w1=13.225268376872723\n",
      "Gradient Descent(6242/9999): loss=2.2306713039952735, w0=72.50624999999974, w1=13.341121360422115\n",
      "Gradient Descent(6243/9999): loss=2.2187709465156296, w0=72.50624999999974, w1=13.16485582951669\n",
      "Gradient Descent(6244/9999): loss=2.4868972287770736, w0=72.39374999999974, w1=13.13393651036016\n",
      "Gradient Descent(6245/9999): loss=2.321193796235938, w0=72.73124999999975, w1=13.146246385119882\n",
      "Gradient Descent(6246/9999): loss=2.373642629521967, w0=72.67499999999974, w1=13.17921776672737\n",
      "Gradient Descent(6247/9999): loss=2.6486755863537255, w0=72.78749999999974, w1=13.116321857512489\n",
      "Gradient Descent(6248/9999): loss=2.1185149448856375, w0=72.84374999999974, w1=13.127084963070676\n",
      "Gradient Descent(6249/9999): loss=2.2673274666168712, w0=73.12499999999974, w1=13.385746000945852\n",
      "Gradient Descent(6250/9999): loss=2.633690305704248, w0=72.89999999999975, w1=13.428854347332036\n",
      "Gradient Descent(6251/9999): loss=2.561662244488595, w0=72.78749999999975, w1=13.40395830239338\n",
      "Gradient Descent(6252/9999): loss=1.821687852370315, w0=72.61874999999975, w1=13.302764419170524\n",
      "Gradient Descent(6253/9999): loss=2.492835856387602, w0=72.78749999999975, w1=13.219132767836534\n",
      "Gradient Descent(6254/9999): loss=1.9727769345507538, w0=72.89999999999975, w1=13.031167970402048\n",
      "Gradient Descent(6255/9999): loss=1.8754328635235142, w0=73.23749999999976, w1=13.281634271950553\n",
      "Gradient Descent(6256/9999): loss=2.3140953430248548, w0=73.29374999999976, w1=13.32916492537058\n",
      "Gradient Descent(6257/9999): loss=2.0052848919349717, w0=73.46249999999976, w1=13.109755544948525\n",
      "Gradient Descent(6258/9999): loss=2.27488437425461, w0=73.74374999999976, w1=13.165895430268954\n",
      "Gradient Descent(6259/9999): loss=1.9090162224617206, w0=73.63124999999977, w1=13.348809772193407\n",
      "Gradient Descent(6260/9999): loss=1.6924776846526053, w0=73.57499999999976, w1=13.24075046603235\n",
      "Gradient Descent(6261/9999): loss=2.2373313279538634, w0=73.63124999999977, w1=13.384037961644793\n",
      "Gradient Descent(6262/9999): loss=2.538754572206046, w0=73.51874999999977, w1=13.34900463819476\n",
      "Gradient Descent(6263/9999): loss=1.6410048034347902, w0=73.34999999999977, w1=13.28929630782424\n",
      "Gradient Descent(6264/9999): loss=2.216517360594545, w0=73.23749999999977, w1=13.34091369631963\n",
      "Gradient Descent(6265/9999): loss=1.7600397840098008, w0=73.06874999999977, w1=13.46017811834285\n",
      "Gradient Descent(6266/9999): loss=2.0156001055465413, w0=73.18124999999976, w1=13.226549914954006\n",
      "Gradient Descent(6267/9999): loss=1.7657230171451908, w0=73.23749999999977, w1=13.38883689515521\n",
      "Gradient Descent(6268/9999): loss=1.9732172632335068, w0=73.12499999999977, w1=13.361221695952397\n",
      "Gradient Descent(6269/9999): loss=2.067033848658836, w0=73.29374999999978, w1=13.280527389881712\n",
      "Gradient Descent(6270/9999): loss=2.078870477230191, w0=73.29374999999978, w1=13.136329249976981\n",
      "Gradient Descent(6271/9999): loss=1.6380340101123032, w0=73.12499999999977, w1=13.424427318342403\n",
      "Gradient Descent(6272/9999): loss=2.4652667159374992, w0=73.23749999999977, w1=13.329277745851405\n",
      "Gradient Descent(6273/9999): loss=1.7760445564282896, w0=73.40624999999977, w1=13.508244650729932\n",
      "Gradient Descent(6274/9999): loss=2.5573007277441158, w0=73.23749999999977, w1=13.59382327434137\n",
      "Gradient Descent(6275/9999): loss=1.9267413232013177, w0=73.12499999999977, w1=13.44978126606293\n",
      "Gradient Descent(6276/9999): loss=1.849354455219125, w0=73.29374999999978, w1=13.555577609086653\n",
      "Gradient Descent(6277/9999): loss=2.0111484293582103, w0=73.29374999999978, w1=13.77200096229789\n",
      "Gradient Descent(6278/9999): loss=2.434580717674862, w0=73.23749999999977, w1=13.934604921996609\n",
      "Gradient Descent(6279/9999): loss=2.053209033594098, w0=73.34999999999977, w1=13.660402355433483\n",
      "Gradient Descent(6280/9999): loss=2.2673165570626734, w0=73.40624999999977, w1=13.366019102590467\n",
      "Gradient Descent(6281/9999): loss=1.8722040762150058, w0=73.40624999999977, w1=13.135765470889218\n",
      "Gradient Descent(6282/9999): loss=1.9571747563101, w0=73.57499999999978, w1=13.532059677617921\n",
      "Gradient Descent(6283/9999): loss=2.098864402085909, w0=73.46249999999978, w1=13.321237688153428\n",
      "Gradient Descent(6284/9999): loss=1.9659584627155622, w0=73.57499999999978, w1=13.20931607735723\n",
      "Gradient Descent(6285/9999): loss=1.8521123574412353, w0=73.79999999999977, w1=13.295010535298582\n",
      "Gradient Descent(6286/9999): loss=1.9531953459983904, w0=73.51874999999977, w1=13.184698576831964\n",
      "Gradient Descent(6287/9999): loss=1.8946318936777042, w0=73.68749999999977, w1=13.482627369316237\n",
      "Gradient Descent(6288/9999): loss=1.7760900284473549, w0=73.34999999999977, w1=13.40673581085268\n",
      "Gradient Descent(6289/9999): loss=2.412578089169578, w0=73.34999999999977, w1=13.389353392930376\n",
      "Gradient Descent(6290/9999): loss=2.266106705623516, w0=73.51874999999977, w1=13.432392329984536\n",
      "Gradient Descent(6291/9999): loss=2.207393855051985, w0=73.63124999999977, w1=13.692328393753458\n",
      "Gradient Descent(6292/9999): loss=2.2905940173520714, w0=73.34999999999977, w1=13.67364631187583\n",
      "Gradient Descent(6293/9999): loss=2.142068050065205, w0=73.23749999999977, w1=13.574080740765552\n",
      "Gradient Descent(6294/9999): loss=2.1336158235262435, w0=73.23749999999977, w1=13.730053970358838\n",
      "Gradient Descent(6295/9999): loss=1.8756551457726658, w0=73.12499999999977, w1=13.512522594936488\n",
      "Gradient Descent(6296/9999): loss=2.3993233330216017, w0=73.34999999999977, w1=13.677369995681643\n",
      "Gradient Descent(6297/9999): loss=1.9638994632358606, w0=73.01249999999976, w1=13.89228956005374\n",
      "Gradient Descent(6298/9999): loss=2.0000080936756657, w0=73.12499999999976, w1=13.952455010542195\n",
      "Gradient Descent(6299/9999): loss=2.1246685906178184, w0=72.73124999999976, w1=13.479897768087081\n",
      "Gradient Descent(6300/9999): loss=2.259273495319925, w0=72.73124999999976, w1=13.74519440736292\n",
      "Gradient Descent(6301/9999): loss=2.0458062246231923, w0=72.78749999999977, w1=13.716500343702673\n",
      "Gradient Descent(6302/9999): loss=2.0014322642237317, w0=72.61874999999976, w1=13.748944509084277\n",
      "Gradient Descent(6303/9999): loss=2.2147104072048407, w0=72.50624999999977, w1=13.673373462224157\n",
      "Gradient Descent(6304/9999): loss=2.6308635018362914, w0=72.89999999999976, w1=13.665958086510273\n",
      "Gradient Descent(6305/9999): loss=2.1737009084205625, w0=72.84374999999976, w1=13.532236369386402\n",
      "Gradient Descent(6306/9999): loss=2.2731891059643576, w0=72.89999999999976, w1=13.751810784372001\n",
      "Gradient Descent(6307/9999): loss=2.1202284494129247, w0=72.95624999999977, w1=14.101683296056905\n",
      "Gradient Descent(6308/9999): loss=2.2674893747386387, w0=72.89999999999976, w1=13.97561135259991\n",
      "Gradient Descent(6309/9999): loss=2.405314033530866, w0=72.84374999999976, w1=13.771063360669952\n",
      "Gradient Descent(6310/9999): loss=1.9194978522119002, w0=73.18124999999976, w1=13.979143155068915\n",
      "Gradient Descent(6311/9999): loss=2.187480774153839, w0=73.29374999999976, w1=13.920470122793649\n",
      "Gradient Descent(6312/9999): loss=1.8421206761373892, w0=73.23749999999976, w1=13.679602869136703\n",
      "Gradient Descent(6313/9999): loss=2.675245338594597, w0=73.18124999999975, w1=13.562222931746009\n",
      "Gradient Descent(6314/9999): loss=2.187984652380854, w0=73.23749999999976, w1=13.52009013615292\n",
      "Gradient Descent(6315/9999): loss=2.233290556900706, w0=72.95624999999976, w1=13.745727633373408\n",
      "Gradient Descent(6316/9999): loss=2.290318739805352, w0=73.18124999999975, w1=13.691056314272227\n",
      "Gradient Descent(6317/9999): loss=2.515725792749277, w0=73.40624999999974, w1=13.489542359478133\n",
      "Gradient Descent(6318/9999): loss=2.2482934601408826, w0=73.46249999999975, w1=13.304448776317743\n",
      "Gradient Descent(6319/9999): loss=2.717678434478291, w0=73.29374999999975, w1=13.287759489112034\n",
      "Gradient Descent(6320/9999): loss=2.448532321110738, w0=73.06874999999975, w1=13.343849484153155\n",
      "Gradient Descent(6321/9999): loss=2.2610767151987172, w0=72.95624999999976, w1=13.283964498947837\n",
      "Gradient Descent(6322/9999): loss=1.856793580570267, w0=73.34999999999975, w1=13.399410866783716\n",
      "Gradient Descent(6323/9999): loss=2.4408461781352173, w0=73.29374999999975, w1=13.319811750975393\n",
      "Gradient Descent(6324/9999): loss=2.4121261029237226, w0=73.46249999999975, w1=13.359555417413059\n",
      "Gradient Descent(6325/9999): loss=2.408905054532074, w0=73.46249999999975, w1=13.141834270699734\n",
      "Gradient Descent(6326/9999): loss=1.9288218242992914, w0=73.57499999999975, w1=13.03478989903095\n",
      "Gradient Descent(6327/9999): loss=2.05961537762675, w0=73.40624999999974, w1=13.143208363947368\n",
      "Gradient Descent(6328/9999): loss=2.3386371524861254, w0=73.57499999999975, w1=13.030316159579575\n",
      "Gradient Descent(6329/9999): loss=2.571959474664302, w0=73.34999999999975, w1=13.181552442038967\n",
      "Gradient Descent(6330/9999): loss=2.540895973493015, w0=73.40624999999976, w1=13.578864015793746\n",
      "Gradient Descent(6331/9999): loss=2.1235936368328883, w0=73.46249999999976, w1=13.577507083357384\n",
      "Gradient Descent(6332/9999): loss=1.7182887209347393, w0=73.46249999999976, w1=13.547748532900787\n",
      "Gradient Descent(6333/9999): loss=2.8377844368028255, w0=73.46249999999976, w1=13.573882058402992\n",
      "Gradient Descent(6334/9999): loss=2.0918560952282985, w0=73.29374999999976, w1=13.621961919386255\n",
      "Gradient Descent(6335/9999): loss=2.0650648706904566, w0=73.18124999999976, w1=13.793919090516146\n",
      "Gradient Descent(6336/9999): loss=1.9929888785732193, w0=73.23749999999977, w1=13.743530565315522\n",
      "Gradient Descent(6337/9999): loss=2.5631651707706173, w0=73.29374999999978, w1=13.66435579376317\n",
      "Gradient Descent(6338/9999): loss=1.8492904270813402, w0=73.18124999999978, w1=13.688861973774753\n",
      "Gradient Descent(6339/9999): loss=2.4597934771729664, w0=73.34999999999978, w1=13.747240137678661\n",
      "Gradient Descent(6340/9999): loss=2.0306075017245173, w0=73.40624999999979, w1=13.633059449641348\n",
      "Gradient Descent(6341/9999): loss=2.202374397964932, w0=73.40624999999979, w1=13.345693610393552\n",
      "Gradient Descent(6342/9999): loss=1.4567887459577726, w0=73.40624999999979, w1=13.42984588317198\n",
      "Gradient Descent(6343/9999): loss=2.2190392326964328, w0=73.57499999999979, w1=13.325759473686281\n",
      "Gradient Descent(6344/9999): loss=2.592010353593211, w0=73.29374999999979, w1=13.43119883030011\n",
      "Gradient Descent(6345/9999): loss=2.3933564691471547, w0=73.29374999999979, w1=13.365782575240177\n",
      "Gradient Descent(6346/9999): loss=2.411761334746296, w0=73.3499999999998, w1=13.468433258760994\n",
      "Gradient Descent(6347/9999): loss=2.6911439109833784, w0=73.29374999999979, w1=13.396018710620382\n",
      "Gradient Descent(6348/9999): loss=2.546870069819768, w0=73.0687499999998, w1=13.42178652333994\n",
      "Gradient Descent(6349/9999): loss=2.0117149505086815, w0=73.3499999999998, w1=13.412158364435175\n",
      "Gradient Descent(6350/9999): loss=2.117968102327894, w0=73.29374999999979, w1=13.689332505582302\n",
      "Gradient Descent(6351/9999): loss=2.041967561662127, w0=72.95624999999978, w1=13.471213030496996\n",
      "Gradient Descent(6352/9999): loss=2.1553820962384735, w0=73.01249999999979, w1=13.228935663797484\n",
      "Gradient Descent(6353/9999): loss=2.6052101101954923, w0=73.12499999999979, w1=13.27394197871268\n",
      "Gradient Descent(6354/9999): loss=2.000409372211643, w0=73.40624999999979, w1=13.308963103173467\n",
      "Gradient Descent(6355/9999): loss=2.5350179829186046, w0=73.29374999999979, w1=13.357193995387478\n",
      "Gradient Descent(6356/9999): loss=1.5474293570318178, w0=73.29374999999979, w1=13.309573077202025\n",
      "Gradient Descent(6357/9999): loss=2.513614051268047, w0=73.12499999999979, w1=13.569201360520978\n",
      "Gradient Descent(6358/9999): loss=2.4051990709969804, w0=73.18124999999979, w1=13.559254293745282\n",
      "Gradient Descent(6359/9999): loss=2.1456685959127144, w0=73.40624999999979, w1=13.33328664826112\n",
      "Gradient Descent(6360/9999): loss=1.8560801009819863, w0=73.68749999999979, w1=13.439354403102591\n",
      "Gradient Descent(6361/9999): loss=2.205577787476909, w0=73.57499999999979, w1=13.460352016664823\n",
      "Gradient Descent(6362/9999): loss=2.2782532166251146, w0=73.57499999999979, w1=13.43506442849816\n",
      "Gradient Descent(6363/9999): loss=2.662556480457408, w0=73.57499999999979, w1=13.253066067672556\n",
      "Gradient Descent(6364/9999): loss=2.284856703907765, w0=73.23749999999978, w1=13.295583466376497\n",
      "Gradient Descent(6365/9999): loss=2.639906659392312, w0=73.23749999999978, w1=13.258728389169443\n",
      "Gradient Descent(6366/9999): loss=2.8672681789857237, w0=73.18124999999978, w1=13.481887469786685\n",
      "Gradient Descent(6367/9999): loss=2.1177170718761595, w0=72.95624999999978, w1=13.352513138329355\n",
      "Gradient Descent(6368/9999): loss=2.3962832667719045, w0=72.61874999999978, w1=13.184672857779754\n",
      "Gradient Descent(6369/9999): loss=2.398212377131986, w0=72.84374999999977, w1=13.129292658241527\n",
      "Gradient Descent(6370/9999): loss=2.268922016185325, w0=73.12499999999977, w1=12.941274493690688\n",
      "Gradient Descent(6371/9999): loss=2.231211191641697, w0=73.34999999999977, w1=12.889699868266852\n",
      "Gradient Descent(6372/9999): loss=2.1029317781039003, w0=73.18124999999976, w1=13.082553152021577\n",
      "Gradient Descent(6373/9999): loss=2.7627743104202063, w0=73.23749999999977, w1=13.094269097344704\n",
      "Gradient Descent(6374/9999): loss=2.499973028159821, w0=73.23749999999977, w1=13.35339382409389\n",
      "Gradient Descent(6375/9999): loss=2.576962025970339, w0=73.29374999999978, w1=13.071550362119192\n",
      "Gradient Descent(6376/9999): loss=2.531971554560799, w0=73.12499999999977, w1=13.124679832201956\n",
      "Gradient Descent(6377/9999): loss=1.827578573726458, w0=73.01249999999978, w1=13.337840293770052\n",
      "Gradient Descent(6378/9999): loss=2.089037007548027, w0=72.89999999999978, w1=13.255607378640153\n",
      "Gradient Descent(6379/9999): loss=2.474184722118598, w0=73.01249999999978, w1=13.410314499577147\n",
      "Gradient Descent(6380/9999): loss=1.8893486372673416, w0=73.23749999999977, w1=13.41338191699464\n",
      "Gradient Descent(6381/9999): loss=2.9410869853266712, w0=73.40624999999977, w1=13.5953715521231\n",
      "Gradient Descent(6382/9999): loss=2.6962176171821146, w0=73.23749999999977, w1=13.893215163609463\n",
      "Gradient Descent(6383/9999): loss=2.069156739839847, w0=73.06874999999977, w1=14.07577434150061\n",
      "Gradient Descent(6384/9999): loss=2.244497128822779, w0=73.18124999999976, w1=13.72242827390034\n",
      "Gradient Descent(6385/9999): loss=1.8170734252138394, w0=73.12499999999976, w1=13.14474592539105\n",
      "Gradient Descent(6386/9999): loss=2.0787065082151166, w0=73.23749999999976, w1=13.223562058104298\n",
      "Gradient Descent(6387/9999): loss=2.386937186171632, w0=73.23749999999976, w1=13.303816991074475\n",
      "Gradient Descent(6388/9999): loss=2.0025517286628634, w0=73.29374999999976, w1=13.334591585960302\n",
      "Gradient Descent(6389/9999): loss=2.4023264973226066, w0=73.23749999999976, w1=13.23072042210292\n",
      "Gradient Descent(6390/9999): loss=2.6034361595669173, w0=73.06874999999975, w1=13.164992753723768\n",
      "Gradient Descent(6391/9999): loss=1.6721170462325836, w0=73.18124999999975, w1=13.321264283344075\n",
      "Gradient Descent(6392/9999): loss=2.0895404695991378, w0=73.57499999999975, w1=13.355901155101451\n",
      "Gradient Descent(6393/9999): loss=1.8087519691534173, w0=73.51874999999974, w1=13.320916933389212\n",
      "Gradient Descent(6394/9999): loss=2.7788805757412263, w0=73.46249999999974, w1=13.496675983161529\n",
      "Gradient Descent(6395/9999): loss=2.1338458645519207, w0=73.46249999999974, w1=13.533480541781895\n",
      "Gradient Descent(6396/9999): loss=2.145461174196865, w0=73.23749999999974, w1=13.694611997726527\n",
      "Gradient Descent(6397/9999): loss=2.0661018361740595, w0=73.18124999999974, w1=13.66817503526194\n",
      "Gradient Descent(6398/9999): loss=2.217133632459026, w0=73.18124999999974, w1=13.594994053423237\n",
      "Gradient Descent(6399/9999): loss=1.7378888789441196, w0=73.29374999999973, w1=13.555620360337938\n",
      "Gradient Descent(6400/9999): loss=2.3326824029963835, w0=73.01249999999973, w1=13.29272861470929\n",
      "Gradient Descent(6401/9999): loss=2.3483784858888948, w0=72.84374999999973, w1=13.492713477705749\n",
      "Gradient Descent(6402/9999): loss=2.485831931611936, w0=72.84374999999973, w1=13.440791510577602\n",
      "Gradient Descent(6403/9999): loss=1.7645205690193033, w0=72.89999999999974, w1=13.29337083390758\n",
      "Gradient Descent(6404/9999): loss=1.7687909341180206, w0=72.95624999999974, w1=12.9732958284294\n",
      "Gradient Descent(6405/9999): loss=2.3345986906096154, w0=73.06874999999974, w1=13.270372099925194\n",
      "Gradient Descent(6406/9999): loss=2.7080202694747753, w0=73.06874999999974, w1=13.270712905466047\n",
      "Gradient Descent(6407/9999): loss=2.004117294954444, w0=73.18124999999974, w1=13.113651334152168\n",
      "Gradient Descent(6408/9999): loss=2.337878489172463, w0=73.18124999999974, w1=12.920545713660964\n",
      "Gradient Descent(6409/9999): loss=1.9831177058112173, w0=73.12499999999973, w1=13.18657520674195\n",
      "Gradient Descent(6410/9999): loss=1.8501270117633104, w0=73.12499999999973, w1=13.10736029345641\n",
      "Gradient Descent(6411/9999): loss=1.8864584049895046, w0=73.06874999999972, w1=13.300399990329772\n",
      "Gradient Descent(6412/9999): loss=2.3786111320721783, w0=72.89999999999972, w1=13.304360125918969\n",
      "Gradient Descent(6413/9999): loss=1.9832962576445763, w0=73.12499999999972, w1=13.434196997941168\n",
      "Gradient Descent(6414/9999): loss=2.3614201657410687, w0=73.29374999999972, w1=13.610582482265123\n",
      "Gradient Descent(6415/9999): loss=2.7816636582449394, w0=73.34999999999972, w1=13.739290935918293\n",
      "Gradient Descent(6416/9999): loss=2.1090681742775264, w0=73.18124999999972, w1=13.595326512639279\n",
      "Gradient Descent(6417/9999): loss=2.056235880676683, w0=72.89999999999972, w1=13.56900357371638\n",
      "Gradient Descent(6418/9999): loss=2.1247685444882487, w0=73.12499999999972, w1=13.870247924582799\n",
      "Gradient Descent(6419/9999): loss=2.3293765850194124, w0=73.06874999999971, w1=14.152002814135633\n",
      "Gradient Descent(6420/9999): loss=2.4057411771206674, w0=73.12499999999972, w1=13.99629338091019\n",
      "Gradient Descent(6421/9999): loss=2.170079260320318, w0=73.23749999999971, w1=14.192041864910713\n",
      "Gradient Descent(6422/9999): loss=2.4051210736949313, w0=73.34999999999971, w1=13.995737585705363\n",
      "Gradient Descent(6423/9999): loss=2.2284206241854903, w0=73.40624999999972, w1=13.945349760689863\n",
      "Gradient Descent(6424/9999): loss=2.4108179679759703, w0=73.51874999999971, w1=13.824598226871371\n",
      "Gradient Descent(6425/9999): loss=3.0904872313957252, w0=73.46249999999971, w1=13.471960305049672\n",
      "Gradient Descent(6426/9999): loss=2.3122146223126014, w0=73.4062499999997, w1=13.62551745922791\n",
      "Gradient Descent(6427/9999): loss=2.6031990776438527, w0=73.2374999999997, w1=13.484324196218486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6428/9999): loss=2.024815362415829, w0=73.0687499999997, w1=13.507850242354609\n",
      "Gradient Descent(6429/9999): loss=2.4678081058392363, w0=73.0687499999997, w1=13.346366833868128\n",
      "Gradient Descent(6430/9999): loss=2.321131650187011, w0=72.8437499999997, w1=13.227439680567478\n",
      "Gradient Descent(6431/9999): loss=1.9623427173428498, w0=72.7874999999997, w1=13.33162692571139\n",
      "Gradient Descent(6432/9999): loss=2.4234207546542343, w0=73.01249999999969, w1=13.369011227846356\n",
      "Gradient Descent(6433/9999): loss=1.8064146561628758, w0=72.95624999999968, w1=13.783704925139752\n",
      "Gradient Descent(6434/9999): loss=2.51813333061992, w0=73.12499999999969, w1=13.88538582532522\n",
      "Gradient Descent(6435/9999): loss=1.873305088456804, w0=73.1812499999997, w1=13.540843258893513\n",
      "Gradient Descent(6436/9999): loss=2.1920773969296055, w0=73.2374999999997, w1=13.327942264153194\n",
      "Gradient Descent(6437/9999): loss=1.9392067747702486, w0=73.1249999999997, w1=13.415596570499869\n",
      "Gradient Descent(6438/9999): loss=2.705896154706877, w0=73.1249999999997, w1=13.47428926698735\n",
      "Gradient Descent(6439/9999): loss=2.0788943032812464, w0=73.18124999999971, w1=13.427543764443334\n",
      "Gradient Descent(6440/9999): loss=2.525829345065818, w0=73.34999999999971, w1=13.640533803331868\n",
      "Gradient Descent(6441/9999): loss=1.7884354279635906, w0=73.34999999999971, w1=13.47042030542408\n",
      "Gradient Descent(6442/9999): loss=2.331160878849685, w0=73.68749999999972, w1=13.53769251109345\n",
      "Gradient Descent(6443/9999): loss=2.121987983538836, w0=73.46249999999972, w1=13.773339657912656\n",
      "Gradient Descent(6444/9999): loss=2.1745065063682385, w0=73.46249999999972, w1=13.728987188713297\n",
      "Gradient Descent(6445/9999): loss=1.7584343871034829, w0=73.46249999999972, w1=14.1511640717856\n",
      "Gradient Descent(6446/9999): loss=1.9356168229626383, w0=73.12499999999972, w1=13.942873768877417\n",
      "Gradient Descent(6447/9999): loss=1.8898567803500064, w0=73.23749999999971, w1=13.879994948817128\n",
      "Gradient Descent(6448/9999): loss=1.7434769380617077, w0=73.06874999999971, w1=14.164085972919379\n",
      "Gradient Descent(6449/9999): loss=2.3726581278328154, w0=73.06874999999971, w1=14.092401515912572\n",
      "Gradient Descent(6450/9999): loss=2.2082229176967028, w0=73.12499999999972, w1=14.655918839746404\n",
      "Gradient Descent(6451/9999): loss=2.028750401325592, w0=73.29374999999972, w1=14.562453820350619\n",
      "Gradient Descent(6452/9999): loss=1.8601088051943566, w0=73.34999999999972, w1=14.631624221244923\n",
      "Gradient Descent(6453/9999): loss=1.989287933060514, w0=73.40624999999973, w1=14.706457198392112\n",
      "Gradient Descent(6454/9999): loss=2.153943748145706, w0=73.34999999999972, w1=14.633463853566978\n",
      "Gradient Descent(6455/9999): loss=1.9808147141758436, w0=73.40624999999973, w1=14.357263706256143\n",
      "Gradient Descent(6456/9999): loss=2.168710706126012, w0=73.51874999999973, w1=14.071193876107236\n",
      "Gradient Descent(6457/9999): loss=2.2795466380402196, w0=73.51874999999973, w1=13.871631061260164\n",
      "Gradient Descent(6458/9999): loss=2.84345124055798, w0=73.79999999999973, w1=13.955811528264931\n",
      "Gradient Descent(6459/9999): loss=1.8790530935447485, w0=73.85624999999973, w1=14.169619119793627\n",
      "Gradient Descent(6460/9999): loss=1.983900695794392, w0=73.91249999999974, w1=14.114573375519983\n",
      "Gradient Descent(6461/9999): loss=1.982286244665645, w0=73.79999999999974, w1=14.23410302764117\n",
      "Gradient Descent(6462/9999): loss=2.255090581666784, w0=73.85624999999975, w1=14.17140863276657\n",
      "Gradient Descent(6463/9999): loss=1.6771198670857286, w0=73.68749999999974, w1=14.131429404215261\n",
      "Gradient Descent(6464/9999): loss=2.062773747780578, w0=73.63124999999974, w1=13.972109883225949\n",
      "Gradient Descent(6465/9999): loss=2.7885764489178175, w0=73.68749999999974, w1=13.796254262958117\n",
      "Gradient Descent(6466/9999): loss=1.849808131897119, w0=73.68749999999974, w1=13.610523788371431\n",
      "Gradient Descent(6467/9999): loss=2.3618240780073867, w0=73.63124999999974, w1=13.664759117256523\n",
      "Gradient Descent(6468/9999): loss=2.6775627938591073, w0=73.68749999999974, w1=13.581711125355636\n",
      "Gradient Descent(6469/9999): loss=2.113771642750693, w0=73.68749999999974, w1=13.480412025444073\n",
      "Gradient Descent(6470/9999): loss=2.2449603397633, w0=73.79999999999974, w1=13.410913063866595\n",
      "Gradient Descent(6471/9999): loss=2.0669383068383933, w0=73.85624999999975, w1=13.369996353292471\n",
      "Gradient Descent(6472/9999): loss=2.0170528786306248, w0=73.63124999999975, w1=13.163311399417807\n",
      "Gradient Descent(6473/9999): loss=2.3628972912940513, w0=73.63124999999975, w1=13.421387578897878\n",
      "Gradient Descent(6474/9999): loss=2.091874137156395, w0=73.74374999999975, w1=13.514511223139491\n",
      "Gradient Descent(6475/9999): loss=2.2619095809930756, w0=73.57499999999975, w1=13.50959535652338\n",
      "Gradient Descent(6476/9999): loss=2.397064636582973, w0=73.79999999999974, w1=13.658018085392909\n",
      "Gradient Descent(6477/9999): loss=1.9291594699980452, w0=73.57499999999975, w1=13.815358068347882\n",
      "Gradient Descent(6478/9999): loss=2.156026810580406, w0=73.34999999999975, w1=13.629996073628066\n",
      "Gradient Descent(6479/9999): loss=1.9538650796013166, w0=73.46249999999975, w1=13.860316346586062\n",
      "Gradient Descent(6480/9999): loss=2.103584829140704, w0=73.63124999999975, w1=13.839756577924009\n",
      "Gradient Descent(6481/9999): loss=2.333717445478184, w0=73.79999999999976, w1=13.867629940738068\n",
      "Gradient Descent(6482/9999): loss=2.124218465341933, w0=73.74374999999975, w1=13.663728347521397\n",
      "Gradient Descent(6483/9999): loss=2.3086022600401543, w0=73.23749999999976, w1=13.722774067303147\n",
      "Gradient Descent(6484/9999): loss=2.5406252282781816, w0=73.34999999999975, w1=13.586113999419808\n",
      "Gradient Descent(6485/9999): loss=2.2788982897776116, w0=73.29374999999975, w1=13.43581037422105\n",
      "Gradient Descent(6486/9999): loss=2.0663843032272946, w0=73.12499999999974, w1=13.390583124746032\n",
      "Gradient Descent(6487/9999): loss=2.3114545930292874, w0=73.12499999999974, w1=13.510293891475893\n",
      "Gradient Descent(6488/9999): loss=2.1404378702733826, w0=73.23749999999974, w1=13.477580569097437\n",
      "Gradient Descent(6489/9999): loss=2.142660333317976, w0=73.34999999999974, w1=13.422564848824736\n",
      "Gradient Descent(6490/9999): loss=2.0386871486182914, w0=73.29374999999973, w1=13.566597824120532\n",
      "Gradient Descent(6491/9999): loss=1.957630120600211, w0=73.29374999999973, w1=13.431685656744966\n",
      "Gradient Descent(6492/9999): loss=2.4319633122184827, w0=73.51874999999973, w1=13.450063681576152\n",
      "Gradient Descent(6493/9999): loss=2.441301391031665, w0=73.57499999999973, w1=13.454351312216232\n",
      "Gradient Descent(6494/9999): loss=1.939325087455402, w0=73.63124999999974, w1=13.39549373320053\n",
      "Gradient Descent(6495/9999): loss=2.188356470743789, w0=74.08124999999974, w1=13.5010120567414\n",
      "Gradient Descent(6496/9999): loss=1.8494827012150394, w0=74.02499999999974, w1=13.5917918723425\n",
      "Gradient Descent(6497/9999): loss=1.9977522498040508, w0=73.74374999999974, w1=13.329801791217925\n",
      "Gradient Descent(6498/9999): loss=3.04028090381932, w0=73.63124999999974, w1=13.524244147963154\n",
      "Gradient Descent(6499/9999): loss=2.0794396600565337, w0=73.79999999999974, w1=13.497087772476192\n",
      "Gradient Descent(6500/9999): loss=2.2684859812534524, w0=73.57499999999975, w1=13.323704606798053\n",
      "Gradient Descent(6501/9999): loss=2.5702720508808006, w0=73.74374999999975, w1=13.319810151531565\n",
      "Gradient Descent(6502/9999): loss=1.7886544788503593, w0=73.46249999999975, w1=13.419022259070703\n",
      "Gradient Descent(6503/9999): loss=2.6829028605754854, w0=73.51874999999976, w1=13.162094357214883\n",
      "Gradient Descent(6504/9999): loss=2.578564210293221, w0=73.74374999999975, w1=13.177479999521218\n",
      "Gradient Descent(6505/9999): loss=2.22296606838078, w0=73.68749999999974, w1=13.187041066997637\n",
      "Gradient Descent(6506/9999): loss=2.3750190965146984, w0=73.68749999999974, w1=13.423530081220738\n",
      "Gradient Descent(6507/9999): loss=2.2995489996432577, w0=73.68749999999974, w1=13.650100877770546\n",
      "Gradient Descent(6508/9999): loss=2.1051594896097225, w0=73.63124999999974, w1=13.875908858014192\n",
      "Gradient Descent(6509/9999): loss=1.859405252610005, w0=73.57499999999973, w1=13.79566176212909\n",
      "Gradient Descent(6510/9999): loss=1.9236095420491846, w0=73.51874999999973, w1=14.111779943709527\n",
      "Gradient Descent(6511/9999): loss=2.1374628225105226, w0=73.63124999999972, w1=14.077768867913338\n",
      "Gradient Descent(6512/9999): loss=2.0596587507856046, w0=73.40624999999973, w1=14.166393415200194\n",
      "Gradient Descent(6513/9999): loss=1.9470925215795933, w0=73.51874999999973, w1=14.148278667285604\n",
      "Gradient Descent(6514/9999): loss=2.3825823153804775, w0=73.51874999999973, w1=14.15920569646357\n",
      "Gradient Descent(6515/9999): loss=2.4133869541238147, w0=73.46249999999972, w1=13.835590696247758\n",
      "Gradient Descent(6516/9999): loss=2.1432378014585693, w0=73.34999999999972, w1=13.691620852782787\n",
      "Gradient Descent(6517/9999): loss=2.095668856160882, w0=73.18124999999972, w1=13.61578725329295\n",
      "Gradient Descent(6518/9999): loss=2.2616122011866104, w0=73.34999999999972, w1=13.591423051332635\n",
      "Gradient Descent(6519/9999): loss=1.912852690464722, w0=73.40624999999973, w1=13.733301492711389\n",
      "Gradient Descent(6520/9999): loss=2.623852653885793, w0=73.40624999999973, w1=13.671969200105394\n",
      "Gradient Descent(6521/9999): loss=2.319579673918053, w0=73.34999999999972, w1=13.442871349806538\n",
      "Gradient Descent(6522/9999): loss=2.4418493366654297, w0=73.29374999999972, w1=13.517471581566339\n",
      "Gradient Descent(6523/9999): loss=2.784177621514959, w0=73.06874999999972, w1=13.768215506512231\n",
      "Gradient Descent(6524/9999): loss=2.272568223454466, w0=73.29374999999972, w1=13.523574177696618\n",
      "Gradient Descent(6525/9999): loss=2.074708237124616, w0=73.29374999999972, w1=13.60511473552129\n",
      "Gradient Descent(6526/9999): loss=2.3487369664267126, w0=73.12499999999972, w1=13.80894051201378\n",
      "Gradient Descent(6527/9999): loss=2.602468403624887, w0=73.06874999999971, w1=13.600205219614853\n",
      "Gradient Descent(6528/9999): loss=2.373274097142893, w0=72.84374999999972, w1=13.707238869196699\n",
      "Gradient Descent(6529/9999): loss=2.036217376084811, w0=72.78749999999971, w1=13.436530469183271\n",
      "Gradient Descent(6530/9999): loss=2.4497561705916624, w0=72.7312499999997, w1=13.471456779025353\n",
      "Gradient Descent(6531/9999): loss=2.25452308648154, w0=72.9562499999997, w1=13.228880699090439\n",
      "Gradient Descent(6532/9999): loss=2.7122835636348555, w0=73.0124999999997, w1=13.452862712351356\n",
      "Gradient Descent(6533/9999): loss=2.077336403934596, w0=72.78749999999971, w1=13.58227003853114\n",
      "Gradient Descent(6534/9999): loss=2.3313704809466964, w0=72.89999999999971, w1=13.571755105737983\n",
      "Gradient Descent(6535/9999): loss=2.4803536644894137, w0=72.78749999999971, w1=13.623099219081823\n",
      "Gradient Descent(6536/9999): loss=2.2741026844711025, w0=72.78749999999971, w1=13.421102190142745\n",
      "Gradient Descent(6537/9999): loss=2.6804883355635756, w0=72.89999999999971, w1=13.500071857706942\n",
      "Gradient Descent(6538/9999): loss=2.636715085929961, w0=72.95624999999971, w1=13.441021187998352\n",
      "Gradient Descent(6539/9999): loss=2.5721126731533235, w0=73.12499999999972, w1=13.412071745703573\n",
      "Gradient Descent(6540/9999): loss=2.4677579862900547, w0=72.95624999999971, w1=13.505258774534788\n",
      "Gradient Descent(6541/9999): loss=2.701072799648049, w0=73.18124999999971, w1=13.601257290372475\n",
      "Gradient Descent(6542/9999): loss=1.8781345889253949, w0=73.5749999999997, w1=13.631460855030255\n",
      "Gradient Descent(6543/9999): loss=2.197919588314498, w0=73.6874999999997, w1=13.698336940505788\n",
      "Gradient Descent(6544/9999): loss=1.9749517139391979, w0=73.9124999999997, w1=13.689451543131428\n",
      "Gradient Descent(6545/9999): loss=2.873062752671472, w0=73.7999999999997, w1=13.72545124823762\n",
      "Gradient Descent(6546/9999): loss=2.1566844728372216, w0=73.4062499999997, w1=13.706447144086592\n",
      "Gradient Descent(6547/9999): loss=2.3470504623095785, w0=73.46249999999971, w1=13.49044663721632\n",
      "Gradient Descent(6548/9999): loss=2.0288913754869107, w0=73.34999999999971, w1=13.753811987896722\n",
      "Gradient Descent(6549/9999): loss=2.3927307393255006, w0=73.23749999999971, w1=13.667542659886422\n",
      "Gradient Descent(6550/9999): loss=2.05432882911257, w0=73.40624999999972, w1=13.64201172050016\n",
      "Gradient Descent(6551/9999): loss=1.9642001042180413, w0=73.57499999999972, w1=14.151057253417632\n",
      "Gradient Descent(6552/9999): loss=2.131442430350701, w0=73.63124999999972, w1=14.178752431551011\n",
      "Gradient Descent(6553/9999): loss=3.0967202127211952, w0=73.29374999999972, w1=13.899811114754344\n",
      "Gradient Descent(6554/9999): loss=1.9815802732173302, w0=73.01249999999972, w1=13.714610605734828\n",
      "Gradient Descent(6555/9999): loss=1.9358998986771465, w0=73.12499999999972, w1=13.675877886624791\n",
      "Gradient Descent(6556/9999): loss=2.1066771911548194, w0=72.95624999999971, w1=13.704885692281904\n",
      "Gradient Descent(6557/9999): loss=1.8355133047376098, w0=72.73124999999972, w1=13.950615522127444\n",
      "Gradient Descent(6558/9999): loss=2.0701281598856, w0=72.67499999999971, w1=13.767183662021754\n",
      "Gradient Descent(6559/9999): loss=2.006391253676975, w0=72.84374999999972, w1=13.857831180635067\n",
      "Gradient Descent(6560/9999): loss=2.1061219694410562, w0=72.84374999999972, w1=13.888705008132483\n",
      "Gradient Descent(6561/9999): loss=2.5883674508018335, w0=72.89999999999972, w1=13.912963353799395\n",
      "Gradient Descent(6562/9999): loss=2.2544134806057765, w0=73.01249999999972, w1=13.831859420804431\n",
      "Gradient Descent(6563/9999): loss=2.455527898434723, w0=73.18124999999972, w1=13.759232090808572\n",
      "Gradient Descent(6564/9999): loss=2.2575418127878004, w0=73.51874999999973, w1=13.468057705608645\n",
      "Gradient Descent(6565/9999): loss=2.123754682483101, w0=73.40624999999973, w1=13.666081243407671\n",
      "Gradient Descent(6566/9999): loss=2.3263904591410647, w0=73.40624999999973, w1=13.623274450461956\n",
      "Gradient Descent(6567/9999): loss=2.4344011954076743, w0=73.40624999999973, w1=13.576801388572283\n",
      "Gradient Descent(6568/9999): loss=2.1874312838610734, w0=73.46249999999974, w1=13.679798124847496\n",
      "Gradient Descent(6569/9999): loss=1.9333284029161875, w0=73.06874999999974, w1=13.52731739321016\n",
      "Gradient Descent(6570/9999): loss=2.2178835609917957, w0=73.29374999999973, w1=13.492364461806197\n",
      "Gradient Descent(6571/9999): loss=1.923488518209107, w0=73.34999999999974, w1=13.65997644722248\n",
      "Gradient Descent(6572/9999): loss=2.1357006505827494, w0=73.34999999999974, w1=13.777200539402562\n",
      "Gradient Descent(6573/9999): loss=3.004422406705067, w0=73.34999999999974, w1=13.7224936872332\n",
      "Gradient Descent(6574/9999): loss=1.8898278601827196, w0=73.46249999999974, w1=13.654129323370807\n",
      "Gradient Descent(6575/9999): loss=2.1869942464798857, w0=73.46249999999974, w1=13.486364510121714\n",
      "Gradient Descent(6576/9999): loss=2.2107281437456616, w0=73.34999999999974, w1=13.721516990685162\n",
      "Gradient Descent(6577/9999): loss=2.0799374966607926, w0=73.34999999999974, w1=13.970431363232512\n",
      "Gradient Descent(6578/9999): loss=1.8995638777641675, w0=73.34999999999974, w1=13.614247624266444\n",
      "Gradient Descent(6579/9999): loss=2.272067110362065, w0=73.34999999999974, w1=13.736179707764284\n",
      "Gradient Descent(6580/9999): loss=2.3516219005480936, w0=73.01249999999973, w1=13.635839543954289\n",
      "Gradient Descent(6581/9999): loss=2.3020868439200113, w0=72.95624999999973, w1=13.63228780606993\n",
      "Gradient Descent(6582/9999): loss=2.2848655038940304, w0=73.23749999999973, w1=13.68157811466328\n",
      "Gradient Descent(6583/9999): loss=2.0277711185171925, w0=73.18124999999972, w1=13.717726413520282\n",
      "Gradient Descent(6584/9999): loss=1.8485013912846113, w0=73.06874999999972, w1=13.602047852734277\n",
      "Gradient Descent(6585/9999): loss=1.947352083337567, w0=73.29374999999972, w1=13.363483172969225\n",
      "Gradient Descent(6586/9999): loss=1.904354663792144, w0=73.34999999999972, w1=13.519958234965944\n",
      "Gradient Descent(6587/9999): loss=2.294423740340033, w0=73.29374999999972, w1=13.377979954511494\n",
      "Gradient Descent(6588/9999): loss=2.5059450415729345, w0=73.51874999999971, w1=13.599945000897138\n",
      "Gradient Descent(6589/9999): loss=2.0172736411669456, w0=73.74374999999971, w1=13.601302891403899\n",
      "Gradient Descent(6590/9999): loss=1.7145079264441145, w0=73.8562499999997, w1=13.548919266620013\n",
      "Gradient Descent(6591/9999): loss=2.106985457693174, w0=73.7999999999997, w1=13.595070619763394\n",
      "Gradient Descent(6592/9999): loss=2.88326943331642, w0=73.7999999999997, w1=13.385398854639453\n",
      "Gradient Descent(6593/9999): loss=2.5042011822572405, w0=73.1812499999997, w1=13.20805028588745\n",
      "Gradient Descent(6594/9999): loss=2.1281794961908003, w0=73.1812499999997, w1=13.41148649602158\n",
      "Gradient Descent(6595/9999): loss=2.202878856910989, w0=73.29374999999969, w1=13.464051627584288\n",
      "Gradient Descent(6596/9999): loss=2.1642930327947267, w0=73.23749999999968, w1=13.387623259206336\n",
      "Gradient Descent(6597/9999): loss=2.1054455282732483, w0=73.29374999999969, w1=13.40686847899664\n",
      "Gradient Descent(6598/9999): loss=2.371444423730454, w0=73.51874999999968, w1=13.455818156978559\n",
      "Gradient Descent(6599/9999): loss=2.8965245803858064, w0=73.46249999999968, w1=13.259930718595633\n",
      "Gradient Descent(6600/9999): loss=2.8270947726904128, w0=73.40624999999967, w1=13.322239208106028\n",
      "Gradient Descent(6601/9999): loss=2.0033558160866556, w0=73.23749999999967, w1=13.217991058048849\n",
      "Gradient Descent(6602/9999): loss=2.355596680472423, w0=73.34999999999967, w1=13.510356343515015\n",
      "Gradient Descent(6603/9999): loss=2.2757560046276706, w0=73.23749999999967, w1=13.422838805088379\n",
      "Gradient Descent(6604/9999): loss=1.984848318632033, w0=73.12499999999967, w1=13.596719417642703\n",
      "Gradient Descent(6605/9999): loss=1.9760487664967208, w0=73.23749999999967, w1=13.872089707360702\n",
      "Gradient Descent(6606/9999): loss=2.3206193767891694, w0=72.95624999999967, w1=13.746305442625095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6607/9999): loss=2.0617825626923088, w0=73.29374999999968, w1=13.317811215488744\n",
      "Gradient Descent(6608/9999): loss=2.605150597685374, w0=73.23749999999967, w1=13.15746007493752\n",
      "Gradient Descent(6609/9999): loss=2.0943210330553503, w0=73.23749999999967, w1=13.159670057671542\n",
      "Gradient Descent(6610/9999): loss=2.4952760665945597, w0=73.57499999999968, w1=13.114032150516095\n",
      "Gradient Descent(6611/9999): loss=2.0224797028156254, w0=73.51874999999967, w1=13.077989993659738\n",
      "Gradient Descent(6612/9999): loss=2.452323313926092, w0=73.74374999999966, w1=13.158487056406102\n",
      "Gradient Descent(6613/9999): loss=2.272386283788266, w0=73.51874999999967, w1=13.31588764264035\n",
      "Gradient Descent(6614/9999): loss=2.1270283372759238, w0=73.51874999999967, w1=13.22305254369313\n",
      "Gradient Descent(6615/9999): loss=2.0468280253862687, w0=73.57499999999968, w1=13.521484746902331\n",
      "Gradient Descent(6616/9999): loss=2.028861414641129, w0=73.63124999999968, w1=13.467846425477466\n",
      "Gradient Descent(6617/9999): loss=2.2735692608390092, w0=73.85624999999968, w1=13.185481636925111\n",
      "Gradient Descent(6618/9999): loss=2.468164859686163, w0=73.68749999999967, w1=13.42819871468428\n",
      "Gradient Descent(6619/9999): loss=2.3888077804534693, w0=73.68749999999967, w1=13.573747630273798\n",
      "Gradient Descent(6620/9999): loss=2.179444278721595, w0=73.63124999999967, w1=13.555827219700994\n",
      "Gradient Descent(6621/9999): loss=2.6452586693707083, w0=73.74374999999966, w1=13.628860909999034\n",
      "Gradient Descent(6622/9999): loss=2.502591450125432, w0=73.51874999999967, w1=13.457206350749239\n",
      "Gradient Descent(6623/9999): loss=2.361970731811607, w0=73.29374999999968, w1=13.294819204135912\n",
      "Gradient Descent(6624/9999): loss=2.13040528162999, w0=73.06874999999968, w1=13.40673003764602\n",
      "Gradient Descent(6625/9999): loss=2.1619602752079965, w0=73.40624999999969, w1=13.388164883567985\n",
      "Gradient Descent(6626/9999): loss=2.873744385439465, w0=73.40624999999969, w1=13.633554965589468\n",
      "Gradient Descent(6627/9999): loss=1.8859950703443968, w0=73.4624999999997, w1=13.68481637048773\n",
      "Gradient Descent(6628/9999): loss=2.18101284330473, w0=73.4624999999997, w1=13.57179059704121\n",
      "Gradient Descent(6629/9999): loss=2.179833792510861, w0=73.4624999999997, w1=13.671346463289106\n",
      "Gradient Descent(6630/9999): loss=2.4294755769584375, w0=73.01249999999969, w1=13.61998183163544\n",
      "Gradient Descent(6631/9999): loss=2.2881514695064613, w0=73.12499999999969, w1=13.42839190560011\n",
      "Gradient Descent(6632/9999): loss=1.8732898719705133, w0=73.1812499999997, w1=13.31388247617912\n",
      "Gradient Descent(6633/9999): loss=2.5133638072520705, w0=72.9562499999997, w1=13.181377338199866\n",
      "Gradient Descent(6634/9999): loss=2.1845514193729594, w0=72.9562499999997, w1=13.182833411434084\n",
      "Gradient Descent(6635/9999): loss=2.136801674505141, w0=72.9562499999997, w1=13.315802536880263\n",
      "Gradient Descent(6636/9999): loss=1.9806049643095553, w0=73.0687499999997, w1=13.443934606412782\n",
      "Gradient Descent(6637/9999): loss=2.1477826103681754, w0=73.2374999999997, w1=13.371378267942767\n",
      "Gradient Descent(6638/9999): loss=2.863147416376514, w0=73.3499999999997, w1=13.31459065036445\n",
      "Gradient Descent(6639/9999): loss=2.013851063188098, w0=73.57499999999969, w1=13.287019895864013\n",
      "Gradient Descent(6640/9999): loss=3.0215830571258757, w0=73.6312499999997, w1=13.218170849981643\n",
      "Gradient Descent(6641/9999): loss=2.207503694671791, w0=73.5187499999997, w1=13.1471555957814\n",
      "Gradient Descent(6642/9999): loss=2.190492272225897, w0=73.6312499999997, w1=12.972105289407592\n",
      "Gradient Descent(6643/9999): loss=2.1204878511472045, w0=73.57499999999969, w1=13.066738713307705\n",
      "Gradient Descent(6644/9999): loss=2.00905084070662, w0=73.29374999999969, w1=12.991043709146476\n",
      "Gradient Descent(6645/9999): loss=2.1337377692478787, w0=73.40624999999969, w1=13.317418639821113\n",
      "Gradient Descent(6646/9999): loss=2.0969596746701256, w0=73.57499999999969, w1=13.258031499106686\n",
      "Gradient Descent(6647/9999): loss=1.8143257443384875, w0=73.3499999999997, w1=13.287799589017153\n",
      "Gradient Descent(6648/9999): loss=2.495410983005572, w0=73.0687499999997, w1=13.273732065081854\n",
      "Gradient Descent(6649/9999): loss=2.3495290845159484, w0=73.2374999999997, w1=13.102344370559399\n",
      "Gradient Descent(6650/9999): loss=2.1570635029396805, w0=73.1249999999997, w1=13.161440952119534\n",
      "Gradient Descent(6651/9999): loss=2.2206024337935473, w0=73.2374999999997, w1=13.562658108739203\n",
      "Gradient Descent(6652/9999): loss=2.4202585468771307, w0=73.4062499999997, w1=13.70237012395042\n",
      "Gradient Descent(6653/9999): loss=2.3657975512070477, w0=73.3499999999997, w1=13.489336722476661\n",
      "Gradient Descent(6654/9999): loss=2.3478025712047437, w0=73.29374999999969, w1=13.072022330699255\n",
      "Gradient Descent(6655/9999): loss=2.4287353366435074, w0=73.0687499999997, w1=13.131090231848397\n",
      "Gradient Descent(6656/9999): loss=2.2857463853538245, w0=73.4062499999997, w1=13.028850935734875\n",
      "Gradient Descent(6657/9999): loss=2.1497720235984694, w0=73.5187499999997, w1=13.26051733552913\n",
      "Gradient Descent(6658/9999): loss=1.8844361963188292, w0=73.4062499999997, w1=13.336818312444324\n",
      "Gradient Descent(6659/9999): loss=2.0414561806000817, w0=73.2374999999997, w1=13.334047949590962\n",
      "Gradient Descent(6660/9999): loss=2.423674632974186, w0=73.4624999999997, w1=13.108189494925448\n",
      "Gradient Descent(6661/9999): loss=2.7803874987267174, w0=73.29374999999969, w1=13.21216280118036\n",
      "Gradient Descent(6662/9999): loss=2.194480704544484, w0=73.1812499999997, w1=13.254239449432749\n",
      "Gradient Descent(6663/9999): loss=2.1283405395109414, w0=73.3499999999997, w1=13.231229944529552\n",
      "Gradient Descent(6664/9999): loss=1.914242091729826, w0=73.1812499999997, w1=13.347230367713498\n",
      "Gradient Descent(6665/9999): loss=2.3117248449921934, w0=73.6312499999997, w1=13.641367092845211\n",
      "Gradient Descent(6666/9999): loss=2.5873792167911387, w0=73.6312499999997, w1=13.518460009698982\n",
      "Gradient Descent(6667/9999): loss=1.8397314043237794, w0=73.85624999999969, w1=13.12853426369231\n",
      "Gradient Descent(6668/9999): loss=2.0722083165614205, w0=73.9124999999997, w1=13.253054667412027\n",
      "Gradient Descent(6669/9999): loss=2.144098964265283, w0=73.7999999999997, w1=13.207759789110384\n",
      "Gradient Descent(6670/9999): loss=1.7893478354047934, w0=74.0812499999997, w1=13.272201707055567\n",
      "Gradient Descent(6671/9999): loss=2.148546355197178, w0=73.7999999999997, w1=13.246892637711841\n",
      "Gradient Descent(6672/9999): loss=2.297086995167645, w0=73.4062499999997, w1=13.513791710382124\n",
      "Gradient Descent(6673/9999): loss=2.814049054584591, w0=73.46249999999971, w1=13.339597676259805\n",
      "Gradient Descent(6674/9999): loss=1.9891099580198361, w0=73.51874999999971, w1=13.602642181549012\n",
      "Gradient Descent(6675/9999): loss=2.296320353772127, w0=73.34999999999971, w1=13.612125935122565\n",
      "Gradient Descent(6676/9999): loss=3.0394861123804056, w0=73.46249999999971, w1=13.470089062389857\n",
      "Gradient Descent(6677/9999): loss=1.886937223236626, w0=73.46249999999971, w1=13.457959377339385\n",
      "Gradient Descent(6678/9999): loss=2.008580793870994, w0=73.46249999999971, w1=13.58679425200635\n",
      "Gradient Descent(6679/9999): loss=2.2161321793186186, w0=73.46249999999971, w1=13.405099505755341\n",
      "Gradient Descent(6680/9999): loss=2.234225298794552, w0=73.4062499999997, w1=13.081983997709438\n",
      "Gradient Descent(6681/9999): loss=2.7979003778527796, w0=73.6312499999997, w1=13.164941536177617\n",
      "Gradient Descent(6682/9999): loss=1.7867571998133611, w0=73.6874999999997, w1=13.12390778632189\n",
      "Gradient Descent(6683/9999): loss=2.4223056734258748, w0=73.6312499999997, w1=13.048911051375423\n",
      "Gradient Descent(6684/9999): loss=1.8722853529918726, w0=73.6312499999997, w1=13.15511293517133\n",
      "Gradient Descent(6685/9999): loss=2.735247412014683, w0=73.57499999999969, w1=13.060251005569032\n",
      "Gradient Descent(6686/9999): loss=1.8290207510691854, w0=73.12499999999969, w1=13.07294564422947\n",
      "Gradient Descent(6687/9999): loss=1.7497020607438636, w0=73.12499999999969, w1=13.230053988930134\n",
      "Gradient Descent(6688/9999): loss=2.801939750193542, w0=72.95624999999968, w1=13.34238217098644\n",
      "Gradient Descent(6689/9999): loss=3.11372154162473, w0=73.18124999999968, w1=13.313471027712604\n",
      "Gradient Descent(6690/9999): loss=2.406074306712525, w0=73.29374999999968, w1=13.365757116280557\n",
      "Gradient Descent(6691/9999): loss=2.2195939236744238, w0=73.34999999999968, w1=13.486296080983777\n",
      "Gradient Descent(6692/9999): loss=1.5231151822479827, w0=73.40624999999969, w1=13.442111790977325\n",
      "Gradient Descent(6693/9999): loss=2.686001494693243, w0=73.34999999999968, w1=13.377936103623872\n",
      "Gradient Descent(6694/9999): loss=2.3037661145365513, w0=73.06874999999968, w1=13.278077472383384\n",
      "Gradient Descent(6695/9999): loss=1.5206906715590058, w0=73.12499999999969, w1=13.219476870613024\n",
      "Gradient Descent(6696/9999): loss=2.2756029236904607, w0=72.78749999999968, w1=13.2250364149367\n",
      "Gradient Descent(6697/9999): loss=2.2180484218137195, w0=72.84374999999969, w1=13.349519304061209\n",
      "Gradient Descent(6698/9999): loss=2.5285530003890186, w0=72.95624999999968, w1=13.48098835759382\n",
      "Gradient Descent(6699/9999): loss=2.6453186805983426, w0=73.06874999999968, w1=13.539827216322832\n",
      "Gradient Descent(6700/9999): loss=2.09367412895578, w0=72.78749999999968, w1=13.581967931937852\n",
      "Gradient Descent(6701/9999): loss=1.7836133108113112, w0=72.61874999999968, w1=13.43729389299825\n",
      "Gradient Descent(6702/9999): loss=2.313296346745396, w0=72.50624999999968, w1=13.6478854960491\n",
      "Gradient Descent(6703/9999): loss=1.804746508565392, w0=72.50624999999968, w1=13.597203560295911\n",
      "Gradient Descent(6704/9999): loss=2.4811409672639892, w0=72.39374999999968, w1=13.73656719255194\n",
      "Gradient Descent(6705/9999): loss=2.461549858290279, w0=72.44999999999969, w1=13.996600606715669\n",
      "Gradient Descent(6706/9999): loss=2.19011257905992, w0=72.5062499999997, w1=14.073793027289302\n",
      "Gradient Descent(6707/9999): loss=2.0960213139892403, w0=72.5062499999997, w1=14.233253553966959\n",
      "Gradient Descent(6708/9999): loss=2.5086762762516908, w0=72.3374999999997, w1=14.047884479729165\n",
      "Gradient Descent(6709/9999): loss=1.995921584491002, w0=72.6187499999997, w1=14.021275548410927\n",
      "Gradient Descent(6710/9999): loss=2.3971188933989804, w0=72.84374999999969, w1=13.922832070080318\n",
      "Gradient Descent(6711/9999): loss=2.3585462781417266, w0=72.73124999999969, w1=13.945986459094806\n",
      "Gradient Descent(6712/9999): loss=2.2520827214628056, w0=72.8999999999997, w1=13.86653047504434\n",
      "Gradient Descent(6713/9999): loss=2.30423558545652, w0=72.7874999999997, w1=13.536904397139695\n",
      "Gradient Descent(6714/9999): loss=2.264990624678532, w0=72.73124999999969, w1=13.617636139860549\n",
      "Gradient Descent(6715/9999): loss=2.224916876800694, w0=72.8999999999997, w1=13.52125492064948\n",
      "Gradient Descent(6716/9999): loss=1.709908907342312, w0=73.12499999999969, w1=13.757739780995678\n",
      "Gradient Descent(6717/9999): loss=1.8503642527011566, w0=73.06874999999968, w1=13.445137920711002\n",
      "Gradient Descent(6718/9999): loss=2.465528265182773, w0=73.12499999999969, w1=13.385719241175234\n",
      "Gradient Descent(6719/9999): loss=2.399601086795459, w0=73.12499999999969, w1=13.644026012682179\n",
      "Gradient Descent(6720/9999): loss=2.321483110920177, w0=73.1812499999997, w1=13.445650005065053\n",
      "Gradient Descent(6721/9999): loss=2.193008485909817, w0=73.29374999999969, w1=13.519109986410859\n",
      "Gradient Descent(6722/9999): loss=2.2410167726720247, w0=73.29374999999969, w1=13.720775096678391\n",
      "Gradient Descent(6723/9999): loss=1.9951382742969117, w0=73.1812499999997, w1=13.544433929648566\n",
      "Gradient Descent(6724/9999): loss=2.417940232897598, w0=73.1812499999997, w1=13.514425528721786\n",
      "Gradient Descent(6725/9999): loss=2.0571813336348903, w0=73.3499999999997, w1=13.55686001250049\n",
      "Gradient Descent(6726/9999): loss=2.2076130901126154, w0=73.3499999999997, w1=13.573551150357114\n",
      "Gradient Descent(6727/9999): loss=2.354176721084036, w0=73.2374999999997, w1=13.414091345711537\n",
      "Gradient Descent(6728/9999): loss=2.2109125098171285, w0=73.1249999999997, w1=13.256169052709405\n",
      "Gradient Descent(6729/9999): loss=2.1402477706714067, w0=73.1249999999997, w1=13.419614842396843\n",
      "Gradient Descent(6730/9999): loss=1.9519183902411672, w0=73.2374999999997, w1=13.655651931736498\n",
      "Gradient Descent(6731/9999): loss=2.0489275704232384, w0=73.0687499999997, w1=13.856035305697171\n",
      "Gradient Descent(6732/9999): loss=1.7186991813269201, w0=73.1812499999997, w1=13.884403998504073\n",
      "Gradient Descent(6733/9999): loss=2.0266930524545717, w0=73.3499999999997, w1=13.762678280311745\n",
      "Gradient Descent(6734/9999): loss=2.2255434709261923, w0=73.4062499999997, w1=13.772886313793496\n",
      "Gradient Descent(6735/9999): loss=2.267135300542056, w0=73.3499999999997, w1=13.403181811787118\n",
      "Gradient Descent(6736/9999): loss=2.14316165203934, w0=73.4062499999997, w1=13.509118663615057\n",
      "Gradient Descent(6737/9999): loss=2.305986414461046, w0=73.6312499999997, w1=13.69894810565557\n",
      "Gradient Descent(6738/9999): loss=2.357474449704516, w0=73.4624999999997, w1=13.664229688441685\n",
      "Gradient Descent(6739/9999): loss=2.552843099731933, w0=73.4624999999997, w1=13.871725790697157\n",
      "Gradient Descent(6740/9999): loss=2.487427799465287, w0=73.2374999999997, w1=13.824384021168118\n",
      "Gradient Descent(6741/9999): loss=2.031261254186994, w0=73.3499999999997, w1=13.722470383142323\n",
      "Gradient Descent(6742/9999): loss=2.4590586337943616, w0=73.2374999999997, w1=13.759465740460533\n",
      "Gradient Descent(6743/9999): loss=1.9970079229971827, w0=73.1249999999997, w1=13.935721572555874\n",
      "Gradient Descent(6744/9999): loss=2.361406441870029, w0=73.0687499999997, w1=13.907810411892411\n",
      "Gradient Descent(6745/9999): loss=2.373624834723679, w0=72.8999999999997, w1=13.674375401004736\n",
      "Gradient Descent(6746/9999): loss=2.0170440502347087, w0=73.01249999999969, w1=13.59091403349281\n",
      "Gradient Descent(6747/9999): loss=2.0283036749970638, w0=72.84374999999969, w1=13.585820169210326\n",
      "Gradient Descent(6748/9999): loss=2.2591936189157327, w0=72.56249999999969, w1=13.716473023898365\n",
      "Gradient Descent(6749/9999): loss=2.2891936246336395, w0=72.6187499999997, w1=13.479011331291563\n",
      "Gradient Descent(6750/9999): loss=2.6300547661333065, w0=72.44999999999969, w1=13.354326742294445\n",
      "Gradient Descent(6751/9999): loss=1.382327385693163, w0=72.3374999999997, w1=13.294956284000133\n",
      "Gradient Descent(6752/9999): loss=2.227274191914019, w0=72.6187499999997, w1=13.506405233021276\n",
      "Gradient Descent(6753/9999): loss=1.8604718188354157, w0=72.3937499999997, w1=13.35558586572916\n",
      "Gradient Descent(6754/9999): loss=2.3183781555174714, w0=72.4499999999997, w1=13.553239671434248\n",
      "Gradient Descent(6755/9999): loss=2.190353615198056, w0=72.61874999999971, w1=13.485567036291938\n",
      "Gradient Descent(6756/9999): loss=2.281098980684609, w0=72.50624999999971, w1=13.443953253059068\n",
      "Gradient Descent(6757/9999): loss=2.1811604181713404, w0=72.78749999999971, w1=13.524044855152917\n",
      "Gradient Descent(6758/9999): loss=2.195647202287055, w0=72.50624999999971, w1=13.725516093362033\n",
      "Gradient Descent(6759/9999): loss=2.7053936943266432, w0=73.0124999999997, w1=13.757619930060939\n",
      "Gradient Descent(6760/9999): loss=2.427406602192482, w0=72.8437499999997, w1=13.745435814499137\n",
      "Gradient Descent(6761/9999): loss=2.8672664399949213, w0=73.1249999999997, w1=13.847039619994522\n",
      "Gradient Descent(6762/9999): loss=1.9435024664911864, w0=73.0124999999997, w1=13.97577316349624\n",
      "Gradient Descent(6763/9999): loss=1.7697516143363716, w0=73.0124999999997, w1=13.952758546779457\n",
      "Gradient Descent(6764/9999): loss=2.4947187781316753, w0=73.2374999999997, w1=13.88954950766554\n",
      "Gradient Descent(6765/9999): loss=2.1587181564297024, w0=73.3499999999997, w1=13.968172665464769\n",
      "Gradient Descent(6766/9999): loss=2.651220993826183, w0=73.01249999999969, w1=14.05061863271759\n",
      "Gradient Descent(6767/9999): loss=2.0886746408248156, w0=73.01249999999969, w1=14.15035022484778\n",
      "Gradient Descent(6768/9999): loss=2.178359222887855, w0=72.8999999999997, w1=14.234310978329283\n",
      "Gradient Descent(6769/9999): loss=2.394728177977652, w0=73.01249999999969, w1=14.234507827986821\n",
      "Gradient Descent(6770/9999): loss=2.462005342694097, w0=73.29374999999969, w1=13.956402856185251\n",
      "Gradient Descent(6771/9999): loss=2.4313118725001783, w0=73.29374999999969, w1=14.007977000108394\n",
      "Gradient Descent(6772/9999): loss=1.9821538910580667, w0=73.1812499999997, w1=13.546774185167246\n",
      "Gradient Descent(6773/9999): loss=1.6915651391672066, w0=73.1812499999997, w1=13.63680906032332\n",
      "Gradient Descent(6774/9999): loss=2.818632911647652, w0=73.0687499999997, w1=13.566468618920926\n",
      "Gradient Descent(6775/9999): loss=2.8585353458900453, w0=73.1812499999997, w1=13.610081730086781\n",
      "Gradient Descent(6776/9999): loss=2.215328959412851, w0=73.1812499999997, w1=13.686916493856009\n",
      "Gradient Descent(6777/9999): loss=1.9498608542919846, w0=72.9562499999997, w1=13.49857885001784\n",
      "Gradient Descent(6778/9999): loss=2.525394381218142, w0=73.1249999999997, w1=13.916910027608257\n",
      "Gradient Descent(6779/9999): loss=2.5072646396638474, w0=73.18124999999971, w1=13.657481888589116\n",
      "Gradient Descent(6780/9999): loss=1.7848942772079555, w0=73.2937499999997, w1=13.695598736177233\n",
      "Gradient Descent(6781/9999): loss=2.1959230856983, w0=73.2937499999997, w1=13.823361701093022\n",
      "Gradient Descent(6782/9999): loss=2.0646630616629302, w0=73.06874999999971, w1=13.742275130088052\n",
      "Gradient Descent(6783/9999): loss=2.205418002004875, w0=73.06874999999971, w1=13.850183389126832\n",
      "Gradient Descent(6784/9999): loss=2.3479308116951674, w0=72.95624999999971, w1=13.83738214491995\n",
      "Gradient Descent(6785/9999): loss=2.1410903279311984, w0=73.06874999999971, w1=13.812586541571383\n",
      "Gradient Descent(6786/9999): loss=2.4633714768836916, w0=73.12499999999972, w1=13.946116379209863\n",
      "Gradient Descent(6787/9999): loss=2.5620592958961064, w0=73.01249999999972, w1=13.823561098373455\n",
      "Gradient Descent(6788/9999): loss=2.044326193718592, w0=72.89999999999972, w1=13.692216328730893\n",
      "Gradient Descent(6789/9999): loss=1.9923682691593254, w0=72.61874999999972, w1=13.57529909060516\n",
      "Gradient Descent(6790/9999): loss=2.322046738831864, w0=73.01249999999972, w1=13.383620530184757\n",
      "Gradient Descent(6791/9999): loss=2.2037421152541663, w0=72.89999999999972, w1=13.385375828796642\n",
      "Gradient Descent(6792/9999): loss=1.7155895602991855, w0=73.01249999999972, w1=13.145027967634435\n",
      "Gradient Descent(6793/9999): loss=2.3344404357718775, w0=72.95624999999971, w1=13.119343214443749\n",
      "Gradient Descent(6794/9999): loss=2.827077802628078, w0=73.01249999999972, w1=13.185918442739867\n",
      "Gradient Descent(6795/9999): loss=1.9408086391194697, w0=73.12499999999972, w1=13.034805031383355\n",
      "Gradient Descent(6796/9999): loss=1.9621258908839465, w0=73.01249999999972, w1=12.901302619088364\n",
      "Gradient Descent(6797/9999): loss=2.0712758953199533, w0=73.06874999999972, w1=12.947769539795393\n",
      "Gradient Descent(6798/9999): loss=2.386600217048598, w0=73.34999999999972, w1=13.11405404674256\n",
      "Gradient Descent(6799/9999): loss=2.08958534280953, w0=73.40624999999973, w1=13.263027023305275\n",
      "Gradient Descent(6800/9999): loss=2.2882649698870425, w0=73.23749999999973, w1=13.0554877468987\n",
      "Gradient Descent(6801/9999): loss=2.0641865990484964, w0=73.51874999999973, w1=13.21715815859902\n",
      "Gradient Descent(6802/9999): loss=2.427543584081543, w0=73.68749999999973, w1=13.25140862629588\n",
      "Gradient Descent(6803/9999): loss=2.4049557754614304, w0=73.40624999999973, w1=13.177237197961459\n",
      "Gradient Descent(6804/9999): loss=1.986365486464256, w0=73.63124999999972, w1=13.200850167515322\n",
      "Gradient Descent(6805/9999): loss=2.593913840139011, w0=73.46249999999972, w1=13.172810656510677\n",
      "Gradient Descent(6806/9999): loss=2.227391454715882, w0=73.29374999999972, w1=13.263330521851366\n",
      "Gradient Descent(6807/9999): loss=2.1465309851637286, w0=73.18124999999972, w1=13.153634689216828\n",
      "Gradient Descent(6808/9999): loss=2.0632387689935383, w0=73.29374999999972, w1=13.144001240979357\n",
      "Gradient Descent(6809/9999): loss=2.8194470038101183, w0=73.34999999999972, w1=13.300828561820444\n",
      "Gradient Descent(6810/9999): loss=1.3943443318902913, w0=73.18124999999972, w1=13.029756720016755\n",
      "Gradient Descent(6811/9999): loss=2.3410795483166735, w0=73.29374999999972, w1=13.172407598825872\n",
      "Gradient Descent(6812/9999): loss=2.5064967802007656, w0=73.12499999999972, w1=13.491105813340717\n",
      "Gradient Descent(6813/9999): loss=2.911550864473826, w0=73.06874999999971, w1=13.54332865485166\n",
      "Gradient Descent(6814/9999): loss=1.6778206770122985, w0=73.0124999999997, w1=13.3204624465551\n",
      "Gradient Descent(6815/9999): loss=2.250416667862375, w0=72.8437499999997, w1=13.326857444167464\n",
      "Gradient Descent(6816/9999): loss=2.604263961447625, w0=72.7874999999997, w1=13.376876752299363\n",
      "Gradient Descent(6817/9999): loss=1.6701010731028887, w0=72.8999999999997, w1=13.72860905131298\n",
      "Gradient Descent(6818/9999): loss=2.012044631145617, w0=72.7874999999997, w1=13.57284218382697\n",
      "Gradient Descent(6819/9999): loss=2.4463935828689456, w0=73.0687499999997, w1=13.301060933061969\n",
      "Gradient Descent(6820/9999): loss=2.3330243449693553, w0=73.1812499999997, w1=13.519450607371818\n",
      "Gradient Descent(6821/9999): loss=2.4050697524324036, w0=73.01249999999969, w1=13.607594392767737\n",
      "Gradient Descent(6822/9999): loss=1.9457674621300203, w0=72.7874999999997, w1=13.52010985322305\n",
      "Gradient Descent(6823/9999): loss=2.0836157579479666, w0=72.8437499999997, w1=13.605439898713993\n",
      "Gradient Descent(6824/9999): loss=1.7818653873420058, w0=72.8437499999997, w1=13.563627141210038\n",
      "Gradient Descent(6825/9999): loss=2.101988950116766, w0=72.9562499999997, w1=13.781008636904916\n",
      "Gradient Descent(6826/9999): loss=1.7483071489163673, w0=72.8999999999997, w1=13.513421133081938\n",
      "Gradient Descent(6827/9999): loss=2.157705210068007, w0=73.3499999999997, w1=13.09253875832254\n",
      "Gradient Descent(6828/9999): loss=2.560446286331803, w0=73.1249999999997, w1=13.105068770297175\n",
      "Gradient Descent(6829/9999): loss=2.809714646021445, w0=73.1249999999997, w1=13.00001822086202\n",
      "Gradient Descent(6830/9999): loss=2.2150068236240417, w0=72.8437499999997, w1=13.232072335468212\n",
      "Gradient Descent(6831/9999): loss=2.5488713383690422, w0=72.8437499999997, w1=13.239912047500619\n",
      "Gradient Descent(6832/9999): loss=2.296879772374168, w0=73.0124999999997, w1=13.482268379241933\n",
      "Gradient Descent(6833/9999): loss=2.626249327492605, w0=73.0124999999997, w1=13.329297882221647\n",
      "Gradient Descent(6834/9999): loss=1.940264163708768, w0=73.0124999999997, w1=13.322480505849969\n",
      "Gradient Descent(6835/9999): loss=2.3007228639594395, w0=72.9562499999997, w1=13.135671008916777\n",
      "Gradient Descent(6836/9999): loss=2.812522692685148, w0=72.9562499999997, w1=13.464214525899157\n",
      "Gradient Descent(6837/9999): loss=1.9266281910745908, w0=72.9562499999997, w1=13.466164157722002\n",
      "Gradient Descent(6838/9999): loss=1.812357353106234, w0=72.8437499999997, w1=13.331484907459068\n",
      "Gradient Descent(6839/9999): loss=1.5052235586831417, w0=73.0124999999997, w1=13.597699942588408\n",
      "Gradient Descent(6840/9999): loss=1.4313301176565434, w0=73.0124999999997, w1=13.551597615982663\n",
      "Gradient Descent(6841/9999): loss=2.203771085948857, w0=73.06874999999971, w1=13.600627199361499\n",
      "Gradient Descent(6842/9999): loss=2.423148866767825, w0=73.34999999999971, w1=13.696658944794192\n",
      "Gradient Descent(6843/9999): loss=2.263991741269645, w0=73.12499999999972, w1=13.381767091306932\n",
      "Gradient Descent(6844/9999): loss=2.369499125292317, w0=73.01249999999972, w1=13.172380423869363\n",
      "Gradient Descent(6845/9999): loss=2.7404053124389853, w0=73.23749999999971, w1=13.238680841504697\n",
      "Gradient Descent(6846/9999): loss=2.309813746836693, w0=73.23749999999971, w1=13.154484413620155\n",
      "Gradient Descent(6847/9999): loss=2.7933761368291927, w0=73.34999999999971, w1=13.143698851971472\n",
      "Gradient Descent(6848/9999): loss=2.035838817844854, w0=73.12499999999972, w1=12.99839382162295\n",
      "Gradient Descent(6849/9999): loss=2.362714105523146, w0=73.12499999999972, w1=13.340285342111292\n",
      "Gradient Descent(6850/9999): loss=2.3346580213316637, w0=73.06874999999971, w1=13.417133633452249\n",
      "Gradient Descent(6851/9999): loss=1.8078109886221791, w0=73.12499999999972, w1=13.572547397279639\n",
      "Gradient Descent(6852/9999): loss=2.269731063922122, w0=72.95624999999971, w1=13.653881752705828\n",
      "Gradient Descent(6853/9999): loss=1.8070308087915148, w0=72.89999999999971, w1=13.710062129796286\n",
      "Gradient Descent(6854/9999): loss=2.449580636988139, w0=73.0124999999997, w1=13.49984542688109\n",
      "Gradient Descent(6855/9999): loss=1.964958008843131, w0=72.89999999999971, w1=13.605909372185875\n",
      "Gradient Descent(6856/9999): loss=2.2414846680377805, w0=73.0124999999997, w1=13.798512349560347\n",
      "Gradient Descent(6857/9999): loss=2.0775525061905533, w0=73.18124999999971, w1=13.801680013825882\n",
      "Gradient Descent(6858/9999): loss=2.457802704372871, w0=73.1249999999997, w1=13.795128246757574\n",
      "Gradient Descent(6859/9999): loss=2.250370015916557, w0=73.2374999999997, w1=13.835086263379598\n",
      "Gradient Descent(6860/9999): loss=2.8157413919043774, w0=73.1812499999997, w1=13.685159419996303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(6861/9999): loss=2.2797839853452224, w0=72.9562499999997, w1=13.557786066229756\n",
      "Gradient Descent(6862/9999): loss=1.8661316040686433, w0=72.7312499999997, w1=13.354629543789374\n",
      "Gradient Descent(6863/9999): loss=2.2359559967382028, w0=72.89999999999971, w1=13.51300476777603\n",
      "Gradient Descent(6864/9999): loss=1.95559212028996, w0=72.89999999999971, w1=13.450812410605586\n",
      "Gradient Descent(6865/9999): loss=2.580597404393055, w0=72.7312499999997, w1=13.603146637107049\n",
      "Gradient Descent(6866/9999): loss=1.8794699070333278, w0=72.50624999999971, w1=13.7178403859712\n",
      "Gradient Descent(6867/9999): loss=1.8151522189052045, w0=72.50624999999971, w1=13.481348449699837\n",
      "Gradient Descent(6868/9999): loss=1.9607025493375314, w0=72.61874999999971, w1=13.64767993198339\n",
      "Gradient Descent(6869/9999): loss=2.3728187629485027, w0=72.7312499999997, w1=13.635367027935406\n",
      "Gradient Descent(6870/9999): loss=2.686598875622656, w0=72.5624999999997, w1=13.347278297357224\n",
      "Gradient Descent(6871/9999): loss=2.333609571083996, w0=72.7312499999997, w1=13.190857617099399\n",
      "Gradient Descent(6872/9999): loss=2.290348390084934, w0=72.78749999999971, w1=13.316055590836449\n",
      "Gradient Descent(6873/9999): loss=1.9457134609227458, w0=72.78749999999971, w1=13.081972618622135\n",
      "Gradient Descent(6874/9999): loss=2.4687783475605185, w0=72.7312499999997, w1=12.926855753725851\n",
      "Gradient Descent(6875/9999): loss=1.8586834883746435, w0=73.0124999999997, w1=12.997673882399186\n",
      "Gradient Descent(6876/9999): loss=2.1115379921080204, w0=73.18124999999971, w1=13.171829011994218\n",
      "Gradient Descent(6877/9999): loss=2.2594243895807478, w0=73.2937499999997, w1=13.448183700181016\n",
      "Gradient Descent(6878/9999): loss=2.238720164018755, w0=73.34999999999971, w1=13.296920078532033\n",
      "Gradient Descent(6879/9999): loss=2.2148908856844765, w0=73.2937499999997, w1=12.922723352095067\n",
      "Gradient Descent(6880/9999): loss=2.157987872089257, w0=73.63124999999971, w1=12.967315715005503\n",
      "Gradient Descent(6881/9999): loss=2.4913976533909317, w0=73.68749999999972, w1=13.165540728926121\n",
      "Gradient Descent(6882/9999): loss=2.0191489315431657, w0=73.51874999999971, w1=13.376342280474503\n",
      "Gradient Descent(6883/9999): loss=1.6496242394817036, w0=73.63124999999971, w1=13.491451740665068\n",
      "Gradient Descent(6884/9999): loss=2.0242077771094467, w0=73.5749999999997, w1=13.234279423308555\n",
      "Gradient Descent(6885/9999): loss=2.1603343258575425, w0=73.6874999999997, w1=13.197944509190163\n",
      "Gradient Descent(6886/9999): loss=2.5616347967447783, w0=73.74374999999971, w1=13.471680515024936\n",
      "Gradient Descent(6887/9999): loss=1.7112047064167535, w0=73.6874999999997, w1=13.308233570699768\n",
      "Gradient Descent(6888/9999): loss=1.9086891206558445, w0=73.8562499999997, w1=13.674952695848464\n",
      "Gradient Descent(6889/9999): loss=2.170227266581504, w0=73.91249999999971, w1=13.661706934131931\n",
      "Gradient Descent(6890/9999): loss=2.018642888219786, w0=73.63124999999971, w1=13.705544752780657\n",
      "Gradient Descent(6891/9999): loss=1.8049729432818988, w0=73.63124999999971, w1=13.825314003465195\n",
      "Gradient Descent(6892/9999): loss=1.999781304273297, w0=73.5749999999997, w1=13.69447532326513\n",
      "Gradient Descent(6893/9999): loss=2.8757990805249323, w0=73.63124999999971, w1=13.785097726844468\n",
      "Gradient Descent(6894/9999): loss=1.7503345533593917, w0=73.63124999999971, w1=13.87481615109835\n",
      "Gradient Descent(6895/9999): loss=2.4235888227289433, w0=73.74374999999971, w1=13.732450774588198\n",
      "Gradient Descent(6896/9999): loss=2.604987645077463, w0=73.74374999999971, w1=13.800086222665403\n",
      "Gradient Descent(6897/9999): loss=1.7839251862762884, w0=73.8562499999997, w1=13.727433647644215\n",
      "Gradient Descent(6898/9999): loss=1.95896487983717, w0=73.91249999999971, w1=13.707385776335546\n",
      "Gradient Descent(6899/9999): loss=2.271274580979014, w0=73.91249999999971, w1=13.662562221867848\n",
      "Gradient Descent(6900/9999): loss=2.394097230048762, w0=73.8562499999997, w1=13.920894391616317\n",
      "Gradient Descent(6901/9999): loss=2.169319998741487, w0=73.74374999999971, w1=13.843191892076922\n",
      "Gradient Descent(6902/9999): loss=2.599496885926242, w0=73.74374999999971, w1=13.669167333372346\n",
      "Gradient Descent(6903/9999): loss=2.3497532996185853, w0=73.46249999999971, w1=13.984461181588632\n",
      "Gradient Descent(6904/9999): loss=1.9612985146735415, w0=73.4062499999997, w1=14.013798155059057\n",
      "Gradient Descent(6905/9999): loss=2.3128211147337616, w0=73.8562499999997, w1=14.00619136088701\n",
      "Gradient Descent(6906/9999): loss=2.4222156504260077, w0=74.02499999999971, w1=13.85730645035479\n",
      "Gradient Descent(6907/9999): loss=2.010652627439292, w0=74.02499999999971, w1=13.970506878378194\n",
      "Gradient Descent(6908/9999): loss=2.458076922161444, w0=73.79999999999971, w1=14.17908929644542\n",
      "Gradient Descent(6909/9999): loss=2.593774056493353, w0=73.51874999999971, w1=14.220808174688056\n",
      "Gradient Descent(6910/9999): loss=2.2653302625621325, w0=73.12499999999972, w1=14.459079519285261\n",
      "Gradient Descent(6911/9999): loss=2.707941816329358, w0=73.29374999999972, w1=14.165833509808909\n",
      "Gradient Descent(6912/9999): loss=2.086504899570848, w0=73.40624999999972, w1=13.811258996919088\n",
      "Gradient Descent(6913/9999): loss=2.2503968043678264, w0=73.46249999999972, w1=13.62722416806152\n",
      "Gradient Descent(6914/9999): loss=1.9519212431358048, w0=73.46249999999972, w1=13.757976443189479\n",
      "Gradient Descent(6915/9999): loss=2.5868069418522355, w0=73.12499999999972, w1=13.835670498575087\n",
      "Gradient Descent(6916/9999): loss=2.835630600382568, w0=73.18124999999972, w1=13.801960811347827\n",
      "Gradient Descent(6917/9999): loss=2.0330716772701862, w0=73.06874999999972, w1=13.748679972905089\n",
      "Gradient Descent(6918/9999): loss=2.056290916039373, w0=73.01249999999972, w1=13.555476782430972\n",
      "Gradient Descent(6919/9999): loss=1.9843157904485085, w0=72.89999999999972, w1=13.413301674040111\n",
      "Gradient Descent(6920/9999): loss=2.614270430154906, w0=73.29374999999972, w1=13.476892884130766\n",
      "Gradient Descent(6921/9999): loss=2.3274146810326264, w0=73.06874999999972, w1=13.158501547506868\n",
      "Gradient Descent(6922/9999): loss=2.0752151342209477, w0=72.89999999999972, w1=13.178415805020856\n",
      "Gradient Descent(6923/9999): loss=2.2461898346028932, w0=73.34999999999972, w1=13.333971207703385\n",
      "Gradient Descent(6924/9999): loss=1.9234382271147392, w0=73.40624999999973, w1=13.63669083316901\n",
      "Gradient Descent(6925/9999): loss=2.2608931452495975, w0=73.34999999999972, w1=13.589541150089584\n",
      "Gradient Descent(6926/9999): loss=1.6113118459117661, w0=73.63124999999972, w1=13.645634502182238\n",
      "Gradient Descent(6927/9999): loss=2.1464968508174076, w0=73.34999999999972, w1=13.784490601035552\n",
      "Gradient Descent(6928/9999): loss=2.005645106145862, w0=73.34999999999972, w1=13.632443529645442\n",
      "Gradient Descent(6929/9999): loss=2.6339411126492713, w0=73.68749999999973, w1=13.784678983975818\n",
      "Gradient Descent(6930/9999): loss=2.4535354367526394, w0=73.79999999999973, w1=14.003708713375735\n",
      "Gradient Descent(6931/9999): loss=2.8512264567384973, w0=73.51874999999973, w1=13.948796498214929\n",
      "Gradient Descent(6932/9999): loss=2.2681397372931063, w0=73.57499999999973, w1=13.831848108425575\n",
      "Gradient Descent(6933/9999): loss=2.187032066205188, w0=73.63124999999974, w1=14.083702260608987\n",
      "Gradient Descent(6934/9999): loss=2.2475378328453735, w0=73.40624999999974, w1=13.896570319217169\n",
      "Gradient Descent(6935/9999): loss=2.5935016736173946, w0=73.23749999999974, w1=13.726128580761257\n",
      "Gradient Descent(6936/9999): loss=2.231551234194373, w0=73.18124999999974, w1=13.889083387971525\n",
      "Gradient Descent(6937/9999): loss=2.318918194110017, w0=73.06874999999974, w1=13.873106290184332\n",
      "Gradient Descent(6938/9999): loss=1.9633189597398517, w0=72.95624999999974, w1=13.621830058713066\n",
      "Gradient Descent(6939/9999): loss=1.510132251608665, w0=73.01249999999975, w1=13.485898946357723\n",
      "Gradient Descent(6940/9999): loss=2.2418819862880204, w0=73.06874999999975, w1=13.581754819666283\n",
      "Gradient Descent(6941/9999): loss=2.0770009410120855, w0=72.84374999999976, w1=13.541499246655556\n",
      "Gradient Descent(6942/9999): loss=2.278075352572065, w0=72.95624999999976, w1=13.586367890327013\n",
      "Gradient Descent(6943/9999): loss=2.2073597420002233, w0=73.18124999999975, w1=13.692255746566339\n",
      "Gradient Descent(6944/9999): loss=2.248478797758941, w0=72.89999999999975, w1=13.5800273120312\n",
      "Gradient Descent(6945/9999): loss=2.104510629474177, w0=73.06874999999975, w1=13.938302353074317\n",
      "Gradient Descent(6946/9999): loss=2.6998702391832663, w0=72.84374999999976, w1=13.939716438056378\n",
      "Gradient Descent(6947/9999): loss=2.666361102993039, w0=72.89999999999976, w1=14.085647729262277\n",
      "Gradient Descent(6948/9999): loss=2.02783706861839, w0=72.95624999999977, w1=14.066705602322592\n",
      "Gradient Descent(6949/9999): loss=2.0530938613726795, w0=73.06874999999977, w1=14.234590125210893\n",
      "Gradient Descent(6950/9999): loss=2.2292870087147554, w0=73.06874999999977, w1=13.940592373205265\n",
      "Gradient Descent(6951/9999): loss=2.574595838972515, w0=72.95624999999977, w1=14.206509549477763\n",
      "Gradient Descent(6952/9999): loss=2.4991258081517893, w0=73.12499999999977, w1=13.843854152901253\n",
      "Gradient Descent(6953/9999): loss=2.0280838784213846, w0=72.84374999999977, w1=13.968213266426787\n",
      "Gradient Descent(6954/9999): loss=2.8551413192814326, w0=73.06874999999977, w1=13.874405410727963\n",
      "Gradient Descent(6955/9999): loss=2.4459567392635853, w0=73.01249999999976, w1=13.607696398848866\n",
      "Gradient Descent(6956/9999): loss=2.324813055712425, w0=73.12499999999976, w1=13.603645169253625\n",
      "Gradient Descent(6957/9999): loss=1.9425500016110886, w0=73.18124999999976, w1=13.735414863159102\n",
      "Gradient Descent(6958/9999): loss=1.9535898057829213, w0=73.12499999999976, w1=13.781318665491467\n",
      "Gradient Descent(6959/9999): loss=2.389888560559408, w0=73.18124999999976, w1=13.87311960811324\n",
      "Gradient Descent(6960/9999): loss=1.9813456863562957, w0=73.18124999999976, w1=13.709879812031348\n",
      "Gradient Descent(6961/9999): loss=1.7081378081752023, w0=73.34999999999977, w1=13.66713863493931\n",
      "Gradient Descent(6962/9999): loss=2.0887334446508197, w0=73.29374999999976, w1=13.74725670209711\n",
      "Gradient Descent(6963/9999): loss=2.363768024504653, w0=73.23749999999976, w1=13.738211110600735\n",
      "Gradient Descent(6964/9999): loss=2.0885923445487116, w0=73.23749999999976, w1=13.63399632127132\n",
      "Gradient Descent(6965/9999): loss=1.7743060433066251, w0=73.01249999999976, w1=13.768658160809995\n",
      "Gradient Descent(6966/9999): loss=1.983937608564608, w0=72.84374999999976, w1=13.715285334513535\n",
      "Gradient Descent(6967/9999): loss=1.6520880501417103, w0=72.84374999999976, w1=13.464008022920876\n",
      "Gradient Descent(6968/9999): loss=1.9444955554719474, w0=73.12499999999976, w1=13.308321474597006\n",
      "Gradient Descent(6969/9999): loss=2.0908013991495973, w0=73.12499999999976, w1=13.576509223242773\n",
      "Gradient Descent(6970/9999): loss=1.6815171875491206, w0=73.12499999999976, w1=13.85011150776582\n",
      "Gradient Descent(6971/9999): loss=1.95857137743306, w0=73.18124999999976, w1=13.848632806435472\n",
      "Gradient Descent(6972/9999): loss=2.0657249846579537, w0=73.23749999999977, w1=13.842531999230712\n",
      "Gradient Descent(6973/9999): loss=2.3441805525828796, w0=73.06874999999977, w1=13.846228392555934\n",
      "Gradient Descent(6974/9999): loss=2.352303796305913, w0=73.23749999999977, w1=13.6161237308375\n",
      "Gradient Descent(6975/9999): loss=1.9349098209216367, w0=73.18124999999976, w1=13.759576376844008\n",
      "Gradient Descent(6976/9999): loss=2.977614584484109, w0=73.06874999999977, w1=13.67682939820134\n",
      "Gradient Descent(6977/9999): loss=1.729015576808212, w0=73.06874999999977, w1=13.644511852734283\n",
      "Gradient Descent(6978/9999): loss=1.8055973893095119, w0=72.95624999999977, w1=13.762556398267128\n",
      "Gradient Descent(6979/9999): loss=2.2618391218272986, w0=73.12499999999977, w1=13.952124937352579\n",
      "Gradient Descent(6980/9999): loss=2.280793858307674, w0=73.12499999999977, w1=13.61608370175147\n",
      "Gradient Descent(6981/9999): loss=2.302036401038422, w0=72.78749999999977, w1=13.414201794245038\n",
      "Gradient Descent(6982/9999): loss=2.1304904651008636, w0=72.89999999999976, w1=13.61201438373173\n",
      "Gradient Descent(6983/9999): loss=2.368236548259084, w0=72.89999999999976, w1=13.528247224790872\n",
      "Gradient Descent(6984/9999): loss=1.9156140136147908, w0=72.95624999999977, w1=13.568104484205408\n",
      "Gradient Descent(6985/9999): loss=2.6086822237430574, w0=72.95624999999977, w1=13.559424346344843\n",
      "Gradient Descent(6986/9999): loss=2.0890653092644764, w0=73.12499999999977, w1=13.54244923302431\n",
      "Gradient Descent(6987/9999): loss=2.4452196155652492, w0=73.34999999999977, w1=13.495701686723704\n",
      "Gradient Descent(6988/9999): loss=2.3175697287712067, w0=73.46249999999976, w1=13.469957166670255\n",
      "Gradient Descent(6989/9999): loss=2.550690451386817, w0=73.23749999999977, w1=13.473491450573038\n",
      "Gradient Descent(6990/9999): loss=1.8006757570614431, w0=73.51874999999977, w1=13.442694955506509\n",
      "Gradient Descent(6991/9999): loss=2.2196944926953237, w0=73.51874999999977, w1=13.322697027314437\n",
      "Gradient Descent(6992/9999): loss=2.6076939222178486, w0=73.23749999999977, w1=13.59380452737821\n",
      "Gradient Descent(6993/9999): loss=2.4240201582914827, w0=73.23749999999977, w1=13.255402432677888\n",
      "Gradient Descent(6994/9999): loss=1.8386245901556966, w0=73.12499999999977, w1=13.444517242255055\n",
      "Gradient Descent(6995/9999): loss=2.8177970621519055, w0=73.06874999999977, w1=13.781001591395448\n",
      "Gradient Descent(6996/9999): loss=2.3849035181108, w0=73.01249999999976, w1=13.65374367655559\n",
      "Gradient Descent(6997/9999): loss=3.0566084035767056, w0=73.01249999999976, w1=13.5838704085531\n",
      "Gradient Descent(6998/9999): loss=2.4780641779660417, w0=72.78749999999977, w1=13.8733776677473\n",
      "Gradient Descent(6999/9999): loss=2.082077706301278, w0=72.73124999999976, w1=14.045263237375758\n",
      "Gradient Descent(7000/9999): loss=1.9713665787722645, w0=72.78749999999977, w1=13.919066575825795\n",
      "Gradient Descent(7001/9999): loss=1.9601578566543145, w0=72.89999999999976, w1=13.80249674236222\n",
      "Gradient Descent(7002/9999): loss=1.9865631914805932, w0=72.84374999999976, w1=13.897808344638007\n",
      "Gradient Descent(7003/9999): loss=2.89144830063533, w0=73.23749999999976, w1=13.727867668894575\n",
      "Gradient Descent(7004/9999): loss=1.9937061369483569, w0=73.23749999999976, w1=13.844938921825904\n",
      "Gradient Descent(7005/9999): loss=1.9659522464153865, w0=73.34999999999975, w1=13.698128327821554\n",
      "Gradient Descent(7006/9999): loss=2.1948302534811472, w0=73.40624999999976, w1=13.73406852112391\n",
      "Gradient Descent(7007/9999): loss=2.23615556822382, w0=73.29374999999976, w1=13.894309291341228\n",
      "Gradient Descent(7008/9999): loss=1.7631815980365135, w0=73.23749999999976, w1=13.991656762094252\n",
      "Gradient Descent(7009/9999): loss=2.2677432736032577, w0=73.40624999999976, w1=13.860229680392246\n",
      "Gradient Descent(7010/9999): loss=2.2855310023519646, w0=73.29374999999976, w1=13.70887341991306\n",
      "Gradient Descent(7011/9999): loss=2.213320148579023, w0=73.12499999999976, w1=13.609754201996989\n",
      "Gradient Descent(7012/9999): loss=1.9006319013732167, w0=73.06874999999975, w1=13.339069201242745\n",
      "Gradient Descent(7013/9999): loss=1.9934961872234045, w0=73.23749999999976, w1=13.413648895977978\n",
      "Gradient Descent(7014/9999): loss=2.1512940046905262, w0=73.63124999999975, w1=13.692799426631383\n",
      "Gradient Descent(7015/9999): loss=2.5866270396871722, w0=73.63124999999975, w1=13.847636849037414\n",
      "Gradient Descent(7016/9999): loss=2.2839033277808225, w0=73.34999999999975, w1=13.531340661635783\n",
      "Gradient Descent(7017/9999): loss=2.365217044939396, w0=73.46249999999975, w1=13.762614618292096\n",
      "Gradient Descent(7018/9999): loss=2.0079392141716865, w0=73.40624999999974, w1=13.807651284777986\n",
      "Gradient Descent(7019/9999): loss=2.0556604602986797, w0=73.46249999999975, w1=13.989489365001258\n",
      "Gradient Descent(7020/9999): loss=2.4454181736879343, w0=73.18124999999975, w1=13.885918736199201\n",
      "Gradient Descent(7021/9999): loss=2.379238798061684, w0=73.23749999999976, w1=13.698669340734297\n",
      "Gradient Descent(7022/9999): loss=1.677466710608727, w0=73.29374999999976, w1=13.911646293033488\n",
      "Gradient Descent(7023/9999): loss=2.714609541124762, w0=73.34999999999977, w1=14.11796436809945\n",
      "Gradient Descent(7024/9999): loss=2.568722514404149, w0=73.06874999999977, w1=13.715431682855037\n",
      "Gradient Descent(7025/9999): loss=2.255221993018396, w0=73.40624999999977, w1=13.6314864865626\n",
      "Gradient Descent(7026/9999): loss=2.06681907059579, w0=73.29374999999978, w1=13.711213589426377\n",
      "Gradient Descent(7027/9999): loss=2.0794623676364847, w0=73.46249999999978, w1=13.686076476120903\n",
      "Gradient Descent(7028/9999): loss=1.7245530228408497, w0=73.23749999999978, w1=13.50101939795618\n",
      "Gradient Descent(7029/9999): loss=1.4536224609611041, w0=73.06874999999978, w1=13.633046429670596\n",
      "Gradient Descent(7030/9999): loss=2.213959977648193, w0=73.23749999999978, w1=13.65606198624062\n",
      "Gradient Descent(7031/9999): loss=1.7560156325828469, w0=73.06874999999978, w1=13.65701118734924\n",
      "Gradient Descent(7032/9999): loss=1.9438744328869253, w0=72.61874999999978, w1=13.780205523962913\n",
      "Gradient Descent(7033/9999): loss=2.3263825531394935, w0=72.56249999999977, w1=13.570136412980068\n",
      "Gradient Descent(7034/9999): loss=1.6287941911127337, w0=72.61874999999978, w1=13.67767967971084\n",
      "Gradient Descent(7035/9999): loss=1.940803008494388, w0=72.78749999999978, w1=13.665066054467832\n",
      "Gradient Descent(7036/9999): loss=2.5888129725840576, w0=73.06874999999978, w1=13.428038662432407\n",
      "Gradient Descent(7037/9999): loss=2.199169520839392, w0=73.06874999999978, w1=13.782527833043876\n",
      "Gradient Descent(7038/9999): loss=2.4112832500295576, w0=73.12499999999979, w1=13.85078597444308\n",
      "Gradient Descent(7039/9999): loss=2.4052514296744203, w0=73.12499999999979, w1=13.89742040127103\n",
      "Gradient Descent(7040/9999): loss=2.3489019199327403, w0=72.95624999999978, w1=13.62583749747912\n",
      "Gradient Descent(7041/9999): loss=2.7867471026965847, w0=73.18124999999978, w1=13.760971058447657\n",
      "Gradient Descent(7042/9999): loss=2.1014952681011416, w0=72.95624999999978, w1=13.863521214064837\n",
      "Gradient Descent(7043/9999): loss=1.9143838885166935, w0=72.89999999999978, w1=14.030694463435243\n",
      "Gradient Descent(7044/9999): loss=1.9602843475783316, w0=72.95624999999978, w1=14.204289575658734\n",
      "Gradient Descent(7045/9999): loss=2.6874975285390437, w0=73.01249999999979, w1=14.369245862799087\n",
      "Gradient Descent(7046/9999): loss=2.462743133529708, w0=73.0687499999998, w1=14.181426705889733\n",
      "Gradient Descent(7047/9999): loss=1.961372817190089, w0=73.01249999999979, w1=14.226634880344589\n",
      "Gradient Descent(7048/9999): loss=2.1079389130759623, w0=72.95624999999978, w1=14.417482346149527\n",
      "Gradient Descent(7049/9999): loss=2.383038634413992, w0=73.18124999999978, w1=14.26974143174948\n",
      "Gradient Descent(7050/9999): loss=2.4208333955148387, w0=72.73124999999978, w1=14.4411908098708\n",
      "Gradient Descent(7051/9999): loss=2.314696363203268, w0=72.67499999999977, w1=14.226852220880028\n",
      "Gradient Descent(7052/9999): loss=2.1720948815257897, w0=72.61874999999976, w1=13.850430831185125\n",
      "Gradient Descent(7053/9999): loss=2.255452582061766, w0=72.73124999999976, w1=13.804492989524142\n",
      "Gradient Descent(7054/9999): loss=2.2718809498143475, w0=72.67499999999976, w1=13.7619215084679\n",
      "Gradient Descent(7055/9999): loss=1.6848122011633668, w0=73.06874999999975, w1=13.652552835340918\n",
      "Gradient Descent(7056/9999): loss=2.617521215384474, w0=73.06874999999975, w1=13.968346744110557\n",
      "Gradient Descent(7057/9999): loss=1.9375689187915515, w0=73.06874999999975, w1=13.858270500018033\n",
      "Gradient Descent(7058/9999): loss=2.5005527845600075, w0=73.12499999999976, w1=13.792726647951085\n",
      "Gradient Descent(7059/9999): loss=2.3057854056077884, w0=73.23749999999976, w1=14.06412975312309\n",
      "Gradient Descent(7060/9999): loss=1.7282376056777367, w0=73.06874999999975, w1=14.213769462958528\n",
      "Gradient Descent(7061/9999): loss=2.1325949139139144, w0=73.34999999999975, w1=14.385384241178606\n",
      "Gradient Descent(7062/9999): loss=2.6721120969818495, w0=73.46249999999975, w1=14.447173137820066\n",
      "Gradient Descent(7063/9999): loss=2.222914740089932, w0=73.40624999999974, w1=14.31275801141951\n",
      "Gradient Descent(7064/9999): loss=2.9052129761470384, w0=73.18124999999975, w1=14.371871536751852\n",
      "Gradient Descent(7065/9999): loss=2.2833368930599613, w0=73.12499999999974, w1=14.022379478309379\n",
      "Gradient Descent(7066/9999): loss=2.2144375032835892, w0=73.12499999999974, w1=13.973877084117738\n",
      "Gradient Descent(7067/9999): loss=2.4918031755379912, w0=72.67499999999974, w1=13.815803023746302\n",
      "Gradient Descent(7068/9999): loss=2.002926737093669, w0=72.84374999999974, w1=13.80036211114525\n",
      "Gradient Descent(7069/9999): loss=1.805551907230142, w0=72.95624999999974, w1=13.537723756430138\n",
      "Gradient Descent(7070/9999): loss=2.20175601895365, w0=73.06874999999974, w1=13.49403522552562\n",
      "Gradient Descent(7071/9999): loss=2.3330909064397543, w0=72.95624999999974, w1=13.404421151066876\n",
      "Gradient Descent(7072/9999): loss=1.9128957118715328, w0=73.12499999999974, w1=13.42865933176771\n",
      "Gradient Descent(7073/9999): loss=2.4435121193634717, w0=73.23749999999974, w1=13.46648994538336\n",
      "Gradient Descent(7074/9999): loss=2.0855460791133877, w0=73.34999999999974, w1=13.653137863902597\n",
      "Gradient Descent(7075/9999): loss=1.658290640301391, w0=73.18124999999974, w1=14.021927315145927\n",
      "Gradient Descent(7076/9999): loss=1.7813612770626883, w0=73.51874999999974, w1=13.606049286198493\n",
      "Gradient Descent(7077/9999): loss=2.4961250735620233, w0=73.51874999999974, w1=13.716557769483774\n",
      "Gradient Descent(7078/9999): loss=2.0597327459953796, w0=73.40624999999974, w1=13.732340259253476\n",
      "Gradient Descent(7079/9999): loss=2.293646531521407, w0=73.29374999999975, w1=13.679183610629298\n",
      "Gradient Descent(7080/9999): loss=2.104488574326347, w0=73.18124999999975, w1=13.732595997407728\n",
      "Gradient Descent(7081/9999): loss=1.7273258320927471, w0=73.29374999999975, w1=13.697058409484407\n",
      "Gradient Descent(7082/9999): loss=2.1708769747245036, w0=73.18124999999975, w1=13.72587508333051\n",
      "Gradient Descent(7083/9999): loss=2.084074065415855, w0=72.89999999999975, w1=13.667068399327537\n",
      "Gradient Descent(7084/9999): loss=2.202616594326288, w0=72.95624999999976, w1=13.461461726659751\n",
      "Gradient Descent(7085/9999): loss=1.8552377315539605, w0=72.95624999999976, w1=13.477056618737713\n",
      "Gradient Descent(7086/9999): loss=2.4559178905418086, w0=72.95624999999976, w1=13.543935690575175\n",
      "Gradient Descent(7087/9999): loss=2.562805160651992, w0=72.84374999999976, w1=13.610649509711466\n",
      "Gradient Descent(7088/9999): loss=2.6406455633968053, w0=72.78749999999975, w1=13.847499548953524\n",
      "Gradient Descent(7089/9999): loss=2.20476685373983, w0=72.95624999999976, w1=13.689753703524147\n",
      "Gradient Descent(7090/9999): loss=2.2975393425151296, w0=72.89999999999975, w1=13.500222582247567\n",
      "Gradient Descent(7091/9999): loss=2.497825544778681, w0=73.01249999999975, w1=13.521213670265151\n",
      "Gradient Descent(7092/9999): loss=2.4008635015575606, w0=73.29374999999975, w1=13.253316154877233\n",
      "Gradient Descent(7093/9999): loss=2.1684128254150203, w0=73.29374999999975, w1=13.394946471394622\n",
      "Gradient Descent(7094/9999): loss=2.241207579029563, w0=73.46249999999975, w1=13.470128745223821\n",
      "Gradient Descent(7095/9999): loss=1.9025094322938407, w0=73.51874999999976, w1=13.687259427424298\n",
      "Gradient Descent(7096/9999): loss=2.073058876483398, w0=73.40624999999976, w1=13.784414997801843\n",
      "Gradient Descent(7097/9999): loss=2.332771357373569, w0=73.23749999999976, w1=13.957299565790866\n",
      "Gradient Descent(7098/9999): loss=2.2618035471090936, w0=73.23749999999976, w1=13.790223919360194\n",
      "Gradient Descent(7099/9999): loss=1.9976840048364155, w0=73.06874999999975, w1=13.908739697491324\n",
      "Gradient Descent(7100/9999): loss=2.711260067093218, w0=73.23749999999976, w1=13.883372085746123\n",
      "Gradient Descent(7101/9999): loss=2.3360195174303025, w0=73.46249999999975, w1=13.743726127787605\n",
      "Gradient Descent(7102/9999): loss=2.265951115456793, w0=73.63124999999975, w1=13.665550415376613\n",
      "Gradient Descent(7103/9999): loss=2.5027619871213185, w0=73.01249999999975, w1=13.721023524228205\n",
      "Gradient Descent(7104/9999): loss=2.4239408197776213, w0=73.18124999999975, w1=13.815207064043394\n",
      "Gradient Descent(7105/9999): loss=2.16133220532602, w0=73.34999999999975, w1=13.681902561346442\n",
      "Gradient Descent(7106/9999): loss=2.302013488671551, w0=73.57499999999975, w1=13.701110894877429\n",
      "Gradient Descent(7107/9999): loss=2.463129420639623, w0=73.51874999999974, w1=13.88346996138139\n",
      "Gradient Descent(7108/9999): loss=2.5709050012282795, w0=73.68749999999974, w1=13.782459251838318\n",
      "Gradient Descent(7109/9999): loss=1.9294462189049006, w0=73.57499999999975, w1=13.74826054716671\n",
      "Gradient Descent(7110/9999): loss=2.308241292115408, w0=73.51874999999974, w1=13.803957872703165\n",
      "Gradient Descent(7111/9999): loss=2.406452300106544, w0=73.46249999999974, w1=13.717855100942007\n",
      "Gradient Descent(7112/9999): loss=2.5409847985644367, w0=73.51874999999974, w1=13.74653016577277\n",
      "Gradient Descent(7113/9999): loss=1.9050815900190714, w0=73.74374999999974, w1=13.759089355956561\n",
      "Gradient Descent(7114/9999): loss=2.575829140881817, w0=73.46249999999974, w1=13.951450516909366\n",
      "Gradient Descent(7115/9999): loss=1.8220196577505623, w0=73.40624999999973, w1=13.992112890536267\n",
      "Gradient Descent(7116/9999): loss=2.2004809246525685, w0=73.79999999999973, w1=14.05677448414557\n",
      "Gradient Descent(7117/9999): loss=2.5307907789241346, w0=73.68749999999973, w1=14.16301742151674\n",
      "Gradient Descent(7118/9999): loss=2.3022526444341276, w0=73.68749999999973, w1=14.095403858236148\n",
      "Gradient Descent(7119/9999): loss=1.987384613931477, w0=73.57499999999973, w1=14.116264934245109\n",
      "Gradient Descent(7120/9999): loss=2.2311285163144046, w0=73.23749999999973, w1=14.021785752248752\n",
      "Gradient Descent(7121/9999): loss=2.4798467183975523, w0=73.34999999999972, w1=13.896858677015537\n",
      "Gradient Descent(7122/9999): loss=2.2307441921413256, w0=73.40624999999973, w1=13.913500905587494\n",
      "Gradient Descent(7123/9999): loss=2.3453661017839353, w0=73.29374999999973, w1=14.154425680174915\n",
      "Gradient Descent(7124/9999): loss=1.8190483108235203, w0=73.12499999999973, w1=13.907771275007352\n",
      "Gradient Descent(7125/9999): loss=1.9325978573627596, w0=72.95624999999973, w1=13.684185159246137\n",
      "Gradient Descent(7126/9999): loss=2.1216749510892248, w0=73.01249999999973, w1=13.718718982453705\n",
      "Gradient Descent(7127/9999): loss=1.9706891129317812, w0=73.12499999999973, w1=13.73763863085261\n",
      "Gradient Descent(7128/9999): loss=2.070346243790808, w0=73.34999999999972, w1=13.631155644026611\n",
      "Gradient Descent(7129/9999): loss=2.12932929215723, w0=73.46249999999972, w1=13.55629027922146\n",
      "Gradient Descent(7130/9999): loss=1.880863893113982, w0=73.51874999999973, w1=13.510420755803285\n",
      "Gradient Descent(7131/9999): loss=1.962464669636998, w0=73.40624999999973, w1=13.50038310799624\n",
      "Gradient Descent(7132/9999): loss=1.86918663940495, w0=73.23749999999973, w1=13.607404986486362\n",
      "Gradient Descent(7133/9999): loss=2.439367498048877, w0=73.23749999999973, w1=13.557438240356309\n",
      "Gradient Descent(7134/9999): loss=1.6696544486467617, w0=73.18124999999972, w1=13.711695316241997\n",
      "Gradient Descent(7135/9999): loss=1.8785918138635052, w0=73.29374999999972, w1=13.590502939138851\n",
      "Gradient Descent(7136/9999): loss=2.185925473176029, w0=73.51874999999971, w1=13.77029435854363\n",
      "Gradient Descent(7137/9999): loss=2.764403690469428, w0=73.46249999999971, w1=13.62917406217036\n",
      "Gradient Descent(7138/9999): loss=2.1232602621933734, w0=73.63124999999971, w1=13.503624815503231\n",
      "Gradient Descent(7139/9999): loss=2.5445186433847984, w0=73.74374999999971, w1=13.323295614559765\n",
      "Gradient Descent(7140/9999): loss=2.3059929496932465, w0=73.5749999999997, w1=13.414038411162554\n",
      "Gradient Descent(7141/9999): loss=2.110571782636475, w0=73.5187499999997, w1=13.436435600672663\n",
      "Gradient Descent(7142/9999): loss=2.437304473096245, w0=73.8562499999997, w1=13.478451796216543\n",
      "Gradient Descent(7143/9999): loss=2.540245383704309, w0=73.74374999999971, w1=13.786283673604224\n",
      "Gradient Descent(7144/9999): loss=2.28871122601926, w0=73.8562499999997, w1=13.617743158988606\n",
      "Gradient Descent(7145/9999): loss=2.059374173630556, w0=73.5187499999997, w1=13.747477510067018\n",
      "Gradient Descent(7146/9999): loss=1.9073351696834648, w0=73.4062499999997, w1=13.931857864492933\n",
      "Gradient Descent(7147/9999): loss=1.9123864305437683, w0=73.46249999999971, w1=13.980526743581876\n",
      "Gradient Descent(7148/9999): loss=2.033840470536142, w0=73.74374999999971, w1=14.127229159683626\n",
      "Gradient Descent(7149/9999): loss=2.2598364734675345, w0=73.51874999999971, w1=14.028396585148833\n",
      "Gradient Descent(7150/9999): loss=1.5599812859367412, w0=73.46249999999971, w1=13.965206118278424\n",
      "Gradient Descent(7151/9999): loss=1.8639854325632426, w0=73.51874999999971, w1=14.110921394537222\n",
      "Gradient Descent(7152/9999): loss=2.399139173550527, w0=73.51874999999971, w1=13.846252060534848\n",
      "Gradient Descent(7153/9999): loss=2.726505311190561, w0=73.57499999999972, w1=13.864573886730621\n",
      "Gradient Descent(7154/9999): loss=2.2704080257526584, w0=73.40624999999972, w1=13.827536952412501\n",
      "Gradient Descent(7155/9999): loss=1.8380786941949623, w0=73.06874999999971, w1=13.988094719697127\n",
      "Gradient Descent(7156/9999): loss=1.6162982983970902, w0=72.78749999999971, w1=14.092304055401337\n",
      "Gradient Descent(7157/9999): loss=2.122722342229311, w0=72.84374999999972, w1=14.11530323817751\n",
      "Gradient Descent(7158/9999): loss=1.811544454248132, w0=73.06874999999971, w1=13.988759045707205\n",
      "Gradient Descent(7159/9999): loss=2.411494952944693, w0=73.12499999999972, w1=13.960661449029327\n",
      "Gradient Descent(7160/9999): loss=2.0039732137673183, w0=73.12499999999972, w1=14.027821528590081\n",
      "Gradient Descent(7161/9999): loss=2.186482534649956, w0=73.18124999999972, w1=14.047597358561179\n",
      "Gradient Descent(7162/9999): loss=2.7563248960466322, w0=73.34999999999972, w1=13.79933298865926\n",
      "Gradient Descent(7163/9999): loss=2.2402965278668563, w0=73.46249999999972, w1=13.670458671307097\n",
      "Gradient Descent(7164/9999): loss=2.2207406961965215, w0=73.23749999999973, w1=13.756688906071352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7165/9999): loss=1.737746498724009, w0=73.12499999999973, w1=13.84839408465706\n",
      "Gradient Descent(7166/9999): loss=2.079037834393961, w0=73.12499999999973, w1=14.011774402037545\n",
      "Gradient Descent(7167/9999): loss=2.745791733693399, w0=73.06874999999972, w1=14.061347121178372\n",
      "Gradient Descent(7168/9999): loss=2.2450929425056545, w0=73.01249999999972, w1=13.871891419030794\n",
      "Gradient Descent(7169/9999): loss=2.665115959845928, w0=73.06874999999972, w1=13.590114218160725\n",
      "Gradient Descent(7170/9999): loss=2.556050994917279, w0=72.73124999999972, w1=13.564184548992301\n",
      "Gradient Descent(7171/9999): loss=2.004743569367796, w0=72.78749999999972, w1=13.590113244825407\n",
      "Gradient Descent(7172/9999): loss=2.4881135013171933, w0=72.67499999999973, w1=13.774288461257907\n",
      "Gradient Descent(7173/9999): loss=2.489184634994743, w0=73.01249999999973, w1=13.90709702554594\n",
      "Gradient Descent(7174/9999): loss=1.7906932732989767, w0=73.01249999999973, w1=13.882314639791876\n",
      "Gradient Descent(7175/9999): loss=2.4180153779878473, w0=73.01249999999973, w1=14.003943447021737\n",
      "Gradient Descent(7176/9999): loss=2.3313989686522154, w0=73.18124999999974, w1=14.142346858207084\n",
      "Gradient Descent(7177/9999): loss=2.0220288963479938, w0=73.23749999999974, w1=14.14126498664007\n",
      "Gradient Descent(7178/9999): loss=1.9033953876205103, w0=73.01249999999975, w1=14.13423162886476\n",
      "Gradient Descent(7179/9999): loss=2.8134188008743775, w0=73.01249999999975, w1=13.936515834705208\n",
      "Gradient Descent(7180/9999): loss=1.8337033377232168, w0=72.95624999999974, w1=13.791760015275798\n",
      "Gradient Descent(7181/9999): loss=2.227714441082526, w0=73.06874999999974, w1=13.712190667372846\n",
      "Gradient Descent(7182/9999): loss=1.792817010821512, w0=73.01249999999973, w1=13.95713667396994\n",
      "Gradient Descent(7183/9999): loss=2.645674480253091, w0=72.95624999999973, w1=14.183857152028214\n",
      "Gradient Descent(7184/9999): loss=2.0465888020165437, w0=72.84374999999973, w1=13.727426760697373\n",
      "Gradient Descent(7185/9999): loss=1.9107644421528354, w0=73.12499999999973, w1=13.766712608178805\n",
      "Gradient Descent(7186/9999): loss=1.7610981399485928, w0=72.95624999999973, w1=13.82321477393143\n",
      "Gradient Descent(7187/9999): loss=2.0279873541847744, w0=73.06874999999972, w1=13.70490344396338\n",
      "Gradient Descent(7188/9999): loss=2.381768048326836, w0=72.95624999999973, w1=13.377014289770587\n",
      "Gradient Descent(7189/9999): loss=2.0784342392952233, w0=73.01249999999973, w1=13.58701049680884\n",
      "Gradient Descent(7190/9999): loss=2.0386584706824085, w0=72.84374999999973, w1=13.88095228634194\n",
      "Gradient Descent(7191/9999): loss=1.9925542563796013, w0=72.61874999999974, w1=13.951919538651692\n",
      "Gradient Descent(7192/9999): loss=2.4950190345177097, w0=72.78749999999974, w1=13.785284042153942\n",
      "Gradient Descent(7193/9999): loss=1.8705158764527927, w0=72.84374999999974, w1=13.815793857054626\n",
      "Gradient Descent(7194/9999): loss=2.1669554029057894, w0=72.78749999999974, w1=13.96567989854662\n",
      "Gradient Descent(7195/9999): loss=2.0617468329731548, w0=72.73124999999973, w1=14.11830452960566\n",
      "Gradient Descent(7196/9999): loss=2.054519508508892, w0=72.78749999999974, w1=14.01315097783245\n",
      "Gradient Descent(7197/9999): loss=2.408485081018496, w0=72.67499999999974, w1=13.957346970695422\n",
      "Gradient Descent(7198/9999): loss=1.7574060295182377, w0=72.56249999999974, w1=13.623629822486455\n",
      "Gradient Descent(7199/9999): loss=2.247516565602118, w0=72.67499999999974, w1=13.561745027449748\n",
      "Gradient Descent(7200/9999): loss=2.482424050649695, w0=72.89999999999974, w1=13.584100852869442\n",
      "Gradient Descent(7201/9999): loss=2.687189166578234, w0=72.89999999999974, w1=13.793659146732258\n",
      "Gradient Descent(7202/9999): loss=2.117484132632212, w0=72.89999999999974, w1=13.784313870750546\n",
      "Gradient Descent(7203/9999): loss=1.5855574371102432, w0=73.06874999999974, w1=13.493485800047145\n",
      "Gradient Descent(7204/9999): loss=2.2779923355924776, w0=73.18124999999974, w1=13.86144126695398\n",
      "Gradient Descent(7205/9999): loss=1.7537459176323071, w0=72.89999999999974, w1=13.587392773204375\n",
      "Gradient Descent(7206/9999): loss=2.7610897583893186, w0=72.89999999999974, w1=13.816285062251074\n",
      "Gradient Descent(7207/9999): loss=2.347083757827566, w0=72.89999999999974, w1=13.703359934081677\n",
      "Gradient Descent(7208/9999): loss=2.5059831965357646, w0=72.95624999999974, w1=13.966248023972314\n",
      "Gradient Descent(7209/9999): loss=1.3179628130525547, w0=73.34999999999974, w1=13.613846205983055\n",
      "Gradient Descent(7210/9999): loss=2.340105889574323, w0=73.12499999999974, w1=13.81212122495043\n",
      "Gradient Descent(7211/9999): loss=2.191908636022665, w0=73.29374999999975, w1=13.7934043668526\n",
      "Gradient Descent(7212/9999): loss=2.146831090097128, w0=73.29374999999975, w1=13.825294168829195\n",
      "Gradient Descent(7213/9999): loss=2.3659619026001604, w0=73.12499999999974, w1=13.609134295496013\n",
      "Gradient Descent(7214/9999): loss=2.1429059127030374, w0=72.84374999999974, w1=13.537282450593084\n",
      "Gradient Descent(7215/9999): loss=2.2815903213151167, w0=72.84374999999974, w1=13.5534328917652\n",
      "Gradient Descent(7216/9999): loss=2.4142026680734197, w0=73.01249999999975, w1=13.349977507323636\n",
      "Gradient Descent(7217/9999): loss=1.9620502866099514, w0=72.89999999999975, w1=13.072521909199612\n",
      "Gradient Descent(7218/9999): loss=2.4243565741253894, w0=72.78749999999975, w1=13.376029271232234\n",
      "Gradient Descent(7219/9999): loss=1.9251012618321544, w0=72.89999999999975, w1=13.379712391770184\n",
      "Gradient Descent(7220/9999): loss=2.1004585174060404, w0=72.78749999999975, w1=13.411222889665803\n",
      "Gradient Descent(7221/9999): loss=2.3263822752163, w0=73.29374999999975, w1=13.640175950172441\n",
      "Gradient Descent(7222/9999): loss=2.0260441227744717, w0=73.29374999999975, w1=13.807210596891649\n",
      "Gradient Descent(7223/9999): loss=2.633063457811472, w0=73.06874999999975, w1=13.669968795483184\n",
      "Gradient Descent(7224/9999): loss=2.4842068445565726, w0=72.84374999999976, w1=13.674350246310539\n",
      "Gradient Descent(7225/9999): loss=2.068268948762726, w0=73.01249999999976, w1=13.585444382679555\n",
      "Gradient Descent(7226/9999): loss=1.9707719948760216, w0=73.12499999999976, w1=13.523111549447682\n",
      "Gradient Descent(7227/9999): loss=2.2533904240549147, w0=73.29374999999976, w1=13.699976691650402\n",
      "Gradient Descent(7228/9999): loss=2.6435054321208096, w0=73.57499999999976, w1=13.79496558923398\n",
      "Gradient Descent(7229/9999): loss=2.3583791505564173, w0=73.68749999999976, w1=13.77156480629925\n",
      "Gradient Descent(7230/9999): loss=2.0735188884425373, w0=73.63124999999975, w1=14.018954193970739\n",
      "Gradient Descent(7231/9999): loss=1.7893347384842069, w0=73.57499999999975, w1=13.881149334808613\n",
      "Gradient Descent(7232/9999): loss=2.380310254506172, w0=73.74374999999975, w1=13.644161064115277\n",
      "Gradient Descent(7233/9999): loss=3.0331172699212017, w0=73.51874999999976, w1=13.769418188485231\n",
      "Gradient Descent(7234/9999): loss=1.7560593200441277, w0=73.34999999999975, w1=13.87087814844047\n",
      "Gradient Descent(7235/9999): loss=2.1863061303102107, w0=73.34999999999975, w1=14.056893020423459\n",
      "Gradient Descent(7236/9999): loss=2.0383943504288915, w0=73.23749999999976, w1=14.163039072985955\n",
      "Gradient Descent(7237/9999): loss=2.1706825421619396, w0=73.12499999999976, w1=13.942566958320514\n",
      "Gradient Descent(7238/9999): loss=2.154693978049796, w0=73.18124999999976, w1=13.738977789168846\n",
      "Gradient Descent(7239/9999): loss=1.9056587537807568, w0=73.40624999999976, w1=13.65297504051914\n",
      "Gradient Descent(7240/9999): loss=2.0794461829325366, w0=73.29374999999976, w1=13.618869831173196\n",
      "Gradient Descent(7241/9999): loss=2.244232977717064, w0=73.46249999999976, w1=13.696660771563721\n",
      "Gradient Descent(7242/9999): loss=2.4076202759641605, w0=73.29374999999976, w1=13.729964835448927\n",
      "Gradient Descent(7243/9999): loss=2.101453395061683, w0=73.18124999999976, w1=13.860351943689516\n",
      "Gradient Descent(7244/9999): loss=1.9974665279980006, w0=73.40624999999976, w1=13.710034036702012\n",
      "Gradient Descent(7245/9999): loss=1.8058149639210717, w0=73.34999999999975, w1=13.838997950087906\n",
      "Gradient Descent(7246/9999): loss=2.4628094391238484, w0=73.18124999999975, w1=13.593430505291797\n",
      "Gradient Descent(7247/9999): loss=1.9824527102421148, w0=72.95624999999976, w1=13.555067864362815\n",
      "Gradient Descent(7248/9999): loss=2.425837345891644, w0=73.06874999999975, w1=13.47110814925736\n",
      "Gradient Descent(7249/9999): loss=2.3910084904348956, w0=72.95624999999976, w1=13.377522239376296\n",
      "Gradient Descent(7250/9999): loss=2.41587763657618, w0=73.18124999999975, w1=13.568691081032398\n",
      "Gradient Descent(7251/9999): loss=2.167016542359515, w0=73.18124999999975, w1=13.272483403160232\n",
      "Gradient Descent(7252/9999): loss=2.2124499510127857, w0=73.51874999999976, w1=13.294439286715368\n",
      "Gradient Descent(7253/9999): loss=2.4620119041284725, w0=73.46249999999975, w1=13.47555584487188\n",
      "Gradient Descent(7254/9999): loss=2.1193989562970446, w0=73.51874999999976, w1=13.760680284613205\n",
      "Gradient Descent(7255/9999): loss=1.925083117669406, w0=73.51874999999976, w1=13.538138470319462\n",
      "Gradient Descent(7256/9999): loss=2.0579900350321267, w0=73.57499999999976, w1=13.285795781474567\n",
      "Gradient Descent(7257/9999): loss=2.424766597301523, w0=73.68749999999976, w1=13.505382005650874\n",
      "Gradient Descent(7258/9999): loss=2.09133833926495, w0=73.40624999999976, w1=13.66560418205566\n",
      "Gradient Descent(7259/9999): loss=2.2365527289990075, w0=73.40624999999976, w1=13.763066761093592\n",
      "Gradient Descent(7260/9999): loss=2.2663395363753454, w0=73.57499999999976, w1=14.035559491890407\n",
      "Gradient Descent(7261/9999): loss=2.4452431516295094, w0=73.29374999999976, w1=13.925658236953934\n",
      "Gradient Descent(7262/9999): loss=2.472978639040985, w0=73.23749999999976, w1=14.087439588294068\n",
      "Gradient Descent(7263/9999): loss=2.2553332314917762, w0=73.40624999999976, w1=13.840558077715361\n",
      "Gradient Descent(7264/9999): loss=2.88589084404733, w0=73.34999999999975, w1=13.869925081043036\n",
      "Gradient Descent(7265/9999): loss=2.5200552839276815, w0=73.29374999999975, w1=13.666187743436002\n",
      "Gradient Descent(7266/9999): loss=1.991556050939383, w0=73.57499999999975, w1=13.654855033934613\n",
      "Gradient Descent(7267/9999): loss=3.009689426136524, w0=73.46249999999975, w1=13.463942516359527\n",
      "Gradient Descent(7268/9999): loss=2.0651536623028957, w0=73.51874999999976, w1=13.39223100170562\n",
      "Gradient Descent(7269/9999): loss=1.7458044001121003, w0=73.34999999999975, w1=13.325000515696976\n",
      "Gradient Descent(7270/9999): loss=2.1925966570087585, w0=73.46249999999975, w1=13.254609137309485\n",
      "Gradient Descent(7271/9999): loss=2.0628032885444036, w0=73.34999999999975, w1=13.212592120359037\n",
      "Gradient Descent(7272/9999): loss=2.714767611676915, w0=73.12499999999976, w1=13.249488847924937\n",
      "Gradient Descent(7273/9999): loss=2.0872326680436277, w0=73.34999999999975, w1=13.364443714583063\n",
      "Gradient Descent(7274/9999): loss=2.2803999089238083, w0=73.40624999999976, w1=13.551866505028155\n",
      "Gradient Descent(7275/9999): loss=2.547917512637869, w0=73.46249999999976, w1=13.35218457388583\n",
      "Gradient Descent(7276/9999): loss=1.811555952366061, w0=73.29374999999976, w1=13.153348820956188\n",
      "Gradient Descent(7277/9999): loss=1.7002143973312371, w0=73.46249999999976, w1=13.099145868699175\n",
      "Gradient Descent(7278/9999): loss=2.3680550720102955, w0=73.40624999999976, w1=13.20752844391362\n",
      "Gradient Descent(7279/9999): loss=1.998949138227892, w0=73.29374999999976, w1=13.007657880621478\n",
      "Gradient Descent(7280/9999): loss=2.3394085710270254, w0=73.18124999999976, w1=13.258570738664655\n",
      "Gradient Descent(7281/9999): loss=1.9366455601145387, w0=73.01249999999976, w1=13.253920416216234\n",
      "Gradient Descent(7282/9999): loss=2.1165087370581, w0=72.95624999999976, w1=13.056232040645769\n",
      "Gradient Descent(7283/9999): loss=1.8657827411440435, w0=73.18124999999975, w1=13.336705779093304\n",
      "Gradient Descent(7284/9999): loss=2.1213601781800806, w0=73.18124999999975, w1=13.487265131565403\n",
      "Gradient Descent(7285/9999): loss=2.653875484952406, w0=73.23749999999976, w1=13.33682094380648\n",
      "Gradient Descent(7286/9999): loss=2.3280940817396125, w0=73.23749999999976, w1=13.49539049647784\n",
      "Gradient Descent(7287/9999): loss=1.7280174749881618, w0=72.89999999999975, w1=13.278793341961357\n",
      "Gradient Descent(7288/9999): loss=1.865232226439003, w0=72.95624999999976, w1=13.427559193909314\n",
      "Gradient Descent(7289/9999): loss=2.176462985488027, w0=72.89999999999975, w1=13.612537609648022\n",
      "Gradient Descent(7290/9999): loss=2.1330283113241153, w0=72.89999999999975, w1=13.751984028347245\n",
      "Gradient Descent(7291/9999): loss=2.528920648706509, w0=72.73124999999975, w1=13.72730340601016\n",
      "Gradient Descent(7292/9999): loss=2.299226795991192, w0=72.84374999999974, w1=13.47105218607517\n",
      "Gradient Descent(7293/9999): loss=2.093405853096734, w0=72.73124999999975, w1=13.573360575627165\n",
      "Gradient Descent(7294/9999): loss=2.238904453873314, w0=72.56249999999974, w1=13.428696783180646\n",
      "Gradient Descent(7295/9999): loss=2.5147688698862023, w0=72.56249999999974, w1=13.146195739422954\n",
      "Gradient Descent(7296/9999): loss=1.986369046765719, w0=72.50624999999974, w1=12.987651747071382\n",
      "Gradient Descent(7297/9999): loss=2.159033881944054, w0=72.50624999999974, w1=12.80601924176067\n",
      "Gradient Descent(7298/9999): loss=2.433329965760729, w0=72.84374999999974, w1=12.935078719979701\n",
      "Gradient Descent(7299/9999): loss=2.136005114077179, w0=72.78749999999974, w1=12.909492030554203\n",
      "Gradient Descent(7300/9999): loss=2.488363838552069, w0=72.84374999999974, w1=12.767052459860816\n",
      "Gradient Descent(7301/9999): loss=2.0768586056668346, w0=72.67499999999974, w1=12.936781248325673\n",
      "Gradient Descent(7302/9999): loss=2.0729723700831135, w0=72.89999999999974, w1=13.053677501307961\n",
      "Gradient Descent(7303/9999): loss=2.0659579048459804, w0=72.95624999999974, w1=13.270486397672597\n",
      "Gradient Descent(7304/9999): loss=2.3268973435486346, w0=72.84374999999974, w1=13.602984405631004\n",
      "Gradient Descent(7305/9999): loss=2.5548319656242633, w0=72.67499999999974, w1=13.53048334407261\n",
      "Gradient Descent(7306/9999): loss=2.2554777829549923, w0=72.84374999999974, w1=14.003655851092384\n",
      "Gradient Descent(7307/9999): loss=1.9739607481689985, w0=72.95624999999974, w1=14.234529784585062\n",
      "Gradient Descent(7308/9999): loss=2.2194119098321567, w0=72.89999999999974, w1=14.190529759462278\n",
      "Gradient Descent(7309/9999): loss=2.7188587932553148, w0=73.12499999999973, w1=14.108637303652628\n",
      "Gradient Descent(7310/9999): loss=1.945323744617149, w0=73.18124999999974, w1=14.009903591239798\n",
      "Gradient Descent(7311/9999): loss=1.9840175697832954, w0=73.12499999999973, w1=13.984716794418564\n",
      "Gradient Descent(7312/9999): loss=1.8256300614708874, w0=73.12499999999973, w1=13.82288772355473\n",
      "Gradient Descent(7313/9999): loss=1.9683143045832667, w0=73.06874999999972, w1=13.722484534822344\n",
      "Gradient Descent(7314/9999): loss=1.4518569635156964, w0=73.12499999999973, w1=13.822002691985693\n",
      "Gradient Descent(7315/9999): loss=2.834160454697019, w0=73.01249999999973, w1=13.852393171066597\n",
      "Gradient Descent(7316/9999): loss=2.2666463123401317, w0=73.01249999999973, w1=13.74397226534988\n",
      "Gradient Descent(7317/9999): loss=2.765993061559844, w0=73.23749999999973, w1=13.577076001276408\n",
      "Gradient Descent(7318/9999): loss=2.2087156745142567, w0=73.29374999999973, w1=13.668557646031498\n",
      "Gradient Descent(7319/9999): loss=2.6076672829619536, w0=73.29374999999973, w1=13.633768644616504\n",
      "Gradient Descent(7320/9999): loss=2.606670916618758, w0=73.46249999999974, w1=13.763779903696795\n",
      "Gradient Descent(7321/9999): loss=2.2944617123545763, w0=73.29374999999973, w1=13.538862501350078\n",
      "Gradient Descent(7322/9999): loss=1.7668997749058837, w0=73.12499999999973, w1=13.611219132755135\n",
      "Gradient Descent(7323/9999): loss=2.436286568937708, w0=72.95624999999973, w1=13.843859496349749\n",
      "Gradient Descent(7324/9999): loss=2.0294664818457138, w0=72.89999999999972, w1=13.888335837238765\n",
      "Gradient Descent(7325/9999): loss=2.251655107132761, w0=72.89999999999972, w1=13.6609757015019\n",
      "Gradient Descent(7326/9999): loss=1.8349985531153414, w0=72.84374999999972, w1=13.231698394129149\n",
      "Gradient Descent(7327/9999): loss=1.690000827929464, w0=72.89999999999972, w1=13.127680906895675\n",
      "Gradient Descent(7328/9999): loss=2.107619382608707, w0=73.06874999999972, w1=13.126031919721553\n",
      "Gradient Descent(7329/9999): loss=2.6154638966762165, w0=73.34999999999972, w1=13.199830777018832\n",
      "Gradient Descent(7330/9999): loss=2.346587612134028, w0=73.46249999999972, w1=13.249875964869254\n",
      "Gradient Descent(7331/9999): loss=2.22507501180807, w0=73.29374999999972, w1=13.791541937321798\n",
      "Gradient Descent(7332/9999): loss=2.2654101092038292, w0=73.12499999999972, w1=13.630903051785277\n",
      "Gradient Descent(7333/9999): loss=1.818567813202246, w0=72.84374999999972, w1=13.668158404928421\n",
      "Gradient Descent(7334/9999): loss=2.417990017826484, w0=72.73124999999972, w1=13.6672178715307\n",
      "Gradient Descent(7335/9999): loss=2.436581399728567, w0=72.56249999999972, w1=13.668356312134998\n",
      "Gradient Descent(7336/9999): loss=2.4347906786907236, w0=72.73124999999972, w1=13.753386545336076\n",
      "Gradient Descent(7337/9999): loss=2.466915781878483, w0=72.95624999999971, w1=14.120720381553433\n",
      "Gradient Descent(7338/9999): loss=2.299535339165361, w0=72.95624999999971, w1=14.10201423431416\n",
      "Gradient Descent(7339/9999): loss=2.42047097861839, w0=73.06874999999971, w1=13.950297125333334\n",
      "Gradient Descent(7340/9999): loss=1.6939797915955723, w0=73.34999999999971, w1=13.935228592833433\n",
      "Gradient Descent(7341/9999): loss=2.3276682628100853, w0=73.5749999999997, w1=13.833491216041569\n",
      "Gradient Descent(7342/9999): loss=2.861085014083921, w0=73.7999999999997, w1=13.799676780791467\n",
      "Gradient Descent(7343/9999): loss=2.2077077329797428, w0=74.0249999999997, w1=13.958882301637269\n",
      "Gradient Descent(7344/9999): loss=1.9722390020811127, w0=73.9124999999997, w1=13.698794773444371\n",
      "Gradient Descent(7345/9999): loss=2.3381250643785725, w0=74.1937499999997, w1=13.610423705188934\n",
      "Gradient Descent(7346/9999): loss=2.474738604311853, w0=74.3062499999997, w1=13.42004285701623\n",
      "Gradient Descent(7347/9999): loss=2.6502701083464646, w0=74.13749999999969, w1=13.451247894689926\n",
      "Gradient Descent(7348/9999): loss=3.160472380216918, w0=74.08124999999968, w1=13.32762668188157\n",
      "Gradient Descent(7349/9999): loss=1.9976648547569056, w0=73.91249999999968, w1=13.398984661239155\n",
      "Gradient Descent(7350/9999): loss=2.708119085702188, w0=73.91249999999968, w1=13.41061915524627\n",
      "Gradient Descent(7351/9999): loss=1.7460004718120017, w0=73.74374999999968, w1=13.389760605516315\n",
      "Gradient Descent(7352/9999): loss=2.3090156523495917, w0=73.63124999999968, w1=13.266698264835512\n",
      "Gradient Descent(7353/9999): loss=1.9849044655945676, w0=73.91249999999968, w1=13.301623333523287\n",
      "Gradient Descent(7354/9999): loss=1.8711250152474255, w0=73.96874999999969, w1=13.497814213417326\n",
      "Gradient Descent(7355/9999): loss=2.3030719654724496, w0=73.85624999999969, w1=13.514416577465852\n",
      "Gradient Descent(7356/9999): loss=2.087510726717255, w0=73.79999999999968, w1=13.526041342615972\n",
      "Gradient Descent(7357/9999): loss=1.9485094468749586, w0=73.85624999999969, w1=13.513714345514726\n",
      "Gradient Descent(7358/9999): loss=1.7987779609076393, w0=73.9124999999997, w1=13.655612060420596\n",
      "Gradient Descent(7359/9999): loss=2.4993788702002684, w0=73.6874999999997, w1=13.554516483437395\n",
      "Gradient Descent(7360/9999): loss=2.0026325849023805, w0=73.46249999999971, w1=13.78851942790681\n",
      "Gradient Descent(7361/9999): loss=2.427444030882377, w0=73.6874999999997, w1=13.89914600266023\n",
      "Gradient Descent(7362/9999): loss=1.9474318746268162, w0=73.6874999999997, w1=13.73351281377112\n",
      "Gradient Descent(7363/9999): loss=2.304883114759777, w0=73.46249999999971, w1=13.6099165118563\n",
      "Gradient Descent(7364/9999): loss=2.563247902354119, w0=73.5749999999997, w1=13.655948728489472\n",
      "Gradient Descent(7365/9999): loss=2.1425416267020965, w0=73.6874999999997, w1=13.573305451956967\n",
      "Gradient Descent(7366/9999): loss=1.9744954188417358, w0=73.2937499999997, w1=13.48106872738541\n",
      "Gradient Descent(7367/9999): loss=3.1462359854793394, w0=73.4062499999997, w1=13.505168430110482\n",
      "Gradient Descent(7368/9999): loss=2.321597800295261, w0=72.89999999999971, w1=13.531900617389296\n",
      "Gradient Descent(7369/9999): loss=2.0439439082464137, w0=72.61874999999971, w1=13.630130156307116\n",
      "Gradient Descent(7370/9999): loss=2.464941099375641, w0=72.61874999999971, w1=13.413493787364752\n",
      "Gradient Descent(7371/9999): loss=2.2319607174665075, w0=72.61874999999971, w1=13.781874024426822\n",
      "Gradient Descent(7372/9999): loss=2.561883970330141, w0=72.78749999999971, w1=13.446015400002324\n",
      "Gradient Descent(7373/9999): loss=2.6432756523047143, w0=73.06874999999971, w1=13.771788520080454\n",
      "Gradient Descent(7374/9999): loss=2.112495007288568, w0=73.23749999999971, w1=13.535164405469514\n",
      "Gradient Descent(7375/9999): loss=1.7441889438125646, w0=73.18124999999971, w1=13.58619681300306\n",
      "Gradient Descent(7376/9999): loss=2.220855502639969, w0=73.06874999999971, w1=13.62216517312991\n",
      "Gradient Descent(7377/9999): loss=2.213932065308648, w0=73.12499999999972, w1=13.453237254926451\n",
      "Gradient Descent(7378/9999): loss=2.039352336450554, w0=72.95624999999971, w1=13.424231826454024\n",
      "Gradient Descent(7379/9999): loss=2.521715650535569, w0=73.18124999999971, w1=13.231104633812468\n",
      "Gradient Descent(7380/9999): loss=1.7385253705142885, w0=73.23749999999971, w1=13.02933971127334\n",
      "Gradient Descent(7381/9999): loss=2.897701038029619, w0=73.18124999999971, w1=13.299568367021697\n",
      "Gradient Descent(7382/9999): loss=2.2206699822907376, w0=73.23749999999971, w1=13.215573880236848\n",
      "Gradient Descent(7383/9999): loss=2.291726695349585, w0=73.40624999999972, w1=13.176212020238019\n",
      "Gradient Descent(7384/9999): loss=2.389574626079206, w0=73.57499999999972, w1=13.383213695581045\n",
      "Gradient Descent(7385/9999): loss=1.7758643189612418, w0=73.57499999999972, w1=13.312151422678008\n",
      "Gradient Descent(7386/9999): loss=2.278940204040955, w0=73.51874999999971, w1=13.321600435088639\n",
      "Gradient Descent(7387/9999): loss=2.194896652068021, w0=73.63124999999971, w1=13.212019386221511\n",
      "Gradient Descent(7388/9999): loss=2.095789810319817, w0=73.74374999999971, w1=13.182218016696904\n",
      "Gradient Descent(7389/9999): loss=2.065114403805187, w0=73.46249999999971, w1=13.248538457802653\n",
      "Gradient Descent(7390/9999): loss=2.6208221249214603, w0=73.46249999999971, w1=13.334647742820083\n",
      "Gradient Descent(7391/9999): loss=1.6446756395486497, w0=73.4062499999997, w1=13.503652105554606\n",
      "Gradient Descent(7392/9999): loss=2.466107597995144, w0=73.2937499999997, w1=13.478187328584212\n",
      "Gradient Descent(7393/9999): loss=2.587338566265127, w0=73.18124999999971, w1=13.34959229512313\n",
      "Gradient Descent(7394/9999): loss=2.0786463515012885, w0=72.89999999999971, w1=13.27830831560774\n",
      "Gradient Descent(7395/9999): loss=2.238065322388772, w0=72.89999999999971, w1=13.21079757437113\n",
      "Gradient Descent(7396/9999): loss=2.521383684255338, w0=72.89999999999971, w1=13.257165285774786\n",
      "Gradient Descent(7397/9999): loss=2.2246855552214573, w0=72.8437499999997, w1=13.170474070575843\n",
      "Gradient Descent(7398/9999): loss=2.6454687529719934, w0=72.6749999999997, w1=13.374693595414962\n",
      "Gradient Descent(7399/9999): loss=2.196006474643182, w0=72.7874999999997, w1=13.473464285518316\n",
      "Gradient Descent(7400/9999): loss=2.1618412561892075, w0=72.7874999999997, w1=13.345158285076572\n",
      "Gradient Descent(7401/9999): loss=2.0603219672440547, w0=72.8999999999997, w1=13.366739615252772\n",
      "Gradient Descent(7402/9999): loss=2.0438681535066667, w0=72.9562499999997, w1=13.61119563585328\n",
      "Gradient Descent(7403/9999): loss=2.059657552311446, w0=72.9562499999997, w1=13.15641519622039\n",
      "Gradient Descent(7404/9999): loss=2.7226986674984195, w0=72.8999999999997, w1=13.001373779370528\n",
      "Gradient Descent(7405/9999): loss=2.0088334586013117, w0=72.9562499999997, w1=13.039554625669078\n",
      "Gradient Descent(7406/9999): loss=1.7914479045953693, w0=73.0687499999997, w1=13.273300352244464\n",
      "Gradient Descent(7407/9999): loss=1.553827791747678, w0=73.29374999999969, w1=12.953585060582572\n",
      "Gradient Descent(7408/9999): loss=2.001221784620162, w0=73.01249999999969, w1=12.868937372631764\n",
      "Gradient Descent(7409/9999): loss=2.3335800053783835, w0=73.29374999999969, w1=13.161751295271168\n",
      "Gradient Descent(7410/9999): loss=2.384995635966207, w0=73.23749999999968, w1=13.36887788788432\n",
      "Gradient Descent(7411/9999): loss=2.3913194255169548, w0=73.29374999999969, w1=13.517410603844548\n",
      "Gradient Descent(7412/9999): loss=2.2494173583159953, w0=73.29374999999969, w1=13.61713690800414\n",
      "Gradient Descent(7413/9999): loss=1.9738561609020124, w0=73.40624999999969, w1=13.55419111220932\n",
      "Gradient Descent(7414/9999): loss=1.9796484919870376, w0=73.63124999999968, w1=13.8278383775397\n",
      "Gradient Descent(7415/9999): loss=2.095732591741654, w0=73.40624999999969, w1=13.720555564883563\n",
      "Gradient Descent(7416/9999): loss=2.281307487045309, w0=73.1812499999997, w1=13.617107418898504\n",
      "Gradient Descent(7417/9999): loss=1.3574346884972592, w0=73.2374999999997, w1=13.510529668148028\n",
      "Gradient Descent(7418/9999): loss=2.146227630279818, w0=72.8437499999997, w1=13.812406770185023\n",
      "Gradient Descent(7419/9999): loss=2.5655995470476247, w0=73.18124999999971, w1=13.497368149863895\n",
      "Gradient Descent(7420/9999): loss=1.8242513518175918, w0=73.18124999999971, w1=13.572181112810352\n",
      "Gradient Descent(7421/9999): loss=2.2338570371633297, w0=73.06874999999971, w1=13.415659969761103\n",
      "Gradient Descent(7422/9999): loss=2.318253032292252, w0=73.0124999999997, w1=13.349165712187899\n",
      "Gradient Descent(7423/9999): loss=2.207248327255736, w0=73.34999999999971, w1=13.353742858126198\n",
      "Gradient Descent(7424/9999): loss=2.3961043390798666, w0=73.46249999999971, w1=13.553379420388938\n",
      "Gradient Descent(7425/9999): loss=2.550029027490293, w0=73.18124999999971, w1=13.436853367282605\n",
      "Gradient Descent(7426/9999): loss=2.3839423698836475, w0=73.23749999999971, w1=13.44815942387482\n",
      "Gradient Descent(7427/9999): loss=2.0096685265634013, w0=73.23749999999971, w1=13.52667326740714\n",
      "Gradient Descent(7428/9999): loss=1.9890644864410951, w0=73.46249999999971, w1=13.638576587043632\n",
      "Gradient Descent(7429/9999): loss=2.270982408478261, w0=73.4062499999997, w1=13.941688507904308\n",
      "Gradient Descent(7430/9999): loss=1.7878879346402885, w0=73.46249999999971, w1=13.980186210051269\n",
      "Gradient Descent(7431/9999): loss=2.0867295718008423, w0=73.4062499999997, w1=14.130131332239072\n",
      "Gradient Descent(7432/9999): loss=1.9300761322238653, w0=73.5187499999997, w1=14.393960590195231\n",
      "Gradient Descent(7433/9999): loss=2.046379016929433, w0=73.2937499999997, w1=14.372447079704996\n",
      "Gradient Descent(7434/9999): loss=2.674664444362555, w0=72.9562499999997, w1=14.393169345611822\n",
      "Gradient Descent(7435/9999): loss=2.0662802373621103, w0=73.0687499999997, w1=14.514368382756066\n",
      "Gradient Descent(7436/9999): loss=2.166162457495471, w0=73.01249999999969, w1=14.26602413667331\n",
      "Gradient Descent(7437/9999): loss=1.8901052364740538, w0=73.01249999999969, w1=14.127220566222922\n",
      "Gradient Descent(7438/9999): loss=2.255974521703118, w0=73.0687499999997, w1=14.080911223076972\n",
      "Gradient Descent(7439/9999): loss=2.236985343500016, w0=73.1812499999997, w1=13.776063800047512\n",
      "Gradient Descent(7440/9999): loss=2.075769654613171, w0=73.5187499999997, w1=13.813654587466276\n",
      "Gradient Descent(7441/9999): loss=2.2095564357296578, w0=73.5749999999997, w1=13.987187711443132\n",
      "Gradient Descent(7442/9999): loss=2.42292072567394, w0=73.1249999999997, w1=13.947558300965666\n",
      "Gradient Descent(7443/9999): loss=2.0082058688908067, w0=73.18124999999971, w1=14.026055986326174\n",
      "Gradient Descent(7444/9999): loss=2.1643885755292427, w0=73.1249999999997, w1=14.080637396253861\n",
      "Gradient Descent(7445/9999): loss=2.2786652045568885, w0=73.18124999999971, w1=14.098304512381278\n",
      "Gradient Descent(7446/9999): loss=2.375916019485971, w0=73.2937499999997, w1=14.033783285798417\n",
      "Gradient Descent(7447/9999): loss=2.3181450376210684, w0=73.2374999999997, w1=14.022683259882832\n",
      "Gradient Descent(7448/9999): loss=1.9172737078076425, w0=73.5749999999997, w1=13.9527785066394\n",
      "Gradient Descent(7449/9999): loss=2.516571046003195, w0=73.63124999999971, w1=14.074619407450895\n",
      "Gradient Descent(7450/9999): loss=2.0715390126320843, w0=73.63124999999971, w1=14.07260983081089\n",
      "Gradient Descent(7451/9999): loss=1.9574319858311648, w0=73.34999999999971, w1=14.199623073358238\n",
      "Gradient Descent(7452/9999): loss=2.0163272031839425, w0=73.63124999999971, w1=14.191667528617833\n",
      "Gradient Descent(7453/9999): loss=2.1744241325069096, w0=73.79999999999971, w1=14.303669583693623\n",
      "Gradient Descent(7454/9999): loss=2.3468128115694853, w0=73.85624999999972, w1=14.189753782167173\n",
      "Gradient Descent(7455/9999): loss=2.067628045192197, w0=74.02499999999972, w1=13.967438328569473\n",
      "Gradient Descent(7456/9999): loss=1.9878007384177379, w0=73.85624999999972, w1=13.670410659817422\n",
      "Gradient Descent(7457/9999): loss=2.649861649785314, w0=73.74374999999972, w1=13.312203969782693\n",
      "Gradient Descent(7458/9999): loss=2.2387111480492186, w0=73.57499999999972, w1=13.265779570938392\n",
      "Gradient Descent(7459/9999): loss=2.1392148331495466, w0=73.79999999999971, w1=13.301331685045106\n",
      "Gradient Descent(7460/9999): loss=2.2816295743738038, w0=73.51874999999971, w1=13.450997376521515\n",
      "Gradient Descent(7461/9999): loss=2.842803803946952, w0=73.29374999999972, w1=13.239916851472701\n",
      "Gradient Descent(7462/9999): loss=2.092162885042015, w0=73.51874999999971, w1=13.5114871183583\n",
      "Gradient Descent(7463/9999): loss=2.1324238341714583, w0=73.40624999999972, w1=13.358308448612174\n",
      "Gradient Descent(7464/9999): loss=1.9251749346112146, w0=73.18124999999972, w1=13.47827794217794\n",
      "Gradient Descent(7465/9999): loss=1.7239687058044004, w0=73.06874999999972, w1=13.440607763829053\n",
      "Gradient Descent(7466/9999): loss=2.2372313209629984, w0=73.51874999999973, w1=13.341902506346543\n",
      "Gradient Descent(7467/9999): loss=2.1432628225385386, w0=73.57499999999973, w1=13.458755141528048\n",
      "Gradient Descent(7468/9999): loss=1.9651054575849418, w0=73.79999999999973, w1=13.593373974142587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7469/9999): loss=2.531056026786761, w0=73.57499999999973, w1=13.416384478156736\n",
      "Gradient Descent(7470/9999): loss=2.600107929995027, w0=73.57499999999973, w1=13.634129747271558\n",
      "Gradient Descent(7471/9999): loss=2.2610252251837792, w0=73.74374999999974, w1=13.690579572655441\n",
      "Gradient Descent(7472/9999): loss=2.263517637004757, w0=73.46249999999974, w1=13.830559550769232\n",
      "Gradient Descent(7473/9999): loss=2.2749908226950346, w0=73.63124999999974, w1=13.823819061774781\n",
      "Gradient Descent(7474/9999): loss=2.489461570550928, w0=73.34999999999974, w1=13.522760982654546\n",
      "Gradient Descent(7475/9999): loss=2.274774435367699, w0=73.18124999999974, w1=13.412428931153904\n",
      "Gradient Descent(7476/9999): loss=2.2283861825540776, w0=73.23749999999974, w1=13.024923147655517\n",
      "Gradient Descent(7477/9999): loss=1.7051303811834182, w0=73.12499999999974, w1=13.269194674140582\n",
      "Gradient Descent(7478/9999): loss=2.518631486894548, w0=72.84374999999974, w1=13.035769530658376\n",
      "Gradient Descent(7479/9999): loss=3.0843207338263117, w0=72.78749999999974, w1=13.058119182524567\n",
      "Gradient Descent(7480/9999): loss=2.57977693690999, w0=72.61874999999974, w1=12.974053719113197\n",
      "Gradient Descent(7481/9999): loss=1.8228275108725027, w0=72.78749999999974, w1=13.241211011666325\n",
      "Gradient Descent(7482/9999): loss=1.8712811638325868, w0=73.12499999999974, w1=13.025749774623566\n",
      "Gradient Descent(7483/9999): loss=2.0923752771512434, w0=73.01249999999975, w1=13.069512512370602\n",
      "Gradient Descent(7484/9999): loss=2.274209997575529, w0=72.95624999999974, w1=12.964378594955727\n",
      "Gradient Descent(7485/9999): loss=1.9671123690135297, w0=73.06874999999974, w1=12.671335360759823\n",
      "Gradient Descent(7486/9999): loss=2.8151794505183427, w0=73.18124999999974, w1=12.895214947711247\n",
      "Gradient Descent(7487/9999): loss=2.357896184630091, w0=73.18124999999974, w1=12.768351957216233\n",
      "Gradient Descent(7488/9999): loss=2.835766760953004, w0=73.23749999999974, w1=13.097144286366438\n",
      "Gradient Descent(7489/9999): loss=2.308275069333902, w0=73.18124999999974, w1=13.407936224201967\n",
      "Gradient Descent(7490/9999): loss=2.782783841710822, w0=73.06874999999974, w1=13.557633015810547\n",
      "Gradient Descent(7491/9999): loss=2.420855030507423, w0=73.12499999999974, w1=13.349968154934283\n",
      "Gradient Descent(7492/9999): loss=2.208041251784178, w0=73.18124999999975, w1=13.410408967202194\n",
      "Gradient Descent(7493/9999): loss=2.4263068378846357, w0=73.06874999999975, w1=13.261093449685996\n",
      "Gradient Descent(7494/9999): loss=2.1549933652084308, w0=73.01249999999975, w1=13.246483301780236\n",
      "Gradient Descent(7495/9999): loss=2.3044557433852386, w0=73.06874999999975, w1=13.132913426058726\n",
      "Gradient Descent(7496/9999): loss=2.5523950754099447, w0=73.18124999999975, w1=13.126895526475261\n",
      "Gradient Descent(7497/9999): loss=1.910527932106669, w0=72.95624999999976, w1=13.071119164398974\n",
      "Gradient Descent(7498/9999): loss=2.166499489340344, w0=73.01249999999976, w1=13.090110389733427\n",
      "Gradient Descent(7499/9999): loss=1.7391041074189086, w0=73.01249999999976, w1=13.155458396994497\n",
      "Gradient Descent(7500/9999): loss=2.3162523077464616, w0=72.73124999999976, w1=13.302092563695417\n",
      "Gradient Descent(7501/9999): loss=1.8027269416449208, w0=72.78749999999977, w1=13.309490382377188\n",
      "Gradient Descent(7502/9999): loss=2.2530321303365675, w0=73.06874999999977, w1=13.594527523576224\n",
      "Gradient Descent(7503/9999): loss=1.8656429118391082, w0=73.23749999999977, w1=13.696895550313045\n",
      "Gradient Descent(7504/9999): loss=2.2875431522385665, w0=73.46249999999976, w1=13.567012718321033\n",
      "Gradient Descent(7505/9999): loss=1.7950237077777056, w0=73.34999999999977, w1=13.790239842088797\n",
      "Gradient Descent(7506/9999): loss=2.4236156527137807, w0=73.18124999999976, w1=13.862954515538915\n",
      "Gradient Descent(7507/9999): loss=1.9788353715726328, w0=73.23749999999977, w1=14.012236784193806\n",
      "Gradient Descent(7508/9999): loss=1.9542373219000806, w0=73.57499999999978, w1=13.531560679593257\n",
      "Gradient Descent(7509/9999): loss=2.004079043440262, w0=73.51874999999977, w1=13.164643261634563\n",
      "Gradient Descent(7510/9999): loss=2.166661634259678, w0=73.46249999999976, w1=13.247089631651995\n",
      "Gradient Descent(7511/9999): loss=2.922781360987238, w0=73.40624999999976, w1=13.178896472559295\n",
      "Gradient Descent(7512/9999): loss=2.284180468147302, w0=73.18124999999976, w1=13.391342324887022\n",
      "Gradient Descent(7513/9999): loss=2.179907288343732, w0=73.18124999999976, w1=13.547355513047906\n",
      "Gradient Descent(7514/9999): loss=2.501865737635646, w0=73.23749999999977, w1=13.593175902157562\n",
      "Gradient Descent(7515/9999): loss=2.9016204271671935, w0=73.29374999999978, w1=13.639500012351839\n",
      "Gradient Descent(7516/9999): loss=1.7769469346430062, w0=73.46249999999978, w1=13.48774413431699\n",
      "Gradient Descent(7517/9999): loss=2.179411372820831, w0=73.68749999999977, w1=13.369932804462781\n",
      "Gradient Descent(7518/9999): loss=2.1855624764575596, w0=73.46249999999978, w1=13.498121777421265\n",
      "Gradient Descent(7519/9999): loss=1.9554376105061606, w0=73.51874999999978, w1=13.607946243172576\n",
      "Gradient Descent(7520/9999): loss=2.6320905768599814, w0=73.57499999999979, w1=13.340572775266406\n",
      "Gradient Descent(7521/9999): loss=2.1367397496811935, w0=73.40624999999979, w1=13.433479949458004\n",
      "Gradient Descent(7522/9999): loss=2.4144183200516536, w0=73.23749999999978, w1=13.55594573920778\n",
      "Gradient Descent(7523/9999): loss=1.9434255610036453, w0=73.23749999999978, w1=13.476599150278973\n",
      "Gradient Descent(7524/9999): loss=2.1299023579389686, w0=73.12499999999979, w1=13.702749078166638\n",
      "Gradient Descent(7525/9999): loss=1.7204869784630419, w0=73.12499999999979, w1=13.905304522551651\n",
      "Gradient Descent(7526/9999): loss=2.0797087413402497, w0=72.84374999999979, w1=13.911947750525467\n",
      "Gradient Descent(7527/9999): loss=2.256549393649683, w0=72.73124999999979, w1=14.049233553426959\n",
      "Gradient Descent(7528/9999): loss=2.021971957823463, w0=72.84374999999979, w1=13.77127862497197\n",
      "Gradient Descent(7529/9999): loss=2.340296400356574, w0=72.89999999999979, w1=13.938632316873981\n",
      "Gradient Descent(7530/9999): loss=2.255536829751543, w0=73.0687499999998, w1=13.766198307282174\n",
      "Gradient Descent(7531/9999): loss=2.1721313761199372, w0=73.18124999999979, w1=13.514569794677202\n",
      "Gradient Descent(7532/9999): loss=1.8899055147176913, w0=72.89999999999979, w1=13.514953710541251\n",
      "Gradient Descent(7533/9999): loss=2.3435506724834223, w0=73.18124999999979, w1=13.302725566428753\n",
      "Gradient Descent(7534/9999): loss=2.9461085398572555, w0=73.46249999999979, w1=13.600701513823418\n",
      "Gradient Descent(7535/9999): loss=2.3244381360647868, w0=73.46249999999979, w1=13.32183734133234\n",
      "Gradient Descent(7536/9999): loss=1.5947446582210665, w0=73.18124999999979, w1=13.23881467233911\n",
      "Gradient Descent(7537/9999): loss=2.545597207996545, w0=72.9562499999998, w1=13.579138477252902\n",
      "Gradient Descent(7538/9999): loss=2.0491174095208358, w0=73.0124999999998, w1=13.351402787341103\n",
      "Gradient Descent(7539/9999): loss=2.2235252881470227, w0=73.1812499999998, w1=13.26218039742879\n",
      "Gradient Descent(7540/9999): loss=2.156553275364575, w0=72.95624999999981, w1=13.444113045174179\n",
      "Gradient Descent(7541/9999): loss=2.3923668892537644, w0=73.06874999999981, w1=13.196456349850292\n",
      "Gradient Descent(7542/9999): loss=2.0782642289598723, w0=73.1812499999998, w1=13.138851989613832\n",
      "Gradient Descent(7543/9999): loss=2.2205741288920455, w0=73.4062499999998, w1=13.252877926612461\n",
      "Gradient Descent(7544/9999): loss=1.6012753423019688, w0=73.4062499999998, w1=13.321944454402395\n",
      "Gradient Descent(7545/9999): loss=1.7137316011447004, w0=73.1812499999998, w1=13.184947448494242\n",
      "Gradient Descent(7546/9999): loss=2.0073431683430196, w0=73.34999999999981, w1=13.33123937615047\n",
      "Gradient Descent(7547/9999): loss=2.0688432422482315, w0=73.23749999999981, w1=13.3688987235818\n",
      "Gradient Descent(7548/9999): loss=2.1314512807991517, w0=73.06874999999981, w1=13.243179351269266\n",
      "Gradient Descent(7549/9999): loss=2.796998722494466, w0=73.40624999999982, w1=13.11175244515471\n",
      "Gradient Descent(7550/9999): loss=2.0853309347547833, w0=73.34999999999981, w1=13.191423360620302\n",
      "Gradient Descent(7551/9999): loss=2.075826945788741, w0=73.4624999999998, w1=13.51608588176401\n",
      "Gradient Descent(7552/9999): loss=2.1559989975511, w0=73.6874999999998, w1=13.366471184249695\n",
      "Gradient Descent(7553/9999): loss=2.031785112348673, w0=73.5187499999998, w1=13.256041070827374\n",
      "Gradient Descent(7554/9999): loss=2.0603379156289057, w0=73.2374999999998, w1=13.56341672017073\n",
      "Gradient Descent(7555/9999): loss=2.3920330904964016, w0=73.18124999999979, w1=13.645555855015898\n",
      "Gradient Descent(7556/9999): loss=2.293824483379531, w0=73.3499999999998, w1=13.667129564670093\n",
      "Gradient Descent(7557/9999): loss=2.2399843632811782, w0=73.6312499999998, w1=13.784647833503364\n",
      "Gradient Descent(7558/9999): loss=2.424427969043955, w0=73.5187499999998, w1=13.470508536087273\n",
      "Gradient Descent(7559/9999): loss=2.036943268643402, w0=73.6312499999998, w1=13.473881396966444\n",
      "Gradient Descent(7560/9999): loss=2.306216835228983, w0=73.6874999999998, w1=13.573232733040578\n",
      "Gradient Descent(7561/9999): loss=2.5768727897556145, w0=73.5749999999998, w1=13.34403786177623\n",
      "Gradient Descent(7562/9999): loss=2.0102449687594905, w0=73.6874999999998, w1=13.3202206290563\n",
      "Gradient Descent(7563/9999): loss=1.9301773625589709, w0=73.3499999999998, w1=13.16718264480962\n",
      "Gradient Descent(7564/9999): loss=2.2237057952379073, w0=73.3499999999998, w1=13.529512678710917\n",
      "Gradient Descent(7565/9999): loss=2.2651253735757706, w0=73.3499999999998, w1=13.580180482866249\n",
      "Gradient Descent(7566/9999): loss=1.9502970476124517, w0=73.5187499999998, w1=13.710522698647702\n",
      "Gradient Descent(7567/9999): loss=1.7452806473769316, w0=73.46249999999979, w1=13.662362639144018\n",
      "Gradient Descent(7568/9999): loss=2.3110822573083567, w0=73.0687499999998, w1=13.76323990689521\n",
      "Gradient Descent(7569/9999): loss=2.038210527510959, w0=73.0687499999998, w1=13.64376732942486\n",
      "Gradient Descent(7570/9999): loss=1.9300255689551569, w0=73.18124999999979, w1=13.747320044119398\n",
      "Gradient Descent(7571/9999): loss=2.7256483561860216, w0=73.40624999999979, w1=13.781290506006115\n",
      "Gradient Descent(7572/9999): loss=2.095547601452237, w0=73.06874999999978, w1=14.005912507324817\n",
      "Gradient Descent(7573/9999): loss=2.3064258058739533, w0=72.89999999999978, w1=13.98278651398345\n",
      "Gradient Descent(7574/9999): loss=1.4360004413412282, w0=72.78749999999978, w1=14.081753410258365\n",
      "Gradient Descent(7575/9999): loss=2.2308550184252587, w0=72.50624999999978, w1=13.854020286269847\n",
      "Gradient Descent(7576/9999): loss=2.543456115001864, w0=72.44999999999978, w1=13.956058185186453\n",
      "Gradient Descent(7577/9999): loss=2.007394430803095, w0=72.50624999999978, w1=13.943624091201091\n",
      "Gradient Descent(7578/9999): loss=2.5244496225846382, w0=72.84374999999979, w1=14.050403434164275\n",
      "Gradient Descent(7579/9999): loss=2.1908950563038245, w0=72.89999999999979, w1=14.091203701846737\n",
      "Gradient Descent(7580/9999): loss=1.9970180778858695, w0=73.12499999999979, w1=13.871310364391633\n",
      "Gradient Descent(7581/9999): loss=1.9332878553955624, w0=73.29374999999979, w1=13.635201892473388\n",
      "Gradient Descent(7582/9999): loss=2.4679109748185204, w0=73.18124999999979, w1=13.780938629334866\n",
      "Gradient Descent(7583/9999): loss=2.119170609717795, w0=73.01249999999979, w1=13.67707023540262\n",
      "Gradient Descent(7584/9999): loss=2.2065933285993546, w0=72.89999999999979, w1=13.742864340265635\n",
      "Gradient Descent(7585/9999): loss=2.559841519671288, w0=73.0687499999998, w1=13.935422075021343\n",
      "Gradient Descent(7586/9999): loss=2.119034758800784, w0=73.0687499999998, w1=13.925035435152635\n",
      "Gradient Descent(7587/9999): loss=1.989319950832077, w0=73.18124999999979, w1=13.860196424224915\n",
      "Gradient Descent(7588/9999): loss=1.8555088969398976, w0=72.84374999999979, w1=13.76534423699993\n",
      "Gradient Descent(7589/9999): loss=2.5856998650274985, w0=72.95624999999978, w1=13.866875502710574\n",
      "Gradient Descent(7590/9999): loss=2.2855161353083315, w0=73.18124999999978, w1=13.62050999899014\n",
      "Gradient Descent(7591/9999): loss=1.8674865747154221, w0=73.12499999999977, w1=13.546017072831733\n",
      "Gradient Descent(7592/9999): loss=2.0906725249576765, w0=73.01249999999978, w1=13.696370936632563\n",
      "Gradient Descent(7593/9999): loss=1.6791958495454429, w0=72.95624999999977, w1=13.613013082110175\n",
      "Gradient Descent(7594/9999): loss=2.449038476919287, w0=72.56249999999977, w1=13.424252944580246\n",
      "Gradient Descent(7595/9999): loss=2.633871649419566, w0=72.39374999999977, w1=13.617655152705511\n",
      "Gradient Descent(7596/9999): loss=2.482190403749871, w0=72.50624999999977, w1=13.496256463270417\n",
      "Gradient Descent(7597/9999): loss=2.452941792252893, w0=72.61874999999976, w1=13.67107173411223\n",
      "Gradient Descent(7598/9999): loss=2.257477224969329, w0=72.73124999999976, w1=13.73759576554934\n",
      "Gradient Descent(7599/9999): loss=1.8914658080748379, w0=72.89999999999976, w1=14.017489116029934\n",
      "Gradient Descent(7600/9999): loss=2.5429307027465016, w0=72.89999999999976, w1=14.122930772187074\n",
      "Gradient Descent(7601/9999): loss=2.106505632280892, w0=72.84374999999976, w1=14.176000769506166\n",
      "Gradient Descent(7602/9999): loss=2.0139852440533845, w0=72.73124999999976, w1=13.929821611740232\n",
      "Gradient Descent(7603/9999): loss=1.7872904018556988, w0=72.95624999999976, w1=13.855013450679518\n",
      "Gradient Descent(7604/9999): loss=2.4115485213779255, w0=72.89999999999975, w1=13.849312438099934\n",
      "Gradient Descent(7605/9999): loss=1.568973774414571, w0=72.56249999999974, w1=13.92235200178089\n",
      "Gradient Descent(7606/9999): loss=2.304445184565677, w0=72.84374999999974, w1=14.098999852080894\n",
      "Gradient Descent(7607/9999): loss=2.6564544095292693, w0=73.29374999999975, w1=14.185149034251172\n",
      "Gradient Descent(7608/9999): loss=2.619454387524134, w0=73.51874999999974, w1=13.784919438201948\n",
      "Gradient Descent(7609/9999): loss=2.030138488286763, w0=73.63124999999974, w1=13.676576025442044\n",
      "Gradient Descent(7610/9999): loss=2.040223950679294, w0=73.74374999999974, w1=13.453368544425212\n",
      "Gradient Descent(7611/9999): loss=2.0476812834971128, w0=73.68749999999973, w1=13.267245425419215\n",
      "Gradient Descent(7612/9999): loss=2.5415754267799433, w0=73.57499999999973, w1=13.326568745346753\n",
      "Gradient Descent(7613/9999): loss=2.070789798624851, w0=73.63124999999974, w1=13.234804127109921\n",
      "Gradient Descent(7614/9999): loss=1.9299336459360255, w0=73.57499999999973, w1=13.189581414926854\n",
      "Gradient Descent(7615/9999): loss=2.563154690618254, w0=73.29374999999973, w1=13.430254292083605\n",
      "Gradient Descent(7616/9999): loss=2.6561684214571653, w0=73.34999999999974, w1=13.617197791197341\n",
      "Gradient Descent(7617/9999): loss=2.6342095539796166, w0=73.46249999999974, w1=13.706555104414031\n",
      "Gradient Descent(7618/9999): loss=2.3341401286691896, w0=73.40624999999973, w1=13.541719416376477\n",
      "Gradient Descent(7619/9999): loss=2.0470541200405847, w0=73.51874999999973, w1=13.621445984441095\n",
      "Gradient Descent(7620/9999): loss=2.4705273573425357, w0=73.74374999999972, w1=13.450422480547891\n",
      "Gradient Descent(7621/9999): loss=2.1928764299651284, w0=73.74374999999972, w1=13.470537758177889\n",
      "Gradient Descent(7622/9999): loss=1.5157536433909953, w0=73.79999999999973, w1=13.482035989094165\n",
      "Gradient Descent(7623/9999): loss=2.7616040907396013, w0=73.51874999999973, w1=13.40485828125404\n",
      "Gradient Descent(7624/9999): loss=2.200967705870368, w0=73.57499999999973, w1=13.381798498568402\n",
      "Gradient Descent(7625/9999): loss=1.9906454366232438, w0=73.46249999999974, w1=13.417432510408199\n",
      "Gradient Descent(7626/9999): loss=1.9626974458986937, w0=73.40624999999973, w1=13.248057491087588\n",
      "Gradient Descent(7627/9999): loss=2.422144933390005, w0=73.12499999999973, w1=13.166455985231249\n",
      "Gradient Descent(7628/9999): loss=2.019378096642725, w0=72.95624999999973, w1=13.261889511081419\n",
      "Gradient Descent(7629/9999): loss=1.9171752228488939, w0=73.23749999999973, w1=13.233800878975348\n",
      "Gradient Descent(7630/9999): loss=2.3504657532676836, w0=73.12499999999973, w1=13.246282300396059\n",
      "Gradient Descent(7631/9999): loss=2.0779269614215505, w0=73.01249999999973, w1=13.271825577855147\n",
      "Gradient Descent(7632/9999): loss=2.3101556866217487, w0=73.01249999999973, w1=13.531556185093496\n",
      "Gradient Descent(7633/9999): loss=2.0693670933417794, w0=73.23749999999973, w1=13.667172639121487\n",
      "Gradient Descent(7634/9999): loss=2.1342476840244853, w0=73.18124999999972, w1=13.957617959109335\n",
      "Gradient Descent(7635/9999): loss=1.9454263882511293, w0=73.23749999999973, w1=13.90160319215925\n",
      "Gradient Descent(7636/9999): loss=1.9303764290873373, w0=73.12499999999973, w1=14.101046787739886\n",
      "Gradient Descent(7637/9999): loss=2.5151543058753414, w0=72.78749999999972, w1=14.130006639628247\n",
      "Gradient Descent(7638/9999): loss=1.8755535392106497, w0=72.84374999999973, w1=13.862397026703617\n",
      "Gradient Descent(7639/9999): loss=2.2898739083846698, w0=72.78749999999972, w1=13.698663931549541\n",
      "Gradient Descent(7640/9999): loss=1.6280806022836465, w0=72.73124999999972, w1=13.849338241145379\n",
      "Gradient Descent(7641/9999): loss=2.1515326223181193, w0=72.84374999999972, w1=13.82896504502505\n",
      "Gradient Descent(7642/9999): loss=2.1971679707245073, w0=72.95624999999971, w1=13.868340586208781\n",
      "Gradient Descent(7643/9999): loss=2.417406730179932, w0=73.18124999999971, w1=13.896004797466201\n",
      "Gradient Descent(7644/9999): loss=2.8401670118793105, w0=73.06874999999971, w1=13.55086972310764\n",
      "Gradient Descent(7645/9999): loss=2.429725161308192, w0=73.18124999999971, w1=13.434729236725548\n",
      "Gradient Descent(7646/9999): loss=2.0983927253677446, w0=73.2937499999997, w1=13.430841878639217\n",
      "Gradient Descent(7647/9999): loss=1.9547460065184474, w0=73.2937499999997, w1=13.400593748275156\n",
      "Gradient Descent(7648/9999): loss=1.6835834355664603, w0=73.06874999999971, w1=13.297366118058857\n",
      "Gradient Descent(7649/9999): loss=1.8375286105946826, w0=73.0124999999997, w1=13.260085489089823\n",
      "Gradient Descent(7650/9999): loss=2.1630870958500816, w0=73.2937499999997, w1=13.656490068466951\n",
      "Gradient Descent(7651/9999): loss=1.9974884515160478, w0=73.4062499999997, w1=13.769068330726682\n",
      "Gradient Descent(7652/9999): loss=2.08411251715806, w0=73.2937499999997, w1=14.131360895424804\n",
      "Gradient Descent(7653/9999): loss=2.48313174008838, w0=73.4062499999997, w1=14.183363620672887\n",
      "Gradient Descent(7654/9999): loss=2.255919097521398, w0=73.5187499999997, w1=14.025438766435762\n",
      "Gradient Descent(7655/9999): loss=2.428644024530988, w0=73.4062499999997, w1=13.865607810236117\n",
      "Gradient Descent(7656/9999): loss=1.9694201857890077, w0=73.2374999999997, w1=13.759326039859378\n",
      "Gradient Descent(7657/9999): loss=2.1185594578953033, w0=73.4624999999997, w1=13.7929787194408\n",
      "Gradient Descent(7658/9999): loss=2.2524147710037443, w0=73.29374999999969, w1=13.662314792345182\n",
      "Gradient Descent(7659/9999): loss=2.5670519832697893, w0=73.23749999999968, w1=13.419486541930546\n",
      "Gradient Descent(7660/9999): loss=2.570037629988904, w0=72.95624999999968, w1=13.561429212586118\n",
      "Gradient Descent(7661/9999): loss=1.9816179471927045, w0=73.01249999999969, w1=13.48152649913879\n",
      "Gradient Descent(7662/9999): loss=2.1916060828289368, w0=73.0687499999997, w1=13.613690457898725\n",
      "Gradient Descent(7663/9999): loss=1.8871914920040556, w0=73.0687499999997, w1=13.56100704688531\n",
      "Gradient Descent(7664/9999): loss=1.942894135901724, w0=72.9562499999997, w1=13.547207213129443\n",
      "Gradient Descent(7665/9999): loss=1.7922168950843758, w0=73.1249999999997, w1=13.477117351406319\n",
      "Gradient Descent(7666/9999): loss=2.077511719287532, w0=73.2374999999997, w1=13.644824746092091\n",
      "Gradient Descent(7667/9999): loss=2.1962799835618547, w0=73.1249999999997, w1=13.666534528584313\n",
      "Gradient Descent(7668/9999): loss=2.1628999200689485, w0=73.3499999999997, w1=13.554514047347888\n",
      "Gradient Descent(7669/9999): loss=2.105244332450712, w0=73.3499999999997, w1=13.654987716039575\n",
      "Gradient Descent(7670/9999): loss=1.615051778062431, w0=73.1812499999997, w1=13.72063036372838\n",
      "Gradient Descent(7671/9999): loss=2.3276079869727284, w0=73.12499999999969, w1=13.685630414815765\n",
      "Gradient Descent(7672/9999): loss=2.190711027341907, w0=72.8999999999997, w1=13.356331399492307\n",
      "Gradient Descent(7673/9999): loss=1.8070758565572702, w0=72.8999999999997, w1=13.530791007351077\n",
      "Gradient Descent(7674/9999): loss=2.309676959259047, w0=72.9562499999997, w1=13.553928509122331\n",
      "Gradient Descent(7675/9999): loss=2.145261071184854, w0=73.0124999999997, w1=13.803619327993077\n",
      "Gradient Descent(7676/9999): loss=1.8380233347775465, w0=73.0124999999997, w1=13.814065453623162\n",
      "Gradient Descent(7677/9999): loss=2.5123148812524714, w0=72.61874999999971, w1=13.842376615601585\n",
      "Gradient Descent(7678/9999): loss=2.0811232075567583, w0=72.61874999999971, w1=13.909321731327381\n",
      "Gradient Descent(7679/9999): loss=2.01875174498286, w0=72.61874999999971, w1=14.268872069858347\n",
      "Gradient Descent(7680/9999): loss=3.1515745556941432, w0=72.95624999999971, w1=14.160683686774632\n",
      "Gradient Descent(7681/9999): loss=2.3268227238865005, w0=72.84374999999972, w1=14.378373893670148\n",
      "Gradient Descent(7682/9999): loss=1.692502388079161, w0=73.06874999999971, w1=14.33044974732437\n",
      "Gradient Descent(7683/9999): loss=2.319211743802575, w0=72.95624999999971, w1=14.307970039935647\n",
      "Gradient Descent(7684/9999): loss=1.9774394877726242, w0=72.89999999999971, w1=14.330983447751946\n",
      "Gradient Descent(7685/9999): loss=2.330875512786132, w0=73.06874999999971, w1=14.232790985119818\n",
      "Gradient Descent(7686/9999): loss=2.0150883566208835, w0=73.23749999999971, w1=13.782455869518799\n",
      "Gradient Descent(7687/9999): loss=1.8967523463541305, w0=73.18124999999971, w1=13.867407106438325\n",
      "Gradient Descent(7688/9999): loss=2.3974134064882846, w0=73.4062499999997, w1=13.731250252243049\n",
      "Gradient Descent(7689/9999): loss=2.2445945790088064, w0=73.5187499999997, w1=13.635359447945454\n",
      "Gradient Descent(7690/9999): loss=2.1870810632929123, w0=73.2937499999997, w1=13.549815921875462\n",
      "Gradient Descent(7691/9999): loss=2.558583928846128, w0=73.34999999999971, w1=13.668072792218853\n",
      "Gradient Descent(7692/9999): loss=2.2458883395256284, w0=73.12499999999972, w1=13.645997866109091\n",
      "Gradient Descent(7693/9999): loss=2.160379074676519, w0=73.23749999999971, w1=13.81908677339954\n",
      "Gradient Descent(7694/9999): loss=2.1475880908864795, w0=73.18124999999971, w1=13.950107860842506\n",
      "Gradient Descent(7695/9999): loss=2.237200161464732, w0=73.23749999999971, w1=14.221478788233094\n",
      "Gradient Descent(7696/9999): loss=2.632714843430161, w0=73.29374999999972, w1=14.171249439716517\n",
      "Gradient Descent(7697/9999): loss=1.8039377619517885, w0=73.18124999999972, w1=13.9837813236357\n",
      "Gradient Descent(7698/9999): loss=2.4722778406105426, w0=72.89999999999972, w1=14.016241870787708\n",
      "Gradient Descent(7699/9999): loss=2.205116008679024, w0=73.01249999999972, w1=13.712538936638934\n",
      "Gradient Descent(7700/9999): loss=2.8432407543876517, w0=72.89999999999972, w1=13.549306627529543\n",
      "Gradient Descent(7701/9999): loss=1.8427322809106303, w0=72.89999999999972, w1=13.575146799537588\n",
      "Gradient Descent(7702/9999): loss=2.844542768691403, w0=72.89999999999972, w1=13.417649822537914\n",
      "Gradient Descent(7703/9999): loss=2.3464156271028958, w0=72.89999999999972, w1=13.623370512095434\n",
      "Gradient Descent(7704/9999): loss=2.3196520716288185, w0=72.95624999999973, w1=13.73624099946287\n",
      "Gradient Descent(7705/9999): loss=2.301941377143822, w0=73.01249999999973, w1=13.801771440394306\n",
      "Gradient Descent(7706/9999): loss=2.277975149923641, w0=72.89999999999974, w1=13.754897809026533\n",
      "Gradient Descent(7707/9999): loss=2.1503361024268424, w0=72.95624999999974, w1=13.8629300380936\n",
      "Gradient Descent(7708/9999): loss=2.1819995264415635, w0=73.23749999999974, w1=13.612363483440376\n",
      "Gradient Descent(7709/9999): loss=2.096318628445576, w0=73.34999999999974, w1=13.411311327950413\n",
      "Gradient Descent(7710/9999): loss=2.5141935972232483, w0=73.06874999999974, w1=13.52224316576717\n",
      "Gradient Descent(7711/9999): loss=2.1786100695845176, w0=73.46249999999974, w1=13.76599978627132\n",
      "Gradient Descent(7712/9999): loss=1.7826874780192323, w0=73.29374999999973, w1=13.712998448060294\n",
      "Gradient Descent(7713/9999): loss=2.2080058739814667, w0=73.23749999999973, w1=13.95851261990608\n",
      "Gradient Descent(7714/9999): loss=2.288604183383135, w0=73.06874999999972, w1=13.955033795839231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7715/9999): loss=1.8389507308341757, w0=72.95624999999973, w1=13.740633343874565\n",
      "Gradient Descent(7716/9999): loss=2.101218108107747, w0=72.89999999999972, w1=13.441818811820841\n",
      "Gradient Descent(7717/9999): loss=1.7718270509138483, w0=72.73124999999972, w1=13.563899588810006\n",
      "Gradient Descent(7718/9999): loss=2.4316564894187107, w0=72.78749999999972, w1=13.441565529062313\n",
      "Gradient Descent(7719/9999): loss=2.5882092574811018, w0=72.78749999999972, w1=13.573782365278236\n",
      "Gradient Descent(7720/9999): loss=2.2212332906454613, w0=72.61874999999972, w1=13.664439926477021\n",
      "Gradient Descent(7721/9999): loss=2.4990761037399922, w0=72.61874999999972, w1=13.827569470637656\n",
      "Gradient Descent(7722/9999): loss=2.776422444418123, w0=72.50624999999972, w1=13.906104761087473\n",
      "Gradient Descent(7723/9999): loss=2.2368873133241864, w0=72.56249999999973, w1=14.169582741700767\n",
      "Gradient Descent(7724/9999): loss=1.7657739410512918, w0=72.84374999999973, w1=13.983285022748342\n",
      "Gradient Descent(7725/9999): loss=2.210036919962703, w0=72.95624999999973, w1=14.156971119513164\n",
      "Gradient Descent(7726/9999): loss=2.1850735344278007, w0=73.18124999999972, w1=13.97171774983953\n",
      "Gradient Descent(7727/9999): loss=2.2918464786529977, w0=73.06874999999972, w1=14.030811734388069\n",
      "Gradient Descent(7728/9999): loss=1.9155058287734614, w0=73.12499999999973, w1=14.009765313812359\n",
      "Gradient Descent(7729/9999): loss=1.8329157871251507, w0=73.06874999999972, w1=14.085065985922194\n",
      "Gradient Descent(7730/9999): loss=1.8834074065102504, w0=73.01249999999972, w1=14.240268625388737\n",
      "Gradient Descent(7731/9999): loss=1.6960135711781639, w0=73.06874999999972, w1=14.055370927220345\n",
      "Gradient Descent(7732/9999): loss=1.8135004513332786, w0=73.18124999999972, w1=14.350703045342916\n",
      "Gradient Descent(7733/9999): loss=1.9020837193550155, w0=72.78749999999972, w1=14.345177414467882\n",
      "Gradient Descent(7734/9999): loss=2.0278805343781756, w0=72.95624999999973, w1=14.148384359256127\n",
      "Gradient Descent(7735/9999): loss=1.8269848906741273, w0=72.78749999999972, w1=14.271775413022212\n",
      "Gradient Descent(7736/9999): loss=2.034041871712696, w0=72.56249999999973, w1=14.251116101105314\n",
      "Gradient Descent(7737/9999): loss=2.6187446703150465, w0=72.78749999999972, w1=14.251126698745363\n",
      "Gradient Descent(7738/9999): loss=2.008859748249061, w0=72.95624999999973, w1=14.158302252286067\n",
      "Gradient Descent(7739/9999): loss=1.6910984509801166, w0=73.01249999999973, w1=13.950162789749415\n",
      "Gradient Descent(7740/9999): loss=2.38863549195386, w0=73.01249999999973, w1=14.203234635328904\n",
      "Gradient Descent(7741/9999): loss=2.1380711039796356, w0=73.12499999999973, w1=14.10887349800297\n",
      "Gradient Descent(7742/9999): loss=2.6476020178689352, w0=72.95624999999973, w1=14.136753133590538\n",
      "Gradient Descent(7743/9999): loss=2.4107015076027167, w0=73.18124999999972, w1=13.9501180024057\n",
      "Gradient Descent(7744/9999): loss=2.5228666577618997, w0=73.12499999999972, w1=14.007641808719233\n",
      "Gradient Descent(7745/9999): loss=2.118529397697692, w0=73.18124999999972, w1=13.933257319777947\n",
      "Gradient Descent(7746/9999): loss=2.1935148988951587, w0=73.34999999999972, w1=13.764491908399405\n",
      "Gradient Descent(7747/9999): loss=2.523582162195171, w0=73.29374999999972, w1=13.543538229174656\n",
      "Gradient Descent(7748/9999): loss=2.161311775752857, w0=73.40624999999972, w1=13.365442564540908\n",
      "Gradient Descent(7749/9999): loss=2.8060353632849053, w0=73.29374999999972, w1=13.69503185022826\n",
      "Gradient Descent(7750/9999): loss=2.1914021672386905, w0=73.12499999999972, w1=13.730674045642612\n",
      "Gradient Descent(7751/9999): loss=2.3248987112052673, w0=73.29374999999972, w1=13.697059475952564\n",
      "Gradient Descent(7752/9999): loss=2.1274867685991437, w0=72.95624999999971, w1=13.62737933821037\n",
      "Gradient Descent(7753/9999): loss=2.193653240799404, w0=73.06874999999971, w1=13.5737819646116\n",
      "Gradient Descent(7754/9999): loss=1.975049074318806, w0=73.18124999999971, w1=13.614400565482848\n",
      "Gradient Descent(7755/9999): loss=1.9270617048385392, w0=73.51874999999971, w1=13.32935726215282\n",
      "Gradient Descent(7756/9999): loss=2.1915020406930528, w0=73.51874999999971, w1=13.373781178922124\n",
      "Gradient Descent(7757/9999): loss=2.1317027184246333, w0=73.51874999999971, w1=13.392764166450505\n",
      "Gradient Descent(7758/9999): loss=2.5604420017122895, w0=73.51874999999971, w1=13.285939853314277\n",
      "Gradient Descent(7759/9999): loss=2.4860504661388743, w0=73.29374999999972, w1=13.321938304164762\n",
      "Gradient Descent(7760/9999): loss=2.1045293044409696, w0=73.46249999999972, w1=13.206360667032937\n",
      "Gradient Descent(7761/9999): loss=2.7220627544664184, w0=73.23749999999973, w1=13.322434714632621\n",
      "Gradient Descent(7762/9999): loss=2.49854289365147, w0=73.06874999999972, w1=13.18013077841312\n",
      "Gradient Descent(7763/9999): loss=1.8791076451996385, w0=73.01249999999972, w1=13.14343786868224\n",
      "Gradient Descent(7764/9999): loss=2.037502991453925, w0=72.84374999999972, w1=13.063221446785978\n",
      "Gradient Descent(7765/9999): loss=1.9195454725279482, w0=73.01249999999972, w1=13.154687080806726\n",
      "Gradient Descent(7766/9999): loss=2.2454060154378546, w0=73.12499999999972, w1=12.973904695951987\n",
      "Gradient Descent(7767/9999): loss=1.7653693400763162, w0=72.84374999999972, w1=13.14105721451415\n",
      "Gradient Descent(7768/9999): loss=2.0324866793260936, w0=73.12499999999972, w1=13.22291731032557\n",
      "Gradient Descent(7769/9999): loss=2.6810011461554555, w0=73.01249999999972, w1=13.16484401859735\n",
      "Gradient Descent(7770/9999): loss=2.8806341913170272, w0=73.18124999999972, w1=13.136271585908867\n",
      "Gradient Descent(7771/9999): loss=2.1460960478562034, w0=72.84374999999972, w1=13.290299558349444\n",
      "Gradient Descent(7772/9999): loss=1.9522494715334002, w0=72.89999999999972, w1=13.452094023289245\n",
      "Gradient Descent(7773/9999): loss=2.3971724939347423, w0=73.01249999999972, w1=13.43646945829618\n",
      "Gradient Descent(7774/9999): loss=2.194899873011136, w0=73.01249999999972, w1=13.30188394446817\n",
      "Gradient Descent(7775/9999): loss=2.633791982820144, w0=72.89999999999972, w1=13.200591670348029\n",
      "Gradient Descent(7776/9999): loss=2.0884565883647213, w0=72.84374999999972, w1=13.441869510827138\n",
      "Gradient Descent(7777/9999): loss=2.428369577378234, w0=72.73124999999972, w1=13.49441929318767\n",
      "Gradient Descent(7778/9999): loss=2.211196834905197, w0=72.78749999999972, w1=13.467642566582615\n",
      "Gradient Descent(7779/9999): loss=2.518624243772296, w0=73.01249999999972, w1=13.411575609351852\n",
      "Gradient Descent(7780/9999): loss=1.9288666767437985, w0=73.12499999999972, w1=13.259022186543135\n",
      "Gradient Descent(7781/9999): loss=2.129598525503546, w0=73.34999999999971, w1=13.472731080319388\n",
      "Gradient Descent(7782/9999): loss=2.345905241171555, w0=72.95624999999971, w1=13.634872560319488\n",
      "Gradient Descent(7783/9999): loss=2.587002450964812, w0=72.78749999999971, w1=13.410622647629076\n",
      "Gradient Descent(7784/9999): loss=1.9799658870665322, w0=73.12499999999972, w1=13.239091532052326\n",
      "Gradient Descent(7785/9999): loss=2.254116337479113, w0=73.06874999999971, w1=13.436327089029932\n",
      "Gradient Descent(7786/9999): loss=2.439962437738282, w0=73.2937499999997, w1=13.386686585871264\n",
      "Gradient Descent(7787/9999): loss=2.2889674666266258, w0=73.0124999999997, w1=13.420292519360471\n",
      "Gradient Descent(7788/9999): loss=2.391216683145732, w0=72.78749999999971, w1=13.16132778716705\n",
      "Gradient Descent(7789/9999): loss=2.3225066120191533, w0=72.67499999999971, w1=13.438847785282627\n",
      "Gradient Descent(7790/9999): loss=1.8281228936619593, w0=72.84374999999972, w1=13.671974664226905\n",
      "Gradient Descent(7791/9999): loss=2.3537240148947585, w0=72.89999999999972, w1=13.817944818065778\n",
      "Gradient Descent(7792/9999): loss=2.0409035205884045, w0=73.06874999999972, w1=13.796784714258402\n",
      "Gradient Descent(7793/9999): loss=2.4093604745108603, w0=73.18124999999972, w1=13.979704892840926\n",
      "Gradient Descent(7794/9999): loss=2.205621412679669, w0=73.18124999999972, w1=13.888397790041932\n",
      "Gradient Descent(7795/9999): loss=2.0537995161233287, w0=73.23749999999973, w1=13.933638850034686\n",
      "Gradient Descent(7796/9999): loss=2.2747884495479695, w0=73.29374999999973, w1=13.65842593594371\n",
      "Gradient Descent(7797/9999): loss=2.388792636741004, w0=73.23749999999973, w1=13.830177587248663\n",
      "Gradient Descent(7798/9999): loss=2.0333335498458567, w0=73.23749999999973, w1=13.711620527637674\n",
      "Gradient Descent(7799/9999): loss=2.4567122503546974, w0=73.12499999999973, w1=13.669647186564484\n",
      "Gradient Descent(7800/9999): loss=2.2887827473282787, w0=73.12499999999973, w1=13.601507487291096\n",
      "Gradient Descent(7801/9999): loss=2.029790489171956, w0=72.95624999999973, w1=13.58585045264295\n",
      "Gradient Descent(7802/9999): loss=2.5094164623963042, w0=72.95624999999973, w1=13.568861961303584\n",
      "Gradient Descent(7803/9999): loss=2.090330136395412, w0=73.01249999999973, w1=13.829411208903478\n",
      "Gradient Descent(7804/9999): loss=2.774121408919955, w0=72.95624999999973, w1=13.479646281547758\n",
      "Gradient Descent(7805/9999): loss=2.6281064152122156, w0=73.12499999999973, w1=13.548388824019879\n",
      "Gradient Descent(7806/9999): loss=2.5474494740596163, w0=73.01249999999973, w1=13.488909389764304\n",
      "Gradient Descent(7807/9999): loss=2.1905005968482705, w0=72.84374999999973, w1=13.984774729447828\n",
      "Gradient Descent(7808/9999): loss=2.331709147190994, w0=72.61874999999974, w1=13.974530569553135\n",
      "Gradient Descent(7809/9999): loss=1.8937376720575418, w0=72.44999999999973, w1=14.170881055314576\n",
      "Gradient Descent(7810/9999): loss=2.2217540391482693, w0=72.44999999999973, w1=13.813681853414481\n",
      "Gradient Descent(7811/9999): loss=1.7212963947645166, w0=72.33749999999974, w1=13.67821785552303\n",
      "Gradient Descent(7812/9999): loss=2.5006615539709074, w0=72.39374999999974, w1=13.645478139606341\n",
      "Gradient Descent(7813/9999): loss=2.661100897637925, w0=72.50624999999974, w1=13.602295917377235\n",
      "Gradient Descent(7814/9999): loss=2.3757753371597268, w0=72.84374999999974, w1=13.820517691874256\n",
      "Gradient Descent(7815/9999): loss=2.426485897222228, w0=72.73124999999975, w1=13.822855264427277\n",
      "Gradient Descent(7816/9999): loss=2.111196939955772, w0=72.78749999999975, w1=13.291373485463724\n",
      "Gradient Descent(7817/9999): loss=2.001496118625587, w0=72.78749999999975, w1=13.25006949855782\n",
      "Gradient Descent(7818/9999): loss=1.8694343347515983, w0=72.84374999999976, w1=13.094957244333532\n",
      "Gradient Descent(7819/9999): loss=1.7760648420696836, w0=72.89999999999976, w1=13.190870195336759\n",
      "Gradient Descent(7820/9999): loss=2.0862120621821947, w0=73.06874999999977, w1=13.442753057086541\n",
      "Gradient Descent(7821/9999): loss=1.9649681473999088, w0=73.01249999999976, w1=13.247958547248183\n",
      "Gradient Descent(7822/9999): loss=2.150230790005952, w0=73.06874999999977, w1=13.203475184974055\n",
      "Gradient Descent(7823/9999): loss=2.1202498684281146, w0=73.29374999999976, w1=13.366591789401314\n",
      "Gradient Descent(7824/9999): loss=2.2606374236802296, w0=73.34999999999977, w1=13.448823133390867\n",
      "Gradient Descent(7825/9999): loss=2.1808709013321614, w0=73.29374999999976, w1=13.46316998861497\n",
      "Gradient Descent(7826/9999): loss=2.03975015073798, w0=73.34999999999977, w1=13.729380322515091\n",
      "Gradient Descent(7827/9999): loss=1.9842079686435687, w0=73.18124999999976, w1=13.666006164436077\n",
      "Gradient Descent(7828/9999): loss=2.0236125076901246, w0=73.18124999999976, w1=13.75500986607351\n",
      "Gradient Descent(7829/9999): loss=1.9605262477694605, w0=73.34999999999977, w1=13.833798372120151\n",
      "Gradient Descent(7830/9999): loss=2.2521876690343934, w0=73.12499999999977, w1=13.508153418998864\n",
      "Gradient Descent(7831/9999): loss=2.13872012066243, w0=73.18124999999978, w1=13.448953181757455\n",
      "Gradient Descent(7832/9999): loss=2.522695707266099, w0=73.01249999999978, w1=13.248645411811758\n",
      "Gradient Descent(7833/9999): loss=2.6268444137784464, w0=72.89999999999978, w1=13.330025858611847\n",
      "Gradient Descent(7834/9999): loss=2.3132122896566214, w0=73.06874999999978, w1=13.383277923372356\n",
      "Gradient Descent(7835/9999): loss=2.8685328956731952, w0=73.01249999999978, w1=13.506101516064938\n",
      "Gradient Descent(7836/9999): loss=1.7368574354718422, w0=72.73124999999978, w1=13.28655983800192\n",
      "Gradient Descent(7837/9999): loss=2.742648702666143, w0=72.78749999999978, w1=13.433491086566399\n",
      "Gradient Descent(7838/9999): loss=1.7435997805257464, w0=72.73124999999978, w1=13.432271738046587\n",
      "Gradient Descent(7839/9999): loss=2.105130977350028, w0=72.84374999999977, w1=13.150431969914148\n",
      "Gradient Descent(7840/9999): loss=1.7200341570362527, w0=72.73124999999978, w1=13.190705741593007\n",
      "Gradient Descent(7841/9999): loss=2.055481697946199, w0=72.73124999999978, w1=13.237456612922655\n",
      "Gradient Descent(7842/9999): loss=2.6984021679861208, w0=72.89999999999978, w1=13.466360573979108\n",
      "Gradient Descent(7843/9999): loss=2.0505345302195046, w0=73.29374999999978, w1=13.386695635036213\n",
      "Gradient Descent(7844/9999): loss=2.0934743729656766, w0=73.40624999999977, w1=13.423368996880946\n",
      "Gradient Descent(7845/9999): loss=2.0847367159908914, w0=73.34999999999977, w1=13.61278800953414\n",
      "Gradient Descent(7846/9999): loss=2.3855098663201915, w0=73.57499999999976, w1=13.448325382513227\n",
      "Gradient Descent(7847/9999): loss=1.9446196326178735, w0=73.79999999999976, w1=13.585949876318113\n",
      "Gradient Descent(7848/9999): loss=2.273775893790681, w0=73.57499999999976, w1=13.55619014888342\n",
      "Gradient Descent(7849/9999): loss=2.114195213727955, w0=73.29374999999976, w1=13.788997818449396\n",
      "Gradient Descent(7850/9999): loss=2.1154331648030964, w0=73.18124999999976, w1=13.488252427412945\n",
      "Gradient Descent(7851/9999): loss=2.4306522021134747, w0=73.34999999999977, w1=13.339773320419056\n",
      "Gradient Descent(7852/9999): loss=1.793694370671842, w0=73.23749999999977, w1=13.494174387854196\n",
      "Gradient Descent(7853/9999): loss=2.306659130623872, w0=73.40624999999977, w1=13.554732936454078\n",
      "Gradient Descent(7854/9999): loss=1.8610141259607604, w0=73.23749999999977, w1=13.751795973492728\n",
      "Gradient Descent(7855/9999): loss=2.0648388224446887, w0=73.18124999999976, w1=13.649353036179065\n",
      "Gradient Descent(7856/9999): loss=2.045060834364377, w0=72.95624999999977, w1=13.741121497316389\n",
      "Gradient Descent(7857/9999): loss=1.6013128636152703, w0=72.73124999999978, w1=13.839752978595818\n",
      "Gradient Descent(7858/9999): loss=2.392141314811045, w0=73.12499999999977, w1=13.729928213271041\n",
      "Gradient Descent(7859/9999): loss=2.1268758170051467, w0=73.23749999999977, w1=13.58212650332906\n",
      "Gradient Descent(7860/9999): loss=2.010045365512984, w0=73.18124999999976, w1=13.701351509868932\n",
      "Gradient Descent(7861/9999): loss=2.4953463404630836, w0=73.18124999999976, w1=13.836473085726617\n",
      "Gradient Descent(7862/9999): loss=2.1317012291382365, w0=73.23749999999977, w1=13.840882153653398\n",
      "Gradient Descent(7863/9999): loss=2.5145519634615985, w0=73.29374999999978, w1=13.806874728814236\n",
      "Gradient Descent(7864/9999): loss=1.9240926646079752, w0=73.23749999999977, w1=13.70222619711603\n",
      "Gradient Descent(7865/9999): loss=1.9499722389824035, w0=73.29374999999978, w1=13.90120519776197\n",
      "Gradient Descent(7866/9999): loss=2.062650899578278, w0=73.34999999999978, w1=13.967618564997174\n",
      "Gradient Descent(7867/9999): loss=1.7137724864272288, w0=73.06874999999978, w1=14.310376755902409\n",
      "Gradient Descent(7868/9999): loss=1.8688164872637296, w0=73.12499999999979, w1=14.223875947866672\n",
      "Gradient Descent(7869/9999): loss=1.9878811536113172, w0=73.23749999999978, w1=14.126388318105088\n",
      "Gradient Descent(7870/9999): loss=2.06643779431952, w0=73.23749999999978, w1=13.87339089223037\n",
      "Gradient Descent(7871/9999): loss=1.8236745310272489, w0=73.18124999999978, w1=13.80066281607099\n",
      "Gradient Descent(7872/9999): loss=2.630531745763813, w0=73.40624999999977, w1=13.663003424240381\n",
      "Gradient Descent(7873/9999): loss=2.0149683295417247, w0=73.46249999999978, w1=13.382289191209134\n",
      "Gradient Descent(7874/9999): loss=2.0465635428614255, w0=73.51874999999978, w1=13.526230977657814\n",
      "Gradient Descent(7875/9999): loss=2.152058788439983, w0=73.34999999999978, w1=13.651480791972832\n",
      "Gradient Descent(7876/9999): loss=1.7075630950521414, w0=73.57499999999978, w1=13.384556022466453\n",
      "Gradient Descent(7877/9999): loss=2.047263392273449, w0=73.46249999999978, w1=13.311277435935168\n",
      "Gradient Descent(7878/9999): loss=1.6619116218498164, w0=73.46249999999978, w1=13.477417085408606\n",
      "Gradient Descent(7879/9999): loss=2.032017955324781, w0=73.46249999999978, w1=13.575535452971\n",
      "Gradient Descent(7880/9999): loss=2.753610310046637, w0=73.23749999999978, w1=13.546385733802529\n",
      "Gradient Descent(7881/9999): loss=2.7693671358037246, w0=73.23749999999978, w1=13.401199341243764\n",
      "Gradient Descent(7882/9999): loss=2.066218354027442, w0=73.23749999999978, w1=13.384283231174514\n",
      "Gradient Descent(7883/9999): loss=1.845472707666951, w0=73.23749999999978, w1=13.297621542923267\n",
      "Gradient Descent(7884/9999): loss=2.130422392950866, w0=73.40624999999979, w1=13.419820569686323\n",
      "Gradient Descent(7885/9999): loss=2.0825069955015483, w0=73.18124999999979, w1=13.418896929656832\n",
      "Gradient Descent(7886/9999): loss=2.271096415715099, w0=72.9562499999998, w1=13.58143739859715\n",
      "Gradient Descent(7887/9999): loss=1.8955952132471223, w0=72.7312499999998, w1=13.879444910627685\n",
      "Gradient Descent(7888/9999): loss=2.402671041582669, w0=72.78749999999981, w1=13.867610023967229\n",
      "Gradient Descent(7889/9999): loss=2.447810545034958, w0=72.7312499999998, w1=13.799348187736825\n",
      "Gradient Descent(7890/9999): loss=2.0577361747330922, w0=72.8437499999998, w1=13.372743983326705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(7891/9999): loss=1.9334934363743013, w0=72.6749999999998, w1=13.539302771476565\n",
      "Gradient Descent(7892/9999): loss=2.5727444270415942, w0=72.9562499999998, w1=13.429534578560105\n",
      "Gradient Descent(7893/9999): loss=1.7188171380699362, w0=72.89999999999979, w1=13.444640212157374\n",
      "Gradient Descent(7894/9999): loss=2.3363765755219124, w0=73.01249999999979, w1=13.59798407643594\n",
      "Gradient Descent(7895/9999): loss=2.1927918578248926, w0=73.01249999999979, w1=13.571433824045634\n",
      "Gradient Descent(7896/9999): loss=2.069731326387489, w0=72.89999999999979, w1=13.592770787521275\n",
      "Gradient Descent(7897/9999): loss=2.078046158265624, w0=72.84374999999979, w1=13.693617275310508\n",
      "Gradient Descent(7898/9999): loss=2.052788660656052, w0=72.95624999999978, w1=13.485557258818718\n",
      "Gradient Descent(7899/9999): loss=1.9332219216900453, w0=72.89999999999978, w1=13.503639173387016\n",
      "Gradient Descent(7900/9999): loss=2.5177019933536804, w0=73.01249999999978, w1=13.597380097366745\n",
      "Gradient Descent(7901/9999): loss=2.7033932937831837, w0=73.23749999999977, w1=13.615911188958766\n",
      "Gradient Descent(7902/9999): loss=2.3053050008790574, w0=73.01249999999978, w1=13.944099178182356\n",
      "Gradient Descent(7903/9999): loss=1.8969141392248454, w0=73.23749999999977, w1=13.944251065564886\n",
      "Gradient Descent(7904/9999): loss=2.4557219013644125, w0=73.40624999999977, w1=13.9295141238735\n",
      "Gradient Descent(7905/9999): loss=2.9198961476470346, w0=73.23749999999977, w1=14.021825213592443\n",
      "Gradient Descent(7906/9999): loss=2.354513130199132, w0=73.23749999999977, w1=13.932009720715104\n",
      "Gradient Descent(7907/9999): loss=2.000056505589889, w0=73.29374999999978, w1=13.886623123025133\n",
      "Gradient Descent(7908/9999): loss=1.9896128025174074, w0=73.51874999999977, w1=13.744362432458122\n",
      "Gradient Descent(7909/9999): loss=2.187249398997753, w0=73.51874999999977, w1=13.834416015209698\n",
      "Gradient Descent(7910/9999): loss=2.4348615128195252, w0=73.40624999999977, w1=13.840080489697867\n",
      "Gradient Descent(7911/9999): loss=1.9163955750604238, w0=73.40624999999977, w1=13.812050522354383\n",
      "Gradient Descent(7912/9999): loss=2.3811287551965687, w0=73.46249999999978, w1=13.848068833846174\n",
      "Gradient Descent(7913/9999): loss=2.6576897592958924, w0=73.46249999999978, w1=13.698070707630217\n",
      "Gradient Descent(7914/9999): loss=1.9908385880386226, w0=73.40624999999977, w1=13.613469009462179\n",
      "Gradient Descent(7915/9999): loss=1.6469479098409776, w0=73.63124999999977, w1=13.870834712750423\n",
      "Gradient Descent(7916/9999): loss=2.184992272903865, w0=73.51874999999977, w1=13.708999867995407\n",
      "Gradient Descent(7917/9999): loss=2.213342817779799, w0=73.63124999999977, w1=13.750918400203293\n",
      "Gradient Descent(7918/9999): loss=2.1297539251455757, w0=73.40624999999977, w1=13.794780231768605\n",
      "Gradient Descent(7919/9999): loss=2.361520908446377, w0=73.12499999999977, w1=13.721363389703802\n",
      "Gradient Descent(7920/9999): loss=2.3222729515901395, w0=73.06874999999977, w1=13.687168963446917\n",
      "Gradient Descent(7921/9999): loss=2.0526686004674835, w0=73.06874999999977, w1=13.612572709018144\n",
      "Gradient Descent(7922/9999): loss=2.362768988594051, w0=73.01249999999976, w1=13.512916155479429\n",
      "Gradient Descent(7923/9999): loss=2.817837400696503, w0=72.95624999999976, w1=13.364031442431882\n",
      "Gradient Descent(7924/9999): loss=2.597573312783027, w0=73.18124999999975, w1=13.329922673113188\n",
      "Gradient Descent(7925/9999): loss=2.3131878006269284, w0=73.40624999999974, w1=13.507765289483002\n",
      "Gradient Descent(7926/9999): loss=2.508278368336895, w0=73.46249999999975, w1=13.508055390478896\n",
      "Gradient Descent(7927/9999): loss=1.674416787672608, w0=73.51874999999976, w1=13.145728672993965\n",
      "Gradient Descent(7928/9999): loss=2.154367716182073, w0=73.46249999999975, w1=12.980556687675415\n",
      "Gradient Descent(7929/9999): loss=2.4717545651483053, w0=73.63124999999975, w1=13.092455083675702\n",
      "Gradient Descent(7930/9999): loss=1.7394551517016088, w0=73.51874999999976, w1=13.291831737892808\n",
      "Gradient Descent(7931/9999): loss=1.906662510224226, w0=73.40624999999976, w1=13.344321648724376\n",
      "Gradient Descent(7932/9999): loss=2.001324684333266, w0=73.40624999999976, w1=13.403010955833846\n",
      "Gradient Descent(7933/9999): loss=2.5805308582402375, w0=73.51874999999976, w1=13.375489575721614\n",
      "Gradient Descent(7934/9999): loss=2.5051333401322413, w0=73.40624999999976, w1=13.147095164892198\n",
      "Gradient Descent(7935/9999): loss=1.7652493922073509, w0=73.46249999999976, w1=13.59362794467955\n",
      "Gradient Descent(7936/9999): loss=2.2902960406697925, w0=73.68749999999976, w1=13.523998940201063\n",
      "Gradient Descent(7937/9999): loss=2.6847385844277225, w0=73.68749999999976, w1=13.634127919426929\n",
      "Gradient Descent(7938/9999): loss=2.165343633026899, w0=73.51874999999976, w1=13.685256140603768\n",
      "Gradient Descent(7939/9999): loss=2.3715695975697555, w0=73.18124999999975, w1=13.818136833473693\n",
      "Gradient Descent(7940/9999): loss=2.6562453518133644, w0=73.23749999999976, w1=13.667652171618645\n",
      "Gradient Descent(7941/9999): loss=2.3653933536197003, w0=73.29374999999976, w1=13.820659454967712\n",
      "Gradient Descent(7942/9999): loss=2.430028012523481, w0=73.06874999999977, w1=13.95969214832694\n",
      "Gradient Descent(7943/9999): loss=2.0649394890196904, w0=73.06874999999977, w1=13.761926693026508\n",
      "Gradient Descent(7944/9999): loss=2.5333774934487208, w0=73.29374999999976, w1=13.737732214339886\n",
      "Gradient Descent(7945/9999): loss=2.07088489712744, w0=73.40624999999976, w1=13.52170551622425\n",
      "Gradient Descent(7946/9999): loss=1.8179685617116106, w0=73.06874999999975, w1=13.53840358661745\n",
      "Gradient Descent(7947/9999): loss=1.8671550209295338, w0=73.01249999999975, w1=13.793160276678709\n",
      "Gradient Descent(7948/9999): loss=2.528522619562155, w0=73.12499999999974, w1=13.828012924998806\n",
      "Gradient Descent(7949/9999): loss=1.680053150665974, w0=73.12499999999974, w1=13.924719898447218\n",
      "Gradient Descent(7950/9999): loss=2.4849005868444065, w0=73.01249999999975, w1=13.932500814573752\n",
      "Gradient Descent(7951/9999): loss=2.1758125630871943, w0=72.84374999999974, w1=14.074774259479158\n",
      "Gradient Descent(7952/9999): loss=2.0742895270552815, w0=72.89999999999975, w1=14.202044463510372\n",
      "Gradient Descent(7953/9999): loss=2.3749671442948976, w0=72.56249999999974, w1=14.2162327070154\n",
      "Gradient Descent(7954/9999): loss=3.057387134405264, w0=72.89999999999975, w1=14.054199648702324\n",
      "Gradient Descent(7955/9999): loss=2.2113394920173426, w0=73.23749999999976, w1=14.050228331657488\n",
      "Gradient Descent(7956/9999): loss=2.427087824490613, w0=73.01249999999976, w1=14.162241539076508\n",
      "Gradient Descent(7957/9999): loss=2.2355756714639865, w0=73.06874999999977, w1=14.281843688395412\n",
      "Gradient Descent(7958/9999): loss=2.0652222513150966, w0=72.89999999999976, w1=14.331162860562083\n",
      "Gradient Descent(7959/9999): loss=2.1346785978002343, w0=72.73124999999976, w1=14.493198479016522\n",
      "Gradient Descent(7960/9999): loss=1.915174081734547, w0=72.67499999999976, w1=14.476031160512466\n",
      "Gradient Descent(7961/9999): loss=2.1385122439580075, w0=72.84374999999976, w1=14.52221357042199\n",
      "Gradient Descent(7962/9999): loss=1.7834185640963365, w0=73.01249999999976, w1=14.223262843646086\n",
      "Gradient Descent(7963/9999): loss=2.455192206406042, w0=73.12499999999976, w1=14.078092835465979\n",
      "Gradient Descent(7964/9999): loss=2.616449024617263, w0=73.06874999999975, w1=14.099787267089663\n",
      "Gradient Descent(7965/9999): loss=2.7129610989653914, w0=72.89999999999975, w1=13.932927918251563\n",
      "Gradient Descent(7966/9999): loss=2.3460374388381005, w0=72.67499999999976, w1=13.844773196950404\n",
      "Gradient Descent(7967/9999): loss=2.729361976230125, w0=72.89999999999975, w1=13.861913813753144\n",
      "Gradient Descent(7968/9999): loss=2.8061363489589297, w0=73.01249999999975, w1=13.953345788558563\n",
      "Gradient Descent(7969/9999): loss=2.239342496779355, w0=72.95624999999974, w1=13.849624185335736\n",
      "Gradient Descent(7970/9999): loss=2.181840584069148, w0=73.23749999999974, w1=13.910939127089248\n",
      "Gradient Descent(7971/9999): loss=2.1681859528577245, w0=73.40624999999974, w1=13.895713151761065\n",
      "Gradient Descent(7972/9999): loss=2.3351032435785815, w0=73.74374999999975, w1=13.780079414530764\n",
      "Gradient Descent(7973/9999): loss=2.3101838334905027, w0=73.85624999999975, w1=14.079150365097101\n",
      "Gradient Descent(7974/9999): loss=2.4497916959367814, w0=73.68749999999974, w1=13.810346806891177\n",
      "Gradient Descent(7975/9999): loss=1.9162075941457473, w0=73.68749999999974, w1=13.916878117901526\n",
      "Gradient Descent(7976/9999): loss=2.454180882293349, w0=73.46249999999975, w1=13.989843011161968\n",
      "Gradient Descent(7977/9999): loss=1.9379120044236557, w0=73.12499999999974, w1=14.404293541391693\n",
      "Gradient Descent(7978/9999): loss=2.046909372878835, w0=73.06874999999974, w1=13.985429029169342\n",
      "Gradient Descent(7979/9999): loss=2.117500790148643, w0=72.84374999999974, w1=13.970442123908976\n",
      "Gradient Descent(7980/9999): loss=2.1637791356485683, w0=73.01249999999975, w1=13.777815198255757\n",
      "Gradient Descent(7981/9999): loss=2.1898988188644757, w0=72.84374999999974, w1=13.766749153561113\n",
      "Gradient Descent(7982/9999): loss=2.183012604774401, w0=72.95624999999974, w1=13.270555787581491\n",
      "Gradient Descent(7983/9999): loss=2.333014568327045, w0=72.50624999999974, w1=13.443776376549431\n",
      "Gradient Descent(7984/9999): loss=1.8895645928763634, w0=72.44999999999973, w1=13.434528760056827\n",
      "Gradient Descent(7985/9999): loss=1.9325992645788113, w0=72.44999999999973, w1=13.303481184032963\n",
      "Gradient Descent(7986/9999): loss=2.149172776287327, w0=72.50624999999974, w1=13.61989479285522\n",
      "Gradient Descent(7987/9999): loss=1.8223283404920654, w0=72.78749999999974, w1=13.718567544740077\n",
      "Gradient Descent(7988/9999): loss=2.3941160893387083, w0=73.12499999999974, w1=13.563215488052256\n",
      "Gradient Descent(7989/9999): loss=2.3589406747907047, w0=72.95624999999974, w1=13.51799678512125\n",
      "Gradient Descent(7990/9999): loss=2.633397902876678, w0=72.89999999999974, w1=13.572001819367332\n",
      "Gradient Descent(7991/9999): loss=2.018186428747747, w0=72.89999999999974, w1=13.51530706914511\n",
      "Gradient Descent(7992/9999): loss=2.077253113422614, w0=72.89999999999974, w1=13.452427821681473\n",
      "Gradient Descent(7993/9999): loss=2.2959124733300924, w0=72.56249999999973, w1=13.494593067573524\n",
      "Gradient Descent(7994/9999): loss=1.881911844018914, w0=72.44999999999973, w1=13.542949547328378\n",
      "Gradient Descent(7995/9999): loss=2.5278845044282434, w0=72.33749999999974, w1=13.508674182365635\n",
      "Gradient Descent(7996/9999): loss=2.2321820988607373, w0=72.39374999999974, w1=13.578764278115765\n",
      "Gradient Descent(7997/9999): loss=2.2114201322150313, w0=72.73124999999975, w1=13.82695153755715\n",
      "Gradient Descent(7998/9999): loss=2.222188210304353, w0=72.84374999999974, w1=14.202195833018267\n",
      "Gradient Descent(7999/9999): loss=2.359475369536559, w0=72.89999999999975, w1=13.96462677882731\n",
      "Gradient Descent(8000/9999): loss=2.4536636175533593, w0=72.89999999999975, w1=13.779141318869632\n",
      "Gradient Descent(8001/9999): loss=2.55280913100545, w0=72.84374999999974, w1=13.774047313988264\n",
      "Gradient Descent(8002/9999): loss=2.149465031994932, w0=72.78749999999974, w1=13.817210967352267\n",
      "Gradient Descent(8003/9999): loss=2.087781083427557, w0=72.73124999999973, w1=13.698547745628009\n",
      "Gradient Descent(8004/9999): loss=2.3871493222673217, w0=72.95624999999973, w1=13.651652778613133\n",
      "Gradient Descent(8005/9999): loss=1.9482044350531305, w0=72.89999999999972, w1=13.39145928535565\n",
      "Gradient Descent(8006/9999): loss=2.265018884134335, w0=73.01249999999972, w1=13.504843199916932\n",
      "Gradient Descent(8007/9999): loss=2.135689634064056, w0=72.56249999999972, w1=13.606827165773822\n",
      "Gradient Descent(8008/9999): loss=1.9380349701361952, w0=72.33749999999972, w1=13.767507693035585\n",
      "Gradient Descent(8009/9999): loss=2.207161988808921, w0=72.22499999999972, w1=13.810549726353475\n",
      "Gradient Descent(8010/9999): loss=1.954496425026836, w0=72.33749999999972, w1=13.754881006175896\n",
      "Gradient Descent(8011/9999): loss=2.4455647570395302, w0=72.44999999999972, w1=13.703641062120642\n",
      "Gradient Descent(8012/9999): loss=1.8315818292773842, w0=72.73124999999972, w1=13.699049411829662\n",
      "Gradient Descent(8013/9999): loss=2.6470525788088994, w0=73.06874999999972, w1=13.49600729801325\n",
      "Gradient Descent(8014/9999): loss=1.9350597874231634, w0=73.23749999999973, w1=13.337204374779079\n",
      "Gradient Descent(8015/9999): loss=1.589819257888687, w0=73.23749999999973, w1=13.23669084420029\n",
      "Gradient Descent(8016/9999): loss=1.873546942543372, w0=73.46249999999972, w1=13.242349120295643\n",
      "Gradient Descent(8017/9999): loss=2.51966048882592, w0=73.68749999999972, w1=13.28919985766354\n",
      "Gradient Descent(8018/9999): loss=2.188075000295217, w0=73.51874999999971, w1=13.010153000390044\n",
      "Gradient Descent(8019/9999): loss=2.632122252866073, w0=73.40624999999972, w1=13.269835062993685\n",
      "Gradient Descent(8020/9999): loss=2.591042144765918, w0=73.40624999999972, w1=13.517005582419724\n",
      "Gradient Descent(8021/9999): loss=2.2693766646959075, w0=73.29374999999972, w1=13.624215140749364\n",
      "Gradient Descent(8022/9999): loss=2.37364446505681, w0=73.34999999999972, w1=13.538328803152877\n",
      "Gradient Descent(8023/9999): loss=1.7577447855441397, w0=73.34999999999972, w1=13.341221961370543\n",
      "Gradient Descent(8024/9999): loss=1.9679915228109763, w0=73.46249999999972, w1=13.390289104367907\n",
      "Gradient Descent(8025/9999): loss=2.049630729360669, w0=73.34999999999972, w1=13.35253986927304\n",
      "Gradient Descent(8026/9999): loss=2.6807155532456837, w0=73.46249999999972, w1=13.350030531336428\n",
      "Gradient Descent(8027/9999): loss=2.5728150903363494, w0=73.29374999999972, w1=13.463422410457014\n",
      "Gradient Descent(8028/9999): loss=2.8010229030936484, w0=73.34999999999972, w1=13.642648094358425\n",
      "Gradient Descent(8029/9999): loss=1.9277207581333091, w0=73.51874999999973, w1=13.48913994208109\n",
      "Gradient Descent(8030/9999): loss=2.6205679758718983, w0=73.46249999999972, w1=13.621948069716941\n",
      "Gradient Descent(8031/9999): loss=2.5618905054072982, w0=73.34999999999972, w1=13.584827528099526\n",
      "Gradient Descent(8032/9999): loss=1.9195751114592527, w0=72.84374999999973, w1=13.668189927856188\n",
      "Gradient Descent(8033/9999): loss=2.3282013930476237, w0=72.73124999999973, w1=13.921352898771032\n",
      "Gradient Descent(8034/9999): loss=2.1207433977456125, w0=72.56249999999973, w1=13.913672597338945\n",
      "Gradient Descent(8035/9999): loss=1.9819370644408878, w0=72.67499999999973, w1=13.984795703379826\n",
      "Gradient Descent(8036/9999): loss=1.729581537715802, w0=72.67499999999973, w1=13.936961270592446\n",
      "Gradient Descent(8037/9999): loss=2.0026516745986496, w0=72.73124999999973, w1=14.214085689980182\n",
      "Gradient Descent(8038/9999): loss=2.4832060249704555, w0=72.95624999999973, w1=13.969138522229015\n",
      "Gradient Descent(8039/9999): loss=2.5665522307006183, w0=73.06874999999972, w1=14.000142209987578\n",
      "Gradient Descent(8040/9999): loss=2.20112907866094, w0=73.06874999999972, w1=14.071539686850155\n",
      "Gradient Descent(8041/9999): loss=1.7942428108811967, w0=73.23749999999973, w1=13.602612805606542\n",
      "Gradient Descent(8042/9999): loss=2.1180730151044624, w0=73.12499999999973, w1=13.720024996234242\n",
      "Gradient Descent(8043/9999): loss=1.882417067312824, w0=73.40624999999973, w1=13.688335048313\n",
      "Gradient Descent(8044/9999): loss=2.0789875148372685, w0=73.40624999999973, w1=13.697523212942219\n",
      "Gradient Descent(8045/9999): loss=2.1245619949198886, w0=73.29374999999973, w1=13.663741301520572\n",
      "Gradient Descent(8046/9999): loss=2.2803740499011167, w0=73.46249999999974, w1=13.948402432297282\n",
      "Gradient Descent(8047/9999): loss=1.9513782450640813, w0=73.79999999999974, w1=13.777118506126437\n",
      "Gradient Descent(8048/9999): loss=2.495622362311985, w0=73.46249999999974, w1=13.557619232781757\n",
      "Gradient Descent(8049/9999): loss=2.155125480483341, w0=73.57499999999973, w1=13.814147967349957\n",
      "Gradient Descent(8050/9999): loss=2.1616099998815286, w0=73.46249999999974, w1=13.683723716850956\n",
      "Gradient Descent(8051/9999): loss=3.0545435725394596, w0=73.51874999999974, w1=13.869239094749277\n",
      "Gradient Descent(8052/9999): loss=2.0556175539968904, w0=73.85624999999975, w1=13.624166458249732\n",
      "Gradient Descent(8053/9999): loss=1.9349162244862876, w0=73.57499999999975, w1=14.057689898843055\n",
      "Gradient Descent(8054/9999): loss=2.6209453146907746, w0=73.23749999999974, w1=13.849373432491879\n",
      "Gradient Descent(8055/9999): loss=2.1414102582056005, w0=73.23749999999974, w1=13.840550987826845\n",
      "Gradient Descent(8056/9999): loss=1.9893326058973433, w0=73.06874999999974, w1=13.746006421566497\n",
      "Gradient Descent(8057/9999): loss=2.3394562505115446, w0=73.34999999999974, w1=13.826403035591913\n",
      "Gradient Descent(8058/9999): loss=2.0683294081012065, w0=73.40624999999974, w1=13.917501947522704\n",
      "Gradient Descent(8059/9999): loss=2.277791699310338, w0=73.29374999999975, w1=13.89294727342667\n",
      "Gradient Descent(8060/9999): loss=2.1919505753201767, w0=73.23749999999974, w1=13.68808830673763\n",
      "Gradient Descent(8061/9999): loss=2.2206974013724627, w0=73.18124999999974, w1=13.845616885833156\n",
      "Gradient Descent(8062/9999): loss=2.339930002074854, w0=73.40624999999973, w1=14.021644311475916\n",
      "Gradient Descent(8063/9999): loss=2.242387819722872, w0=73.63124999999972, w1=13.833942820048284\n",
      "Gradient Descent(8064/9999): loss=2.3141586880443406, w0=73.63124999999972, w1=13.7865636716095\n",
      "Gradient Descent(8065/9999): loss=1.9771544753343102, w0=73.46249999999972, w1=13.832112509218339\n",
      "Gradient Descent(8066/9999): loss=2.368491110704416, w0=73.34999999999972, w1=13.69007531561178\n",
      "Gradient Descent(8067/9999): loss=2.2776270989786482, w0=73.18124999999972, w1=13.90817691789519\n",
      "Gradient Descent(8068/9999): loss=2.606102460602388, w0=73.01249999999972, w1=13.662066262263739\n",
      "Gradient Descent(8069/9999): loss=2.723527666770711, w0=73.18124999999972, w1=13.596839371711088\n",
      "Gradient Descent(8070/9999): loss=2.099754358737291, w0=73.12499999999972, w1=13.490961697347526\n",
      "Gradient Descent(8071/9999): loss=2.4043501435639736, w0=73.06874999999971, w1=13.46462435198078\n",
      "Gradient Descent(8072/9999): loss=1.5865803819619808, w0=73.0124999999997, w1=13.249981875506638\n",
      "Gradient Descent(8073/9999): loss=2.3405126655649537, w0=73.06874999999971, w1=13.432078664588412\n",
      "Gradient Descent(8074/9999): loss=2.191773522287198, w0=72.84374999999972, w1=13.54682716310781\n",
      "Gradient Descent(8075/9999): loss=2.3567143240773594, w0=72.73124999999972, w1=13.55828957211095\n",
      "Gradient Descent(8076/9999): loss=2.4196999344354126, w0=72.84374999999972, w1=13.458001201952905\n",
      "Gradient Descent(8077/9999): loss=2.18618639142608, w0=72.67499999999971, w1=13.37856310982673\n",
      "Gradient Descent(8078/9999): loss=2.876503368721519, w0=72.95624999999971, w1=13.300909635742629\n",
      "Gradient Descent(8079/9999): loss=2.4926184646350418, w0=72.95624999999971, w1=13.614002010813932\n",
      "Gradient Descent(8080/9999): loss=1.8358740381334255, w0=73.06874999999971, w1=13.68384487255838\n",
      "Gradient Descent(8081/9999): loss=2.7617012152381957, w0=73.18124999999971, w1=13.746065555696484\n",
      "Gradient Descent(8082/9999): loss=2.450710668874737, w0=73.1249999999997, w1=13.521868476844055\n",
      "Gradient Descent(8083/9999): loss=1.9070105872081295, w0=72.6749999999997, w1=13.350008608251585\n",
      "Gradient Descent(8084/9999): loss=2.414811293909654, w0=73.0124999999997, w1=13.21251841038701\n",
      "Gradient Descent(8085/9999): loss=2.3281273019727333, w0=72.9562499999997, w1=13.15986873810055\n",
      "Gradient Descent(8086/9999): loss=1.836434170361181, w0=72.8999999999997, w1=13.223792475671024\n",
      "Gradient Descent(8087/9999): loss=2.002246357404318, w0=72.8999999999997, w1=13.23217176541626\n",
      "Gradient Descent(8088/9999): loss=1.961841758080319, w0=72.6749999999997, w1=13.386629231693883\n",
      "Gradient Descent(8089/9999): loss=2.463872189174473, w0=72.8999999999997, w1=13.543779961789655\n",
      "Gradient Descent(8090/9999): loss=2.2462607701240196, w0=72.84374999999969, w1=13.314014885729321\n",
      "Gradient Descent(8091/9999): loss=2.070514566877237, w0=73.01249999999969, w1=13.378029799605288\n",
      "Gradient Descent(8092/9999): loss=2.101289986555464, w0=72.95624999999968, w1=13.648058280905548\n",
      "Gradient Descent(8093/9999): loss=2.341419691573874, w0=72.95624999999968, w1=13.71985337650436\n",
      "Gradient Descent(8094/9999): loss=2.275006713917842, w0=72.84374999999969, w1=13.801224588342352\n",
      "Gradient Descent(8095/9999): loss=1.7787292361999574, w0=72.73124999999969, w1=13.979468832158169\n",
      "Gradient Descent(8096/9999): loss=1.817416218823095, w0=72.44999999999969, w1=14.027790104338434\n",
      "Gradient Descent(8097/9999): loss=2.128378681505849, w0=72.73124999999969, w1=14.049822617319505\n",
      "Gradient Descent(8098/9999): loss=2.569478920853853, w0=72.84374999999969, w1=14.078800887268695\n",
      "Gradient Descent(8099/9999): loss=2.2155505978015353, w0=72.8999999999997, w1=14.014184868210407\n",
      "Gradient Descent(8100/9999): loss=2.168396460238525, w0=73.01249999999969, w1=13.86107992986638\n",
      "Gradient Descent(8101/9999): loss=1.7635134213828876, w0=73.0687499999997, w1=13.980056262548787\n",
      "Gradient Descent(8102/9999): loss=2.2578549547298046, w0=73.1249999999997, w1=14.065469774828212\n",
      "Gradient Descent(8103/9999): loss=1.983259167750471, w0=73.18124999999971, w1=13.836779735493732\n",
      "Gradient Descent(8104/9999): loss=2.606165924052318, w0=73.34999999999971, w1=13.779583301188348\n",
      "Gradient Descent(8105/9999): loss=1.8518408704689537, w0=73.23749999999971, w1=13.6237018739601\n",
      "Gradient Descent(8106/9999): loss=2.8248323210382007, w0=73.23749999999971, w1=13.640482123927244\n",
      "Gradient Descent(8107/9999): loss=2.543164412173058, w0=73.29374999999972, w1=13.814727507483271\n",
      "Gradient Descent(8108/9999): loss=2.1114272471197086, w0=73.34999999999972, w1=13.878457089798735\n",
      "Gradient Descent(8109/9999): loss=1.9540175332816876, w0=73.46249999999972, w1=13.638114777596657\n",
      "Gradient Descent(8110/9999): loss=1.9844116541543995, w0=73.34999999999972, w1=13.529608366611841\n",
      "Gradient Descent(8111/9999): loss=2.480563840023736, w0=73.12499999999973, w1=13.76561907559363\n",
      "Gradient Descent(8112/9999): loss=1.961132420361712, w0=72.89999999999974, w1=13.54225554257165\n",
      "Gradient Descent(8113/9999): loss=2.044537394460682, w0=72.84374999999973, w1=13.500103780635126\n",
      "Gradient Descent(8114/9999): loss=3.0792749766917673, w0=73.06874999999972, w1=13.657187898937787\n",
      "Gradient Descent(8115/9999): loss=2.5282518683184145, w0=73.12499999999973, w1=13.816581153995033\n",
      "Gradient Descent(8116/9999): loss=2.1909637103878072, w0=73.12499999999973, w1=13.785115241511397\n",
      "Gradient Descent(8117/9999): loss=1.810190734729777, w0=73.51874999999973, w1=13.7624306512865\n",
      "Gradient Descent(8118/9999): loss=2.4096136215186648, w0=73.18124999999972, w1=13.846384409052781\n",
      "Gradient Descent(8119/9999): loss=2.419799582246176, w0=73.40624999999972, w1=13.969001415536573\n",
      "Gradient Descent(8120/9999): loss=2.3321914383488984, w0=73.79999999999971, w1=13.929128161250418\n",
      "Gradient Descent(8121/9999): loss=2.0017448014661388, w0=73.96874999999972, w1=13.683474542780065\n",
      "Gradient Descent(8122/9999): loss=2.502554112693716, w0=74.02499999999972, w1=13.44738139740336\n",
      "Gradient Descent(8123/9999): loss=1.717356543508602, w0=74.08124999999973, w1=13.573959859840343\n",
      "Gradient Descent(8124/9999): loss=1.9544149411067755, w0=73.96874999999973, w1=13.477149089889775\n",
      "Gradient Descent(8125/9999): loss=2.291181619530733, w0=73.85624999999973, w1=13.366358444159097\n",
      "Gradient Descent(8126/9999): loss=2.3591564295949756, w0=73.96874999999973, w1=13.42861142566878\n",
      "Gradient Descent(8127/9999): loss=2.124569188237782, w0=73.85624999999973, w1=13.331206750199634\n",
      "Gradient Descent(8128/9999): loss=2.2014842951117406, w0=73.68749999999973, w1=13.238320345925384\n",
      "Gradient Descent(8129/9999): loss=2.3600867554050753, w0=73.96874999999973, w1=13.48545804688681\n",
      "Gradient Descent(8130/9999): loss=1.9728599995319265, w0=73.91249999999972, w1=13.288547800351486\n",
      "Gradient Descent(8131/9999): loss=2.3565448163197296, w0=73.74374999999972, w1=13.226793630873138\n",
      "Gradient Descent(8132/9999): loss=2.9357903984608695, w0=73.57499999999972, w1=13.223383669726719\n",
      "Gradient Descent(8133/9999): loss=1.939162023442794, w0=73.57499999999972, w1=13.567973113136674\n",
      "Gradient Descent(8134/9999): loss=2.528250265885971, w0=73.57499999999972, w1=13.386935516366965\n",
      "Gradient Descent(8135/9999): loss=2.418971251045915, w0=73.57499999999972, w1=13.411020454158507\n",
      "Gradient Descent(8136/9999): loss=2.334390571323535, w0=73.63124999999972, w1=13.321058597812515\n",
      "Gradient Descent(8137/9999): loss=1.524318368578133, w0=73.46249999999972, w1=13.59479387431166\n",
      "Gradient Descent(8138/9999): loss=2.1639927989081302, w0=73.40624999999972, w1=13.38401614675898\n",
      "Gradient Descent(8139/9999): loss=2.249665220613489, w0=73.40624999999972, w1=13.087838019102026\n",
      "Gradient Descent(8140/9999): loss=1.7958216914926728, w0=73.51874999999971, w1=13.194030613286882\n",
      "Gradient Descent(8141/9999): loss=2.1841368094471365, w0=73.51874999999971, w1=13.18470200107298\n",
      "Gradient Descent(8142/9999): loss=3.312031321003402, w0=73.51874999999971, w1=13.230115632408904\n",
      "Gradient Descent(8143/9999): loss=1.9035622307531939, w0=73.68749999999972, w1=12.87461885723642\n",
      "Gradient Descent(8144/9999): loss=2.5564952310261404, w0=73.63124999999971, w1=12.775559080453409\n",
      "Gradient Descent(8145/9999): loss=1.7759935046199102, w0=73.68749999999972, w1=12.852067236123675\n",
      "Gradient Descent(8146/9999): loss=2.348858690828603, w0=73.68749999999972, w1=12.964090190493\n",
      "Gradient Descent(8147/9999): loss=2.7542216360941993, w0=73.68749999999972, w1=12.976222641741732\n",
      "Gradient Descent(8148/9999): loss=2.2550369062176627, w0=73.57499999999972, w1=12.902660077273975\n",
      "Gradient Descent(8149/9999): loss=2.4726165693713247, w0=73.46249999999972, w1=13.115370558804585\n",
      "Gradient Descent(8150/9999): loss=2.1692041477178, w0=73.68749999999972, w1=13.056048366901411\n",
      "Gradient Descent(8151/9999): loss=2.2472784992797696, w0=73.51874999999971, w1=13.145015336835252\n",
      "Gradient Descent(8152/9999): loss=1.743543031847296, w0=72.95624999999971, w1=13.049495079047029\n",
      "Gradient Descent(8153/9999): loss=2.160135192223528, w0=72.84374999999972, w1=12.88197667496676\n",
      "Gradient Descent(8154/9999): loss=1.991639992456259, w0=72.84374999999972, w1=13.092789106567356\n",
      "Gradient Descent(8155/9999): loss=2.748826625890527, w0=72.84374999999972, w1=13.135553282411857\n",
      "Gradient Descent(8156/9999): loss=1.9935157858974129, w0=72.67499999999971, w1=12.97658916083132\n",
      "Gradient Descent(8157/9999): loss=2.17375626778739, w0=72.89999999999971, w1=12.919178160940984\n",
      "Gradient Descent(8158/9999): loss=2.262140068868839, w0=72.8437499999997, w1=12.76913375835273\n",
      "Gradient Descent(8159/9999): loss=1.9570663545702889, w0=72.7312499999997, w1=12.97429941609701\n",
      "Gradient Descent(8160/9999): loss=2.1281631855574528, w0=73.0124999999997, w1=13.026719521789971\n",
      "Gradient Descent(8161/9999): loss=1.8026712837767012, w0=73.0124999999997, w1=13.067569997027542\n",
      "Gradient Descent(8162/9999): loss=1.6347090577416283, w0=73.0124999999997, w1=13.052441065243684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8163/9999): loss=2.817492430863588, w0=73.1249999999997, w1=13.17523425588126\n",
      "Gradient Descent(8164/9999): loss=2.857449192196534, w0=72.8437499999997, w1=13.295488661467614\n",
      "Gradient Descent(8165/9999): loss=1.9239835977710176, w0=73.2937499999997, w1=13.428417614096322\n",
      "Gradient Descent(8166/9999): loss=2.4463266759143636, w0=73.06874999999971, w1=13.552478446765964\n",
      "Gradient Descent(8167/9999): loss=2.3046412716488796, w0=73.23749999999971, w1=13.489290661339954\n",
      "Gradient Descent(8168/9999): loss=2.3315642735159816, w0=73.40624999999972, w1=13.537463873826141\n",
      "Gradient Descent(8169/9999): loss=2.6563917778795787, w0=73.51874999999971, w1=13.711250731800389\n",
      "Gradient Descent(8170/9999): loss=2.5355113310968505, w0=73.57499999999972, w1=13.777424333593837\n",
      "Gradient Descent(8171/9999): loss=2.3535812628592927, w0=73.63124999999972, w1=13.885163023226283\n",
      "Gradient Descent(8172/9999): loss=2.585017468356261, w0=73.51874999999973, w1=13.82231321383439\n",
      "Gradient Descent(8173/9999): loss=2.0653430599594476, w0=73.29374999999973, w1=13.812182136770266\n",
      "Gradient Descent(8174/9999): loss=2.1764544073915992, w0=73.34999999999974, w1=13.746966780198603\n",
      "Gradient Descent(8175/9999): loss=2.1740426701812217, w0=73.46249999999974, w1=13.673395247021114\n",
      "Gradient Descent(8176/9999): loss=2.3751802351618623, w0=73.34999999999974, w1=13.8165538784548\n",
      "Gradient Descent(8177/9999): loss=2.1000398006281253, w0=73.12499999999974, w1=13.650811339957903\n",
      "Gradient Descent(8178/9999): loss=1.7562874034781755, w0=72.95624999999974, w1=13.367733345332873\n",
      "Gradient Descent(8179/9999): loss=2.6841023715889634, w0=72.78749999999974, w1=13.651043309089802\n",
      "Gradient Descent(8180/9999): loss=2.1833941578111045, w0=73.06874999999974, w1=13.636598802598986\n",
      "Gradient Descent(8181/9999): loss=1.9748839174599147, w0=72.78749999999974, w1=13.533063921778869\n",
      "Gradient Descent(8182/9999): loss=2.6370322215691693, w0=73.01249999999973, w1=13.347425513169394\n",
      "Gradient Descent(8183/9999): loss=1.7263718480286263, w0=73.06874999999974, w1=13.478211356463657\n",
      "Gradient Descent(8184/9999): loss=1.5575396927459317, w0=73.12499999999974, w1=13.527149188387892\n",
      "Gradient Descent(8185/9999): loss=2.0625997680597123, w0=73.12499999999974, w1=13.57945450645914\n",
      "Gradient Descent(8186/9999): loss=2.677271973159292, w0=73.06874999999974, w1=13.507640218832636\n",
      "Gradient Descent(8187/9999): loss=2.2046822366885603, w0=73.18124999999974, w1=13.387571447199917\n",
      "Gradient Descent(8188/9999): loss=2.074405278814715, w0=73.29374999999973, w1=13.496019197611803\n",
      "Gradient Descent(8189/9999): loss=1.8778508773743683, w0=73.18124999999974, w1=13.460080514337148\n",
      "Gradient Descent(8190/9999): loss=1.7142953184349925, w0=73.23749999999974, w1=13.616809046704459\n",
      "Gradient Descent(8191/9999): loss=2.0217625608237713, w0=73.06874999999974, w1=13.279300816820191\n",
      "Gradient Descent(8192/9999): loss=1.8485936151062878, w0=73.06874999999974, w1=13.469892559425704\n",
      "Gradient Descent(8193/9999): loss=1.7799724642496038, w0=73.06874999999974, w1=13.692921931767803\n",
      "Gradient Descent(8194/9999): loss=2.523541158109774, w0=73.12499999999974, w1=13.707592915721916\n",
      "Gradient Descent(8195/9999): loss=2.730417329763856, w0=73.01249999999975, w1=13.784375430652576\n",
      "Gradient Descent(8196/9999): loss=2.0647672979530625, w0=72.89999999999975, w1=13.983496173489813\n",
      "Gradient Descent(8197/9999): loss=2.4763691915970254, w0=72.84374999999974, w1=13.773192537623087\n",
      "Gradient Descent(8198/9999): loss=1.9835886452307396, w0=72.89999999999975, w1=13.465257707099353\n",
      "Gradient Descent(8199/9999): loss=2.351406464480108, w0=72.67499999999976, w1=13.79624264452343\n",
      "Gradient Descent(8200/9999): loss=2.088070316235436, w0=72.56249999999976, w1=13.882931423731428\n",
      "Gradient Descent(8201/9999): loss=2.2014361590751834, w0=72.50624999999975, w1=13.879348210340067\n",
      "Gradient Descent(8202/9999): loss=2.7999156435977706, w0=72.73124999999975, w1=13.781823008364293\n",
      "Gradient Descent(8203/9999): loss=2.882297025746001, w0=72.61874999999975, w1=13.921130568599004\n",
      "Gradient Descent(8204/9999): loss=2.0675718904467595, w0=72.89999999999975, w1=13.759733999827567\n",
      "Gradient Descent(8205/9999): loss=2.576297454243765, w0=73.01249999999975, w1=13.735701058815255\n",
      "Gradient Descent(8206/9999): loss=2.0818018407450083, w0=73.01249999999975, w1=13.824325853881648\n",
      "Gradient Descent(8207/9999): loss=2.3067223732866475, w0=73.12499999999974, w1=13.686960552138117\n",
      "Gradient Descent(8208/9999): loss=2.1826499313485392, w0=73.18124999999975, w1=13.429972929619106\n",
      "Gradient Descent(8209/9999): loss=1.8457834325991551, w0=73.06874999999975, w1=13.626057246449719\n",
      "Gradient Descent(8210/9999): loss=1.9016266715191912, w0=72.95624999999976, w1=13.598021519982174\n",
      "Gradient Descent(8211/9999): loss=1.8749245188469645, w0=73.18124999999975, w1=13.476755633120476\n",
      "Gradient Descent(8212/9999): loss=2.1252964320543724, w0=73.01249999999975, w1=13.454092196198031\n",
      "Gradient Descent(8213/9999): loss=2.1428547346397755, w0=73.18124999999975, w1=13.335719481071195\n",
      "Gradient Descent(8214/9999): loss=2.507828003839637, w0=73.29374999999975, w1=13.365461367714177\n",
      "Gradient Descent(8215/9999): loss=1.6826904670734666, w0=73.29374999999975, w1=13.456018227775317\n",
      "Gradient Descent(8216/9999): loss=2.7729228557653864, w0=73.63124999999975, w1=13.473046048153286\n",
      "Gradient Descent(8217/9999): loss=2.4808181407009537, w0=73.51874999999976, w1=13.216984551459772\n",
      "Gradient Descent(8218/9999): loss=2.287518336168186, w0=73.51874999999976, w1=13.260634511969805\n",
      "Gradient Descent(8219/9999): loss=2.1938334669038886, w0=73.40624999999976, w1=13.390560262850547\n",
      "Gradient Descent(8220/9999): loss=2.376947840922182, w0=73.51874999999976, w1=13.329929838345642\n",
      "Gradient Descent(8221/9999): loss=2.3012650385374864, w0=73.34999999999975, w1=13.430816734430822\n",
      "Gradient Descent(8222/9999): loss=1.9757477564971488, w0=73.51874999999976, w1=13.731239162520273\n",
      "Gradient Descent(8223/9999): loss=2.2775381882885015, w0=73.40624999999976, w1=13.494785802833347\n",
      "Gradient Descent(8224/9999): loss=2.48575106216396, w0=73.34999999999975, w1=13.87733279966547\n",
      "Gradient Descent(8225/9999): loss=2.175285358495673, w0=73.51874999999976, w1=13.776686710200286\n",
      "Gradient Descent(8226/9999): loss=2.3557449263899994, w0=73.46249999999975, w1=13.765752022701179\n",
      "Gradient Descent(8227/9999): loss=1.7347357265243206, w0=73.06874999999975, w1=13.663409623189692\n",
      "Gradient Descent(8228/9999): loss=1.909085214499292, w0=72.78749999999975, w1=13.325970534996966\n",
      "Gradient Descent(8229/9999): loss=2.32139553784789, w0=72.84374999999976, w1=13.374239111513598\n",
      "Gradient Descent(8230/9999): loss=2.260673305852878, w0=72.67499999999976, w1=13.409934162873622\n",
      "Gradient Descent(8231/9999): loss=2.2048908905414217, w0=72.67499999999976, w1=13.557405450896395\n",
      "Gradient Descent(8232/9999): loss=2.2587463469665026, w0=72.56249999999976, w1=13.363140125601992\n",
      "Gradient Descent(8233/9999): loss=2.1812284751119835, w0=72.61874999999976, w1=13.237202453616218\n",
      "Gradient Descent(8234/9999): loss=2.3188806164687774, w0=72.67499999999977, w1=13.602430841252936\n",
      "Gradient Descent(8235/9999): loss=2.055505991301734, w0=72.56249999999977, w1=13.680154426102677\n",
      "Gradient Descent(8236/9999): loss=2.7044978412867655, w0=72.22499999999977, w1=13.626228153009478\n",
      "Gradient Descent(8237/9999): loss=2.2070227292266797, w0=72.44999999999976, w1=13.698339393466698\n",
      "Gradient Descent(8238/9999): loss=2.2662587951192044, w0=72.78749999999977, w1=13.555101277994392\n",
      "Gradient Descent(8239/9999): loss=1.4075356055431403, w0=72.78749999999977, w1=13.34348318572692\n",
      "Gradient Descent(8240/9999): loss=2.070158610136932, w0=72.84374999999977, w1=13.17503754782012\n",
      "Gradient Descent(8241/9999): loss=2.0565163368394024, w0=72.56249999999977, w1=13.033253891594319\n",
      "Gradient Descent(8242/9999): loss=1.5274107214545505, w0=72.33749999999978, w1=12.93898433369086\n",
      "Gradient Descent(8243/9999): loss=2.4174617575954462, w0=72.28124999999977, w1=12.865184270592701\n",
      "Gradient Descent(8244/9999): loss=2.312495430571452, w0=72.33749999999978, w1=13.104775112463653\n",
      "Gradient Descent(8245/9999): loss=2.054619469985994, w0=72.22499999999978, w1=12.871760770598186\n",
      "Gradient Descent(8246/9999): loss=2.3404719930470006, w0=72.16874999999978, w1=12.82122878821535\n",
      "Gradient Descent(8247/9999): loss=1.9515797344430796, w0=72.22499999999978, w1=12.852180892309285\n",
      "Gradient Descent(8248/9999): loss=2.8699426467404936, w0=72.33749999999978, w1=13.061484165503725\n",
      "Gradient Descent(8249/9999): loss=2.2654225777653747, w0=72.50624999999978, w1=13.255135636792206\n",
      "Gradient Descent(8250/9999): loss=2.7504266614188184, w0=72.33749999999978, w1=13.174041649878154\n",
      "Gradient Descent(8251/9999): loss=1.489513121113047, w0=72.44999999999978, w1=12.70941285375175\n",
      "Gradient Descent(8252/9999): loss=1.9770750016113785, w0=72.67499999999977, w1=12.923699853907557\n",
      "Gradient Descent(8253/9999): loss=1.802404147126462, w0=72.67499999999977, w1=12.522919998452364\n",
      "Gradient Descent(8254/9999): loss=2.2569384411556603, w0=72.73124999999978, w1=12.814972839788513\n",
      "Gradient Descent(8255/9999): loss=2.4375416780832104, w0=72.84374999999977, w1=12.770200774480726\n",
      "Gradient Descent(8256/9999): loss=2.145395935186939, w0=72.89999999999978, w1=12.934682600325422\n",
      "Gradient Descent(8257/9999): loss=1.8502241513814444, w0=72.95624999999978, w1=13.19974520795644\n",
      "Gradient Descent(8258/9999): loss=1.5898786032417453, w0=73.01249999999979, w1=13.246183970755634\n",
      "Gradient Descent(8259/9999): loss=1.9063866812315253, w0=73.18124999999979, w1=13.547122947599759\n",
      "Gradient Descent(8260/9999): loss=2.1416424758870747, w0=73.40624999999979, w1=13.738041225150939\n",
      "Gradient Descent(8261/9999): loss=1.7486948150071682, w0=73.12499999999979, w1=13.832786465085942\n",
      "Gradient Descent(8262/9999): loss=1.8712060437687035, w0=73.29374999999979, w1=13.927078637773139\n",
      "Gradient Descent(8263/9999): loss=2.059779154468899, w0=73.29374999999979, w1=13.780061038999499\n",
      "Gradient Descent(8264/9999): loss=2.192044844969597, w0=73.3499999999998, w1=13.806748007217847\n",
      "Gradient Descent(8265/9999): loss=1.9198133193423086, w0=73.3499999999998, w1=13.813928843521476\n",
      "Gradient Descent(8266/9999): loss=2.3642281663902596, w0=73.2374999999998, w1=13.775754067915162\n",
      "Gradient Descent(8267/9999): loss=2.490998618618239, w0=73.18124999999979, w1=13.807670464229947\n",
      "Gradient Descent(8268/9999): loss=1.71610880585593, w0=73.3499999999998, w1=13.712690211483\n",
      "Gradient Descent(8269/9999): loss=1.9890999121850956, w0=73.4062499999998, w1=13.519692384962285\n",
      "Gradient Descent(8270/9999): loss=2.287523228117413, w0=73.5749999999998, w1=13.37998419596319\n",
      "Gradient Descent(8271/9999): loss=1.7947694935580194, w0=73.34999999999981, w1=13.23973296904798\n",
      "Gradient Descent(8272/9999): loss=2.2994021986839335, w0=73.1812499999998, w1=13.210003178139344\n",
      "Gradient Descent(8273/9999): loss=2.2937459863135077, w0=73.4062499999998, w1=12.972705941383436\n",
      "Gradient Descent(8274/9999): loss=2.12048112607847, w0=73.2374999999998, w1=13.019759316672957\n",
      "Gradient Descent(8275/9999): loss=1.9312280150184704, w0=73.0687499999998, w1=12.865957243930856\n",
      "Gradient Descent(8276/9999): loss=2.7739193452420987, w0=72.9562499999998, w1=13.164866698067298\n",
      "Gradient Descent(8277/9999): loss=1.9512741221056704, w0=73.1249999999998, w1=13.259227879204559\n",
      "Gradient Descent(8278/9999): loss=1.6237428861424972, w0=73.1249999999998, w1=13.431652131862958\n",
      "Gradient Descent(8279/9999): loss=2.2205624396025563, w0=73.1812499999998, w1=13.303405107007594\n",
      "Gradient Descent(8280/9999): loss=2.1965626513479233, w0=73.23749999999981, w1=13.26784851225254\n",
      "Gradient Descent(8281/9999): loss=2.027561980543001, w0=73.34999999999981, w1=13.477730137709855\n",
      "Gradient Descent(8282/9999): loss=2.33007471500028, w0=73.5749999999998, w1=13.611664405707154\n",
      "Gradient Descent(8283/9999): loss=2.1980954813145446, w0=73.4062499999998, w1=13.554933238625699\n",
      "Gradient Descent(8284/9999): loss=1.8498308092672715, w0=73.4624999999998, w1=13.578637380632536\n",
      "Gradient Descent(8285/9999): loss=2.0956542626057986, w0=73.4624999999998, w1=13.737835545695164\n",
      "Gradient Descent(8286/9999): loss=2.0288901994231763, w0=73.4062499999998, w1=13.745594507438692\n",
      "Gradient Descent(8287/9999): loss=2.773666033788743, w0=73.0687499999998, w1=13.547099061402768\n",
      "Gradient Descent(8288/9999): loss=1.9727793226716794, w0=73.1249999999998, w1=13.46864462248558\n",
      "Gradient Descent(8289/9999): loss=2.1324912571378327, w0=73.0124999999998, w1=13.208559610196613\n",
      "Gradient Descent(8290/9999): loss=1.8234288291448273, w0=73.0124999999998, w1=13.21353487873694\n",
      "Gradient Descent(8291/9999): loss=3.112413066715061, w0=73.1249999999998, w1=13.357544063921779\n",
      "Gradient Descent(8292/9999): loss=1.6858443022533822, w0=73.0687499999998, w1=13.537953605424743\n",
      "Gradient Descent(8293/9999): loss=1.654139589462058, w0=72.73124999999979, w1=13.546768616561529\n",
      "Gradient Descent(8294/9999): loss=2.4230850992392146, w0=72.61874999999979, w1=13.877463934962918\n",
      "Gradient Descent(8295/9999): loss=2.9952099220582364, w0=72.89999999999979, w1=13.902031226624667\n",
      "Gradient Descent(8296/9999): loss=2.3782439438046152, w0=72.9562499999998, w1=14.147869498729312\n",
      "Gradient Descent(8297/9999): loss=1.8119796213503228, w0=72.89999999999979, w1=14.14709133385237\n",
      "Gradient Descent(8298/9999): loss=2.1889746737622584, w0=72.84374999999979, w1=14.381125635728337\n",
      "Gradient Descent(8299/9999): loss=2.578051588240732, w0=72.89999999999979, w1=14.155203664254525\n",
      "Gradient Descent(8300/9999): loss=2.1972648296801003, w0=73.01249999999979, w1=14.084142898057099\n",
      "Gradient Descent(8301/9999): loss=1.8242028064123168, w0=72.95624999999978, w1=14.233114042767992\n",
      "Gradient Descent(8302/9999): loss=2.106284288781919, w0=73.12499999999979, w1=14.36387698219029\n",
      "Gradient Descent(8303/9999): loss=2.3796657490860875, w0=73.12499999999979, w1=14.095412614067603\n",
      "Gradient Descent(8304/9999): loss=2.1068941294655326, w0=73.12499999999979, w1=14.220829103939193\n",
      "Gradient Descent(8305/9999): loss=2.1775948664235925, w0=73.06874999999978, w1=14.162485477715135\n",
      "Gradient Descent(8306/9999): loss=2.111167292010057, w0=72.95624999999978, w1=14.044133647796293\n",
      "Gradient Descent(8307/9999): loss=2.3801026884704495, w0=73.06874999999978, w1=14.087664411465203\n",
      "Gradient Descent(8308/9999): loss=2.5012182860295047, w0=72.84374999999979, w1=14.283287294103483\n",
      "Gradient Descent(8309/9999): loss=2.7047894809415274, w0=73.40624999999979, w1=14.207200720279376\n",
      "Gradient Descent(8310/9999): loss=2.1641804803392675, w0=73.29374999999979, w1=14.300929846884394\n",
      "Gradient Descent(8311/9999): loss=2.065002767239001, w0=73.3499999999998, w1=14.077307364430245\n",
      "Gradient Descent(8312/9999): loss=1.8966175060324004, w0=73.18124999999979, w1=13.947467319670235\n",
      "Gradient Descent(8313/9999): loss=2.900607776460669, w0=73.3499999999998, w1=13.818355051764769\n",
      "Gradient Descent(8314/9999): loss=1.7545676129747236, w0=73.57499999999979, w1=13.61152519129153\n",
      "Gradient Descent(8315/9999): loss=2.6121146430815503, w0=73.46249999999979, w1=13.474969254744307\n",
      "Gradient Descent(8316/9999): loss=2.2299968519691484, w0=73.6312499999998, w1=13.65896781457013\n",
      "Gradient Descent(8317/9999): loss=2.0829139552099836, w0=73.57499999999979, w1=13.758187386077383\n",
      "Gradient Descent(8318/9999): loss=1.6393425835637976, w0=73.6312499999998, w1=13.953215732245752\n",
      "Gradient Descent(8319/9999): loss=2.314914091244562, w0=73.3499999999998, w1=14.291086012703635\n",
      "Gradient Descent(8320/9999): loss=2.3045864815723096, w0=73.5187499999998, w1=14.137668044033859\n",
      "Gradient Descent(8321/9999): loss=1.9740795665854434, w0=73.4062499999998, w1=13.934609335802026\n",
      "Gradient Descent(8322/9999): loss=2.376442832921719, w0=73.4062499999998, w1=13.873683369063077\n",
      "Gradient Descent(8323/9999): loss=2.322720628497307, w0=73.4624999999998, w1=14.052257776011688\n",
      "Gradient Descent(8324/9999): loss=1.9647191540590598, w0=73.34999999999981, w1=13.909193319191504\n",
      "Gradient Descent(8325/9999): loss=2.597529990172683, w0=73.4624999999998, w1=13.911473163846933\n",
      "Gradient Descent(8326/9999): loss=2.1403595116996073, w0=73.63124999999981, w1=13.691709156985366\n",
      "Gradient Descent(8327/9999): loss=2.15730433944706, w0=73.5749999999998, w1=13.788340016436454\n",
      "Gradient Descent(8328/9999): loss=1.9336712248115702, w0=73.63124999999981, w1=13.6738272810056\n",
      "Gradient Descent(8329/9999): loss=2.0348164627386556, w0=73.51874999999981, w1=13.81313646105615\n",
      "Gradient Descent(8330/9999): loss=2.151350719741608, w0=73.57499999999982, w1=13.88177548281548\n",
      "Gradient Descent(8331/9999): loss=1.6636213356730127, w0=73.40624999999982, w1=13.852443184310285\n",
      "Gradient Descent(8332/9999): loss=2.4327139269138223, w0=73.74374999999982, w1=13.807625563045338\n",
      "Gradient Descent(8333/9999): loss=2.221419560946973, w0=73.63124999999982, w1=14.021422271248822\n",
      "Gradient Descent(8334/9999): loss=1.9405210627274476, w0=73.57499999999982, w1=14.005337583355002\n",
      "Gradient Descent(8335/9999): loss=2.255265741174497, w0=73.57499999999982, w1=13.684405790583353\n",
      "Gradient Descent(8336/9999): loss=1.7905634509695894, w0=73.06874999999982, w1=13.683168680976015\n",
      "Gradient Descent(8337/9999): loss=1.9349143520550833, w0=73.18124999999982, w1=13.501941596484059\n",
      "Gradient Descent(8338/9999): loss=2.599374330510315, w0=73.57499999999982, w1=13.483068164512432\n",
      "Gradient Descent(8339/9999): loss=2.2941147695016215, w0=73.34999999999982, w1=13.420194322543104\n",
      "Gradient Descent(8340/9999): loss=2.2140058840939236, w0=73.12499999999983, w1=13.443130816246272\n",
      "Gradient Descent(8341/9999): loss=2.272252308129816, w0=73.40624999999983, w1=13.648618076820139\n",
      "Gradient Descent(8342/9999): loss=2.243651107114693, w0=73.40624999999983, w1=13.741277500995707\n",
      "Gradient Descent(8343/9999): loss=1.9086479375894525, w0=73.34999999999982, w1=13.65534143018591\n",
      "Gradient Descent(8344/9999): loss=2.598586422568465, w0=73.40624999999983, w1=13.622282611252098\n",
      "Gradient Descent(8345/9999): loss=2.274645198443298, w0=73.12499999999983, w1=13.6102812011629\n",
      "Gradient Descent(8346/9999): loss=2.1640389980887895, w0=72.84374999999983, w1=13.741446704645547\n",
      "Gradient Descent(8347/9999): loss=2.1056592722864886, w0=73.23749999999983, w1=13.780373674691107\n",
      "Gradient Descent(8348/9999): loss=2.5335799081164194, w0=73.40624999999983, w1=13.644476660456094\n",
      "Gradient Descent(8349/9999): loss=2.574524104779613, w0=73.18124999999984, w1=13.504796722947109\n",
      "Gradient Descent(8350/9999): loss=1.9661845273551681, w0=73.18124999999984, w1=13.43728139730514\n",
      "Gradient Descent(8351/9999): loss=1.8035833915973767, w0=73.12499999999983, w1=13.398205729083786\n",
      "Gradient Descent(8352/9999): loss=2.1686446384598224, w0=73.06874999999982, w1=13.753456965480382\n",
      "Gradient Descent(8353/9999): loss=2.124354075163148, w0=73.01249999999982, w1=13.724082205035405\n",
      "Gradient Descent(8354/9999): loss=2.4941472618349705, w0=73.01249999999982, w1=13.636614101118793\n",
      "Gradient Descent(8355/9999): loss=1.9999729824544903, w0=72.67499999999981, w1=13.77718052301281\n",
      "Gradient Descent(8356/9999): loss=2.082719167972486, w0=72.78749999999981, w1=13.464722945763924\n",
      "Gradient Descent(8357/9999): loss=2.1853399617682703, w0=73.1812499999998, w1=13.638665938309233\n",
      "Gradient Descent(8358/9999): loss=2.546792413275942, w0=73.4062499999998, w1=13.28389993527069\n",
      "Gradient Descent(8359/9999): loss=2.5434532210558807, w0=73.4062499999998, w1=13.638072273539024\n",
      "Gradient Descent(8360/9999): loss=2.4953619200689747, w0=73.5187499999998, w1=13.716946043535797\n",
      "Gradient Descent(8361/9999): loss=2.5721369642386565, w0=73.46249999999979, w1=13.851643822544812\n",
      "Gradient Descent(8362/9999): loss=1.9330900756644207, w0=73.3499999999998, w1=13.859557965618924\n",
      "Gradient Descent(8363/9999): loss=2.6826103920596456, w0=73.29374999999979, w1=13.875041398601706\n",
      "Gradient Descent(8364/9999): loss=2.0490297345241135, w0=73.23749999999978, w1=13.781004338054904\n",
      "Gradient Descent(8365/9999): loss=2.341298036434516, w0=73.46249999999978, w1=13.593582700893043\n",
      "Gradient Descent(8366/9999): loss=2.208771071283201, w0=73.34999999999978, w1=13.592151765522427\n",
      "Gradient Descent(8367/9999): loss=2.474593189498975, w0=73.23749999999978, w1=13.694372539567205\n",
      "Gradient Descent(8368/9999): loss=2.318326550314798, w0=73.40624999999979, w1=13.791356746574564\n",
      "Gradient Descent(8369/9999): loss=2.2302321959554403, w0=73.34999999999978, w1=13.969695556271121\n",
      "Gradient Descent(8370/9999): loss=1.9234449004587042, w0=73.23749999999978, w1=13.76527598758571\n",
      "Gradient Descent(8371/9999): loss=2.380006345316055, w0=72.95624999999978, w1=13.64059078394279\n",
      "Gradient Descent(8372/9999): loss=2.518459340960286, w0=73.12499999999979, w1=13.742104299177948\n",
      "Gradient Descent(8373/9999): loss=1.9988417889556143, w0=73.18124999999979, w1=13.693715298937912\n",
      "Gradient Descent(8374/9999): loss=2.1072523702756056, w0=73.3499999999998, w1=13.566585294569432\n",
      "Gradient Descent(8375/9999): loss=1.8830031622495547, w0=73.4062499999998, w1=13.502232414204274\n",
      "Gradient Descent(8376/9999): loss=2.268658973146249, w0=73.5749999999998, w1=13.435201232000976\n",
      "Gradient Descent(8377/9999): loss=2.1042846567160494, w0=73.4624999999998, w1=13.534675995492385\n",
      "Gradient Descent(8378/9999): loss=2.7337904341735153, w0=73.5749999999998, w1=13.408911800702795\n",
      "Gradient Descent(8379/9999): loss=2.154335703228318, w0=73.63124999999981, w1=13.508686116787255\n",
      "Gradient Descent(8380/9999): loss=2.784677225889771, w0=73.5749999999998, w1=13.309172296433422\n",
      "Gradient Descent(8381/9999): loss=2.2765540410992067, w0=73.2937499999998, w1=13.224373743022515\n",
      "Gradient Descent(8382/9999): loss=2.320228171168276, w0=73.4062499999998, w1=13.22659444952672\n",
      "Gradient Descent(8383/9999): loss=1.6293540133160578, w0=73.4062499999998, w1=13.567593228822558\n",
      "Gradient Descent(8384/9999): loss=2.4327398776321028, w0=73.2937499999998, w1=13.433113240597693\n",
      "Gradient Descent(8385/9999): loss=2.5981987512499645, w0=72.9562499999998, w1=13.384840478140658\n",
      "Gradient Descent(8386/9999): loss=2.1101895813798714, w0=73.0124999999998, w1=13.175544471975314\n",
      "Gradient Descent(8387/9999): loss=2.0872513727173123, w0=73.1249999999998, w1=12.993664323007017\n",
      "Gradient Descent(8388/9999): loss=2.6030075897729623, w0=72.9562499999998, w1=12.83944946521324\n",
      "Gradient Descent(8389/9999): loss=2.5748709819077504, w0=72.6749999999998, w1=12.97857979337066\n",
      "Gradient Descent(8390/9999): loss=2.2197146345494936, w0=72.6749999999998, w1=13.043533852292562\n",
      "Gradient Descent(8391/9999): loss=2.0674717028123126, w0=72.4499999999998, w1=13.31403245132347\n",
      "Gradient Descent(8392/9999): loss=2.330521625556576, w0=72.3937499999998, w1=13.40796640097088\n",
      "Gradient Descent(8393/9999): loss=1.9010595315799428, w0=72.1124999999998, w1=13.538165791899477\n",
      "Gradient Descent(8394/9999): loss=2.6673437881493474, w0=72.5062499999998, w1=13.57674761401512\n",
      "Gradient Descent(8395/9999): loss=2.1024789048408117, w0=72.5062499999998, w1=13.422012265972887\n",
      "Gradient Descent(8396/9999): loss=2.5748852301025953, w0=72.5624999999998, w1=13.608822686201297\n",
      "Gradient Descent(8397/9999): loss=2.697183780607557, w0=72.6187499999998, w1=13.713323441205722\n",
      "Gradient Descent(8398/9999): loss=2.0425846764432514, w0=72.5624999999998, w1=13.646243225700866\n",
      "Gradient Descent(8399/9999): loss=2.0100810168087526, w0=72.7874999999998, w1=13.483491773712776\n",
      "Gradient Descent(8400/9999): loss=2.1384112071579175, w0=73.1249999999998, w1=13.504984630543436\n",
      "Gradient Descent(8401/9999): loss=2.3961001456005184, w0=72.8437499999998, w1=13.400380808672201\n",
      "Gradient Descent(8402/9999): loss=1.9744125101497758, w0=73.2374999999998, w1=13.31541546633175\n",
      "Gradient Descent(8403/9999): loss=1.6093708697372562, w0=73.3499999999998, w1=13.207946804734405\n",
      "Gradient Descent(8404/9999): loss=2.4624465673180005, w0=73.2374999999998, w1=13.121627719958822\n",
      "Gradient Descent(8405/9999): loss=1.7694373154537852, w0=73.2937499999998, w1=13.32987074796859\n",
      "Gradient Descent(8406/9999): loss=2.718603866419552, w0=73.0124999999998, w1=13.447549740014317\n",
      "Gradient Descent(8407/9999): loss=2.976080980182923, w0=73.0124999999998, w1=13.27094903396564\n",
      "Gradient Descent(8408/9999): loss=1.7987339702497398, w0=73.2937499999998, w1=13.065153859556201\n",
      "Gradient Descent(8409/9999): loss=2.179643255128278, w0=73.4624999999998, w1=13.08348069735733\n",
      "Gradient Descent(8410/9999): loss=2.4363668129567833, w0=73.4624999999998, w1=12.999868551195956\n",
      "Gradient Descent(8411/9999): loss=2.2693940349622674, w0=73.4624999999998, w1=12.901400255411392\n",
      "Gradient Descent(8412/9999): loss=1.766938272977414, w0=73.4624999999998, w1=13.179267813802024\n",
      "Gradient Descent(8413/9999): loss=1.9146279868025484, w0=73.4062499999998, w1=13.140075936727804\n",
      "Gradient Descent(8414/9999): loss=2.2154576578304384, w0=73.5749999999998, w1=13.348801637819202\n",
      "Gradient Descent(8415/9999): loss=2.2489061506529358, w0=73.4624999999998, w1=13.354167638961904\n",
      "Gradient Descent(8416/9999): loss=2.6476000363126264, w0=73.4062499999998, w1=13.161229364071442\n",
      "Gradient Descent(8417/9999): loss=2.2301612883187234, w0=73.2937499999998, w1=13.141199040347127\n",
      "Gradient Descent(8418/9999): loss=2.281120903707198, w0=73.63124999999981, w1=13.133105254955161\n",
      "Gradient Descent(8419/9999): loss=1.8539588760805499, w0=73.34999999999981, w1=13.031417193944785\n",
      "Gradient Descent(8420/9999): loss=2.682397330550809, w0=73.34999999999981, w1=13.1935911705254\n",
      "Gradient Descent(8421/9999): loss=1.8465826058805992, w0=73.12499999999982, w1=13.337913957655859\n",
      "Gradient Descent(8422/9999): loss=1.8860722986285459, w0=73.12499999999982, w1=13.495306005521076\n",
      "Gradient Descent(8423/9999): loss=1.8407405586646453, w0=72.95624999999981, w1=13.47482323289153\n",
      "Gradient Descent(8424/9999): loss=2.301018824891216, w0=73.01249999999982, w1=13.589723825586471\n",
      "Gradient Descent(8425/9999): loss=2.071302977466395, w0=72.78749999999982, w1=13.703568042860798\n",
      "Gradient Descent(8426/9999): loss=2.33512137296583, w0=72.89999999999982, w1=13.753197090682434\n",
      "Gradient Descent(8427/9999): loss=2.504038238000998, w0=72.78749999999982, w1=13.887230986633051\n",
      "Gradient Descent(8428/9999): loss=2.2731085614991775, w0=72.78749999999982, w1=14.055780404762872\n",
      "Gradient Descent(8429/9999): loss=2.6846147267200235, w0=72.89999999999982, w1=14.161423659971064\n",
      "Gradient Descent(8430/9999): loss=1.8236042632711964, w0=72.95624999999983, w1=14.35751676966267\n",
      "Gradient Descent(8431/9999): loss=2.0939367261642103, w0=73.01249999999983, w1=14.430023133371526\n",
      "Gradient Descent(8432/9999): loss=2.3652016743456983, w0=73.29374999999983, w1=14.470723449530958\n",
      "Gradient Descent(8433/9999): loss=1.5014160319893441, w0=73.06874999999984, w1=14.54754683510784\n",
      "Gradient Descent(8434/9999): loss=1.8722935389399151, w0=72.84374999999984, w1=14.294286079210966\n",
      "Gradient Descent(8435/9999): loss=1.8336827349193072, w0=72.89999999999985, w1=14.21685024873559\n",
      "Gradient Descent(8436/9999): loss=2.0785782819969802, w0=72.73124999999985, w1=14.47195322525816\n",
      "Gradient Descent(8437/9999): loss=2.317202307275777, w0=73.01249999999985, w1=14.366001010446004\n",
      "Gradient Descent(8438/9999): loss=2.283775060512464, w0=73.23749999999984, w1=14.241097525286994\n",
      "Gradient Descent(8439/9999): loss=1.6621334933052117, w0=73.29374999999985, w1=14.239182399054894\n",
      "Gradient Descent(8440/9999): loss=1.6274088244368046, w0=73.18124999999985, w1=14.137094758251507\n",
      "Gradient Descent(8441/9999): loss=1.9499459023532983, w0=73.29374999999985, w1=13.895934014477975\n",
      "Gradient Descent(8442/9999): loss=2.250567303447112, w0=73.46249999999985, w1=13.791143960161637\n",
      "Gradient Descent(8443/9999): loss=2.236867833925807, w0=73.12499999999984, w1=13.64387712610915\n",
      "Gradient Descent(8444/9999): loss=1.7866524272996769, w0=72.84374999999984, w1=13.87634863016197\n",
      "Gradient Descent(8445/9999): loss=2.0971756980161347, w0=73.12499999999984, w1=13.64468919373458\n",
      "Gradient Descent(8446/9999): loss=1.743157944503341, w0=73.12499999999984, w1=13.643496289945142\n",
      "Gradient Descent(8447/9999): loss=2.1729725608281107, w0=73.34999999999984, w1=13.39442032390495\n",
      "Gradient Descent(8448/9999): loss=2.3624994779652537, w0=73.29374999999983, w1=13.445903378019093\n",
      "Gradient Descent(8449/9999): loss=1.8532232209499413, w0=73.46249999999984, w1=13.186928183990052\n",
      "Gradient Descent(8450/9999): loss=2.248581563602272, w0=73.51874999999984, w1=13.111123710094246\n",
      "Gradient Descent(8451/9999): loss=2.7498829649229, w0=73.51874999999984, w1=13.05863657008241\n",
      "Gradient Descent(8452/9999): loss=1.7180421875090581, w0=73.46249999999984, w1=13.49791579451586\n",
      "Gradient Descent(8453/9999): loss=2.3279923601646004, w0=73.63124999999984, w1=13.394162333553181\n",
      "Gradient Descent(8454/9999): loss=1.79092387837277, w0=73.63124999999984, w1=13.26168800054041\n",
      "Gradient Descent(8455/9999): loss=1.8961877670711034, w0=73.63124999999984, w1=13.216248296222622\n",
      "Gradient Descent(8456/9999): loss=1.8838960513149965, w0=73.74374999999984, w1=13.45618290476851\n",
      "Gradient Descent(8457/9999): loss=2.460661349408131, w0=73.74374999999984, w1=13.637066411405197\n",
      "Gradient Descent(8458/9999): loss=2.0863739828188854, w0=73.57499999999983, w1=13.653455466403988\n",
      "Gradient Descent(8459/9999): loss=2.4915383383653325, w0=73.34999999999984, w1=13.733591777001656\n",
      "Gradient Descent(8460/9999): loss=1.6843366869560985, w0=73.40624999999984, w1=13.524604681189555\n",
      "Gradient Descent(8461/9999): loss=2.2335470892347398, w0=73.51874999999984, w1=13.414220226022204\n",
      "Gradient Descent(8462/9999): loss=2.2871373872345027, w0=73.51874999999984, w1=12.918819022404321\n",
      "Gradient Descent(8463/9999): loss=2.086989422459596, w0=73.34999999999984, w1=12.894395829558963\n",
      "Gradient Descent(8464/9999): loss=2.5465639474918706, w0=73.29374999999983, w1=13.12696371040941\n",
      "Gradient Descent(8465/9999): loss=1.737652128004652, w0=73.06874999999984, w1=13.066632001971227\n",
      "Gradient Descent(8466/9999): loss=2.5282389012722994, w0=73.06874999999984, w1=12.892490662182347\n",
      "Gradient Descent(8467/9999): loss=2.56081670282122, w0=72.84374999999984, w1=13.044946926281472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8468/9999): loss=2.0230113230915796, w0=72.78749999999984, w1=13.056078472411507\n",
      "Gradient Descent(8469/9999): loss=2.089847610613462, w0=73.01249999999983, w1=13.102727111754943\n",
      "Gradient Descent(8470/9999): loss=2.1001195401114385, w0=72.95624999999983, w1=13.29518249786977\n",
      "Gradient Descent(8471/9999): loss=2.2146110194532556, w0=73.29374999999983, w1=13.30629653807716\n",
      "Gradient Descent(8472/9999): loss=2.8259722861547267, w0=73.34999999999984, w1=13.578431390727097\n",
      "Gradient Descent(8473/9999): loss=2.1183103127647973, w0=73.40624999999984, w1=13.398799811977337\n",
      "Gradient Descent(8474/9999): loss=2.7609435398301008, w0=73.46249999999985, w1=13.321681531922586\n",
      "Gradient Descent(8475/9999): loss=1.8023470760523221, w0=73.23749999999986, w1=13.29657887237255\n",
      "Gradient Descent(8476/9999): loss=2.4264130164170394, w0=73.40624999999986, w1=13.228400597327227\n",
      "Gradient Descent(8477/9999): loss=2.5341749540201377, w0=73.18124999999986, w1=13.214911227141762\n",
      "Gradient Descent(8478/9999): loss=2.247428135483009, w0=73.29374999999986, w1=13.413575645275445\n",
      "Gradient Descent(8479/9999): loss=2.5675330162463874, w0=73.23749999999986, w1=13.256087935854861\n",
      "Gradient Descent(8480/9999): loss=2.382650088924636, w0=73.29374999999986, w1=13.294410027017987\n",
      "Gradient Descent(8481/9999): loss=2.449603654505365, w0=73.46249999999986, w1=13.323395874589202\n",
      "Gradient Descent(8482/9999): loss=2.1722029114280823, w0=73.46249999999986, w1=13.542714019649193\n",
      "Gradient Descent(8483/9999): loss=2.4455335924820183, w0=73.40624999999986, w1=13.31516796779789\n",
      "Gradient Descent(8484/9999): loss=2.0779367716347417, w0=73.40624999999986, w1=13.172015121521957\n",
      "Gradient Descent(8485/9999): loss=1.9618737960214039, w0=73.23749999999986, w1=13.205390231255574\n",
      "Gradient Descent(8486/9999): loss=2.148028121804502, w0=73.40624999999986, w1=12.993284062177745\n",
      "Gradient Descent(8487/9999): loss=1.9816562647824145, w0=73.12499999999986, w1=13.15849212129327\n",
      "Gradient Descent(8488/9999): loss=2.6036687630602584, w0=73.29374999999986, w1=13.204797982507232\n",
      "Gradient Descent(8489/9999): loss=2.2557460730836105, w0=73.23749999999986, w1=12.800817623676695\n",
      "Gradient Descent(8490/9999): loss=2.3066212995209074, w0=73.34999999999985, w1=13.333094361070241\n",
      "Gradient Descent(8491/9999): loss=2.2776893465811785, w0=73.46249999999985, w1=13.023435652977572\n",
      "Gradient Descent(8492/9999): loss=1.7495937187866437, w0=73.85624999999985, w1=12.952881651310605\n",
      "Gradient Descent(8493/9999): loss=2.313478970627363, w0=73.79999999999984, w1=12.86662337715484\n",
      "Gradient Descent(8494/9999): loss=2.011277429388619, w0=73.46249999999984, w1=12.882130582132245\n",
      "Gradient Descent(8495/9999): loss=1.6646524443272335, w0=73.29374999999983, w1=13.114598904152771\n",
      "Gradient Descent(8496/9999): loss=2.035836798410881, w0=73.34999999999984, w1=13.038573811246971\n",
      "Gradient Descent(8497/9999): loss=2.0659604080231926, w0=73.29374999999983, w1=13.310080338831582\n",
      "Gradient Descent(8498/9999): loss=1.6750987077441757, w0=73.18124999999984, w1=13.050973858245575\n",
      "Gradient Descent(8499/9999): loss=1.8968988255606622, w0=73.34999999999984, w1=12.933143186794716\n",
      "Gradient Descent(8500/9999): loss=2.012722490220802, w0=73.46249999999984, w1=12.997864853217475\n",
      "Gradient Descent(8501/9999): loss=1.7531790058840977, w0=73.63124999999984, w1=13.479069320710872\n",
      "Gradient Descent(8502/9999): loss=2.084671123900299, w0=73.79999999999984, w1=13.404817216067606\n",
      "Gradient Descent(8503/9999): loss=1.7455754987419003, w0=73.63124999999984, w1=13.387972211561621\n",
      "Gradient Descent(8504/9999): loss=2.9399661158402934, w0=73.63124999999984, w1=13.55967986296369\n",
      "Gradient Descent(8505/9999): loss=2.10680564324338, w0=73.74374999999984, w1=13.125010303804917\n",
      "Gradient Descent(8506/9999): loss=1.8244928074187834, w0=73.63124999999984, w1=13.218615616305856\n",
      "Gradient Descent(8507/9999): loss=1.9125643012756137, w0=73.51874999999984, w1=13.337951837183935\n",
      "Gradient Descent(8508/9999): loss=2.2127592489436188, w0=73.51874999999984, w1=12.981554033460487\n",
      "Gradient Descent(8509/9999): loss=2.339280945962744, w0=73.68749999999984, w1=13.119709057243291\n",
      "Gradient Descent(8510/9999): loss=1.8066527636001712, w0=73.63124999999984, w1=13.3377449285388\n",
      "Gradient Descent(8511/9999): loss=2.0677471559735077, w0=73.51874999999984, w1=13.480670575735028\n",
      "Gradient Descent(8512/9999): loss=2.2955872572998484, w0=73.23749999999984, w1=13.440296066920567\n",
      "Gradient Descent(8513/9999): loss=1.8839070670819753, w0=73.29374999999985, w1=13.337011850663162\n",
      "Gradient Descent(8514/9999): loss=1.858381253722692, w0=73.12499999999984, w1=13.443733667935792\n",
      "Gradient Descent(8515/9999): loss=2.2046172814015446, w0=73.40624999999984, w1=13.32315907949115\n",
      "Gradient Descent(8516/9999): loss=2.623646661584324, w0=73.57499999999985, w1=13.348183520837638\n",
      "Gradient Descent(8517/9999): loss=1.7945694418156248, w0=73.40624999999984, w1=13.409678980121587\n",
      "Gradient Descent(8518/9999): loss=2.3635106793735066, w0=73.46249999999985, w1=13.345530408609768\n",
      "Gradient Descent(8519/9999): loss=2.101675941546575, w0=73.63124999999985, w1=13.513433596749525\n",
      "Gradient Descent(8520/9999): loss=2.0038321338435234, w0=74.02499999999985, w1=13.454831387189689\n",
      "Gradient Descent(8521/9999): loss=2.2742479409388348, w0=74.24999999999984, w1=13.500812603421425\n",
      "Gradient Descent(8522/9999): loss=1.8557859216573203, w0=73.96874999999984, w1=13.629612654647127\n",
      "Gradient Descent(8523/9999): loss=2.4846650179915875, w0=73.85624999999985, w1=13.644202148411633\n",
      "Gradient Descent(8524/9999): loss=2.377967329945199, w0=73.34999999999985, w1=13.629652290247805\n",
      "Gradient Descent(8525/9999): loss=2.0996734612812045, w0=73.18124999999985, w1=13.681439107750741\n",
      "Gradient Descent(8526/9999): loss=2.0933007013011653, w0=73.18124999999985, w1=13.666677697116759\n",
      "Gradient Descent(8527/9999): loss=2.7299805167742948, w0=72.95624999999986, w1=13.624027181091245\n",
      "Gradient Descent(8528/9999): loss=2.364537573503782, w0=72.95624999999986, w1=13.471238438916185\n",
      "Gradient Descent(8529/9999): loss=2.5522195471333706, w0=73.23749999999986, w1=13.177760542957493\n",
      "Gradient Descent(8530/9999): loss=2.1839680612425156, w0=73.46249999999985, w1=13.287411248811082\n",
      "Gradient Descent(8531/9999): loss=2.5519308470839155, w0=73.51874999999986, w1=13.150732006482144\n",
      "Gradient Descent(8532/9999): loss=2.342959493370251, w0=73.34999999999985, w1=13.082215217338756\n",
      "Gradient Descent(8533/9999): loss=2.6625959797179863, w0=73.18124999999985, w1=13.275965328743466\n",
      "Gradient Descent(8534/9999): loss=2.215478020234089, w0=73.06874999999985, w1=13.383103451959267\n",
      "Gradient Descent(8535/9999): loss=2.1726606476318593, w0=73.12499999999986, w1=13.342784066543734\n",
      "Gradient Descent(8536/9999): loss=1.9723855935204928, w0=72.84374999999986, w1=13.086756543091678\n",
      "Gradient Descent(8537/9999): loss=2.43530400047844, w0=73.01249999999986, w1=13.012242002136954\n",
      "Gradient Descent(8538/9999): loss=2.0201823087119632, w0=73.29374999999986, w1=13.279297162722848\n",
      "Gradient Descent(8539/9999): loss=2.584748549127613, w0=73.12499999999986, w1=13.38391286566103\n",
      "Gradient Descent(8540/9999): loss=2.2927373162563347, w0=73.23749999999986, w1=13.321478950840394\n",
      "Gradient Descent(8541/9999): loss=1.8411029160615153, w0=73.23749999999986, w1=13.702099328916875\n",
      "Gradient Descent(8542/9999): loss=2.266410381128, w0=73.23749999999986, w1=13.891687046737559\n",
      "Gradient Descent(8543/9999): loss=2.19532900499795, w0=73.51874999999986, w1=13.893114126868445\n",
      "Gradient Descent(8544/9999): loss=2.1334001376046317, w0=73.63124999999985, w1=13.689973356173189\n",
      "Gradient Descent(8545/9999): loss=2.4230301866909643, w0=73.57499999999985, w1=13.653388546967403\n",
      "Gradient Descent(8546/9999): loss=2.337665829201397, w0=73.46249999999985, w1=13.707701918641106\n",
      "Gradient Descent(8547/9999): loss=1.7573693991549275, w0=73.46249999999985, w1=13.510373968418305\n",
      "Gradient Descent(8548/9999): loss=2.0974190352723148, w0=73.57499999999985, w1=13.652437620062576\n",
      "Gradient Descent(8549/9999): loss=2.059605198534209, w0=73.51874999999984, w1=13.62647737669627\n",
      "Gradient Descent(8550/9999): loss=2.39117095319915, w0=73.51874999999984, w1=13.691861129317692\n",
      "Gradient Descent(8551/9999): loss=2.4800002681796762, w0=73.23749999999984, w1=13.83104207417919\n",
      "Gradient Descent(8552/9999): loss=1.9625645846990052, w0=73.29374999999985, w1=13.960415579220614\n",
      "Gradient Descent(8553/9999): loss=1.8600222083350644, w0=73.40624999999984, w1=13.893611733916044\n",
      "Gradient Descent(8554/9999): loss=2.157845774776999, w0=73.51874999999984, w1=14.137792023473008\n",
      "Gradient Descent(8555/9999): loss=2.2100371830099523, w0=73.34999999999984, w1=13.93193841743522\n",
      "Gradient Descent(8556/9999): loss=2.2399418796802797, w0=73.29374999999983, w1=13.84717734219921\n",
      "Gradient Descent(8557/9999): loss=2.604981850500855, w0=73.23749999999983, w1=13.747865421564775\n",
      "Gradient Descent(8558/9999): loss=1.7625579674989955, w0=73.18124999999982, w1=13.750108071898435\n",
      "Gradient Descent(8559/9999): loss=2.26876241060181, w0=73.12499999999982, w1=13.594569347586129\n",
      "Gradient Descent(8560/9999): loss=2.1720204413847997, w0=73.29374999999982, w1=13.80921738717723\n",
      "Gradient Descent(8561/9999): loss=1.8676056195546589, w0=73.29374999999982, w1=13.688782112200693\n",
      "Gradient Descent(8562/9999): loss=1.9757742409270986, w0=73.23749999999981, w1=13.496838347519057\n",
      "Gradient Descent(8563/9999): loss=1.9516285210424784, w0=73.23749999999981, w1=13.35029424967738\n",
      "Gradient Descent(8564/9999): loss=1.707313808266831, w0=73.29374999999982, w1=13.240858585335758\n",
      "Gradient Descent(8565/9999): loss=2.2267079682555666, w0=73.12499999999982, w1=13.413247743194075\n",
      "Gradient Descent(8566/9999): loss=2.7731994431599687, w0=73.23749999999981, w1=13.301473737152099\n",
      "Gradient Descent(8567/9999): loss=2.590799537906115, w0=73.29374999999982, w1=13.448046011528668\n",
      "Gradient Descent(8568/9999): loss=2.240048421332337, w0=73.29374999999982, w1=13.43562522384416\n",
      "Gradient Descent(8569/9999): loss=2.2477648671954897, w0=73.06874999999982, w1=13.351509413831334\n",
      "Gradient Descent(8570/9999): loss=2.181591814130309, w0=73.06874999999982, w1=13.096887164285551\n",
      "Gradient Descent(8571/9999): loss=2.199151035772948, w0=73.12499999999983, w1=13.230754954620005\n",
      "Gradient Descent(8572/9999): loss=2.6123680002944063, w0=73.29374999999983, w1=13.488491415875533\n",
      "Gradient Descent(8573/9999): loss=2.1597623014010017, w0=73.29374999999983, w1=13.572192105190712\n",
      "Gradient Descent(8574/9999): loss=1.8744339174693647, w0=73.40624999999983, w1=13.256736574155056\n",
      "Gradient Descent(8575/9999): loss=1.8037483336133786, w0=73.46249999999984, w1=13.272587992642418\n",
      "Gradient Descent(8576/9999): loss=2.38897833616966, w0=73.51874999999984, w1=13.501696835665413\n",
      "Gradient Descent(8577/9999): loss=1.821546163231765, w0=73.57499999999985, w1=13.398880943017694\n",
      "Gradient Descent(8578/9999): loss=2.63294485265494, w0=73.57499999999985, w1=13.573649529610414\n",
      "Gradient Descent(8579/9999): loss=2.275103808771422, w0=73.40624999999984, w1=13.808734258557955\n",
      "Gradient Descent(8580/9999): loss=2.6197839488435486, w0=73.63124999999984, w1=13.678455730838973\n",
      "Gradient Descent(8581/9999): loss=2.3985422490084467, w0=73.74374999999984, w1=13.601932690717666\n",
      "Gradient Descent(8582/9999): loss=1.4603327924647398, w0=73.51874999999984, w1=13.571723925455895\n",
      "Gradient Descent(8583/9999): loss=2.3927235855055304, w0=73.63124999999984, w1=13.474865030684692\n",
      "Gradient Descent(8584/9999): loss=2.2157870601147094, w0=73.46249999999984, w1=13.36766627055835\n",
      "Gradient Descent(8585/9999): loss=2.4886433413038795, w0=73.46249999999984, w1=13.515468870540989\n",
      "Gradient Descent(8586/9999): loss=2.4592064827931583, w0=73.29374999999983, w1=13.454155966975433\n",
      "Gradient Descent(8587/9999): loss=2.3569585436832643, w0=73.46249999999984, w1=13.589583671931376\n",
      "Gradient Descent(8588/9999): loss=1.9570124292371567, w0=73.34999999999984, w1=13.704281078036823\n",
      "Gradient Descent(8589/9999): loss=2.456924505632785, w0=73.18124999999984, w1=13.645380865953895\n",
      "Gradient Descent(8590/9999): loss=2.208295464294665, w0=73.12499999999983, w1=13.691365109783284\n",
      "Gradient Descent(8591/9999): loss=2.1428923552902646, w0=73.12499999999983, w1=13.66715283504734\n",
      "Gradient Descent(8592/9999): loss=2.172191052446661, w0=73.01249999999983, w1=13.626126364812112\n",
      "Gradient Descent(8593/9999): loss=2.2056801767964354, w0=73.06874999999984, w1=13.732917048709846\n",
      "Gradient Descent(8594/9999): loss=2.0935424475640243, w0=73.06874999999984, w1=13.792538142850168\n",
      "Gradient Descent(8595/9999): loss=2.293295115624565, w0=73.01249999999983, w1=13.652931873687429\n",
      "Gradient Descent(8596/9999): loss=1.6616526276020556, w0=73.34999999999984, w1=13.737878996065806\n",
      "Gradient Descent(8597/9999): loss=1.9022738020770762, w0=73.68749999999984, w1=13.647103725574043\n",
      "Gradient Descent(8598/9999): loss=2.165191951227495, w0=73.74374999999985, w1=13.657040203859474\n",
      "Gradient Descent(8599/9999): loss=2.45605746391282, w0=73.74374999999985, w1=13.725282637783447\n",
      "Gradient Descent(8600/9999): loss=2.4006210859440227, w0=73.68749999999984, w1=13.763806737483648\n",
      "Gradient Descent(8601/9999): loss=2.4823714935464603, w0=73.63124999999984, w1=13.898375922266258\n",
      "Gradient Descent(8602/9999): loss=2.3988350982802054, w0=73.79999999999984, w1=14.039318839992497\n",
      "Gradient Descent(8603/9999): loss=2.6885463690729967, w0=73.63124999999984, w1=14.384998533295592\n",
      "Gradient Descent(8604/9999): loss=1.9828911678119705, w0=73.74374999999984, w1=14.36776243589548\n",
      "Gradient Descent(8605/9999): loss=2.4517843396810237, w0=73.51874999999984, w1=14.12150227011196\n",
      "Gradient Descent(8606/9999): loss=2.7948644393444213, w0=73.51874999999984, w1=14.114384332240874\n",
      "Gradient Descent(8607/9999): loss=1.6904022581324243, w0=73.23749999999984, w1=14.22116316485877\n",
      "Gradient Descent(8608/9999): loss=2.378465801975005, w0=73.18124999999984, w1=14.096516559398747\n",
      "Gradient Descent(8609/9999): loss=2.0317048517453045, w0=73.40624999999983, w1=13.886841369395405\n",
      "Gradient Descent(8610/9999): loss=1.8132966551266652, w0=73.34999999999982, w1=13.943206066613834\n",
      "Gradient Descent(8611/9999): loss=2.468595651904171, w0=73.12499999999983, w1=13.756325016665043\n",
      "Gradient Descent(8612/9999): loss=2.5652130874228254, w0=73.23749999999983, w1=13.488751007098275\n",
      "Gradient Descent(8613/9999): loss=2.035586631149328, w0=73.12499999999983, w1=13.787272143870847\n",
      "Gradient Descent(8614/9999): loss=2.481023697461084, w0=73.29374999999983, w1=13.459864648036687\n",
      "Gradient Descent(8615/9999): loss=2.3510899695684317, w0=73.34999999999984, w1=13.479078553669718\n",
      "Gradient Descent(8616/9999): loss=1.936723902459054, w0=73.51874999999984, w1=13.573894196408999\n",
      "Gradient Descent(8617/9999): loss=2.0668114205356956, w0=73.51874999999984, w1=13.796332534933141\n",
      "Gradient Descent(8618/9999): loss=2.550150879035982, w0=73.46249999999984, w1=13.87486366990251\n",
      "Gradient Descent(8619/9999): loss=2.0683647483840613, w0=73.18124999999984, w1=13.967507755975468\n",
      "Gradient Descent(8620/9999): loss=2.126980533740692, w0=73.34999999999984, w1=13.70209159358363\n",
      "Gradient Descent(8621/9999): loss=2.210057756103195, w0=73.40624999999984, w1=13.913071118409647\n",
      "Gradient Descent(8622/9999): loss=1.9476688506387814, w0=73.40624999999984, w1=14.02197859495528\n",
      "Gradient Descent(8623/9999): loss=2.58135611026032, w0=73.29374999999985, w1=13.951197462811416\n",
      "Gradient Descent(8624/9999): loss=2.3114962315764855, w0=73.23749999999984, w1=13.819043378506729\n",
      "Gradient Descent(8625/9999): loss=1.747406963729822, w0=73.12499999999984, w1=13.74826265624168\n",
      "Gradient Descent(8626/9999): loss=2.436434494977882, w0=73.12499999999984, w1=14.025274901735848\n",
      "Gradient Descent(8627/9999): loss=1.6898880795037077, w0=73.01249999999985, w1=14.043530318792884\n",
      "Gradient Descent(8628/9999): loss=1.7580936418606101, w0=73.18124999999985, w1=14.111036496601223\n",
      "Gradient Descent(8629/9999): loss=2.592271325739011, w0=73.51874999999986, w1=13.925836556985315\n",
      "Gradient Descent(8630/9999): loss=2.4666154706932257, w0=73.29374999999986, w1=13.85966175547506\n",
      "Gradient Descent(8631/9999): loss=2.1524465748388137, w0=73.46249999999986, w1=13.771085641685232\n",
      "Gradient Descent(8632/9999): loss=2.0791015713534815, w0=73.63124999999987, w1=13.544185030926215\n",
      "Gradient Descent(8633/9999): loss=2.0846189278129126, w0=73.68749999999987, w1=13.387324928409793\n",
      "Gradient Descent(8634/9999): loss=2.32299336161758, w0=73.63124999999987, w1=13.549827375770974\n",
      "Gradient Descent(8635/9999): loss=2.614919297562211, w0=73.51874999999987, w1=13.303129186504231\n",
      "Gradient Descent(8636/9999): loss=2.1043042450719147, w0=73.51874999999987, w1=12.950817191716613\n",
      "Gradient Descent(8637/9999): loss=2.432150983282603, w0=73.12499999999987, w1=13.054709432355178\n",
      "Gradient Descent(8638/9999): loss=2.1170459533329025, w0=73.06874999999987, w1=13.27374120662004\n",
      "Gradient Descent(8639/9999): loss=2.319349000594504, w0=73.06874999999987, w1=13.343474933584163\n",
      "Gradient Descent(8640/9999): loss=2.3563383309198227, w0=73.29374999999986, w1=13.305037882585806\n",
      "Gradient Descent(8641/9999): loss=2.3561044072808888, w0=73.34999999999987, w1=13.69664909062336\n",
      "Gradient Descent(8642/9999): loss=1.9593799482332424, w0=73.57499999999986, w1=13.661441230270924\n",
      "Gradient Descent(8643/9999): loss=1.640688486688115, w0=73.40624999999986, w1=13.592340963995508\n",
      "Gradient Descent(8644/9999): loss=1.8331095084551865, w0=73.29374999999986, w1=13.97710184027895\n",
      "Gradient Descent(8645/9999): loss=2.153827532759669, w0=73.40624999999986, w1=14.238355022859784\n",
      "Gradient Descent(8646/9999): loss=2.1711344377401716, w0=73.23749999999986, w1=14.119031375290987\n",
      "Gradient Descent(8647/9999): loss=2.0474459663533056, w0=72.89999999999985, w1=13.989627783718168\n",
      "Gradient Descent(8648/9999): loss=2.01985999692286, w0=73.06874999999985, w1=13.771207933680577\n",
      "Gradient Descent(8649/9999): loss=2.2487450163093103, w0=72.84374999999986, w1=13.772238911215322\n",
      "Gradient Descent(8650/9999): loss=2.3395696897280454, w0=72.84374999999986, w1=13.789412342989555\n",
      "Gradient Descent(8651/9999): loss=2.5810177303119564, w0=73.12499999999986, w1=13.631861240468595\n",
      "Gradient Descent(8652/9999): loss=1.7329445254351978, w0=73.06874999999985, w1=13.620163490213749\n",
      "Gradient Descent(8653/9999): loss=2.2613710416220254, w0=73.06874999999985, w1=13.592066507513296\n",
      "Gradient Descent(8654/9999): loss=2.403110290076908, w0=73.01249999999985, w1=13.293911922809379\n",
      "Gradient Descent(8655/9999): loss=2.560828526372271, w0=72.84374999999984, w1=13.35195969645359\n",
      "Gradient Descent(8656/9999): loss=2.47171518233933, w0=73.06874999999984, w1=13.426018970432652\n",
      "Gradient Descent(8657/9999): loss=2.7488569699500665, w0=72.89999999999984, w1=13.296969278067515\n",
      "Gradient Descent(8658/9999): loss=2.3860005696211415, w0=73.01249999999983, w1=13.262899668733168\n",
      "Gradient Descent(8659/9999): loss=2.016549591433343, w0=72.95624999999983, w1=13.354873366559618\n",
      "Gradient Descent(8660/9999): loss=1.9910878566050103, w0=72.89999999999982, w1=13.369393438764954\n",
      "Gradient Descent(8661/9999): loss=1.9388104803398272, w0=73.01249999999982, w1=13.584513454829874\n",
      "Gradient Descent(8662/9999): loss=2.172021399379977, w0=73.12499999999982, w1=13.514662480580132\n",
      "Gradient Descent(8663/9999): loss=2.564896822611472, w0=73.23749999999981, w1=13.519786641967984\n",
      "Gradient Descent(8664/9999): loss=1.8176406609966813, w0=73.29374999999982, w1=13.655490906291881\n",
      "Gradient Descent(8665/9999): loss=1.9865778650747947, w0=73.12499999999982, w1=13.517382270048719\n",
      "Gradient Descent(8666/9999): loss=2.6435539302554654, w0=73.23749999999981, w1=13.583427434485571\n",
      "Gradient Descent(8667/9999): loss=2.1429593896824457, w0=73.1812499999998, w1=13.730877930828246\n",
      "Gradient Descent(8668/9999): loss=1.9972622825201973, w0=73.1249999999998, w1=14.150280319281507\n",
      "Gradient Descent(8669/9999): loss=2.191979678541139, w0=73.0687499999998, w1=13.882638866656203\n",
      "Gradient Descent(8670/9999): loss=2.0774998430505347, w0=73.0687499999998, w1=13.946060921681458\n",
      "Gradient Descent(8671/9999): loss=2.380090334349871, w0=73.2374999999998, w1=14.084413998121342\n",
      "Gradient Descent(8672/9999): loss=1.9410456837947763, w0=73.1249999999998, w1=14.247352850201787\n",
      "Gradient Descent(8673/9999): loss=1.9924910792011714, w0=73.0124999999998, w1=14.17409454776916\n",
      "Gradient Descent(8674/9999): loss=2.3563590697452743, w0=73.06874999999981, w1=13.967295850299374\n",
      "Gradient Descent(8675/9999): loss=2.8070815306557773, w0=73.06874999999981, w1=13.914902813254168\n",
      "Gradient Descent(8676/9999): loss=1.9080860376844546, w0=73.12499999999982, w1=13.820664894686315\n",
      "Gradient Descent(8677/9999): loss=2.408615581095306, w0=72.84374999999982, w1=13.746106984907431\n",
      "Gradient Descent(8678/9999): loss=2.5729958269814728, w0=73.23749999999981, w1=13.832520834240931\n",
      "Gradient Descent(8679/9999): loss=2.1775812761327042, w0=73.40624999999982, w1=13.901721109517569\n",
      "Gradient Descent(8680/9999): loss=2.1244192639142856, w0=73.40624999999982, w1=14.031072531804194\n",
      "Gradient Descent(8681/9999): loss=2.1918430274764082, w0=73.29374999999982, w1=13.729028232054885\n",
      "Gradient Descent(8682/9999): loss=1.7958127103123338, w0=73.23749999999981, w1=13.761877371187765\n",
      "Gradient Descent(8683/9999): loss=1.5015888203761825, w0=73.23749999999981, w1=13.671485946054933\n",
      "Gradient Descent(8684/9999): loss=1.9165440157487705, w0=73.23749999999981, w1=13.55282941209067\n",
      "Gradient Descent(8685/9999): loss=1.9007916012247965, w0=73.23749999999981, w1=13.478527734524963\n",
      "Gradient Descent(8686/9999): loss=2.3894022944131734, w0=73.06874999999981, w1=13.423517656365098\n",
      "Gradient Descent(8687/9999): loss=2.0810073984163893, w0=72.95624999999981, w1=13.327909789098797\n",
      "Gradient Descent(8688/9999): loss=2.1533239831193987, w0=72.78749999999981, w1=13.332982617362603\n",
      "Gradient Descent(8689/9999): loss=2.567582804968855, w0=72.7312499999998, w1=13.255346116930523\n",
      "Gradient Descent(8690/9999): loss=2.7890437847831944, w0=72.78749999999981, w1=13.252593506874303\n",
      "Gradient Descent(8691/9999): loss=2.12359167548924, w0=72.78749999999981, w1=13.25461151604337\n",
      "Gradient Descent(8692/9999): loss=2.0167221678311873, w0=72.95624999999981, w1=13.631656537540188\n",
      "Gradient Descent(8693/9999): loss=2.3190978957791164, w0=73.06874999999981, w1=13.474410662575364\n",
      "Gradient Descent(8694/9999): loss=2.3430641698794656, w0=73.12499999999982, w1=13.329394628613477\n",
      "Gradient Descent(8695/9999): loss=2.0375285662727096, w0=73.23749999999981, w1=13.620875187095042\n",
      "Gradient Descent(8696/9999): loss=2.434299231624228, w0=73.12499999999982, w1=13.855018952808503\n",
      "Gradient Descent(8697/9999): loss=2.1713489394305148, w0=72.89999999999982, w1=13.790765800070652\n",
      "Gradient Descent(8698/9999): loss=2.408741947391974, w0=72.89999999999982, w1=13.848956458626846\n",
      "Gradient Descent(8699/9999): loss=2.3640682768660257, w0=72.95624999999983, w1=14.020458646421348\n",
      "Gradient Descent(8700/9999): loss=1.8316071481972427, w0=72.89999999999982, w1=14.172387817468014\n",
      "Gradient Descent(8701/9999): loss=2.452132115581398, w0=72.84374999999982, w1=14.124032470378703\n",
      "Gradient Descent(8702/9999): loss=1.7007564501320769, w0=72.89999999999982, w1=14.260770024188508\n",
      "Gradient Descent(8703/9999): loss=2.155636585835031, w0=72.84374999999982, w1=14.23152257176923\n",
      "Gradient Descent(8704/9999): loss=2.3033606897084944, w0=72.73124999999982, w1=14.155982976767403\n",
      "Gradient Descent(8705/9999): loss=2.4166711142874124, w0=72.84374999999982, w1=13.957898158729803\n",
      "Gradient Descent(8706/9999): loss=1.8849657725112055, w0=73.06874999999981, w1=13.820988225283477\n",
      "Gradient Descent(8707/9999): loss=1.8226860077227072, w0=73.06874999999981, w1=13.908145068550942\n",
      "Gradient Descent(8708/9999): loss=1.7976787666602099, w0=73.06874999999981, w1=13.911241343901787\n",
      "Gradient Descent(8709/9999): loss=1.7469931287141764, w0=73.2937499999998, w1=13.547956388672391\n",
      "Gradient Descent(8710/9999): loss=2.8467196517306195, w0=73.2374999999998, w1=13.679274693689262\n",
      "Gradient Descent(8711/9999): loss=2.2157338073896, w0=73.2374999999998, w1=13.8258387283188\n",
      "Gradient Descent(8712/9999): loss=1.9800795341751258, w0=73.1249999999998, w1=13.578256927330182\n",
      "Gradient Descent(8713/9999): loss=1.6365346570201003, w0=72.8437499999998, w1=13.415154600837424\n",
      "Gradient Descent(8714/9999): loss=2.5236258027576817, w0=72.8437499999998, w1=13.413325886948046\n",
      "Gradient Descent(8715/9999): loss=1.6537119646146308, w0=72.7874999999998, w1=13.700166746118253\n",
      "Gradient Descent(8716/9999): loss=2.540989120984632, w0=72.73124999999979, w1=13.921756610183392\n",
      "Gradient Descent(8717/9999): loss=2.497512118101905, w0=73.0687499999998, w1=13.860683198630538\n",
      "Gradient Descent(8718/9999): loss=1.7404402069892702, w0=73.18124999999979, w1=13.7194262438987\n",
      "Gradient Descent(8719/9999): loss=2.256002985887422, w0=73.18124999999979, w1=13.720334163938997\n",
      "Gradient Descent(8720/9999): loss=2.4358781877157236, w0=73.18124999999979, w1=13.463946105463114\n",
      "Gradient Descent(8721/9999): loss=2.0845826813423356, w0=73.3499999999998, w1=13.237713150944046\n",
      "Gradient Descent(8722/9999): loss=2.2849570472121967, w0=73.2374999999998, w1=13.381712193587175\n",
      "Gradient Descent(8723/9999): loss=2.704878998261071, w0=73.0687499999998, w1=13.545801654964734\n",
      "Gradient Descent(8724/9999): loss=2.1548398201437657, w0=72.7874999999998, w1=13.772388639264516\n",
      "Gradient Descent(8725/9999): loss=2.0151826518395506, w0=72.89999999999979, w1=13.742045061889675\n",
      "Gradient Descent(8726/9999): loss=2.3459747188552758, w0=72.89999999999979, w1=13.919331299223902\n",
      "Gradient Descent(8727/9999): loss=1.7766795578527568, w0=72.6749999999998, w1=13.811674169284386\n",
      "Gradient Descent(8728/9999): loss=2.491796493504939, w0=72.7874999999998, w1=13.731173347140079\n",
      "Gradient Descent(8729/9999): loss=2.345864309887348, w0=72.8437499999998, w1=13.729455123319477\n",
      "Gradient Descent(8730/9999): loss=2.5927315835651004, w0=72.8999999999998, w1=13.679462271605061\n",
      "Gradient Descent(8731/9999): loss=1.7369860402040689, w0=72.7312499999998, w1=13.50214320255895\n",
      "Gradient Descent(8732/9999): loss=2.3273332993005855, w0=73.06874999999981, w1=13.552209110435426\n",
      "Gradient Descent(8733/9999): loss=2.7000153411974788, w0=73.23749999999981, w1=13.725619019910738\n",
      "Gradient Descent(8734/9999): loss=2.360218135999031, w0=73.34999999999981, w1=13.76186524946164\n",
      "Gradient Descent(8735/9999): loss=2.8276632020756205, w0=73.23749999999981, w1=13.956187927465091\n",
      "Gradient Descent(8736/9999): loss=2.2038826040540997, w0=73.06874999999981, w1=14.028017686914188\n",
      "Gradient Descent(8737/9999): loss=2.6251843009305844, w0=73.06874999999981, w1=14.07206508030124\n",
      "Gradient Descent(8738/9999): loss=2.284102210706724, w0=72.8999999999998, w1=14.039541522623676\n",
      "Gradient Descent(8739/9999): loss=2.0395479270950867, w0=72.8437499999998, w1=14.052205748281661\n",
      "Gradient Descent(8740/9999): loss=2.672693707135646, w0=72.7874999999998, w1=13.984986147741663\n",
      "Gradient Descent(8741/9999): loss=2.260971496162317, w0=72.6749999999998, w1=14.18054017987065\n",
      "Gradient Descent(8742/9999): loss=2.861818840716458, w0=72.7312499999998, w1=14.17568443578167\n",
      "Gradient Descent(8743/9999): loss=2.919956197833395, w0=72.8437499999998, w1=14.16837847528002\n",
      "Gradient Descent(8744/9999): loss=2.2216881876241974, w0=72.9562499999998, w1=14.006151702942626\n",
      "Gradient Descent(8745/9999): loss=2.3716884396304083, w0=73.0687499999998, w1=13.623096277440464\n",
      "Gradient Descent(8746/9999): loss=1.8790203901511766, w0=73.3499999999998, w1=13.585181971903177\n",
      "Gradient Descent(8747/9999): loss=2.7897612712974684, w0=73.2374999999998, w1=13.61734352578036\n",
      "Gradient Descent(8748/9999): loss=2.614183375515888, w0=73.2937499999998, w1=13.653998030128792\n",
      "Gradient Descent(8749/9999): loss=2.8167180645659715, w0=73.1812499999998, w1=13.427735234775778\n",
      "Gradient Descent(8750/9999): loss=2.5317036907390014, w0=73.06874999999981, w1=13.470907662328612\n",
      "Gradient Descent(8751/9999): loss=1.489642760885296, w0=73.06874999999981, w1=13.47264084920604\n",
      "Gradient Descent(8752/9999): loss=2.4555440487130844, w0=73.0124999999998, w1=13.404694662899116\n",
      "Gradient Descent(8753/9999): loss=1.7628274638339088, w0=73.06874999999981, w1=13.55340001825662\n",
      "Gradient Descent(8754/9999): loss=2.0072783164705665, w0=72.8999999999998, w1=13.504823197451925\n",
      "Gradient Descent(8755/9999): loss=2.3750804195898154, w0=73.1249999999998, w1=13.591994894956253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(8756/9999): loss=2.082171030818772, w0=73.3499999999998, w1=13.55987631327425\n",
      "Gradient Descent(8757/9999): loss=2.3656615773264114, w0=73.6312499999998, w1=13.540685987945945\n",
      "Gradient Descent(8758/9999): loss=2.095375023480627, w0=73.46249999999979, w1=13.785580910691053\n",
      "Gradient Descent(8759/9999): loss=2.332426750510122, w0=73.40624999999979, w1=13.595379715822377\n",
      "Gradient Descent(8760/9999): loss=2.09520577414584, w0=73.46249999999979, w1=13.805346824070718\n",
      "Gradient Descent(8761/9999): loss=1.740993870626014, w0=73.57499999999979, w1=13.902288370000173\n",
      "Gradient Descent(8762/9999): loss=2.494052494834103, w0=73.6312499999998, w1=13.9375150888879\n",
      "Gradient Descent(8763/9999): loss=2.3588449082057217, w0=73.6312499999998, w1=13.792259229411702\n",
      "Gradient Descent(8764/9999): loss=2.2901767868113954, w0=73.5187499999998, w1=13.880769199624995\n",
      "Gradient Descent(8765/9999): loss=2.1816212410631737, w0=73.5749999999998, w1=13.704095237519718\n",
      "Gradient Descent(8766/9999): loss=2.6262511246433076, w0=73.5749999999998, w1=13.462548924268505\n",
      "Gradient Descent(8767/9999): loss=1.9353137092629307, w0=73.5749999999998, w1=13.522680104045246\n",
      "Gradient Descent(8768/9999): loss=2.3314659963389097, w0=73.5187499999998, w1=13.78699429046128\n",
      "Gradient Descent(8769/9999): loss=2.133077865022026, w0=73.74374999999979, w1=13.775530201025086\n",
      "Gradient Descent(8770/9999): loss=2.7766938408793935, w0=73.68749999999979, w1=13.641559903892704\n",
      "Gradient Descent(8771/9999): loss=2.326090520840298, w0=73.63124999999978, w1=13.82296775463489\n",
      "Gradient Descent(8772/9999): loss=2.4544939809354585, w0=73.46249999999978, w1=14.1311749213\n",
      "Gradient Descent(8773/9999): loss=2.3999126526983448, w0=73.40624999999977, w1=13.939734044960183\n",
      "Gradient Descent(8774/9999): loss=2.405477602621156, w0=73.18124999999978, w1=13.950529834061753\n",
      "Gradient Descent(8775/9999): loss=2.238168571535783, w0=73.29374999999978, w1=14.112286540030121\n",
      "Gradient Descent(8776/9999): loss=1.9923414140657387, w0=73.29374999999978, w1=14.08745944975772\n",
      "Gradient Descent(8777/9999): loss=2.3305939300218643, w0=73.40624999999977, w1=14.068188434276278\n",
      "Gradient Descent(8778/9999): loss=2.1793523637498167, w0=73.12499999999977, w1=13.829997510542615\n",
      "Gradient Descent(8779/9999): loss=2.49420415033549, w0=73.18124999999978, w1=13.83453114864628\n",
      "Gradient Descent(8780/9999): loss=2.536585259118418, w0=72.95624999999978, w1=13.8320157678319\n",
      "Gradient Descent(8781/9999): loss=1.9244522823153831, w0=73.01249999999979, w1=13.913822372527624\n",
      "Gradient Descent(8782/9999): loss=1.8086585368420722, w0=72.95624999999978, w1=13.887045373851665\n",
      "Gradient Descent(8783/9999): loss=1.9367591934186301, w0=73.01249999999979, w1=13.904289989426449\n",
      "Gradient Descent(8784/9999): loss=1.894935501054469, w0=73.0687499999998, w1=14.17835661976711\n",
      "Gradient Descent(8785/9999): loss=2.568675478419568, w0=73.3499999999998, w1=13.944221552757227\n",
      "Gradient Descent(8786/9999): loss=2.080908175916232, w0=73.4062499999998, w1=13.950828251098832\n",
      "Gradient Descent(8787/9999): loss=2.5654308998521484, w0=73.5187499999998, w1=13.819135095566304\n",
      "Gradient Descent(8788/9999): loss=2.7244815855132556, w0=73.46249999999979, w1=14.087193495536932\n",
      "Gradient Descent(8789/9999): loss=2.4776261088966014, w0=73.3499999999998, w1=14.015628351173392\n",
      "Gradient Descent(8790/9999): loss=1.8199570562548046, w0=73.6874999999998, w1=13.731408580084576\n",
      "Gradient Descent(8791/9999): loss=2.3024665727817157, w0=73.6874999999998, w1=13.631353515454373\n",
      "Gradient Descent(8792/9999): loss=2.3685165067940144, w0=73.6874999999998, w1=13.630617328159875\n",
      "Gradient Descent(8793/9999): loss=1.7359693574387836, w0=73.3499999999998, w1=13.661066704506446\n",
      "Gradient Descent(8794/9999): loss=2.5412185141035017, w0=73.4062499999998, w1=13.414073155254144\n",
      "Gradient Descent(8795/9999): loss=2.1992923326722407, w0=73.2937499999998, w1=13.318897316335905\n",
      "Gradient Descent(8796/9999): loss=2.5815806090880606, w0=72.9562499999998, w1=13.495626394322278\n",
      "Gradient Descent(8797/9999): loss=1.8283247863802807, w0=72.9562499999998, w1=13.554313665030755\n",
      "Gradient Descent(8798/9999): loss=2.4924446604199404, w0=73.1249999999998, w1=13.491981291170966\n",
      "Gradient Descent(8799/9999): loss=2.3676328641755697, w0=73.1249999999998, w1=13.209175879123299\n",
      "Gradient Descent(8800/9999): loss=2.4928918617940217, w0=72.8999999999998, w1=13.184182981829135\n",
      "Gradient Descent(8801/9999): loss=2.626495423568206, w0=72.8437499999998, w1=13.020002111017345\n",
      "Gradient Descent(8802/9999): loss=2.0808203335359456, w0=72.7874999999998, w1=13.146779643618672\n",
      "Gradient Descent(8803/9999): loss=2.593900328275468, w0=73.01249999999979, w1=13.016254367753136\n",
      "Gradient Descent(8804/9999): loss=2.2153203771716026, w0=73.18124999999979, w1=13.110492397135491\n",
      "Gradient Descent(8805/9999): loss=1.9174570513338043, w0=73.18124999999979, w1=13.340326871051905\n",
      "Gradient Descent(8806/9999): loss=2.1991367736205456, w0=73.18124999999979, w1=13.407687071734646\n",
      "Gradient Descent(8807/9999): loss=2.1931486078725855, w0=73.2374999999998, w1=13.668923215705265\n",
      "Gradient Descent(8808/9999): loss=2.1827052408928664, w0=73.18124999999979, w1=13.87436501501853\n",
      "Gradient Descent(8809/9999): loss=2.427138881271728, w0=73.2374999999998, w1=13.915311266326134\n",
      "Gradient Descent(8810/9999): loss=2.231573241296753, w0=73.0687499999998, w1=14.119968826357638\n",
      "Gradient Descent(8811/9999): loss=1.9969759135875633, w0=73.18124999999979, w1=13.926801757769667\n",
      "Gradient Descent(8812/9999): loss=2.402506702471217, w0=73.0687499999998, w1=13.93978028302253\n",
      "Gradient Descent(8813/9999): loss=1.8998663263350641, w0=72.9562499999998, w1=13.752115051289389\n",
      "Gradient Descent(8814/9999): loss=2.4154943615081157, w0=72.7312499999998, w1=13.701087497390894\n",
      "Gradient Descent(8815/9999): loss=1.2225883204640402, w0=73.2374999999998, w1=13.663199609946368\n",
      "Gradient Descent(8816/9999): loss=1.849159120073302, w0=73.18124999999979, w1=13.544623373734455\n",
      "Gradient Descent(8817/9999): loss=3.064890347886851, w0=73.5187499999998, w1=13.280108919168667\n",
      "Gradient Descent(8818/9999): loss=2.278195545837888, w0=73.4062499999998, w1=13.332629206228617\n",
      "Gradient Descent(8819/9999): loss=2.360196326172545, w0=73.0687499999998, w1=13.479868580191946\n",
      "Gradient Descent(8820/9999): loss=1.9886200006856, w0=73.01249999999979, w1=13.920945337740337\n",
      "Gradient Descent(8821/9999): loss=2.4206496922933187, w0=72.95624999999978, w1=14.083272625920578\n",
      "Gradient Descent(8822/9999): loss=2.0540376611178495, w0=72.78749999999978, w1=13.925379116767141\n",
      "Gradient Descent(8823/9999): loss=2.3651875181501234, w0=72.84374999999979, w1=13.810585546107886\n",
      "Gradient Descent(8824/9999): loss=1.7141816706182618, w0=72.50624999999978, w1=13.800548075545354\n",
      "Gradient Descent(8825/9999): loss=2.140195441164745, w0=72.78749999999978, w1=13.703324278074748\n",
      "Gradient Descent(8826/9999): loss=1.928451130650445, w0=72.84374999999979, w1=13.25158572694057\n",
      "Gradient Descent(8827/9999): loss=2.3718264590401157, w0=73.29374999999979, w1=13.27237094459238\n",
      "Gradient Descent(8828/9999): loss=2.2813804491284815, w0=73.23749999999978, w1=13.477253772972357\n",
      "Gradient Descent(8829/9999): loss=2.22182842171587, w0=73.23749999999978, w1=13.369880806984094\n",
      "Gradient Descent(8830/9999): loss=1.8597375066726514, w0=73.01249999999979, w1=13.488772435138861\n",
      "Gradient Descent(8831/9999): loss=2.237415953633236, w0=72.95624999999978, w1=13.380969375165192\n",
      "Gradient Descent(8832/9999): loss=1.9833693304537074, w0=72.89999999999978, w1=13.432359686259588\n",
      "Gradient Descent(8833/9999): loss=2.160891575777656, w0=72.84374999999977, w1=13.483610299999128\n",
      "Gradient Descent(8834/9999): loss=2.479287011994658, w0=72.89999999999978, w1=13.584824379237359\n",
      "Gradient Descent(8835/9999): loss=2.1705139853355258, w0=72.89999999999978, w1=13.558083536895964\n",
      "Gradient Descent(8836/9999): loss=1.989224319898357, w0=72.73124999999978, w1=13.293523846110123\n",
      "Gradient Descent(8837/9999): loss=2.5031029665738216, w0=73.23749999999977, w1=13.039578982718679\n",
      "Gradient Descent(8838/9999): loss=2.0818308426951324, w0=73.23749999999977, w1=13.057399498354892\n",
      "Gradient Descent(8839/9999): loss=2.660499710248893, w0=73.46249999999976, w1=13.035209434458453\n",
      "Gradient Descent(8840/9999): loss=2.001680864873247, w0=73.34999999999977, w1=12.864062704241142\n",
      "Gradient Descent(8841/9999): loss=1.8966926766778314, w0=73.29374999999976, w1=12.585787742869634\n",
      "Gradient Descent(8842/9999): loss=2.1982488876913666, w0=73.06874999999977, w1=12.369722343500626\n",
      "Gradient Descent(8843/9999): loss=2.4579215430472567, w0=72.95624999999977, w1=12.549267421503481\n",
      "Gradient Descent(8844/9999): loss=2.313981771726401, w0=72.95624999999977, w1=12.645749944146896\n",
      "Gradient Descent(8845/9999): loss=2.5115143491286935, w0=72.67499999999977, w1=12.65718245749443\n",
      "Gradient Descent(8846/9999): loss=2.4830190367495146, w0=72.67499999999977, w1=12.93843037021448\n",
      "Gradient Descent(8847/9999): loss=2.2311961138805727, w0=72.73124999999978, w1=13.19870088451078\n",
      "Gradient Descent(8848/9999): loss=2.210794854381046, w0=72.84374999999977, w1=13.19614881542421\n",
      "Gradient Descent(8849/9999): loss=1.9651926046825356, w0=73.01249999999978, w1=13.118528472191896\n",
      "Gradient Descent(8850/9999): loss=2.082047696628223, w0=73.01249999999978, w1=13.291615804552029\n",
      "Gradient Descent(8851/9999): loss=2.3598773194648723, w0=73.34999999999978, w1=13.359470894481955\n",
      "Gradient Descent(8852/9999): loss=2.2574904784595864, w0=73.18124999999978, w1=13.175897097324054\n",
      "Gradient Descent(8853/9999): loss=2.5654082065832506, w0=73.29374999999978, w1=13.466749695721749\n",
      "Gradient Descent(8854/9999): loss=1.7104640856504867, w0=73.51874999999977, w1=13.623026178063908\n",
      "Gradient Descent(8855/9999): loss=1.6692538155663517, w0=73.74374999999976, w1=13.676895098506455\n",
      "Gradient Descent(8856/9999): loss=1.9548408883980972, w0=73.51874999999977, w1=13.940197548933922\n",
      "Gradient Descent(8857/9999): loss=2.228823150578952, w0=73.51874999999977, w1=13.880003926666696\n",
      "Gradient Descent(8858/9999): loss=1.9016510086503657, w0=73.34999999999977, w1=13.973491889573172\n",
      "Gradient Descent(8859/9999): loss=2.302469537821014, w0=73.51874999999977, w1=13.994772918218542\n",
      "Gradient Descent(8860/9999): loss=1.8963787010131432, w0=73.51874999999977, w1=14.059854343909393\n",
      "Gradient Descent(8861/9999): loss=2.4441663285839064, w0=73.74374999999976, w1=13.888805499543134\n",
      "Gradient Descent(8862/9999): loss=1.8693279751523517, w0=73.51874999999977, w1=13.86422058330953\n",
      "Gradient Descent(8863/9999): loss=2.6598478789623154, w0=73.12499999999977, w1=13.435840464513232\n",
      "Gradient Descent(8864/9999): loss=1.8630157933423985, w0=72.95624999999977, w1=13.605101033911158\n",
      "Gradient Descent(8865/9999): loss=2.3703806024512764, w0=72.73124999999978, w1=13.633199138734849\n",
      "Gradient Descent(8866/9999): loss=2.744381408974558, w0=73.01249999999978, w1=13.558896099502023\n",
      "Gradient Descent(8867/9999): loss=2.5507720379801215, w0=73.06874999999978, w1=13.690397429774915\n",
      "Gradient Descent(8868/9999): loss=2.0939546546188774, w0=73.18124999999978, w1=13.715560848697605\n",
      "Gradient Descent(8869/9999): loss=2.0555714578603963, w0=73.23749999999978, w1=13.874623691085878\n",
      "Gradient Descent(8870/9999): loss=2.5855907786908108, w0=73.51874999999978, w1=13.716793499316632\n",
      "Gradient Descent(8871/9999): loss=2.540923743351355, w0=73.68749999999979, w1=13.624083028474601\n",
      "Gradient Descent(8872/9999): loss=2.4341824286971625, w0=73.68749999999979, w1=13.423975293280739\n",
      "Gradient Descent(8873/9999): loss=2.173601284750302, w0=73.79999999999978, w1=13.515357810372224\n",
      "Gradient Descent(8874/9999): loss=2.6601620903361543, w0=73.40624999999979, w1=13.495492942159814\n",
      "Gradient Descent(8875/9999): loss=2.531450005647849, w0=73.29374999999979, w1=13.503664794049566\n",
      "Gradient Descent(8876/9999): loss=2.3714062579418527, w0=73.29374999999979, w1=13.613529710272697\n",
      "Gradient Descent(8877/9999): loss=2.2693769833780673, w0=73.0687499999998, w1=13.666832965253157\n",
      "Gradient Descent(8878/9999): loss=2.0092717597529837, w0=73.18124999999979, w1=13.3595940013856\n",
      "Gradient Descent(8879/9999): loss=2.647727539306369, w0=73.18124999999979, w1=13.487097747928166\n",
      "Gradient Descent(8880/9999): loss=1.8879747225579346, w0=73.12499999999979, w1=13.328396833450524\n",
      "Gradient Descent(8881/9999): loss=1.8719106363189986, w0=73.06874999999978, w1=13.361365794636034\n",
      "Gradient Descent(8882/9999): loss=2.017567563451051, w0=73.12499999999979, w1=13.468133959088377\n",
      "Gradient Descent(8883/9999): loss=1.931536160276902, w0=73.29374999999979, w1=13.456845923728393\n",
      "Gradient Descent(8884/9999): loss=1.8986240954131037, w0=73.57499999999979, w1=13.501759032134109\n",
      "Gradient Descent(8885/9999): loss=2.5017501314464488, w0=73.51874999999978, w1=13.447215389113898\n",
      "Gradient Descent(8886/9999): loss=2.3189512235059717, w0=73.51874999999978, w1=13.15064945206411\n",
      "Gradient Descent(8887/9999): loss=2.598305143378415, w0=73.46249999999978, w1=13.200569043379826\n",
      "Gradient Descent(8888/9999): loss=2.3895594885458156, w0=73.51874999999978, w1=13.31253921530954\n",
      "Gradient Descent(8889/9999): loss=2.1682538972814416, w0=73.51874999999978, w1=13.474204483892692\n",
      "Gradient Descent(8890/9999): loss=2.140272675191711, w0=73.74374999999978, w1=13.365247946269202\n",
      "Gradient Descent(8891/9999): loss=2.0603764253181622, w0=73.74374999999978, w1=13.615202924514323\n",
      "Gradient Descent(8892/9999): loss=2.6232703711224072, w0=73.85624999999978, w1=13.345307402555354\n",
      "Gradient Descent(8893/9999): loss=2.1020526585728203, w0=73.96874999999977, w1=13.753571919850598\n",
      "Gradient Descent(8894/9999): loss=1.906820796966442, w0=73.74374999999978, w1=13.704102605546064\n",
      "Gradient Descent(8895/9999): loss=1.9138382945030643, w0=73.68749999999977, w1=13.625428149472164\n",
      "Gradient Descent(8896/9999): loss=2.0111375545620023, w0=73.74374999999978, w1=13.45634855707829\n",
      "Gradient Descent(8897/9999): loss=2.560973356436091, w0=73.57499999999978, w1=13.40055520589017\n",
      "Gradient Descent(8898/9999): loss=2.6239968384798003, w0=73.85624999999978, w1=13.284448044147503\n",
      "Gradient Descent(8899/9999): loss=2.119443970133097, w0=73.57499999999978, w1=13.249205489450025\n",
      "Gradient Descent(8900/9999): loss=2.256400912994247, w0=73.18124999999978, w1=13.392519141030492\n",
      "Gradient Descent(8901/9999): loss=2.0748156812198575, w0=73.18124999999978, w1=13.093088545052057\n",
      "Gradient Descent(8902/9999): loss=2.2832559981440186, w0=73.12499999999977, w1=13.168077504551512\n",
      "Gradient Descent(8903/9999): loss=2.1728219852803057, w0=73.01249999999978, w1=13.121746815041638\n",
      "Gradient Descent(8904/9999): loss=2.5006861553979065, w0=73.18124999999978, w1=12.967534708578077\n",
      "Gradient Descent(8905/9999): loss=2.624246761909576, w0=73.12499999999977, w1=13.236350327735313\n",
      "Gradient Descent(8906/9999): loss=2.304475023564766, w0=73.06874999999977, w1=12.962714150252157\n",
      "Gradient Descent(8907/9999): loss=1.771382093540193, w0=73.29374999999976, w1=13.23664462257333\n",
      "Gradient Descent(8908/9999): loss=2.532250037364707, w0=73.23749999999976, w1=13.245906037491805\n",
      "Gradient Descent(8909/9999): loss=2.159303313782776, w0=73.06874999999975, w1=13.267045025098211\n",
      "Gradient Descent(8910/9999): loss=1.8342435242040387, w0=73.06874999999975, w1=13.210791550635559\n",
      "Gradient Descent(8911/9999): loss=2.304721273369463, w0=73.06874999999975, w1=13.233702088444447\n",
      "Gradient Descent(8912/9999): loss=2.4698625337266997, w0=72.89999999999975, w1=13.177237427903954\n",
      "Gradient Descent(8913/9999): loss=2.2912219588986034, w0=72.95624999999976, w1=13.388344767357513\n",
      "Gradient Descent(8914/9999): loss=2.6538469708334134, w0=72.89999999999975, w1=13.279570189946366\n",
      "Gradient Descent(8915/9999): loss=2.669738106814287, w0=72.89999999999975, w1=13.16182060361074\n",
      "Gradient Descent(8916/9999): loss=2.6663584155087294, w0=73.23749999999976, w1=12.981067512196216\n",
      "Gradient Descent(8917/9999): loss=2.57997783013763, w0=73.29374999999976, w1=13.027786917257073\n",
      "Gradient Descent(8918/9999): loss=2.3593644867346506, w0=73.23749999999976, w1=12.945165555706263\n",
      "Gradient Descent(8919/9999): loss=2.642031581410298, w0=73.01249999999976, w1=12.97844410722051\n",
      "Gradient Descent(8920/9999): loss=2.554911003162622, w0=73.06874999999977, w1=13.16850357692749\n",
      "Gradient Descent(8921/9999): loss=2.291497347873016, w0=72.95624999999977, w1=13.273090684933123\n",
      "Gradient Descent(8922/9999): loss=2.0157006536833317, w0=73.06874999999977, w1=13.133400826065474\n",
      "Gradient Descent(8923/9999): loss=2.312748299054981, w0=73.06874999999977, w1=13.303185541496005\n",
      "Gradient Descent(8924/9999): loss=1.973096532332591, w0=72.89999999999976, w1=13.17582820963661\n",
      "Gradient Descent(8925/9999): loss=2.0979477885319, w0=72.95624999999977, w1=13.092663033825843\n",
      "Gradient Descent(8926/9999): loss=2.2233150753011133, w0=73.06874999999977, w1=13.194672120348786\n",
      "Gradient Descent(8927/9999): loss=2.126277382564744, w0=73.40624999999977, w1=13.15015156707114\n",
      "Gradient Descent(8928/9999): loss=2.773495420938643, w0=73.29374999999978, w1=13.34919971870404\n",
      "Gradient Descent(8929/9999): loss=2.4986493166040415, w0=73.29374999999978, w1=13.422256982181747\n",
      "Gradient Descent(8930/9999): loss=2.447014244988981, w0=73.51874999999977, w1=13.257063953466208\n",
      "Gradient Descent(8931/9999): loss=2.105224888499143, w0=73.46249999999976, w1=13.062243781258505\n",
      "Gradient Descent(8932/9999): loss=1.9459089431539789, w0=73.18124999999976, w1=12.852865493500483\n",
      "Gradient Descent(8933/9999): loss=2.23293300562218, w0=73.01249999999976, w1=12.993821893750791\n",
      "Gradient Descent(8934/9999): loss=2.2010746524316587, w0=73.18124999999976, w1=13.183398027449387\n",
      "Gradient Descent(8935/9999): loss=1.9988229369480273, w0=73.12499999999976, w1=13.012622674198312\n",
      "Gradient Descent(8936/9999): loss=1.8843586551051095, w0=73.23749999999976, w1=13.197121563570708\n",
      "Gradient Descent(8937/9999): loss=1.7456370673130293, w0=73.06874999999975, w1=13.18595927152395\n",
      "Gradient Descent(8938/9999): loss=2.5546108950686715, w0=73.18124999999975, w1=12.86786609077593\n",
      "Gradient Descent(8939/9999): loss=2.960855105205358, w0=73.23749999999976, w1=13.006464273125001\n",
      "Gradient Descent(8940/9999): loss=2.186663952321755, w0=73.23749999999976, w1=12.89666120888548\n",
      "Gradient Descent(8941/9999): loss=2.1316633783675227, w0=73.12499999999976, w1=13.029818159977026\n",
      "Gradient Descent(8942/9999): loss=2.0031401358968473, w0=73.01249999999976, w1=13.0799059496975\n",
      "Gradient Descent(8943/9999): loss=2.066174585180078, w0=73.23749999999976, w1=13.438711725296924\n",
      "Gradient Descent(8944/9999): loss=1.6739204061581794, w0=72.95624999999976, w1=13.394976816456092\n",
      "Gradient Descent(8945/9999): loss=2.5114640734956577, w0=73.23749999999976, w1=13.330280086462578\n",
      "Gradient Descent(8946/9999): loss=1.790609448620757, w0=73.40624999999976, w1=13.351062352980016\n",
      "Gradient Descent(8947/9999): loss=2.0611679682675197, w0=73.51874999999976, w1=13.600638872969762\n",
      "Gradient Descent(8948/9999): loss=2.0298391027639813, w0=73.57499999999976, w1=13.618778583962948\n",
      "Gradient Descent(8949/9999): loss=1.7966071141801832, w0=73.23749999999976, w1=13.408433342298592\n",
      "Gradient Descent(8950/9999): loss=1.993621106459206, w0=73.18124999999975, w1=13.501903407197773\n",
      "Gradient Descent(8951/9999): loss=2.2973681921672933, w0=73.01249999999975, w1=13.264958544516821\n",
      "Gradient Descent(8952/9999): loss=1.9889055292358078, w0=72.89999999999975, w1=13.122809006091181\n",
      "Gradient Descent(8953/9999): loss=1.9836805318117576, w0=72.89999999999975, w1=13.40653294829354\n",
      "Gradient Descent(8954/9999): loss=1.8730992692358304, w0=73.06874999999975, w1=13.466749551108027\n",
      "Gradient Descent(8955/9999): loss=2.3263275929591343, w0=73.12499999999976, w1=13.575141925363107\n",
      "Gradient Descent(8956/9999): loss=2.4838051758312636, w0=73.29374999999976, w1=13.618481253590305\n",
      "Gradient Descent(8957/9999): loss=2.13345116360263, w0=73.18124999999976, w1=13.693698224899634\n",
      "Gradient Descent(8958/9999): loss=2.20878758877393, w0=73.18124999999976, w1=13.756690193793434\n",
      "Gradient Descent(8959/9999): loss=1.7748469999412877, w0=73.51874999999977, w1=13.642879545042817\n",
      "Gradient Descent(8960/9999): loss=2.396598684678902, w0=73.79999999999977, w1=13.73907475890765\n",
      "Gradient Descent(8961/9999): loss=1.8954415063403616, w0=73.68749999999977, w1=13.613781794297568\n",
      "Gradient Descent(8962/9999): loss=1.8368115104094302, w0=73.63124999999977, w1=13.667997481792222\n",
      "Gradient Descent(8963/9999): loss=2.0700565629922063, w0=73.46249999999976, w1=13.38617256230313\n",
      "Gradient Descent(8964/9999): loss=2.153158252495995, w0=73.40624999999976, w1=13.570383121792757\n",
      "Gradient Descent(8965/9999): loss=2.5041735228111417, w0=73.06874999999975, w1=13.35365729607936\n",
      "Gradient Descent(8966/9999): loss=1.5491810220805924, w0=72.73124999999975, w1=13.63942297909399\n",
      "Gradient Descent(8967/9999): loss=2.395236939145988, w0=72.78749999999975, w1=13.306693198908233\n",
      "Gradient Descent(8968/9999): loss=1.9495204004571174, w0=72.78749999999975, w1=13.308505446940009\n",
      "Gradient Descent(8969/9999): loss=2.224300995774123, w0=72.84374999999976, w1=13.34339389573925\n",
      "Gradient Descent(8970/9999): loss=1.8815314547481228, w0=72.73124999999976, w1=13.235185983471247\n",
      "Gradient Descent(8971/9999): loss=2.2025698870707493, w0=72.84374999999976, w1=13.3181254456596\n",
      "Gradient Descent(8972/9999): loss=2.4031554601411567, w0=72.89999999999976, w1=13.28149698321532\n",
      "Gradient Descent(8973/9999): loss=1.9463509608644352, w0=73.06874999999977, w1=13.32138248736269\n",
      "Gradient Descent(8974/9999): loss=2.460890630201397, w0=73.40624999999977, w1=13.181411714445714\n",
      "Gradient Descent(8975/9999): loss=2.022809407955737, w0=73.79999999999977, w1=12.877550528558825\n",
      "Gradient Descent(8976/9999): loss=2.319074297688685, w0=73.63124999999977, w1=13.16226925046168\n",
      "Gradient Descent(8977/9999): loss=2.0209338165553072, w0=73.63124999999977, w1=13.112432200611877\n",
      "Gradient Descent(8978/9999): loss=1.7964902477796356, w0=73.29374999999976, w1=13.233445008800052\n",
      "Gradient Descent(8979/9999): loss=1.8617919152465539, w0=73.12499999999976, w1=13.15377697001145\n",
      "Gradient Descent(8980/9999): loss=2.32269808646872, w0=73.34999999999975, w1=13.196144177819114\n",
      "Gradient Descent(8981/9999): loss=2.2467138986280535, w0=73.18124999999975, w1=13.426540416015776\n",
      "Gradient Descent(8982/9999): loss=1.9365822735788716, w0=73.12499999999974, w1=13.667045917772079\n",
      "Gradient Descent(8983/9999): loss=2.0694277541726676, w0=73.06874999999974, w1=13.405168305107175\n",
      "Gradient Descent(8984/9999): loss=2.482457080781937, w0=73.12499999999974, w1=13.366421570141389\n",
      "Gradient Descent(8985/9999): loss=1.8666706751216964, w0=73.23749999999974, w1=13.27286305320083\n",
      "Gradient Descent(8986/9999): loss=2.4651978993605237, w0=73.23749999999974, w1=13.371378283626822\n",
      "Gradient Descent(8987/9999): loss=1.4971154926090071, w0=73.01249999999975, w1=13.495287711192324\n",
      "Gradient Descent(8988/9999): loss=2.202798939610458, w0=72.95624999999974, w1=13.776277979715871\n",
      "Gradient Descent(8989/9999): loss=2.3253627898682963, w0=72.73124999999975, w1=13.43515699185775\n",
      "Gradient Descent(8990/9999): loss=1.8270573575318525, w0=72.78749999999975, w1=13.497226093753813\n",
      "Gradient Descent(8991/9999): loss=2.094729585016013, w0=72.78749999999975, w1=13.48991559497026\n",
      "Gradient Descent(8992/9999): loss=2.242038329685789, w0=72.78749999999975, w1=13.185097506314875\n",
      "Gradient Descent(8993/9999): loss=2.566947744005902, w0=72.73124999999975, w1=13.163064985335243\n",
      "Gradient Descent(8994/9999): loss=1.5308151279761824, w0=72.78749999999975, w1=13.29841783093839\n",
      "Gradient Descent(8995/9999): loss=2.7287355222992042, w0=72.78749999999975, w1=13.548551662046256\n",
      "Gradient Descent(8996/9999): loss=2.176765936791827, w0=72.89999999999975, w1=13.660384720588292\n",
      "Gradient Descent(8997/9999): loss=1.9671111551632894, w0=72.89999999999975, w1=13.528958189567893\n",
      "Gradient Descent(8998/9999): loss=1.582718102548374, w0=73.01249999999975, w1=13.322323392771848\n",
      "Gradient Descent(8999/9999): loss=2.894782929095525, w0=73.06874999999975, w1=13.4651637823733\n",
      "Gradient Descent(9000/9999): loss=2.2677669540923415, w0=73.29374999999975, w1=13.322054525560734\n",
      "Gradient Descent(9001/9999): loss=2.068176921922123, w0=73.06874999999975, w1=13.418708687146593\n",
      "Gradient Descent(9002/9999): loss=2.8826818874919615, w0=73.06874999999975, w1=13.662228018489449\n",
      "Gradient Descent(9003/9999): loss=2.1784581330870783, w0=73.23749999999976, w1=13.4987400804728\n",
      "Gradient Descent(9004/9999): loss=2.324530508583174, w0=73.23749999999976, w1=13.597729938516542\n",
      "Gradient Descent(9005/9999): loss=2.902309167927152, w0=73.12499999999976, w1=13.837538999066654\n",
      "Gradient Descent(9006/9999): loss=3.3740461426179915, w0=72.89999999999976, w1=13.847530099503812\n",
      "Gradient Descent(9007/9999): loss=2.2157774280313425, w0=73.18124999999976, w1=13.722392508227639\n",
      "Gradient Descent(9008/9999): loss=2.5599051970694933, w0=73.12499999999976, w1=13.587360873354124\n",
      "Gradient Descent(9009/9999): loss=2.123636446850455, w0=73.12499999999976, w1=13.671878477416385\n",
      "Gradient Descent(9010/9999): loss=2.033556036109773, w0=72.95624999999976, w1=13.593233995207243\n",
      "Gradient Descent(9011/9999): loss=2.33085767511955, w0=72.84374999999976, w1=13.768518261677064\n",
      "Gradient Descent(9012/9999): loss=2.6190040444912412, w0=73.01249999999976, w1=13.751360751646672\n",
      "Gradient Descent(9013/9999): loss=2.2698267208236573, w0=72.84374999999976, w1=13.929025891503052\n",
      "Gradient Descent(9014/9999): loss=2.3795807198656096, w0=72.73124999999976, w1=13.717343517468395\n",
      "Gradient Descent(9015/9999): loss=1.7373027332426418, w0=72.73124999999976, w1=13.671945967747593\n",
      "Gradient Descent(9016/9999): loss=2.1996868151815043, w0=72.67499999999976, w1=13.677618438742892\n",
      "Gradient Descent(9017/9999): loss=1.6673356962133954, w0=73.01249999999976, w1=13.51658200109046\n",
      "Gradient Descent(9018/9999): loss=2.382295739958825, w0=73.23749999999976, w1=13.462833160100772\n",
      "Gradient Descent(9019/9999): loss=1.726156908901497, w0=73.18124999999975, w1=13.444724028595697\n",
      "Gradient Descent(9020/9999): loss=2.1548422248149954, w0=73.12499999999974, w1=13.36458173575962\n",
      "Gradient Descent(9021/9999): loss=2.1459182292307126, w0=73.29374999999975, w1=13.221563680607874\n",
      "Gradient Descent(9022/9999): loss=2.937499340472079, w0=73.29374999999975, w1=13.316223979835254\n",
      "Gradient Descent(9023/9999): loss=2.0852238939375467, w0=73.29374999999975, w1=13.198247151091241\n",
      "Gradient Descent(9024/9999): loss=2.151818582403668, w0=73.63124999999975, w1=13.418429550809078\n",
      "Gradient Descent(9025/9999): loss=2.1696109245971313, w0=73.85624999999975, w1=13.24332108449108\n",
      "Gradient Descent(9026/9999): loss=2.587996039881875, w0=73.63124999999975, w1=13.630926051862524\n",
      "Gradient Descent(9027/9999): loss=2.3774220485656077, w0=73.74374999999975, w1=13.47442960849866\n",
      "Gradient Descent(9028/9999): loss=2.141654831899757, w0=73.79999999999976, w1=13.32767339872652\n",
      "Gradient Descent(9029/9999): loss=2.1736163624850398, w0=73.63124999999975, w1=13.214391060735172\n",
      "Gradient Descent(9030/9999): loss=2.1248823971494692, w0=73.51874999999976, w1=13.47825639229707\n",
      "Gradient Descent(9031/9999): loss=1.9522985719188906, w0=73.51874999999976, w1=13.625237445734323\n",
      "Gradient Descent(9032/9999): loss=2.57579543518107, w0=73.57499999999976, w1=13.585142061411206\n",
      "Gradient Descent(9033/9999): loss=1.3920759744997055, w0=73.12499999999976, w1=13.599457967078575\n",
      "Gradient Descent(9034/9999): loss=1.7764577345611998, w0=73.12499999999976, w1=13.452363491978984\n",
      "Gradient Descent(9035/9999): loss=2.303693275468832, w0=73.18124999999976, w1=13.385472613070975\n",
      "Gradient Descent(9036/9999): loss=2.5283039995741188, w0=73.06874999999977, w1=13.503326190965078\n",
      "Gradient Descent(9037/9999): loss=2.3172188421431574, w0=73.06874999999977, w1=13.621462339822614\n",
      "Gradient Descent(9038/9999): loss=1.916051305293554, w0=73.23749999999977, w1=13.446592947930423\n",
      "Gradient Descent(9039/9999): loss=2.7810566601556674, w0=73.46249999999976, w1=13.393970678520622\n",
      "Gradient Descent(9040/9999): loss=1.6826394000188163, w0=73.12499999999976, w1=13.371461289994249\n",
      "Gradient Descent(9041/9999): loss=2.3939852626877824, w0=73.12499999999976, w1=13.000758780251429\n",
      "Gradient Descent(9042/9999): loss=2.580900995020187, w0=73.06874999999975, w1=13.164934187739053\n",
      "Gradient Descent(9043/9999): loss=2.407379951042956, w0=73.12499999999976, w1=13.418878929594442\n",
      "Gradient Descent(9044/9999): loss=2.5435082555622346, w0=72.95624999999976, w1=13.444641257124307\n",
      "Gradient Descent(9045/9999): loss=1.810322605424232, w0=72.84374999999976, w1=13.5395832911647\n",
      "Gradient Descent(9046/9999): loss=2.2554832741322515, w0=72.67499999999976, w1=13.831794385053975\n",
      "Gradient Descent(9047/9999): loss=2.1095903017188773, w0=72.67499999999976, w1=14.04822155774163\n",
      "Gradient Descent(9048/9999): loss=2.0168631906160703, w0=72.56249999999976, w1=13.925577731459805\n",
      "Gradient Descent(9049/9999): loss=2.257925966347913, w0=72.61874999999976, w1=13.976902392150187\n",
      "Gradient Descent(9050/9999): loss=1.8205902951442114, w0=72.95624999999977, w1=13.868283077172363\n",
      "Gradient Descent(9051/9999): loss=2.8191854094722, w0=72.78749999999977, w1=13.82550810529054\n",
      "Gradient Descent(9052/9999): loss=2.5264974065001518, w0=72.73124999999976, w1=13.822385628560617\n",
      "Gradient Descent(9053/9999): loss=2.423007982275669, w0=72.78749999999977, w1=13.961466447943238\n",
      "Gradient Descent(9054/9999): loss=2.6379967169091523, w0=72.78749999999977, w1=13.945347585221656\n",
      "Gradient Descent(9055/9999): loss=2.5551744268058676, w0=72.89999999999976, w1=14.17010038493051\n",
      "Gradient Descent(9056/9999): loss=3.0859975816665357, w0=73.18124999999976, w1=14.119224686408177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9057/9999): loss=2.132886178043298, w0=73.18124999999976, w1=14.126376617207777\n",
      "Gradient Descent(9058/9999): loss=2.359999313454091, w0=73.18124999999976, w1=13.980163870861098\n",
      "Gradient Descent(9059/9999): loss=2.3403231665032878, w0=73.29374999999976, w1=13.855066967614354\n",
      "Gradient Descent(9060/9999): loss=2.661244056169556, w0=73.06874999999977, w1=13.763576150786353\n",
      "Gradient Descent(9061/9999): loss=2.252993544527919, w0=73.06874999999977, w1=13.843177150803163\n",
      "Gradient Descent(9062/9999): loss=2.0865024549229867, w0=73.12499999999977, w1=13.735130956980663\n",
      "Gradient Descent(9063/9999): loss=1.6092833094278407, w0=73.12499999999977, w1=13.96894693106356\n",
      "Gradient Descent(9064/9999): loss=3.1959988983351275, w0=73.06874999999977, w1=14.04351674558979\n",
      "Gradient Descent(9065/9999): loss=1.7277986151140854, w0=72.89999999999976, w1=13.78740419193858\n",
      "Gradient Descent(9066/9999): loss=1.8385355675020982, w0=73.18124999999976, w1=13.939017290734713\n",
      "Gradient Descent(9067/9999): loss=2.5454478366037296, w0=72.89999999999976, w1=14.032008581049315\n",
      "Gradient Descent(9068/9999): loss=2.6116929138994665, w0=72.78749999999977, w1=14.108641460991974\n",
      "Gradient Descent(9069/9999): loss=2.214834475764312, w0=72.89999999999976, w1=14.093893048505194\n",
      "Gradient Descent(9070/9999): loss=2.3556051330638605, w0=72.78749999999977, w1=13.920120235538558\n",
      "Gradient Descent(9071/9999): loss=2.218558976199412, w0=72.78749999999977, w1=13.921484816164858\n",
      "Gradient Descent(9072/9999): loss=2.154103250705628, w0=72.89999999999976, w1=13.809029402283251\n",
      "Gradient Descent(9073/9999): loss=2.069186799420369, w0=73.06874999999977, w1=13.799896439191526\n",
      "Gradient Descent(9074/9999): loss=1.9417873631564395, w0=73.18124999999976, w1=13.642011893957058\n",
      "Gradient Descent(9075/9999): loss=2.5556458995493445, w0=73.18124999999976, w1=13.832364655304138\n",
      "Gradient Descent(9076/9999): loss=3.099390660173177, w0=73.29374999999976, w1=13.732155274364581\n",
      "Gradient Descent(9077/9999): loss=2.182543124390378, w0=73.40624999999976, w1=13.815548883197108\n",
      "Gradient Descent(9078/9999): loss=2.500034567713201, w0=73.46249999999976, w1=13.758325904130595\n",
      "Gradient Descent(9079/9999): loss=2.222210266627499, w0=73.57499999999976, w1=13.84879167244424\n",
      "Gradient Descent(9080/9999): loss=2.281409158670835, w0=73.23749999999976, w1=13.58201261760804\n",
      "Gradient Descent(9081/9999): loss=2.4125391850055427, w0=73.12499999999976, w1=13.596878482344081\n",
      "Gradient Descent(9082/9999): loss=2.2416854152892247, w0=73.29374999999976, w1=13.672083718405785\n",
      "Gradient Descent(9083/9999): loss=1.801646716982586, w0=73.18124999999976, w1=13.365808851135357\n",
      "Gradient Descent(9084/9999): loss=1.9877790099259518, w0=73.46249999999976, w1=13.292812677926705\n",
      "Gradient Descent(9085/9999): loss=2.8409709885146177, w0=73.34999999999977, w1=13.390852714376859\n",
      "Gradient Descent(9086/9999): loss=2.336239854603132, w0=73.23749999999977, w1=13.790106370746884\n",
      "Gradient Descent(9087/9999): loss=2.2471768196579935, w0=72.89999999999976, w1=13.442849088197255\n",
      "Gradient Descent(9088/9999): loss=2.005690390091273, w0=72.89999999999976, w1=13.576431406953407\n",
      "Gradient Descent(9089/9999): loss=1.908725582025597, w0=72.78749999999977, w1=13.543832527971938\n",
      "Gradient Descent(9090/9999): loss=2.5811663444510997, w0=72.89999999999976, w1=13.39703513075412\n",
      "Gradient Descent(9091/9999): loss=2.9007785031383833, w0=72.84374999999976, w1=13.510264031516845\n",
      "Gradient Descent(9092/9999): loss=1.7136306158871983, w0=72.84374999999976, w1=13.56787509708758\n",
      "Gradient Descent(9093/9999): loss=2.1871511126399508, w0=72.84374999999976, w1=13.822977385689441\n",
      "Gradient Descent(9094/9999): loss=2.0509580106588947, w0=72.78749999999975, w1=13.798927434446227\n",
      "Gradient Descent(9095/9999): loss=2.3920297973521634, w0=73.01249999999975, w1=13.85740527317343\n",
      "Gradient Descent(9096/9999): loss=2.1669094673172373, w0=72.95624999999974, w1=13.962398424813376\n",
      "Gradient Descent(9097/9999): loss=2.444016986241995, w0=73.06874999999974, w1=14.185154307980735\n",
      "Gradient Descent(9098/9999): loss=2.9739587457462147, w0=72.95624999999974, w1=14.117400464634565\n",
      "Gradient Descent(9099/9999): loss=2.126396616069762, w0=72.95624999999974, w1=13.906780796735315\n",
      "Gradient Descent(9100/9999): loss=2.127583860260521, w0=72.95624999999974, w1=13.876021283606\n",
      "Gradient Descent(9101/9999): loss=2.1975816474433234, w0=73.01249999999975, w1=13.790509302069145\n",
      "Gradient Descent(9102/9999): loss=2.4592778077621658, w0=72.95624999999974, w1=13.577226062711386\n",
      "Gradient Descent(9103/9999): loss=1.8219380114821637, w0=73.01249999999975, w1=13.635215113708941\n",
      "Gradient Descent(9104/9999): loss=2.970202758945713, w0=73.01249999999975, w1=13.49274788014532\n",
      "Gradient Descent(9105/9999): loss=2.5703794642799336, w0=73.06874999999975, w1=13.49977929940033\n",
      "Gradient Descent(9106/9999): loss=2.050721896691323, w0=72.84374999999976, w1=13.724628885474463\n",
      "Gradient Descent(9107/9999): loss=2.4425810465578124, w0=72.84374999999976, w1=13.580470406110235\n",
      "Gradient Descent(9108/9999): loss=2.729992377453814, w0=72.95624999999976, w1=13.654306162747496\n",
      "Gradient Descent(9109/9999): loss=2.647770937620907, w0=73.18124999999975, w1=13.283716977905032\n",
      "Gradient Descent(9110/9999): loss=2.0110065621294, w0=72.89999999999975, w1=13.2786730785803\n",
      "Gradient Descent(9111/9999): loss=2.529504446627081, w0=73.18124999999975, w1=13.040388422945545\n",
      "Gradient Descent(9112/9999): loss=2.0788483033990572, w0=72.84374999999974, w1=13.204066393486723\n",
      "Gradient Descent(9113/9999): loss=2.727350634659201, w0=72.78749999999974, w1=13.446013454750545\n",
      "Gradient Descent(9114/9999): loss=1.9990939350346248, w0=72.78749999999974, w1=13.581270777407967\n",
      "Gradient Descent(9115/9999): loss=2.36528297669111, w0=72.95624999999974, w1=13.807287885529997\n",
      "Gradient Descent(9116/9999): loss=2.7159862680839177, w0=72.95624999999974, w1=13.730329102307167\n",
      "Gradient Descent(9117/9999): loss=2.3559784855958332, w0=72.84374999999974, w1=13.791946867997613\n",
      "Gradient Descent(9118/9999): loss=2.3485093580682452, w0=73.06874999999974, w1=13.48323455738534\n",
      "Gradient Descent(9119/9999): loss=2.3874656042851097, w0=73.01249999999973, w1=13.499356335618879\n",
      "Gradient Descent(9120/9999): loss=2.23673066651532, w0=73.01249999999973, w1=13.69233325083836\n",
      "Gradient Descent(9121/9999): loss=2.6815096820751245, w0=73.01249999999973, w1=13.69921280840402\n",
      "Gradient Descent(9122/9999): loss=2.0861662988243914, w0=73.01249999999973, w1=13.647030177162094\n",
      "Gradient Descent(9123/9999): loss=2.068820183499472, w0=73.01249999999973, w1=14.146393953460263\n",
      "Gradient Descent(9124/9999): loss=1.7227006838829806, w0=73.18124999999974, w1=13.925022181464014\n",
      "Gradient Descent(9125/9999): loss=1.903377838946042, w0=73.29374999999973, w1=13.762759442251086\n",
      "Gradient Descent(9126/9999): loss=1.9116685977721417, w0=73.18124999999974, w1=13.850814169832447\n",
      "Gradient Descent(9127/9999): loss=2.687015845515525, w0=73.18124999999974, w1=13.67264261949613\n",
      "Gradient Descent(9128/9999): loss=2.389185944993335, w0=73.01249999999973, w1=13.546743284361977\n",
      "Gradient Descent(9129/9999): loss=2.1505703979050503, w0=72.95624999999973, w1=13.484762297477618\n",
      "Gradient Descent(9130/9999): loss=2.2154351734821653, w0=72.95624999999973, w1=13.463622510135274\n",
      "Gradient Descent(9131/9999): loss=2.1597923439974727, w0=73.06874999999972, w1=13.39400963767386\n",
      "Gradient Descent(9132/9999): loss=2.9146570709294233, w0=73.23749999999973, w1=13.216287214461486\n",
      "Gradient Descent(9133/9999): loss=2.0780355078660224, w0=72.89999999999972, w1=13.399249819637609\n",
      "Gradient Descent(9134/9999): loss=2.067784278971922, w0=73.01249999999972, w1=13.41767550485718\n",
      "Gradient Descent(9135/9999): loss=2.3988423594106276, w0=72.84374999999972, w1=13.625941097443132\n",
      "Gradient Descent(9136/9999): loss=2.451774167956308, w0=72.61874999999972, w1=13.483908427932265\n",
      "Gradient Descent(9137/9999): loss=2.1936877363677323, w0=72.44999999999972, w1=13.398435904226707\n",
      "Gradient Descent(9138/9999): loss=2.3980870302203945, w0=72.67499999999971, w1=13.660017695333114\n",
      "Gradient Descent(9139/9999): loss=2.692141244817993, w0=72.73124999999972, w1=13.662406694440778\n",
      "Gradient Descent(9140/9999): loss=2.5223919537209416, w0=72.89999999999972, w1=13.573574232113309\n",
      "Gradient Descent(9141/9999): loss=2.1082840616827614, w0=72.73124999999972, w1=13.787174906357558\n",
      "Gradient Descent(9142/9999): loss=2.5707903002584773, w0=72.67499999999971, w1=13.801831716000692\n",
      "Gradient Descent(9143/9999): loss=2.3710763319488963, w0=72.50624999999971, w1=13.574459537842069\n",
      "Gradient Descent(9144/9999): loss=2.234889043662725, w0=72.56249999999972, w1=13.663512111219976\n",
      "Gradient Descent(9145/9999): loss=2.103732077970637, w0=72.39374999999971, w1=13.742226625662557\n",
      "Gradient Descent(9146/9999): loss=2.072426232179645, w0=72.56249999999972, w1=13.875183304809404\n",
      "Gradient Descent(9147/9999): loss=2.3205682826992335, w0=72.67499999999971, w1=14.150542617728705\n",
      "Gradient Descent(9148/9999): loss=2.6806822831195083, w0=72.89999999999971, w1=14.05568434723918\n",
      "Gradient Descent(9149/9999): loss=2.235957778253031, w0=72.78749999999971, w1=13.781767574010896\n",
      "Gradient Descent(9150/9999): loss=1.8359375893304388, w0=72.95624999999971, w1=13.57240466063656\n",
      "Gradient Descent(9151/9999): loss=2.017446220588168, w0=72.89999999999971, w1=13.577983886921222\n",
      "Gradient Descent(9152/9999): loss=2.1342721717045996, w0=73.1249999999997, w1=13.556303267500777\n",
      "Gradient Descent(9153/9999): loss=2.3761538382093472, w0=73.0124999999997, w1=13.870336572488153\n",
      "Gradient Descent(9154/9999): loss=2.1516429488749305, w0=73.06874999999971, w1=13.69758474478487\n",
      "Gradient Descent(9155/9999): loss=2.3603569994400426, w0=73.18124999999971, w1=13.942377585691155\n",
      "Gradient Descent(9156/9999): loss=2.6064380678261965, w0=73.1249999999997, w1=13.963163337062584\n",
      "Gradient Descent(9157/9999): loss=1.774310298438126, w0=73.1249999999997, w1=13.987363716415686\n",
      "Gradient Descent(9158/9999): loss=2.3723977259239835, w0=73.18124999999971, w1=14.008368903304715\n",
      "Gradient Descent(9159/9999): loss=1.9293740889525257, w0=73.1249999999997, w1=13.96948481481194\n",
      "Gradient Descent(9160/9999): loss=2.5458891247269673, w0=73.4062499999997, w1=13.784665682254884\n",
      "Gradient Descent(9161/9999): loss=2.4113949970290847, w0=73.5187499999997, w1=13.51599125106289\n",
      "Gradient Descent(9162/9999): loss=2.2392721928014896, w0=73.5187499999997, w1=13.642057266354783\n",
      "Gradient Descent(9163/9999): loss=2.007088648254827, w0=73.4062499999997, w1=13.477518542767765\n",
      "Gradient Descent(9164/9999): loss=2.2929570014543468, w0=73.6312499999997, w1=13.53113859714412\n",
      "Gradient Descent(9165/9999): loss=2.472227448223982, w0=73.6312499999997, w1=13.466902005936312\n",
      "Gradient Descent(9166/9999): loss=2.7445169991998695, w0=73.3499999999997, w1=13.456334873191139\n",
      "Gradient Descent(9167/9999): loss=1.8681265132421903, w0=73.3499999999997, w1=13.512713302731836\n",
      "Gradient Descent(9168/9999): loss=2.7089861941009814, w0=73.2374999999997, w1=13.490613316106074\n",
      "Gradient Descent(9169/9999): loss=1.9254821906405304, w0=73.2937499999997, w1=13.546244057139223\n",
      "Gradient Descent(9170/9999): loss=2.4894497653745598, w0=73.34999999999971, w1=13.397806099171417\n",
      "Gradient Descent(9171/9999): loss=1.6537455215847472, w0=73.18124999999971, w1=13.571019426881715\n",
      "Gradient Descent(9172/9999): loss=2.501535992459726, w0=73.18124999999971, w1=13.486185331501101\n",
      "Gradient Descent(9173/9999): loss=2.047193451297036, w0=73.18124999999971, w1=13.560793498775649\n",
      "Gradient Descent(9174/9999): loss=1.7986475763266083, w0=73.18124999999971, w1=13.750770681846006\n",
      "Gradient Descent(9175/9999): loss=2.511576720913844, w0=73.51874999999971, w1=13.78905486669118\n",
      "Gradient Descent(9176/9999): loss=2.1996687493318263, w0=73.23749999999971, w1=13.73783733606125\n",
      "Gradient Descent(9177/9999): loss=2.2141021012976516, w0=73.40624999999972, w1=13.760215073600417\n",
      "Gradient Descent(9178/9999): loss=2.390655082033124, w0=73.23749999999971, w1=13.52785211027007\n",
      "Gradient Descent(9179/9999): loss=2.0296920494811683, w0=73.34999999999971, w1=13.545563685123794\n",
      "Gradient Descent(9180/9999): loss=1.7496906478947691, w0=73.34999999999971, w1=13.69619443875545\n",
      "Gradient Descent(9181/9999): loss=2.526349925504805, w0=73.40624999999972, w1=13.895472636241262\n",
      "Gradient Descent(9182/9999): loss=2.305723659994067, w0=73.46249999999972, w1=13.749013446871144\n",
      "Gradient Descent(9183/9999): loss=1.8212020557970732, w0=73.74374999999972, w1=14.071616422114031\n",
      "Gradient Descent(9184/9999): loss=1.9686039720020554, w0=73.79999999999973, w1=14.061486433618638\n",
      "Gradient Descent(9185/9999): loss=2.112503457053435, w0=73.74374999999972, w1=13.96925758110855\n",
      "Gradient Descent(9186/9999): loss=2.641546152192998, w0=73.79999999999973, w1=14.147892551909738\n",
      "Gradient Descent(9187/9999): loss=2.472550058729066, w0=73.79999999999973, w1=13.905633859133559\n",
      "Gradient Descent(9188/9999): loss=2.2925328904273945, w0=73.74374999999972, w1=13.895856037303954\n",
      "Gradient Descent(9189/9999): loss=2.021331899312818, w0=73.96874999999972, w1=13.737326815108059\n",
      "Gradient Descent(9190/9999): loss=2.808200215748489, w0=73.91249999999971, w1=13.861270483923024\n",
      "Gradient Descent(9191/9999): loss=2.152105541442855, w0=73.68749999999972, w1=13.874387867887219\n",
      "Gradient Descent(9192/9999): loss=2.076370975560541, w0=73.79999999999971, w1=13.614267614032762\n",
      "Gradient Descent(9193/9999): loss=2.1918964834714716, w0=73.91249999999971, w1=13.791104262836065\n",
      "Gradient Descent(9194/9999): loss=2.36740891248838, w0=73.91249999999971, w1=13.756229760498226\n",
      "Gradient Descent(9195/9999): loss=1.934815307234147, w0=73.91249999999971, w1=13.665032000734183\n",
      "Gradient Descent(9196/9999): loss=1.8005226864753936, w0=73.5749999999997, w1=13.781576054543317\n",
      "Gradient Descent(9197/9999): loss=2.286438933282761, w0=73.5187499999997, w1=13.845747451778173\n",
      "Gradient Descent(9198/9999): loss=2.219143088734852, w0=73.6312499999997, w1=13.55694955668785\n",
      "Gradient Descent(9199/9999): loss=2.2020502409211176, w0=73.29374999999969, w1=13.533690268019225\n",
      "Gradient Descent(9200/9999): loss=2.3065919498570358, w0=73.0687499999997, w1=13.620343592175349\n",
      "Gradient Descent(9201/9999): loss=1.9735602739259264, w0=72.8437499999997, w1=13.751032263749726\n",
      "Gradient Descent(9202/9999): loss=2.2029617014279874, w0=72.9562499999997, w1=13.546075209762755\n",
      "Gradient Descent(9203/9999): loss=2.9434785582574428, w0=73.0687499999997, w1=13.542432912749744\n",
      "Gradient Descent(9204/9999): loss=2.629296891083983, w0=73.01249999999969, w1=13.575475451822731\n",
      "Gradient Descent(9205/9999): loss=2.1451908707180953, w0=73.12499999999969, w1=13.451977018885724\n",
      "Gradient Descent(9206/9999): loss=2.089281655151805, w0=73.23749999999968, w1=13.443303633489155\n",
      "Gradient Descent(9207/9999): loss=2.399684438372751, w0=73.29374999999969, w1=13.440864649321535\n",
      "Gradient Descent(9208/9999): loss=1.935777052407254, w0=72.95624999999968, w1=13.75097685757185\n",
      "Gradient Descent(9209/9999): loss=1.855503713733804, w0=72.95624999999968, w1=13.574023921099931\n",
      "Gradient Descent(9210/9999): loss=2.5428264437373325, w0=72.89999999999968, w1=13.374736675931572\n",
      "Gradient Descent(9211/9999): loss=1.8104370460266876, w0=72.89999999999968, w1=13.525886085585388\n",
      "Gradient Descent(9212/9999): loss=2.0987737651084832, w0=73.23749999999968, w1=13.699037541997754\n",
      "Gradient Descent(9213/9999): loss=1.5251544105699086, w0=72.89999999999968, w1=13.69787200996217\n",
      "Gradient Descent(9214/9999): loss=2.076314080855907, w0=73.06874999999968, w1=13.479107272844935\n",
      "Gradient Descent(9215/9999): loss=2.1353744643335775, w0=72.73124999999968, w1=13.179320737918331\n",
      "Gradient Descent(9216/9999): loss=2.102930403132002, w0=72.89999999999968, w1=13.289197731819877\n",
      "Gradient Descent(9217/9999): loss=2.4815389519834694, w0=72.89999999999968, w1=13.350471904043836\n",
      "Gradient Descent(9218/9999): loss=2.30647055455319, w0=73.06874999999968, w1=13.500034539524169\n",
      "Gradient Descent(9219/9999): loss=2.0690708472858925, w0=72.89999999999968, w1=13.429687876026717\n",
      "Gradient Descent(9220/9999): loss=2.104793779206075, w0=72.61874999999968, w1=13.451793332172448\n",
      "Gradient Descent(9221/9999): loss=2.096404498961223, w0=72.78749999999968, w1=13.635092865827193\n",
      "Gradient Descent(9222/9999): loss=2.3704712091003453, w0=72.73124999999968, w1=13.628779890536334\n",
      "Gradient Descent(9223/9999): loss=2.5324637403770143, w0=73.06874999999968, w1=13.729129682232033\n",
      "Gradient Descent(9224/9999): loss=2.10317421356185, w0=73.12499999999969, w1=13.499576835119859\n",
      "Gradient Descent(9225/9999): loss=2.1763153580105588, w0=73.1812499999997, w1=13.31156733746968\n",
      "Gradient Descent(9226/9999): loss=2.318480615891227, w0=73.1812499999997, w1=13.369484372718157\n",
      "Gradient Descent(9227/9999): loss=2.098397118372534, w0=73.29374999999969, w1=13.454563981609736\n",
      "Gradient Descent(9228/9999): loss=1.985898791823784, w0=73.23749999999968, w1=13.394172865900261\n",
      "Gradient Descent(9229/9999): loss=2.901611015707055, w0=73.51874999999968, w1=13.249887071342398\n",
      "Gradient Descent(9230/9999): loss=2.2508252626741747, w0=73.63124999999968, w1=13.358654521946164\n",
      "Gradient Descent(9231/9999): loss=2.252162705847176, w0=73.51874999999968, w1=13.21386967374944\n",
      "Gradient Descent(9232/9999): loss=2.0702515504091847, w0=73.57499999999969, w1=13.028242216434505\n",
      "Gradient Descent(9233/9999): loss=2.126232006474732, w0=73.6312499999997, w1=12.975023826720998\n",
      "Gradient Descent(9234/9999): loss=2.1223386751363496, w0=73.57499999999969, w1=12.942121341838055\n",
      "Gradient Descent(9235/9999): loss=1.8530371703349486, w0=73.3499999999997, w1=13.14165792613571\n",
      "Gradient Descent(9236/9999): loss=1.7721956432249848, w0=73.29374999999969, w1=13.146463451567199\n",
      "Gradient Descent(9237/9999): loss=2.1320336382633114, w0=73.1812499999997, w1=13.070935260598008\n",
      "Gradient Descent(9238/9999): loss=2.597052007262306, w0=72.9562499999997, w1=13.393245056408409\n",
      "Gradient Descent(9239/9999): loss=1.8627477995599135, w0=73.0687499999997, w1=13.428367677007806\n",
      "Gradient Descent(9240/9999): loss=2.5376141993982766, w0=73.1812499999997, w1=13.622448275023379\n",
      "Gradient Descent(9241/9999): loss=2.3169185487346615, w0=73.2374999999997, w1=13.70509195103865\n",
      "Gradient Descent(9242/9999): loss=2.2989912575394507, w0=73.2374999999997, w1=13.881454443884554\n",
      "Gradient Descent(9243/9999): loss=1.9858785355747663, w0=73.2937499999997, w1=14.035439802036674\n",
      "Gradient Descent(9244/9999): loss=2.0795745438361206, w0=73.2937499999997, w1=13.880282282456978\n",
      "Gradient Descent(9245/9999): loss=2.4121192744521163, w0=73.1249999999997, w1=13.776073612595072\n",
      "Gradient Descent(9246/9999): loss=2.448190371324732, w0=73.2937499999997, w1=13.74661941933083\n",
      "Gradient Descent(9247/9999): loss=2.3396709403052953, w0=73.18124999999971, w1=13.706056835535657\n",
      "Gradient Descent(9248/9999): loss=2.295066515620693, w0=73.0124999999997, w1=13.636898310627588\n",
      "Gradient Descent(9249/9999): loss=2.1115112765639195, w0=73.2937499999997, w1=13.646323685290847\n",
      "Gradient Descent(9250/9999): loss=2.1333456644580684, w0=73.34999999999971, w1=13.689874900846016\n",
      "Gradient Descent(9251/9999): loss=2.2990688451550305, w0=73.46249999999971, w1=13.674663346202824\n",
      "Gradient Descent(9252/9999): loss=2.117805672853801, w0=73.2937499999997, w1=13.70925554405261\n",
      "Gradient Descent(9253/9999): loss=2.4216354557871678, w0=73.4062499999997, w1=13.733385026364365\n",
      "Gradient Descent(9254/9999): loss=2.556704564727231, w0=73.2937499999997, w1=13.823570735413334\n",
      "Gradient Descent(9255/9999): loss=1.9105956897733896, w0=73.1249999999997, w1=13.848920616913905\n",
      "Gradient Descent(9256/9999): loss=2.064296389188767, w0=73.18124999999971, w1=13.64486541206149\n",
      "Gradient Descent(9257/9999): loss=2.3385457748550715, w0=73.18124999999971, w1=13.378608147964655\n",
      "Gradient Descent(9258/9999): loss=2.254428102942801, w0=73.23749999999971, w1=13.53115935453155\n",
      "Gradient Descent(9259/9999): loss=1.7226443537511278, w0=73.18124999999971, w1=13.48072800752719\n",
      "Gradient Descent(9260/9999): loss=2.039173470538643, w0=73.0124999999997, w1=13.242768764201307\n",
      "Gradient Descent(9261/9999): loss=1.9999168770203926, w0=72.9562499999997, w1=13.072219397483297\n",
      "Gradient Descent(9262/9999): loss=2.573698424196701, w0=73.0124999999997, w1=12.97143806120591\n",
      "Gradient Descent(9263/9999): loss=1.5570636882435696, w0=72.9562499999997, w1=12.88592815229226\n",
      "Gradient Descent(9264/9999): loss=2.228199896701878, w0=72.8999999999997, w1=12.765578959126886\n",
      "Gradient Descent(9265/9999): loss=2.0286003077054913, w0=73.0687499999997, w1=13.025538692968805\n",
      "Gradient Descent(9266/9999): loss=2.7379905710721815, w0=72.9562499999997, w1=13.222854670947727\n",
      "Gradient Descent(9267/9999): loss=1.965588822450183, w0=73.2937499999997, w1=13.208669015897303\n",
      "Gradient Descent(9268/9999): loss=2.1161450324834647, w0=73.4062499999997, w1=13.290011855054424\n",
      "Gradient Descent(9269/9999): loss=1.8387613713908273, w0=73.5187499999997, w1=13.38508325219197\n",
      "Gradient Descent(9270/9999): loss=2.685953588534181, w0=73.3499999999997, w1=13.468112073671222\n",
      "Gradient Descent(9271/9999): loss=2.700074104184945, w0=73.3499999999997, w1=13.778139629137343\n",
      "Gradient Descent(9272/9999): loss=2.423457699252179, w0=73.5187499999997, w1=13.707194476269363\n",
      "Gradient Descent(9273/9999): loss=2.218925535067303, w0=73.5187499999997, w1=13.502955119939628\n",
      "Gradient Descent(9274/9999): loss=2.395768883420838, w0=73.4062499999997, w1=13.384106165650948\n",
      "Gradient Descent(9275/9999): loss=2.3914572067534667, w0=73.5749999999997, w1=13.218229596171303\n",
      "Gradient Descent(9276/9999): loss=2.065676342993791, w0=73.6874999999997, w1=13.40825326392044\n",
      "Gradient Descent(9277/9999): loss=1.62060652992623, w0=73.6312499999997, w1=13.67547173556143\n",
      "Gradient Descent(9278/9999): loss=2.220088157972879, w0=73.57499999999969, w1=13.669945252543833\n",
      "Gradient Descent(9279/9999): loss=1.9202880548070935, w0=73.68749999999969, w1=13.772991511332048\n",
      "Gradient Descent(9280/9999): loss=2.8786395682501222, w0=73.57499999999969, w1=13.379057202390605\n",
      "Gradient Descent(9281/9999): loss=2.267080579766641, w0=73.68749999999969, w1=13.613324210342032\n",
      "Gradient Descent(9282/9999): loss=2.4984074694565095, w0=73.63124999999968, w1=13.346811061433195\n",
      "Gradient Descent(9283/9999): loss=2.4596689651728267, w0=73.63124999999968, w1=13.283256448761302\n",
      "Gradient Descent(9284/9999): loss=2.3676082286957376, w0=73.23749999999968, w1=13.328888403848097\n",
      "Gradient Descent(9285/9999): loss=2.1155328278287158, w0=73.34999999999968, w1=13.250476904202563\n",
      "Gradient Descent(9286/9999): loss=2.241863143950534, w0=73.57499999999968, w1=13.598231192549179\n",
      "Gradient Descent(9287/9999): loss=2.11600960027132, w0=73.12499999999967, w1=13.459929735901158\n",
      "Gradient Descent(9288/9999): loss=2.4040819308210697, w0=72.95624999999967, w1=13.548287162183398\n",
      "Gradient Descent(9289/9999): loss=2.1732847956627586, w0=72.84374999999967, w1=14.148142180906342\n",
      "Gradient Descent(9290/9999): loss=2.3002565624263567, w0=72.84374999999967, w1=14.159557625539218\n",
      "Gradient Descent(9291/9999): loss=2.079842440242399, w0=72.95624999999967, w1=14.236571611936812\n",
      "Gradient Descent(9292/9999): loss=2.2223583123263895, w0=72.95624999999967, w1=14.158054260389559\n",
      "Gradient Descent(9293/9999): loss=2.0884135848682233, w0=73.06874999999967, w1=14.023507359057025\n",
      "Gradient Descent(9294/9999): loss=2.775745972398565, w0=73.23749999999967, w1=13.765258635108564\n",
      "Gradient Descent(9295/9999): loss=2.766760436002368, w0=73.06874999999967, w1=13.664131997849003\n",
      "Gradient Descent(9296/9999): loss=2.222986489700417, w0=73.12499999999967, w1=13.5150363482274\n",
      "Gradient Descent(9297/9999): loss=2.220546866942991, w0=73.40624999999967, w1=13.588838812155496\n",
      "Gradient Descent(9298/9999): loss=2.5187535522694295, w0=73.34999999999967, w1=13.510810912109452\n",
      "Gradient Descent(9299/9999): loss=1.811961346015512, w0=73.57499999999966, w1=13.695183544901171\n",
      "Gradient Descent(9300/9999): loss=2.006667384349763, w0=73.51874999999966, w1=13.73398537332376\n",
      "Gradient Descent(9301/9999): loss=1.8830722119989602, w0=73.63124999999965, w1=13.604528300243713\n",
      "Gradient Descent(9302/9999): loss=2.4352894379401238, w0=73.63124999999965, w1=13.238190620805028\n",
      "Gradient Descent(9303/9999): loss=2.574488247458606, w0=73.57499999999965, w1=12.958530966054878\n",
      "Gradient Descent(9304/9999): loss=2.146442865979782, w0=73.68749999999964, w1=13.021498764353655\n",
      "Gradient Descent(9305/9999): loss=2.304373974555054, w0=73.68749999999964, w1=12.957542227042758\n",
      "Gradient Descent(9306/9999): loss=1.718062479159851, w0=73.34999999999964, w1=12.959017073330743\n",
      "Gradient Descent(9307/9999): loss=2.644770582391123, w0=73.29374999999963, w1=12.91196419463406\n",
      "Gradient Descent(9308/9999): loss=2.1309856490391104, w0=73.63124999999964, w1=12.99441771905123\n",
      "Gradient Descent(9309/9999): loss=2.1021884078381037, w0=73.79999999999964, w1=13.224840291298378\n",
      "Gradient Descent(9310/9999): loss=1.9365607329022516, w0=73.63124999999964, w1=13.582675275012255\n",
      "Gradient Descent(9311/9999): loss=2.0790536989018085, w0=73.68749999999964, w1=13.733696479238121\n",
      "Gradient Descent(9312/9999): loss=2.376803097052949, w0=73.74374999999965, w1=13.888828556703395\n",
      "Gradient Descent(9313/9999): loss=1.5139613596384311, w0=73.91249999999965, w1=13.737206147819501\n",
      "Gradient Descent(9314/9999): loss=2.146796106235535, w0=73.79999999999966, w1=13.77022884184697\n",
      "Gradient Descent(9315/9999): loss=2.339638693507216, w0=73.68749999999966, w1=13.8634676027978\n",
      "Gradient Descent(9316/9999): loss=2.0780682731034554, w0=73.68749999999966, w1=13.938023153203208\n",
      "Gradient Descent(9317/9999): loss=1.3913326897860043, w0=73.79999999999966, w1=14.089326390878085\n",
      "Gradient Descent(9318/9999): loss=1.6443401914763311, w0=73.68749999999966, w1=14.00354099980576\n",
      "Gradient Descent(9319/9999): loss=1.888053934913504, w0=73.57499999999966, w1=13.9766951814746\n",
      "Gradient Descent(9320/9999): loss=2.018935271732758, w0=73.29374999999966, w1=13.947826307447736\n",
      "Gradient Descent(9321/9999): loss=2.451133423136527, w0=73.06874999999967, w1=14.062137566836155\n",
      "Gradient Descent(9322/9999): loss=2.083405047654658, w0=73.06874999999967, w1=14.062241904568264\n",
      "Gradient Descent(9323/9999): loss=2.6506460895911603, w0=73.06874999999967, w1=14.359054487102673\n",
      "Gradient Descent(9324/9999): loss=2.582925554986644, w0=72.95624999999967, w1=14.568869133649315\n",
      "Gradient Descent(9325/9999): loss=2.1404405289435404, w0=73.06874999999967, w1=14.199403457202088\n",
      "Gradient Descent(9326/9999): loss=2.251769724782905, w0=73.01249999999966, w1=13.944517847842848\n",
      "Gradient Descent(9327/9999): loss=1.688194869542131, w0=72.89999999999966, w1=14.040721161685104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9328/9999): loss=2.152600539837472, w0=72.84374999999966, w1=13.894786042595898\n",
      "Gradient Descent(9329/9999): loss=2.7727897248974376, w0=72.67499999999966, w1=14.196515779694943\n",
      "Gradient Descent(9330/9999): loss=2.86325973367797, w0=72.89999999999965, w1=13.806890096481023\n",
      "Gradient Descent(9331/9999): loss=2.1528505175772015, w0=72.73124999999965, w1=13.768069483948057\n",
      "Gradient Descent(9332/9999): loss=2.385115956797919, w0=72.95624999999964, w1=13.596514184141416\n",
      "Gradient Descent(9333/9999): loss=2.1773853460153387, w0=72.67499999999964, w1=13.730276802106328\n",
      "Gradient Descent(9334/9999): loss=2.2074644200247002, w0=73.23749999999964, w1=13.604912895858826\n",
      "Gradient Descent(9335/9999): loss=1.9323367574683044, w0=73.34999999999964, w1=13.493769390043537\n",
      "Gradient Descent(9336/9999): loss=2.2026276936839, w0=73.51874999999964, w1=13.468818646983369\n",
      "Gradient Descent(9337/9999): loss=2.3641391538695946, w0=73.51874999999964, w1=13.267426513783011\n",
      "Gradient Descent(9338/9999): loss=2.498325507355115, w0=73.34999999999964, w1=13.285920781545514\n",
      "Gradient Descent(9339/9999): loss=2.3204036947470037, w0=73.40624999999964, w1=13.412011570923308\n",
      "Gradient Descent(9340/9999): loss=2.1191054218745107, w0=73.40624999999964, w1=13.364243332959205\n",
      "Gradient Descent(9341/9999): loss=2.4880253393192495, w0=73.29374999999965, w1=13.601271846831692\n",
      "Gradient Descent(9342/9999): loss=2.5587332540487973, w0=72.95624999999964, w1=13.76044874061906\n",
      "Gradient Descent(9343/9999): loss=2.291860068533487, w0=72.89999999999964, w1=13.696183806700933\n",
      "Gradient Descent(9344/9999): loss=1.9085837279029259, w0=72.95624999999964, w1=13.598512972655794\n",
      "Gradient Descent(9345/9999): loss=2.5072617794935645, w0=72.89999999999964, w1=13.796987741787209\n",
      "Gradient Descent(9346/9999): loss=1.7794614391640429, w0=73.06874999999964, w1=13.72028417912014\n",
      "Gradient Descent(9347/9999): loss=2.1611817374589553, w0=73.29374999999963, w1=13.73824962351814\n",
      "Gradient Descent(9348/9999): loss=2.4216073656493364, w0=73.01249999999963, w1=13.753545002169728\n",
      "Gradient Descent(9349/9999): loss=2.159441448558399, w0=73.06874999999964, w1=13.811828125958476\n",
      "Gradient Descent(9350/9999): loss=1.6325637084596023, w0=73.06874999999964, w1=13.771197372638591\n",
      "Gradient Descent(9351/9999): loss=2.3047600420230623, w0=73.23749999999964, w1=13.994638554392562\n",
      "Gradient Descent(9352/9999): loss=2.2620774814948748, w0=73.57499999999965, w1=14.045428823946596\n",
      "Gradient Descent(9353/9999): loss=2.776082933970035, w0=73.46249999999965, w1=14.040125460973451\n",
      "Gradient Descent(9354/9999): loss=2.181067293849665, w0=73.57499999999965, w1=13.8602542763981\n",
      "Gradient Descent(9355/9999): loss=2.4677306014671094, w0=73.51874999999964, w1=13.716364543746728\n",
      "Gradient Descent(9356/9999): loss=2.099389409699021, w0=73.34999999999964, w1=13.714893825673133\n",
      "Gradient Descent(9357/9999): loss=2.5088938482928653, w0=73.18124999999964, w1=13.682096801598277\n",
      "Gradient Descent(9358/9999): loss=2.2537491263570946, w0=73.23749999999964, w1=13.995677456931194\n",
      "Gradient Descent(9359/9999): loss=2.297370149072895, w0=72.95624999999964, w1=14.14609894222888\n",
      "Gradient Descent(9360/9999): loss=2.497410491601944, w0=72.89999999999964, w1=13.900089574989599\n",
      "Gradient Descent(9361/9999): loss=2.9653947068559736, w0=73.06874999999964, w1=13.71369178324419\n",
      "Gradient Descent(9362/9999): loss=2.3824943684471975, w0=73.23749999999964, w1=13.915025972027614\n",
      "Gradient Descent(9363/9999): loss=2.56159146948193, w0=73.12499999999964, w1=13.7374555250629\n",
      "Gradient Descent(9364/9999): loss=1.826795552265403, w0=73.12499999999964, w1=13.791750559012995\n",
      "Gradient Descent(9365/9999): loss=2.449804114620183, w0=73.18124999999965, w1=13.86788085432972\n",
      "Gradient Descent(9366/9999): loss=2.8267384696305786, w0=73.06874999999965, w1=13.930663266135877\n",
      "Gradient Descent(9367/9999): loss=2.2806640459305174, w0=73.12499999999966, w1=14.136326904573842\n",
      "Gradient Descent(9368/9999): loss=2.20200494901113, w0=73.12499999999966, w1=14.052182404483299\n",
      "Gradient Descent(9369/9999): loss=2.3438294841651333, w0=72.95624999999966, w1=14.194839936615\n",
      "Gradient Descent(9370/9999): loss=2.1237156071255603, w0=72.73124999999966, w1=14.119484105298474\n",
      "Gradient Descent(9371/9999): loss=2.249991031276039, w0=72.89999999999966, w1=14.117250992685806\n",
      "Gradient Descent(9372/9999): loss=2.4893371819344963, w0=73.06874999999967, w1=14.332163726543962\n",
      "Gradient Descent(9373/9999): loss=1.9451266843006434, w0=73.06874999999967, w1=14.114614992431276\n",
      "Gradient Descent(9374/9999): loss=2.255033431616453, w0=73.12499999999967, w1=13.921159578601047\n",
      "Gradient Descent(9375/9999): loss=2.5008394569810735, w0=73.12499999999967, w1=13.97124993637836\n",
      "Gradient Descent(9376/9999): loss=2.2578561561124157, w0=72.89999999999968, w1=13.979395631616041\n",
      "Gradient Descent(9377/9999): loss=2.531055335518401, w0=72.89999999999968, w1=13.99240700833614\n",
      "Gradient Descent(9378/9999): loss=1.928096636177443, w0=72.61874999999968, w1=13.920618490896768\n",
      "Gradient Descent(9379/9999): loss=2.068099451231372, w0=72.67499999999968, w1=13.670597184120915\n",
      "Gradient Descent(9380/9999): loss=2.2385321288261695, w0=72.78749999999968, w1=13.554932813805365\n",
      "Gradient Descent(9381/9999): loss=1.6557833632347365, w0=72.89999999999968, w1=13.48427978987026\n",
      "Gradient Descent(9382/9999): loss=2.401850216572292, w0=72.84374999999967, w1=13.743858244840034\n",
      "Gradient Descent(9383/9999): loss=1.988599542308653, w0=72.73124999999968, w1=13.867261412006036\n",
      "Gradient Descent(9384/9999): loss=2.0052714943424244, w0=73.06874999999968, w1=13.867711453596227\n",
      "Gradient Descent(9385/9999): loss=2.296959682003002, w0=73.01249999999968, w1=14.210992111442039\n",
      "Gradient Descent(9386/9999): loss=2.3036987448219532, w0=73.23749999999967, w1=14.166738319420059\n",
      "Gradient Descent(9387/9999): loss=2.299417654842361, w0=73.34999999999967, w1=14.222762942520882\n",
      "Gradient Descent(9388/9999): loss=1.8838961410867021, w0=73.46249999999966, w1=14.179644901368922\n",
      "Gradient Descent(9389/9999): loss=2.120699761344097, w0=73.23749999999967, w1=14.106406514167736\n",
      "Gradient Descent(9390/9999): loss=2.2279667263193716, w0=73.18124999999966, w1=13.983910381577173\n",
      "Gradient Descent(9391/9999): loss=2.16088392930377, w0=73.18124999999966, w1=13.892610098296727\n",
      "Gradient Descent(9392/9999): loss=2.150408225124071, w0=73.34999999999967, w1=13.677530340151261\n",
      "Gradient Descent(9393/9999): loss=2.355234289362388, w0=73.29374999999966, w1=13.309774775067407\n",
      "Gradient Descent(9394/9999): loss=1.8549210963827252, w0=73.06874999999967, w1=13.519100159711801\n",
      "Gradient Descent(9395/9999): loss=2.3730669246224743, w0=73.29374999999966, w1=13.748220723167822\n",
      "Gradient Descent(9396/9999): loss=2.784976803146378, w0=73.18124999999966, w1=13.647840906695102\n",
      "Gradient Descent(9397/9999): loss=2.338208701979326, w0=73.40624999999966, w1=13.788418985753676\n",
      "Gradient Descent(9398/9999): loss=2.5011971532725985, w0=73.51874999999966, w1=13.885033202955151\n",
      "Gradient Descent(9399/9999): loss=1.9284231879830944, w0=73.68749999999966, w1=14.117935959722097\n",
      "Gradient Descent(9400/9999): loss=2.0555616278649596, w0=73.74374999999966, w1=14.059202381036346\n",
      "Gradient Descent(9401/9999): loss=2.388156531990734, w0=73.68749999999966, w1=13.780849648967658\n",
      "Gradient Descent(9402/9999): loss=2.0961649316338673, w0=73.85624999999966, w1=13.70006328779665\n",
      "Gradient Descent(9403/9999): loss=3.026605743675618, w0=73.96874999999966, w1=13.671447114398084\n",
      "Gradient Descent(9404/9999): loss=2.2258312588522333, w0=74.02499999999966, w1=13.548670016791167\n",
      "Gradient Descent(9405/9999): loss=2.257669293433789, w0=73.85624999999966, w1=13.603669641717419\n",
      "Gradient Descent(9406/9999): loss=2.4614665933429123, w0=73.68749999999966, w1=13.619588225334715\n",
      "Gradient Descent(9407/9999): loss=2.144147596054185, w0=73.74374999999966, w1=13.838645273549165\n",
      "Gradient Descent(9408/9999): loss=2.407541016986205, w0=73.85624999999966, w1=13.643079896034985\n",
      "Gradient Descent(9409/9999): loss=1.8599732139699054, w0=74.02499999999966, w1=13.85413035297793\n",
      "Gradient Descent(9410/9999): loss=2.034178519322415, w0=73.79999999999967, w1=13.778364994559428\n",
      "Gradient Descent(9411/9999): loss=2.0764258612176487, w0=73.85624999999968, w1=13.740272357209488\n",
      "Gradient Descent(9412/9999): loss=2.1542115936341855, w0=73.74374999999968, w1=13.733879327875126\n",
      "Gradient Descent(9413/9999): loss=2.457511707279007, w0=73.79999999999968, w1=13.40711109787963\n",
      "Gradient Descent(9414/9999): loss=2.3107700724626454, w0=73.74374999999968, w1=13.674301038200877\n",
      "Gradient Descent(9415/9999): loss=2.3451132026340735, w0=73.68749999999967, w1=13.698719159082343\n",
      "Gradient Descent(9416/9999): loss=2.0394795553581977, w0=73.51874999999967, w1=13.70655961710969\n",
      "Gradient Descent(9417/9999): loss=2.604753034214841, w0=73.57499999999968, w1=13.730224925562228\n",
      "Gradient Descent(9418/9999): loss=2.081456444757132, w0=73.40624999999967, w1=13.512714849782427\n",
      "Gradient Descent(9419/9999): loss=1.965609312959652, w0=73.12499999999967, w1=13.647111904131435\n",
      "Gradient Descent(9420/9999): loss=2.3209091741943255, w0=73.12499999999967, w1=13.782152367225333\n",
      "Gradient Descent(9421/9999): loss=1.96560601897273, w0=73.12499999999967, w1=13.579236232609524\n",
      "Gradient Descent(9422/9999): loss=2.3502221170050945, w0=73.01249999999968, w1=13.454744031664164\n",
      "Gradient Descent(9423/9999): loss=1.8652271603145927, w0=73.01249999999968, w1=13.433056538324438\n",
      "Gradient Descent(9424/9999): loss=2.3090925954269963, w0=73.06874999999968, w1=13.388763118153234\n",
      "Gradient Descent(9425/9999): loss=2.4288517405244336, w0=73.23749999999968, w1=13.598430958786027\n",
      "Gradient Descent(9426/9999): loss=2.266850392257397, w0=73.18124999999968, w1=13.296950985326994\n",
      "Gradient Descent(9427/9999): loss=2.266769734799426, w0=73.18124999999968, w1=13.466289055557757\n",
      "Gradient Descent(9428/9999): loss=2.3730960005487427, w0=73.34999999999968, w1=13.427322705660558\n",
      "Gradient Descent(9429/9999): loss=1.8877086801251293, w0=73.57499999999968, w1=13.482591736568743\n",
      "Gradient Descent(9430/9999): loss=2.7165562936983876, w0=73.85624999999968, w1=13.559350224903527\n",
      "Gradient Descent(9431/9999): loss=2.1731061147032165, w0=73.63124999999968, w1=12.908564674452414\n",
      "Gradient Descent(9432/9999): loss=2.246072252595032, w0=73.46249999999968, w1=13.118127150722685\n",
      "Gradient Descent(9433/9999): loss=2.327594619515091, w0=73.40624999999967, w1=13.119013462175946\n",
      "Gradient Descent(9434/9999): loss=2.6778123428886524, w0=73.46249999999968, w1=13.329476231661168\n",
      "Gradient Descent(9435/9999): loss=1.8800715991361585, w0=73.63124999999968, w1=13.676352282621952\n",
      "Gradient Descent(9436/9999): loss=2.631844063007965, w0=73.63124999999968, w1=13.658293957382673\n",
      "Gradient Descent(9437/9999): loss=2.032062453749433, w0=73.63124999999968, w1=13.432143011619216\n",
      "Gradient Descent(9438/9999): loss=2.2873436563312053, w0=73.74374999999968, w1=13.33318729208322\n",
      "Gradient Descent(9439/9999): loss=1.8182304452584508, w0=74.02499999999968, w1=13.203808329827478\n",
      "Gradient Descent(9440/9999): loss=1.8864777296307262, w0=73.79999999999968, w1=13.196549386581447\n",
      "Gradient Descent(9441/9999): loss=2.2961310678033784, w0=73.57499999999969, w1=12.997285143971435\n",
      "Gradient Descent(9442/9999): loss=1.997744148803533, w0=73.9124999999997, w1=13.062326214853288\n",
      "Gradient Descent(9443/9999): loss=1.6962247098401395, w0=73.85624999999969, w1=13.426799375547999\n",
      "Gradient Descent(9444/9999): loss=1.8889689023477723, w0=73.68749999999969, w1=13.356253915694706\n",
      "Gradient Descent(9445/9999): loss=2.292156450491968, w0=73.57499999999969, w1=12.952843354082006\n",
      "Gradient Descent(9446/9999): loss=2.4714608414647157, w0=73.4624999999997, w1=12.962164671876462\n",
      "Gradient Descent(9447/9999): loss=2.3302427428911665, w0=73.6312499999997, w1=13.011883090340817\n",
      "Gradient Descent(9448/9999): loss=2.0316492705241638, w0=73.57499999999969, w1=12.94908762012699\n",
      "Gradient Descent(9449/9999): loss=2.2875510600988296, w0=73.4624999999997, w1=12.958480161011735\n",
      "Gradient Descent(9450/9999): loss=2.266670872412349, w0=73.12499999999969, w1=13.09481274450353\n",
      "Gradient Descent(9451/9999): loss=2.2135516819104604, w0=73.01249999999969, w1=12.797729414090112\n",
      "Gradient Descent(9452/9999): loss=2.6119021921348264, w0=73.01249999999969, w1=13.149584113094651\n",
      "Gradient Descent(9453/9999): loss=2.1854785150546947, w0=73.29374999999969, w1=13.413013446855405\n",
      "Gradient Descent(9454/9999): loss=2.0504443257766685, w0=73.12499999999969, w1=13.362836899349107\n",
      "Gradient Descent(9455/9999): loss=2.26055104879194, w0=73.29374999999969, w1=13.417394942323659\n",
      "Gradient Descent(9456/9999): loss=1.924744502927742, w0=73.1812499999997, w1=13.84693009913677\n",
      "Gradient Descent(9457/9999): loss=2.3573206553515504, w0=73.3499999999997, w1=13.652304562002717\n",
      "Gradient Descent(9458/9999): loss=2.368162004412002, w0=73.4062499999997, w1=13.720061767183436\n",
      "Gradient Descent(9459/9999): loss=2.213345774703937, w0=73.1249999999997, w1=13.689940310550115\n",
      "Gradient Descent(9460/9999): loss=2.7955650332629602, w0=73.1249999999997, w1=13.719501215957308\n",
      "Gradient Descent(9461/9999): loss=2.217562678013726, w0=73.4062499999997, w1=13.646591096388478\n",
      "Gradient Descent(9462/9999): loss=2.078077061055202, w0=73.3499999999997, w1=13.481214450690505\n",
      "Gradient Descent(9463/9999): loss=1.857196089370505, w0=73.2374999999997, w1=13.619104182123571\n",
      "Gradient Descent(9464/9999): loss=2.770999873826577, w0=73.0687499999997, w1=13.5316991136951\n",
      "Gradient Descent(9465/9999): loss=1.9710439002642515, w0=73.1249999999997, w1=13.467649941884435\n",
      "Gradient Descent(9466/9999): loss=2.2338886392953734, w0=73.0124999999997, w1=13.300389740968091\n",
      "Gradient Descent(9467/9999): loss=2.0839041717109117, w0=72.89999999999971, w1=13.461470068544264\n",
      "Gradient Descent(9468/9999): loss=2.6126103278425763, w0=73.0124999999997, w1=13.749499212981975\n",
      "Gradient Descent(9469/9999): loss=1.9093939101463004, w0=73.18124999999971, w1=13.928014016460454\n",
      "Gradient Descent(9470/9999): loss=2.392371103613772, w0=73.06874999999971, w1=13.93394217490057\n",
      "Gradient Descent(9471/9999): loss=2.403243118040179, w0=73.0124999999997, w1=14.02486060117652\n",
      "Gradient Descent(9472/9999): loss=2.312995255862825, w0=73.2374999999997, w1=13.833410506395662\n",
      "Gradient Descent(9473/9999): loss=2.261002843337283, w0=73.0124999999997, w1=13.484051708471634\n",
      "Gradient Descent(9474/9999): loss=2.2169560317025705, w0=72.9562499999997, w1=13.483239108940325\n",
      "Gradient Descent(9475/9999): loss=2.00645944909126, w0=73.0687499999997, w1=13.726483120417022\n",
      "Gradient Descent(9476/9999): loss=1.850968830344398, w0=73.2374999999997, w1=13.68503771400552\n",
      "Gradient Descent(9477/9999): loss=1.9866587689072004, w0=73.2374999999997, w1=13.417442070092324\n",
      "Gradient Descent(9478/9999): loss=2.2264850904360083, w0=73.2374999999997, w1=13.325500400154455\n",
      "Gradient Descent(9479/9999): loss=2.4234654173198145, w0=73.2937499999997, w1=13.575229657227279\n",
      "Gradient Descent(9480/9999): loss=2.0512191699821085, w0=73.18124999999971, w1=13.740377622904006\n",
      "Gradient Descent(9481/9999): loss=1.6723311865017527, w0=73.1249999999997, w1=13.588807810493469\n",
      "Gradient Descent(9482/9999): loss=1.7826474641592396, w0=73.18124999999971, w1=13.949780514270909\n",
      "Gradient Descent(9483/9999): loss=2.242292198566877, w0=73.34999999999971, w1=13.755720271222875\n",
      "Gradient Descent(9484/9999): loss=2.2706411409729217, w0=73.34999999999971, w1=13.63702340612129\n",
      "Gradient Descent(9485/9999): loss=1.883724466940636, w0=73.34999999999971, w1=13.24979333554292\n",
      "Gradient Descent(9486/9999): loss=2.2636296654781356, w0=73.23749999999971, w1=13.376422525302667\n",
      "Gradient Descent(9487/9999): loss=2.124750268881728, w0=73.34999999999971, w1=13.515612713382332\n",
      "Gradient Descent(9488/9999): loss=1.909763719576133, w0=73.12499999999972, w1=13.568691983418185\n",
      "Gradient Descent(9489/9999): loss=1.8308099881688642, w0=73.18124999999972, w1=13.517807775234461\n",
      "Gradient Descent(9490/9999): loss=2.3288063408155244, w0=72.95624999999973, w1=13.44952289509111\n",
      "Gradient Descent(9491/9999): loss=1.8887253171623333, w0=72.84374999999973, w1=13.320736444558493\n",
      "Gradient Descent(9492/9999): loss=2.6520673565756567, w0=72.89999999999974, w1=13.536197842890763\n",
      "Gradient Descent(9493/9999): loss=1.6604840236889233, w0=72.95624999999974, w1=13.41522895628572\n",
      "Gradient Descent(9494/9999): loss=1.978770390926316, w0=72.89999999999974, w1=13.555093825518167\n",
      "Gradient Descent(9495/9999): loss=2.208681981592335, w0=73.01249999999973, w1=13.527557649148976\n",
      "Gradient Descent(9496/9999): loss=2.148378334284904, w0=73.01249999999973, w1=13.649034330626682\n",
      "Gradient Descent(9497/9999): loss=1.415976927611026, w0=73.06874999999974, w1=13.408590371819244\n",
      "Gradient Descent(9498/9999): loss=2.306172360001081, w0=72.95624999999974, w1=13.526016601358828\n",
      "Gradient Descent(9499/9999): loss=1.7289095866480007, w0=72.95624999999974, w1=13.474689839413141\n",
      "Gradient Descent(9500/9999): loss=2.3898131755425833, w0=72.89999999999974, w1=13.542069934225987\n",
      "Gradient Descent(9501/9999): loss=2.038956922922658, w0=72.61874999999974, w1=13.660820551641939\n",
      "Gradient Descent(9502/9999): loss=2.3115792511643685, w0=72.84374999999973, w1=13.655056933539138\n",
      "Gradient Descent(9503/9999): loss=1.5806164593161718, w0=72.73124999999973, w1=13.550043504507068\n",
      "Gradient Descent(9504/9999): loss=2.0851957336780282, w0=72.84374999999973, w1=13.471990189955733\n",
      "Gradient Descent(9505/9999): loss=2.5442055539773496, w0=72.73124999999973, w1=13.74613731991641\n",
      "Gradient Descent(9506/9999): loss=2.2084171390998826, w0=72.84374999999973, w1=14.027673457312323\n",
      "Gradient Descent(9507/9999): loss=2.278216669224239, w0=73.06874999999972, w1=13.844553877184365\n",
      "Gradient Descent(9508/9999): loss=2.321386186567742, w0=72.95624999999973, w1=13.789352782799881\n",
      "Gradient Descent(9509/9999): loss=2.425149780507805, w0=73.06874999999972, w1=13.942343662647211\n",
      "Gradient Descent(9510/9999): loss=2.5617552789407467, w0=73.06874999999972, w1=13.62728354998714\n",
      "Gradient Descent(9511/9999): loss=2.1650648771205816, w0=73.01249999999972, w1=13.400096904797428\n",
      "Gradient Descent(9512/9999): loss=2.489971869195208, w0=73.12499999999972, w1=13.564435196094447\n",
      "Gradient Descent(9513/9999): loss=2.3151834216684493, w0=73.40624999999972, w1=13.43820401172848\n",
      "Gradient Descent(9514/9999): loss=3.17477069722984, w0=73.23749999999971, w1=13.240587152446679\n",
      "Gradient Descent(9515/9999): loss=2.4490483652567887, w0=73.18124999999971, w1=13.300991027282713\n",
      "Gradient Descent(9516/9999): loss=2.3498554355796513, w0=72.95624999999971, w1=13.528211926843762\n",
      "Gradient Descent(9517/9999): loss=2.1915707444281702, w0=73.01249999999972, w1=13.320346089023863\n",
      "Gradient Descent(9518/9999): loss=1.9100689709141414, w0=73.06874999999972, w1=13.39608167871474\n",
      "Gradient Descent(9519/9999): loss=2.124096962391879, w0=72.95624999999973, w1=13.497488687877867\n",
      "Gradient Descent(9520/9999): loss=2.508642330778987, w0=72.84374999999973, w1=13.760130807452626\n",
      "Gradient Descent(9521/9999): loss=2.182516259197107, w0=72.73124999999973, w1=13.613655239782569\n",
      "Gradient Descent(9522/9999): loss=2.370274631063188, w0=72.67499999999973, w1=13.624895565857344\n",
      "Gradient Descent(9523/9999): loss=2.3073762982626613, w0=72.89999999999972, w1=13.404724459575974\n",
      "Gradient Descent(9524/9999): loss=1.5883625317951118, w0=72.84374999999972, w1=13.70905150701013\n",
      "Gradient Descent(9525/9999): loss=2.5183764088803375, w0=73.06874999999971, w1=13.557849993908594\n",
      "Gradient Descent(9526/9999): loss=2.515997188909926, w0=73.0124999999997, w1=14.104469123247073\n",
      "Gradient Descent(9527/9999): loss=2.4933668600084857, w0=73.06874999999971, w1=14.023422523438724\n",
      "Gradient Descent(9528/9999): loss=2.1577123132114018, w0=73.12499999999972, w1=13.910310083564864\n",
      "Gradient Descent(9529/9999): loss=2.622477024145357, w0=73.23749999999971, w1=13.651898328652246\n",
      "Gradient Descent(9530/9999): loss=2.299570958494162, w0=73.40624999999972, w1=13.935072511059207\n",
      "Gradient Descent(9531/9999): loss=2.780149279058539, w0=73.57499999999972, w1=13.714809548356342\n",
      "Gradient Descent(9532/9999): loss=2.3213839799178606, w0=73.34999999999972, w1=13.541250723193919\n",
      "Gradient Descent(9533/9999): loss=2.0996739144947085, w0=73.29374999999972, w1=13.57097857513417\n",
      "Gradient Descent(9534/9999): loss=1.8255444636587628, w0=73.23749999999971, w1=13.490951386844673\n",
      "Gradient Descent(9535/9999): loss=2.5024638634699468, w0=73.29374999999972, w1=13.42251398187153\n",
      "Gradient Descent(9536/9999): loss=2.4499935262306423, w0=73.23749999999971, w1=13.421039398462678\n",
      "Gradient Descent(9537/9999): loss=1.8799774586841327, w0=73.18124999999971, w1=13.388129421276485\n",
      "Gradient Descent(9538/9999): loss=1.801606186438617, w0=73.46249999999971, w1=13.343711097788834\n",
      "Gradient Descent(9539/9999): loss=3.2438897616745255, w0=73.5749999999997, w1=13.153363743962364\n",
      "Gradient Descent(9540/9999): loss=2.5282883588448986, w0=73.46249999999971, w1=12.81557986433923\n",
      "Gradient Descent(9541/9999): loss=2.2558637484461728, w0=73.4062499999997, w1=13.116999756424642\n",
      "Gradient Descent(9542/9999): loss=1.786274233692918, w0=73.5749999999997, w1=12.996091191256637\n",
      "Gradient Descent(9543/9999): loss=2.223229375004637, w0=73.2374999999997, w1=13.030840244355103\n",
      "Gradient Descent(9544/9999): loss=2.5234996303317914, w0=73.3499999999997, w1=13.189941988535146\n",
      "Gradient Descent(9545/9999): loss=2.595083626182036, w0=73.5187499999997, w1=13.217166984847399\n",
      "Gradient Descent(9546/9999): loss=2.038698995769746, w0=73.5187499999997, w1=13.425616021819103\n",
      "Gradient Descent(9547/9999): loss=2.4077986079816593, w0=73.6312499999997, w1=13.711348342032451\n",
      "Gradient Descent(9548/9999): loss=2.252713737104245, w0=73.4624999999997, w1=13.726015327951206\n",
      "Gradient Descent(9549/9999): loss=1.6267339078895848, w0=73.29374999999969, w1=13.480510656871813\n",
      "Gradient Descent(9550/9999): loss=2.205651392089129, w0=73.29374999999969, w1=13.199222762933879\n",
      "Gradient Descent(9551/9999): loss=2.1810656763483474, w0=73.23749999999968, w1=13.232800054309639\n",
      "Gradient Descent(9552/9999): loss=2.0193788315480434, w0=73.40624999999969, w1=13.326555183989434\n",
      "Gradient Descent(9553/9999): loss=1.9955860792887632, w0=73.91249999999968, w1=13.472532069655339\n",
      "Gradient Descent(9554/9999): loss=2.56087508867377, w0=73.85624999999968, w1=13.336998436932955\n",
      "Gradient Descent(9555/9999): loss=2.2949616930331875, w0=73.74374999999968, w1=13.486911391644506\n",
      "Gradient Descent(9556/9999): loss=1.9101783304727902, w0=73.68749999999967, w1=13.648236187963366\n",
      "Gradient Descent(9557/9999): loss=2.4081553205732256, w0=73.85624999999968, w1=13.486270276564522\n",
      "Gradient Descent(9558/9999): loss=2.1511927956378565, w0=73.79999999999967, w1=13.357042835525393\n",
      "Gradient Descent(9559/9999): loss=2.2584902510900227, w0=73.79999999999967, w1=13.588481383969615\n",
      "Gradient Descent(9560/9999): loss=2.1613975265172924, w0=73.57499999999968, w1=13.847104737629566\n",
      "Gradient Descent(9561/9999): loss=2.183373040686475, w0=73.85624999999968, w1=13.827923099884385\n",
      "Gradient Descent(9562/9999): loss=2.580386835888075, w0=73.91249999999968, w1=13.912424132755683\n",
      "Gradient Descent(9563/9999): loss=2.3100157576546563, w0=73.91249999999968, w1=13.814122318843745\n",
      "Gradient Descent(9564/9999): loss=2.190126104335977, w0=73.91249999999968, w1=13.78437931185872\n",
      "Gradient Descent(9565/9999): loss=1.8915077463006265, w0=74.02499999999968, w1=13.557532371934741\n",
      "Gradient Descent(9566/9999): loss=1.961064738892758, w0=73.96874999999967, w1=13.566627300136208\n",
      "Gradient Descent(9567/9999): loss=2.3086532708185343, w0=73.74374999999968, w1=13.678463714990153\n",
      "Gradient Descent(9568/9999): loss=2.038167207306099, w0=73.79999999999968, w1=13.55389225072763\n",
      "Gradient Descent(9569/9999): loss=2.4488614927061416, w0=73.63124999999968, w1=13.436510185098838\n",
      "Gradient Descent(9570/9999): loss=2.3782653367402173, w0=73.57499999999968, w1=13.616230432192086\n",
      "Gradient Descent(9571/9999): loss=1.7719667700787072, w0=73.68749999999967, w1=13.305490044835821\n",
      "Gradient Descent(9572/9999): loss=2.4858125550238173, w0=73.46249999999968, w1=13.443415615860632\n",
      "Gradient Descent(9573/9999): loss=2.953828350818737, w0=73.34999999999968, w1=13.405824225920217\n",
      "Gradient Descent(9574/9999): loss=2.557647551361342, w0=73.46249999999968, w1=13.430968676338502\n",
      "Gradient Descent(9575/9999): loss=2.309228906259645, w0=73.79999999999968, w1=13.755971275882583\n",
      "Gradient Descent(9576/9999): loss=2.3743460040226365, w0=73.96874999999969, w1=13.312458493806922\n",
      "Gradient Descent(9577/9999): loss=2.341375736848378, w0=73.79999999999968, w1=13.08551499697574\n",
      "Gradient Descent(9578/9999): loss=1.8649319796754005, w0=73.79999999999968, w1=13.131582336980754\n",
      "Gradient Descent(9579/9999): loss=2.4434547520141496, w0=73.63124999999968, w1=13.09808387785293\n",
      "Gradient Descent(9580/9999): loss=2.265303228270331, w0=73.51874999999968, w1=13.045868052612313\n",
      "Gradient Descent(9581/9999): loss=2.322771119307883, w0=73.40624999999969, w1=12.924512466057942\n",
      "Gradient Descent(9582/9999): loss=2.5774863373430446, w0=73.40624999999969, w1=13.329420094009865\n",
      "Gradient Descent(9583/9999): loss=2.4748978706490994, w0=73.51874999999968, w1=13.416005818592781\n",
      "Gradient Descent(9584/9999): loss=2.4799409718579097, w0=73.46249999999968, w1=13.449524029976024\n",
      "Gradient Descent(9585/9999): loss=2.2352746799916057, w0=73.23749999999968, w1=13.611441230911913\n",
      "Gradient Descent(9586/9999): loss=2.1059184171206646, w0=73.29374999999969, w1=13.62219291777547\n",
      "Gradient Descent(9587/9999): loss=1.8244490171085486, w0=73.29374999999969, w1=13.597593850631524\n",
      "Gradient Descent(9588/9999): loss=2.5366219599484285, w0=73.6312499999997, w1=13.630895629102698\n",
      "Gradient Descent(9589/9999): loss=2.0059621239947436, w0=73.4624999999997, w1=13.688965758854337\n",
      "Gradient Descent(9590/9999): loss=2.4432731097114844, w0=73.5187499999997, w1=13.587292265982454\n",
      "Gradient Descent(9591/9999): loss=2.1018147994493366, w0=73.5187499999997, w1=13.709263798892419\n",
      "Gradient Descent(9592/9999): loss=2.460802726195297, w0=73.4624999999997, w1=13.83756457820905\n",
      "Gradient Descent(9593/9999): loss=1.8282621878846312, w0=73.5187499999997, w1=13.903858625827123\n",
      "Gradient Descent(9594/9999): loss=2.0796771638917386, w0=73.3499999999997, w1=13.735076617794572\n",
      "Gradient Descent(9595/9999): loss=2.8110577980398226, w0=73.2374999999997, w1=13.6103683148448\n",
      "Gradient Descent(9596/9999): loss=2.3887693558082335, w0=73.2374999999997, w1=13.618590434327869\n",
      "Gradient Descent(9597/9999): loss=2.633971609191944, w0=73.3499999999997, w1=13.950202490243772\n",
      "Gradient Descent(9598/9999): loss=2.6174245718832108, w0=73.4062499999997, w1=14.046281965047628\n",
      "Gradient Descent(9599/9999): loss=1.9393417168576157, w0=73.4062499999997, w1=13.92066210652787\n",
      "Gradient Descent(9600/9999): loss=2.188859226235734, w0=73.18124999999971, w1=13.789493521963866\n",
      "Gradient Descent(9601/9999): loss=2.0443725323191635, w0=73.18124999999971, w1=13.718546580599252\n",
      "Gradient Descent(9602/9999): loss=2.1010783619374367, w0=73.51874999999971, w1=13.637935335463164\n",
      "Gradient Descent(9603/9999): loss=2.3195748062676684, w0=73.51874999999971, w1=13.343344462735919\n",
      "Gradient Descent(9604/9999): loss=2.1214255064976912, w0=73.57499999999972, w1=13.366386402749871\n",
      "Gradient Descent(9605/9999): loss=2.3623484464734483, w0=73.23749999999971, w1=13.468538407426978\n",
      "Gradient Descent(9606/9999): loss=1.7702979129188265, w0=73.06874999999971, w1=13.29567193448116\n",
      "Gradient Descent(9607/9999): loss=1.9361643017835333, w0=73.23749999999971, w1=13.169349849064167\n",
      "Gradient Descent(9608/9999): loss=2.031638160889199, w0=73.29374999999972, w1=13.400140177841458\n",
      "Gradient Descent(9609/9999): loss=2.420813720235034, w0=73.23749999999971, w1=13.380407879076577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9610/9999): loss=2.37460077491548, w0=73.12499999999972, w1=13.366407191499825\n",
      "Gradient Descent(9611/9999): loss=2.2572622293391937, w0=73.18124999999972, w1=13.27588106967585\n",
      "Gradient Descent(9612/9999): loss=2.3914200601756646, w0=73.06874999999972, w1=13.359934901987778\n",
      "Gradient Descent(9613/9999): loss=1.9012260007381945, w0=73.06874999999972, w1=13.156908122941301\n",
      "Gradient Descent(9614/9999): loss=1.8116927117291364, w0=72.84374999999973, w1=13.22195422032082\n",
      "Gradient Descent(9615/9999): loss=2.432944754634562, w0=73.06874999999972, w1=13.31809881316178\n",
      "Gradient Descent(9616/9999): loss=2.188603897880247, w0=73.12499999999973, w1=13.380607104454917\n",
      "Gradient Descent(9617/9999): loss=1.6656707587087232, w0=72.95624999999973, w1=13.295838938184035\n",
      "Gradient Descent(9618/9999): loss=2.1266135748617843, w0=73.01249999999973, w1=13.34001339991091\n",
      "Gradient Descent(9619/9999): loss=2.1791339168794464, w0=73.01249999999973, w1=13.024638982542085\n",
      "Gradient Descent(9620/9999): loss=1.7762092616340126, w0=73.01249999999973, w1=13.264844584942365\n",
      "Gradient Descent(9621/9999): loss=2.298572710908293, w0=73.18124999999974, w1=13.313362776045668\n",
      "Gradient Descent(9622/9999): loss=1.95124237378265, w0=73.29374999999973, w1=13.394856321452128\n",
      "Gradient Descent(9623/9999): loss=1.9657740711300087, w0=73.34999999999974, w1=13.438269059645085\n",
      "Gradient Descent(9624/9999): loss=2.37833774629513, w0=73.12499999999974, w1=13.280524751643688\n",
      "Gradient Descent(9625/9999): loss=2.5284378492898476, w0=73.34999999999974, w1=13.218223058046217\n",
      "Gradient Descent(9626/9999): loss=1.9841632501698774, w0=73.51874999999974, w1=13.24153821202664\n",
      "Gradient Descent(9627/9999): loss=2.1784366137082736, w0=73.91249999999974, w1=13.543526648387747\n",
      "Gradient Descent(9628/9999): loss=1.6407663191423065, w0=74.08124999999974, w1=13.650527016389571\n",
      "Gradient Descent(9629/9999): loss=2.202125538677598, w0=73.74374999999974, w1=13.749852076207263\n",
      "Gradient Descent(9630/9999): loss=1.9783325001598102, w0=73.51874999999974, w1=13.670481111109135\n",
      "Gradient Descent(9631/9999): loss=2.3206065604369934, w0=73.40624999999974, w1=13.118155025999673\n",
      "Gradient Descent(9632/9999): loss=2.5545076823172446, w0=73.29374999999975, w1=13.404220045970343\n",
      "Gradient Descent(9633/9999): loss=1.9257151848979859, w0=73.01249999999975, w1=13.252397789836097\n",
      "Gradient Descent(9634/9999): loss=2.4033024488794665, w0=73.01249999999975, w1=13.350364108306254\n",
      "Gradient Descent(9635/9999): loss=2.9370935950531347, w0=73.18124999999975, w1=13.439858881506275\n",
      "Gradient Descent(9636/9999): loss=2.1175196979679534, w0=73.12499999999974, w1=13.372530714499197\n",
      "Gradient Descent(9637/9999): loss=1.8804032587485469, w0=73.18124999999975, w1=13.553372881234612\n",
      "Gradient Descent(9638/9999): loss=2.292182457314817, w0=73.12499999999974, w1=13.527127411661992\n",
      "Gradient Descent(9639/9999): loss=2.1151560010584896, w0=73.12499999999974, w1=13.414160208810568\n",
      "Gradient Descent(9640/9999): loss=1.9262263882368489, w0=73.34999999999974, w1=13.214341701656974\n",
      "Gradient Descent(9641/9999): loss=1.9428631142332748, w0=73.18124999999974, w1=13.426363963561505\n",
      "Gradient Descent(9642/9999): loss=2.317257917908244, w0=73.01249999999973, w1=13.307591037602222\n",
      "Gradient Descent(9643/9999): loss=1.828384425790508, w0=73.12499999999973, w1=13.30657632026776\n",
      "Gradient Descent(9644/9999): loss=1.7208048174867676, w0=73.12499999999973, w1=13.474687233233894\n",
      "Gradient Descent(9645/9999): loss=1.8473935338609477, w0=73.23749999999973, w1=13.59761377835406\n",
      "Gradient Descent(9646/9999): loss=1.7407290786833678, w0=73.34999999999972, w1=13.427048702596382\n",
      "Gradient Descent(9647/9999): loss=2.4555642125921158, w0=73.40624999999973, w1=13.62977446470266\n",
      "Gradient Descent(9648/9999): loss=2.1888217640543375, w0=73.51874999999973, w1=13.720325198582577\n",
      "Gradient Descent(9649/9999): loss=1.7881046961712608, w0=73.46249999999972, w1=13.678728737131607\n",
      "Gradient Descent(9650/9999): loss=2.3550867722860303, w0=73.29374999999972, w1=13.646277055867902\n",
      "Gradient Descent(9651/9999): loss=2.7297539463992884, w0=73.23749999999971, w1=13.886671661493182\n",
      "Gradient Descent(9652/9999): loss=2.296029891826992, w0=73.18124999999971, w1=13.855859702155152\n",
      "Gradient Descent(9653/9999): loss=2.4701764778641717, w0=73.23749999999971, w1=13.415701498703285\n",
      "Gradient Descent(9654/9999): loss=2.2861663906454384, w0=73.46249999999971, w1=13.336302529972686\n",
      "Gradient Descent(9655/9999): loss=2.3029549255473762, w0=73.74374999999971, w1=13.226548746253245\n",
      "Gradient Descent(9656/9999): loss=2.291400767876426, w0=73.74374999999971, w1=13.237306415036658\n",
      "Gradient Descent(9657/9999): loss=2.1077369394358403, w0=73.79999999999971, w1=13.077866466180764\n",
      "Gradient Descent(9658/9999): loss=2.182193622625122, w0=73.79999999999971, w1=13.13665733971783\n",
      "Gradient Descent(9659/9999): loss=2.00144076039836, w0=74.08124999999971, w1=13.233377928057328\n",
      "Gradient Descent(9660/9999): loss=2.0712257749480445, w0=73.96874999999972, w1=13.512084368008296\n",
      "Gradient Descent(9661/9999): loss=2.019804419905623, w0=74.02499999999972, w1=13.565607913652395\n",
      "Gradient Descent(9662/9999): loss=2.2912053430125443, w0=73.68749999999972, w1=13.729229627726461\n",
      "Gradient Descent(9663/9999): loss=1.859135237462699, w0=73.57499999999972, w1=13.770324664136345\n",
      "Gradient Descent(9664/9999): loss=2.2765183416319204, w0=73.46249999999972, w1=13.805010345902343\n",
      "Gradient Descent(9665/9999): loss=1.8148752265711146, w0=73.51874999999973, w1=13.806911181957057\n",
      "Gradient Descent(9666/9999): loss=2.0036250620639793, w0=73.96874999999973, w1=13.716413240236555\n",
      "Gradient Descent(9667/9999): loss=2.1119020114724774, w0=73.68749999999973, w1=13.764930959295526\n",
      "Gradient Descent(9668/9999): loss=2.2212211862835933, w0=73.51874999999973, w1=13.874374332550671\n",
      "Gradient Descent(9669/9999): loss=2.0769848043187444, w0=73.40624999999973, w1=13.716282584485931\n",
      "Gradient Descent(9670/9999): loss=2.059873373926923, w0=73.34999999999972, w1=13.513774312738695\n",
      "Gradient Descent(9671/9999): loss=2.1702558566373815, w0=73.12499999999973, w1=13.366601116164508\n",
      "Gradient Descent(9672/9999): loss=2.6026446346551797, w0=73.23749999999973, w1=13.354964175836157\n",
      "Gradient Descent(9673/9999): loss=1.8831929073914615, w0=73.12499999999973, w1=13.281023874773004\n",
      "Gradient Descent(9674/9999): loss=2.01303091247442, w0=73.06874999999972, w1=13.227956526884261\n",
      "Gradient Descent(9675/9999): loss=2.2181327146118317, w0=73.18124999999972, w1=13.241528063941086\n",
      "Gradient Descent(9676/9999): loss=2.1393697400935867, w0=73.23749999999973, w1=12.89591593325126\n",
      "Gradient Descent(9677/9999): loss=2.5222478053909145, w0=73.34999999999972, w1=12.934093043887273\n",
      "Gradient Descent(9678/9999): loss=1.665595926118733, w0=73.18124999999972, w1=13.098986202399548\n",
      "Gradient Descent(9679/9999): loss=2.011643153383005, w0=73.12499999999972, w1=13.31731579992426\n",
      "Gradient Descent(9680/9999): loss=2.0065439255824575, w0=73.29374999999972, w1=13.243344961035962\n",
      "Gradient Descent(9681/9999): loss=2.20526841397085, w0=73.06874999999972, w1=13.376012911095215\n",
      "Gradient Descent(9682/9999): loss=2.5065075731028674, w0=73.06874999999972, w1=13.103266000979247\n",
      "Gradient Descent(9683/9999): loss=2.4961665697983246, w0=73.01249999999972, w1=12.830563425402898\n",
      "Gradient Descent(9684/9999): loss=2.806492181030799, w0=73.06874999999972, w1=12.866877865031782\n",
      "Gradient Descent(9685/9999): loss=2.5645657907399775, w0=72.78749999999972, w1=12.782025892609143\n",
      "Gradient Descent(9686/9999): loss=2.1771661412841183, w0=72.73124999999972, w1=12.922266821147554\n",
      "Gradient Descent(9687/9999): loss=2.5198889818523575, w0=72.78749999999972, w1=12.987081412875952\n",
      "Gradient Descent(9688/9999): loss=1.920106752210195, w0=72.84374999999973, w1=13.112573684487028\n",
      "Gradient Descent(9689/9999): loss=2.590206183308424, w0=72.73124999999973, w1=13.258375297141987\n",
      "Gradient Descent(9690/9999): loss=2.392376308146152, w0=73.01249999999973, w1=13.382774363390407\n",
      "Gradient Descent(9691/9999): loss=2.256225387343027, w0=73.12499999999973, w1=13.536691335088447\n",
      "Gradient Descent(9692/9999): loss=1.5236499309455094, w0=73.06874999999972, w1=13.267438661971582\n",
      "Gradient Descent(9693/9999): loss=1.9602638099696739, w0=72.95624999999973, w1=13.662409550152708\n",
      "Gradient Descent(9694/9999): loss=1.9953671602434557, w0=72.89999999999972, w1=13.789431749950538\n",
      "Gradient Descent(9695/9999): loss=1.8624955012351208, w0=73.34999999999972, w1=13.736954466297002\n",
      "Gradient Descent(9696/9999): loss=1.764355338210585, w0=73.34999999999972, w1=13.92378406956306\n",
      "Gradient Descent(9697/9999): loss=2.118853069318778, w0=73.23749999999973, w1=13.73851653603274\n",
      "Gradient Descent(9698/9999): loss=2.4971425547459463, w0=73.40624999999973, w1=13.385883779730122\n",
      "Gradient Descent(9699/9999): loss=2.0120105569676845, w0=73.23749999999973, w1=13.721019163191421\n",
      "Gradient Descent(9700/9999): loss=2.1459196606860944, w0=73.34999999999972, w1=13.888339875343428\n",
      "Gradient Descent(9701/9999): loss=1.8916091645450952, w0=73.23749999999973, w1=14.011923749851862\n",
      "Gradient Descent(9702/9999): loss=1.9106644412094644, w0=72.95624999999973, w1=13.80401986105777\n",
      "Gradient Descent(9703/9999): loss=2.270901833524626, w0=72.67499999999973, w1=13.99725500923142\n",
      "Gradient Descent(9704/9999): loss=1.8368590219687932, w0=72.84374999999973, w1=14.020687542645677\n",
      "Gradient Descent(9705/9999): loss=1.73771805062804, w0=72.78749999999972, w1=13.895087580462732\n",
      "Gradient Descent(9706/9999): loss=2.0001159022913266, w0=72.73124999999972, w1=14.079960900005464\n",
      "Gradient Descent(9707/9999): loss=2.3216909215536887, w0=73.01249999999972, w1=13.937538769810583\n",
      "Gradient Descent(9708/9999): loss=2.281830975248197, w0=73.18124999999972, w1=13.838940139092044\n",
      "Gradient Descent(9709/9999): loss=1.8495009093786534, w0=73.34999999999972, w1=14.10948101328653\n",
      "Gradient Descent(9710/9999): loss=2.0832504010950403, w0=73.12499999999973, w1=14.174723553193871\n",
      "Gradient Descent(9711/9999): loss=2.6629923438429577, w0=73.29374999999973, w1=14.319312593415306\n",
      "Gradient Descent(9712/9999): loss=2.218620971933235, w0=73.40624999999973, w1=14.189506747834844\n",
      "Gradient Descent(9713/9999): loss=2.248556641691967, w0=73.34999999999972, w1=14.057917050848657\n",
      "Gradient Descent(9714/9999): loss=1.775011571949056, w0=73.34999999999972, w1=14.39543616827942\n",
      "Gradient Descent(9715/9999): loss=2.6099522272078746, w0=73.57499999999972, w1=14.197313582013226\n",
      "Gradient Descent(9716/9999): loss=1.9423042625049511, w0=73.40624999999972, w1=14.148117318560123\n",
      "Gradient Descent(9717/9999): loss=2.267683550011701, w0=73.40624999999972, w1=13.843845723450825\n",
      "Gradient Descent(9718/9999): loss=2.2065249804092897, w0=73.51874999999971, w1=13.873953146434307\n",
      "Gradient Descent(9719/9999): loss=2.910242936926334, w0=73.57499999999972, w1=13.895723390414046\n",
      "Gradient Descent(9720/9999): loss=1.88557576845489, w0=73.57499999999972, w1=13.702391445596732\n",
      "Gradient Descent(9721/9999): loss=1.9210951295589873, w0=73.40624999999972, w1=13.52519837335335\n",
      "Gradient Descent(9722/9999): loss=1.8802440439371007, w0=73.34999999999971, w1=13.57789521048992\n",
      "Gradient Descent(9723/9999): loss=2.3143418281906123, w0=73.18124999999971, w1=13.36697588724235\n",
      "Gradient Descent(9724/9999): loss=2.1604024431865305, w0=73.34999999999971, w1=13.036561004853096\n",
      "Gradient Descent(9725/9999): loss=2.2590542698593747, w0=73.2937499999997, w1=13.140857764270782\n",
      "Gradient Descent(9726/9999): loss=2.2257709791784377, w0=73.06874999999971, w1=13.42719895722452\n",
      "Gradient Descent(9727/9999): loss=2.2588051086025818, w0=73.06874999999971, w1=13.589703337177236\n",
      "Gradient Descent(9728/9999): loss=2.3619764437267206, w0=73.12499999999972, w1=13.659506015239556\n",
      "Gradient Descent(9729/9999): loss=2.045167982782021, w0=73.29374999999972, w1=13.504750856693935\n",
      "Gradient Descent(9730/9999): loss=2.2645930835023194, w0=73.51874999999971, w1=13.820285451239412\n",
      "Gradient Descent(9731/9999): loss=2.42783070592243, w0=73.40624999999972, w1=13.726639654954447\n",
      "Gradient Descent(9732/9999): loss=2.563268184213793, w0=73.68749999999972, w1=13.66898011400139\n",
      "Gradient Descent(9733/9999): loss=2.2467491798825137, w0=73.46249999999972, w1=13.767875417860926\n",
      "Gradient Descent(9734/9999): loss=2.7424955443532166, w0=73.57499999999972, w1=13.774603552620178\n",
      "Gradient Descent(9735/9999): loss=2.032153824948658, w0=73.85624999999972, w1=13.593286880001145\n",
      "Gradient Descent(9736/9999): loss=2.222464854672887, w0=73.63124999999972, w1=13.612521883500923\n",
      "Gradient Descent(9737/9999): loss=2.511768416753732, w0=73.40624999999973, w1=13.646102434453587\n",
      "Gradient Descent(9738/9999): loss=2.198810440654769, w0=73.29374999999973, w1=13.595900466539295\n",
      "Gradient Descent(9739/9999): loss=2.092830798110011, w0=73.29374999999973, w1=13.331133223943848\n",
      "Gradient Descent(9740/9999): loss=1.6664056926608584, w0=73.40624999999973, w1=13.391819606544965\n",
      "Gradient Descent(9741/9999): loss=2.5899367006756835, w0=73.34999999999972, w1=13.76654960545089\n",
      "Gradient Descent(9742/9999): loss=2.007542126119379, w0=73.29374999999972, w1=13.904537300734347\n",
      "Gradient Descent(9743/9999): loss=2.725966024387849, w0=73.34999999999972, w1=13.79980443365037\n",
      "Gradient Descent(9744/9999): loss=2.1553641833424493, w0=73.23749999999973, w1=13.516947393983799\n",
      "Gradient Descent(9745/9999): loss=2.090030725076147, w0=73.01249999999973, w1=13.616404074164443\n",
      "Gradient Descent(9746/9999): loss=2.7190184153991233, w0=73.29374999999973, w1=13.916070212507915\n",
      "Gradient Descent(9747/9999): loss=1.8239229084032982, w0=73.34999999999974, w1=13.804568147955594\n",
      "Gradient Descent(9748/9999): loss=2.0295545757850713, w0=73.06874999999974, w1=13.916356554796783\n",
      "Gradient Descent(9749/9999): loss=2.1033843518273843, w0=73.23749999999974, w1=13.798515681280483\n",
      "Gradient Descent(9750/9999): loss=1.747348961067135, w0=73.29374999999975, w1=13.955251796862175\n",
      "Gradient Descent(9751/9999): loss=2.3169359661074127, w0=73.40624999999974, w1=13.91142513967023\n",
      "Gradient Descent(9752/9999): loss=2.432594038267957, w0=73.12499999999974, w1=13.89663930161302\n",
      "Gradient Descent(9753/9999): loss=1.8140938712377508, w0=73.12499999999974, w1=13.75489766364198\n",
      "Gradient Descent(9754/9999): loss=2.7759607646327984, w0=72.95624999999974, w1=13.322899622188887\n",
      "Gradient Descent(9755/9999): loss=2.0042376538805415, w0=72.67499999999974, w1=13.483504775529186\n",
      "Gradient Descent(9756/9999): loss=2.2552290489853037, w0=72.67499999999974, w1=13.630370366629444\n",
      "Gradient Descent(9757/9999): loss=2.5627826872580597, w0=72.56249999999974, w1=13.751673915979715\n",
      "Gradient Descent(9758/9999): loss=1.9931677370968406, w0=72.61874999999975, w1=13.965653342064632\n",
      "Gradient Descent(9759/9999): loss=2.717133906096624, w0=73.06874999999975, w1=13.970470052337237\n",
      "Gradient Descent(9760/9999): loss=2.1810230794289582, w0=73.01249999999975, w1=13.902378176858132\n",
      "Gradient Descent(9761/9999): loss=2.4421374621960403, w0=73.34999999999975, w1=13.774646652934694\n",
      "Gradient Descent(9762/9999): loss=2.0198116331458458, w0=73.29374999999975, w1=13.673137319387518\n",
      "Gradient Descent(9763/9999): loss=2.245088869174348, w0=72.95624999999974, w1=13.861814030921787\n",
      "Gradient Descent(9764/9999): loss=1.894200789128897, w0=72.73124999999975, w1=13.753774222646447\n",
      "Gradient Descent(9765/9999): loss=2.0944427603480036, w0=72.67499999999974, w1=13.828969813989485\n",
      "Gradient Descent(9766/9999): loss=2.0518612213270124, w0=72.61874999999974, w1=13.854481303296577\n",
      "Gradient Descent(9767/9999): loss=2.1510646019314184, w0=72.61874999999974, w1=13.826796311038994\n",
      "Gradient Descent(9768/9999): loss=2.273397054931296, w0=72.84374999999973, w1=14.099273023533495\n",
      "Gradient Descent(9769/9999): loss=2.2645635856654844, w0=72.61874999999974, w1=14.100463500590578\n",
      "Gradient Descent(9770/9999): loss=2.280025199624162, w0=72.95624999999974, w1=13.895344339113455\n",
      "Gradient Descent(9771/9999): loss=2.220284197687101, w0=73.18124999999974, w1=13.921567695394526\n",
      "Gradient Descent(9772/9999): loss=2.389632195205042, w0=72.89999999999974, w1=14.021268941180189\n",
      "Gradient Descent(9773/9999): loss=1.8689687030815734, w0=72.89999999999974, w1=14.080778628163584\n",
      "Gradient Descent(9774/9999): loss=2.2003178690260343, w0=72.89999999999974, w1=13.6547124187322\n",
      "Gradient Descent(9775/9999): loss=2.141789945281218, w0=72.78749999999974, w1=13.60025408920385\n",
      "Gradient Descent(9776/9999): loss=2.2767910535557903, w0=72.89999999999974, w1=13.73375203776935\n",
      "Gradient Descent(9777/9999): loss=2.071476819130388, w0=72.89999999999974, w1=13.61268118912679\n",
      "Gradient Descent(9778/9999): loss=2.400590672204539, w0=73.01249999999973, w1=13.743217307815204\n",
      "Gradient Descent(9779/9999): loss=2.375111597854699, w0=72.84374999999973, w1=13.956487292624562\n",
      "Gradient Descent(9780/9999): loss=1.9670916135976428, w0=73.18124999999974, w1=14.187363345358651\n",
      "Gradient Descent(9781/9999): loss=2.2208469360550103, w0=73.23749999999974, w1=14.080426776540826\n",
      "Gradient Descent(9782/9999): loss=2.13536415345599, w0=73.29374999999975, w1=14.137334289332575\n",
      "Gradient Descent(9783/9999): loss=2.259297073055077, w0=73.40624999999974, w1=13.959567099385232\n",
      "Gradient Descent(9784/9999): loss=2.5529116914384105, w0=73.40624999999974, w1=13.54903513847156\n",
      "Gradient Descent(9785/9999): loss=1.800148566087642, w0=73.23749999999974, w1=13.489852870264015\n",
      "Gradient Descent(9786/9999): loss=1.8925555706620218, w0=73.18124999999974, w1=13.166883510985446\n",
      "Gradient Descent(9787/9999): loss=2.440640396680706, w0=73.06874999999974, w1=13.147329573955947\n",
      "Gradient Descent(9788/9999): loss=2.7828164693860096, w0=73.01249999999973, w1=13.395794172761587\n",
      "Gradient Descent(9789/9999): loss=2.0794136478926535, w0=73.12499999999973, w1=13.602123140537726\n",
      "Gradient Descent(9790/9999): loss=2.090318760736735, w0=73.18124999999974, w1=13.495910308071405\n",
      "Gradient Descent(9791/9999): loss=1.9326444069528512, w0=72.89999999999974, w1=13.666497187962921\n",
      "Gradient Descent(9792/9999): loss=2.320296273190216, w0=72.89999999999974, w1=13.682758775011475\n",
      "Gradient Descent(9793/9999): loss=2.4545133319776715, w0=72.89999999999974, w1=13.548208177652828\n",
      "Gradient Descent(9794/9999): loss=1.8341987952943328, w0=72.84374999999973, w1=13.327135426411035\n",
      "Gradient Descent(9795/9999): loss=1.8352354639164496, w0=72.67499999999973, w1=13.336373696701074\n",
      "Gradient Descent(9796/9999): loss=1.9390460046957079, w0=72.50624999999972, w1=13.449673097944595\n",
      "Gradient Descent(9797/9999): loss=1.984515209713611, w0=72.28124999999973, w1=13.46513530131547\n",
      "Gradient Descent(9798/9999): loss=2.2626355491016463, w0=72.44999999999973, w1=13.456132781916237\n",
      "Gradient Descent(9799/9999): loss=2.7130170252493473, w0=72.33749999999974, w1=13.486289791660091\n",
      "Gradient Descent(9800/9999): loss=1.9966720847457093, w0=72.56249999999973, w1=13.35699359206793\n",
      "Gradient Descent(9801/9999): loss=2.348421941164151, w0=72.44999999999973, w1=13.499257423872884\n",
      "Gradient Descent(9802/9999): loss=2.3092322760178474, w0=72.73124999999973, w1=13.265264177593975\n",
      "Gradient Descent(9803/9999): loss=2.1847225660555774, w0=73.01249999999973, w1=13.210539067917585\n",
      "Gradient Descent(9804/9999): loss=2.507094648664883, w0=73.06874999999974, w1=13.000948059956276\n",
      "Gradient Descent(9805/9999): loss=1.3569934399802241, w0=73.06874999999974, w1=13.084396909905566\n",
      "Gradient Descent(9806/9999): loss=2.1736126124942703, w0=73.18124999999974, w1=13.017716180918663\n",
      "Gradient Descent(9807/9999): loss=2.063469236915945, w0=73.18124999999974, w1=12.909684503945961\n",
      "Gradient Descent(9808/9999): loss=2.064528726344683, w0=73.34999999999974, w1=12.633403821024489\n",
      "Gradient Descent(9809/9999): loss=1.9392774311938605, w0=73.34999999999974, w1=12.846710528326216\n",
      "Gradient Descent(9810/9999): loss=2.162152882543639, w0=73.51874999999974, w1=13.153904801040802\n",
      "Gradient Descent(9811/9999): loss=2.3136899757729745, w0=73.40624999999974, w1=13.085300473583262\n",
      "Gradient Descent(9812/9999): loss=2.270045506065774, w0=73.23749999999974, w1=13.063727320509601\n",
      "Gradient Descent(9813/9999): loss=1.9311928512394732, w0=73.06874999999974, w1=13.132822296284841\n",
      "Gradient Descent(9814/9999): loss=2.308788930076232, w0=73.12499999999974, w1=12.937135564106812\n",
      "Gradient Descent(9815/9999): loss=2.334162760593082, w0=72.84374999999974, w1=12.960090686555821\n",
      "Gradient Descent(9816/9999): loss=2.2073134278441766, w0=73.06874999999974, w1=12.777412456631202\n",
      "Gradient Descent(9817/9999): loss=2.335942535113118, w0=72.89999999999974, w1=12.888255525842462\n",
      "Gradient Descent(9818/9999): loss=2.182305195966772, w0=72.95624999999974, w1=12.987681664621231\n",
      "Gradient Descent(9819/9999): loss=2.552189956172576, w0=73.06874999999974, w1=13.337839721646011\n",
      "Gradient Descent(9820/9999): loss=1.5570253505858735, w0=72.95624999999974, w1=13.409713655320385\n",
      "Gradient Descent(9821/9999): loss=2.826379906885637, w0=73.12499999999974, w1=13.546917681659252\n",
      "Gradient Descent(9822/9999): loss=2.19018003964014, w0=73.01249999999975, w1=13.508667823171622\n",
      "Gradient Descent(9823/9999): loss=2.4773759019693364, w0=73.06874999999975, w1=13.29408320100691\n",
      "Gradient Descent(9824/9999): loss=2.2326620571109554, w0=73.12499999999976, w1=13.477716689897829\n",
      "Gradient Descent(9825/9999): loss=2.0785283310227056, w0=73.12499999999976, w1=13.813737303983514\n",
      "Gradient Descent(9826/9999): loss=2.0136694201485126, w0=73.51874999999976, w1=13.982730247308458\n",
      "Gradient Descent(9827/9999): loss=2.1052342681889984, w0=73.40624999999976, w1=13.981948689810128\n",
      "Gradient Descent(9828/9999): loss=1.9301992982248097, w0=73.29374999999976, w1=13.89580963215044\n",
      "Gradient Descent(9829/9999): loss=1.9496258086608582, w0=73.23749999999976, w1=13.608470175034478\n",
      "Gradient Descent(9830/9999): loss=1.9055036588140173, w0=73.46249999999975, w1=13.499837864114056\n",
      "Gradient Descent(9831/9999): loss=2.169479177176695, w0=73.29374999999975, w1=13.55688012195536\n",
      "Gradient Descent(9832/9999): loss=2.4841221599890524, w0=73.29374999999975, w1=13.4228197941206\n",
      "Gradient Descent(9833/9999): loss=1.932044617523903, w0=73.63124999999975, w1=13.674597661014031\n",
      "Gradient Descent(9834/9999): loss=2.429785882893648, w0=73.51874999999976, w1=13.794865408831862\n",
      "Gradient Descent(9835/9999): loss=2.857353736872594, w0=73.51874999999976, w1=13.834468543760421\n",
      "Gradient Descent(9836/9999): loss=2.3230354978542724, w0=73.51874999999976, w1=13.933357804339057\n",
      "Gradient Descent(9837/9999): loss=1.8489799349744704, w0=73.63124999999975, w1=13.919644093873872\n",
      "Gradient Descent(9838/9999): loss=1.9997296226966097, w0=73.63124999999975, w1=13.96197303385906\n",
      "Gradient Descent(9839/9999): loss=2.229880349379716, w0=73.57499999999975, w1=13.923526727032442\n",
      "Gradient Descent(9840/9999): loss=1.32051638268781, w0=73.46249999999975, w1=14.111125939513801\n",
      "Gradient Descent(9841/9999): loss=2.487634462712008, w0=73.34999999999975, w1=14.054931175842263\n",
      "Gradient Descent(9842/9999): loss=2.194443689873301, w0=73.46249999999975, w1=13.962707073477958\n",
      "Gradient Descent(9843/9999): loss=2.4453014575198306, w0=73.29374999999975, w1=13.804412234826485\n",
      "Gradient Descent(9844/9999): loss=2.320757323068071, w0=73.34999999999975, w1=13.942948956516428\n",
      "Gradient Descent(9845/9999): loss=2.626770155104231, w0=73.29374999999975, w1=13.788474985132405\n",
      "Gradient Descent(9846/9999): loss=2.2971533846323995, w0=73.23749999999974, w1=13.704869978829922\n",
      "Gradient Descent(9847/9999): loss=2.6115743393597444, w0=73.40624999999974, w1=13.672166642935201\n",
      "Gradient Descent(9848/9999): loss=2.4726561893753667, w0=73.40624999999974, w1=13.444444094374068\n",
      "Gradient Descent(9849/9999): loss=1.8930287059638424, w0=73.57499999999975, w1=13.162203981534981\n",
      "Gradient Descent(9850/9999): loss=1.9674916635768995, w0=73.74374999999975, w1=13.271016884939149\n",
      "Gradient Descent(9851/9999): loss=2.0475710196009347, w0=73.85624999999975, w1=13.271371691361844\n",
      "Gradient Descent(9852/9999): loss=1.958692335313784, w0=73.74374999999975, w1=13.248718201575858\n",
      "Gradient Descent(9853/9999): loss=2.634084336156148, w0=73.57499999999975, w1=13.303728218035436\n",
      "Gradient Descent(9854/9999): loss=2.406600134562162, w0=73.63124999999975, w1=13.51864939942015\n",
      "Gradient Descent(9855/9999): loss=2.010189691022029, w0=73.46249999999975, w1=13.07597565821313\n",
      "Gradient Descent(9856/9999): loss=2.4039509705612896, w0=73.51874999999976, w1=13.11394194007753\n",
      "Gradient Descent(9857/9999): loss=1.8112416600268704, w0=73.51874999999976, w1=13.108134157840528\n",
      "Gradient Descent(9858/9999): loss=2.799430671832486, w0=73.29374999999976, w1=13.010907649213122\n",
      "Gradient Descent(9859/9999): loss=2.3775261947221553, w0=73.40624999999976, w1=13.064135279102615\n",
      "Gradient Descent(9860/9999): loss=1.9180985350208777, w0=73.40624999999976, w1=13.122648312583172\n",
      "Gradient Descent(9861/9999): loss=2.320770624367736, w0=72.89999999999976, w1=13.289020079952186\n",
      "Gradient Descent(9862/9999): loss=2.3573244193574436, w0=72.89999999999976, w1=13.261064661056405\n",
      "Gradient Descent(9863/9999): loss=2.269864798024194, w0=72.84374999999976, w1=13.196440131992874\n",
      "Gradient Descent(9864/9999): loss=2.253737383976215, w0=73.12499999999976, w1=13.187302546507674\n",
      "Gradient Descent(9865/9999): loss=1.9037430995273665, w0=73.06874999999975, w1=13.114339030951347\n",
      "Gradient Descent(9866/9999): loss=2.493577884977076, w0=72.95624999999976, w1=13.042882541313245\n",
      "Gradient Descent(9867/9999): loss=2.363119402929579, w0=72.89999999999975, w1=12.903090552922132\n",
      "Gradient Descent(9868/9999): loss=2.0465780785397265, w0=73.01249999999975, w1=12.9108294754077\n",
      "Gradient Descent(9869/9999): loss=1.6155950899082663, w0=72.84374999999974, w1=13.170398207714998\n",
      "Gradient Descent(9870/9999): loss=2.3774894098627324, w0=72.89999999999975, w1=12.874197620366376\n",
      "Gradient Descent(9871/9999): loss=2.130254263297867, w0=73.06874999999975, w1=12.691706955302058\n",
      "Gradient Descent(9872/9999): loss=2.4211871664427496, w0=73.40624999999976, w1=12.836706910674645\n",
      "Gradient Descent(9873/9999): loss=2.5980224547752977, w0=73.40624999999976, w1=13.214530959873937\n",
      "Gradient Descent(9874/9999): loss=3.1928161150436933, w0=73.34999999999975, w1=13.055307056253358\n",
      "Gradient Descent(9875/9999): loss=1.9903221662925554, w0=73.29374999999975, w1=13.193564348403926\n",
      "Gradient Descent(9876/9999): loss=2.7438805340287002, w0=73.40624999999974, w1=13.604644882875244\n",
      "Gradient Descent(9877/9999): loss=1.9012396414784356, w0=73.51874999999974, w1=13.77738778393995\n",
      "Gradient Descent(9878/9999): loss=2.4660202088742826, w0=73.74374999999974, w1=13.882939437467579\n",
      "Gradient Descent(9879/9999): loss=2.4740323762183243, w0=73.57499999999973, w1=13.9186978919858\n",
      "Gradient Descent(9880/9999): loss=1.9636778512278081, w0=73.63124999999974, w1=14.06868627918711\n",
      "Gradient Descent(9881/9999): loss=2.295939887573932, w0=73.74374999999974, w1=13.598140732484483\n",
      "Gradient Descent(9882/9999): loss=1.9360020280932577, w0=74.13749999999973, w1=13.65470548629212\n",
      "Gradient Descent(9883/9999): loss=2.0076334710673143, w0=73.96874999999973, w1=13.616173509383236\n",
      "Gradient Descent(9884/9999): loss=2.4144592360957784, w0=73.96874999999973, w1=13.626792774666848\n",
      "Gradient Descent(9885/9999): loss=2.217201480658953, w0=73.91249999999972, w1=13.71657751406977\n",
      "Gradient Descent(9886/9999): loss=2.195260293159052, w0=73.74374999999972, w1=13.614101568216041\n",
      "Gradient Descent(9887/9999): loss=2.1680501047986573, w0=73.96874999999972, w1=13.540121294861782\n",
      "Gradient Descent(9888/9999): loss=2.3820413159996843, w0=73.79999999999971, w1=13.58357638072953\n",
      "Gradient Descent(9889/9999): loss=2.726517281022602, w0=73.91249999999971, w1=13.459630906194663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(9890/9999): loss=2.250689233277429, w0=73.63124999999971, w1=13.56040207849984\n",
      "Gradient Descent(9891/9999): loss=2.5369174666954737, w0=73.63124999999971, w1=13.727320689815903\n",
      "Gradient Descent(9892/9999): loss=2.319255038180751, w0=73.74374999999971, w1=13.938515421341892\n",
      "Gradient Descent(9893/9999): loss=1.9536765044579818, w0=73.9687499999997, w1=14.017966574438452\n",
      "Gradient Descent(9894/9999): loss=1.9868396139897027, w0=73.8562499999997, w1=14.014392395438072\n",
      "Gradient Descent(9895/9999): loss=2.4998560551825655, w0=73.74374999999971, w1=13.605469335801143\n",
      "Gradient Descent(9896/9999): loss=2.32348201730011, w0=73.74374999999971, w1=13.557432164129343\n",
      "Gradient Descent(9897/9999): loss=2.9320373144235035, w0=73.46249999999971, w1=13.367940794485452\n",
      "Gradient Descent(9898/9999): loss=1.7896749202925935, w0=73.23749999999971, w1=13.242905158315835\n",
      "Gradient Descent(9899/9999): loss=2.240975001282055, w0=73.18124999999971, w1=13.213831960687976\n",
      "Gradient Descent(9900/9999): loss=1.847782505557752, w0=73.18124999999971, w1=13.24513500172493\n",
      "Gradient Descent(9901/9999): loss=2.168576199510135, w0=73.18124999999971, w1=13.113226128596342\n",
      "Gradient Descent(9902/9999): loss=2.2034822562656684, w0=73.0124999999997, w1=13.149236221190472\n",
      "Gradient Descent(9903/9999): loss=2.5262464156116, w0=73.06874999999971, w1=13.347282581856113\n",
      "Gradient Descent(9904/9999): loss=2.5101486078589605, w0=73.40624999999972, w1=13.557877966024762\n",
      "Gradient Descent(9905/9999): loss=1.8484787825595759, w0=73.63124999999971, w1=13.34954823939125\n",
      "Gradient Descent(9906/9999): loss=2.262195285015352, w0=73.51874999999971, w1=13.498568224383755\n",
      "Gradient Descent(9907/9999): loss=1.994388233398889, w0=73.63124999999971, w1=13.376044025463697\n",
      "Gradient Descent(9908/9999): loss=2.423011826807101, w0=73.68749999999972, w1=13.095723707406384\n",
      "Gradient Descent(9909/9999): loss=2.229638930978635, w0=73.96874999999972, w1=13.069727023013225\n",
      "Gradient Descent(9910/9999): loss=2.261217269842163, w0=73.79999999999971, w1=13.176756647074523\n",
      "Gradient Descent(9911/9999): loss=2.104849566111516, w0=73.74374999999971, w1=13.272654801836483\n",
      "Gradient Descent(9912/9999): loss=2.3925590759287214, w0=73.8562499999997, w1=13.207092788318342\n",
      "Gradient Descent(9913/9999): loss=1.9593259223446067, w0=73.5749999999997, w1=13.268825497231038\n",
      "Gradient Descent(9914/9999): loss=1.8073128646176264, w0=73.91249999999971, w1=13.424822389468307\n",
      "Gradient Descent(9915/9999): loss=2.1846613313624843, w0=74.02499999999971, w1=13.562417472672779\n",
      "Gradient Descent(9916/9999): loss=2.1554340964572862, w0=73.8562499999997, w1=13.758218558022397\n",
      "Gradient Descent(9917/9999): loss=2.2052756339554938, w0=73.4062499999997, w1=13.770421818306717\n",
      "Gradient Descent(9918/9999): loss=1.810610368512826, w0=73.2937499999997, w1=13.490304090561057\n",
      "Gradient Descent(9919/9999): loss=2.2160666010955143, w0=73.1249999999997, w1=13.612366161429847\n",
      "Gradient Descent(9920/9999): loss=2.4113449602528383, w0=73.1249999999997, w1=13.431199138336092\n",
      "Gradient Descent(9921/9999): loss=2.21728511872646, w0=73.1249999999997, w1=13.456542008624337\n",
      "Gradient Descent(9922/9999): loss=2.5783223141755935, w0=73.1249999999997, w1=13.688968812203418\n",
      "Gradient Descent(9923/9999): loss=1.9740423828741185, w0=73.2937499999997, w1=13.818846304321346\n",
      "Gradient Descent(9924/9999): loss=2.4602182339576624, w0=73.1249999999997, w1=13.996537424780954\n",
      "Gradient Descent(9925/9999): loss=1.9983198918068263, w0=73.0687499999997, w1=13.900476443149515\n",
      "Gradient Descent(9926/9999): loss=1.7454029671871418, w0=73.1249999999997, w1=13.650170016887008\n",
      "Gradient Descent(9927/9999): loss=1.863019510278396, w0=73.0687499999997, w1=13.635731436055188\n",
      "Gradient Descent(9928/9999): loss=2.0078102861668627, w0=73.01249999999969, w1=13.4906102620399\n",
      "Gradient Descent(9929/9999): loss=1.576254452261942, w0=72.8999999999997, w1=13.668668498422765\n",
      "Gradient Descent(9930/9999): loss=2.2836600570814407, w0=72.84374999999969, w1=13.7491756447795\n",
      "Gradient Descent(9931/9999): loss=2.007108567112401, w0=72.8999999999997, w1=13.695648348328993\n",
      "Gradient Descent(9932/9999): loss=1.8549982628222663, w0=72.73124999999969, w1=13.519668767519981\n",
      "Gradient Descent(9933/9999): loss=2.64567620426614, w0=73.1812499999997, w1=13.301263757488297\n",
      "Gradient Descent(9934/9999): loss=2.440693930285322, w0=73.1812499999997, w1=13.356442452938328\n",
      "Gradient Descent(9935/9999): loss=2.5318760454258586, w0=73.2374999999997, w1=13.143162082256426\n",
      "Gradient Descent(9936/9999): loss=2.390967822047604, w0=73.2937499999997, w1=12.95134087764435\n",
      "Gradient Descent(9937/9999): loss=2.247690362915475, w0=73.5749999999997, w1=13.175336966568775\n",
      "Gradient Descent(9938/9999): loss=2.0395782527859243, w0=73.63124999999971, w1=13.196751952322687\n",
      "Gradient Descent(9939/9999): loss=2.0748885246267905, w0=73.96874999999972, w1=13.241017018857871\n",
      "Gradient Descent(9940/9999): loss=2.6218658905533516, w0=73.96874999999972, w1=13.514545762515581\n",
      "Gradient Descent(9941/9999): loss=2.2485319517583244, w0=73.85624999999972, w1=13.197859030773316\n",
      "Gradient Descent(9942/9999): loss=2.120479629653127, w0=73.74374999999972, w1=13.380236441163417\n",
      "Gradient Descent(9943/9999): loss=2.423918586718609, w0=73.91249999999972, w1=13.619586880012095\n",
      "Gradient Descent(9944/9999): loss=2.1791951464390698, w0=73.85624999999972, w1=13.781684013586398\n",
      "Gradient Descent(9945/9999): loss=2.2493690731254983, w0=73.79999999999971, w1=13.64756554056164\n",
      "Gradient Descent(9946/9999): loss=1.8709142162330712, w0=73.40624999999972, w1=13.541440667387706\n",
      "Gradient Descent(9947/9999): loss=2.1830410328293546, w0=73.34999999999971, w1=13.499677813960853\n",
      "Gradient Descent(9948/9999): loss=2.1269862740416903, w0=73.34999999999971, w1=13.588602234931944\n",
      "Gradient Descent(9949/9999): loss=2.728567315350388, w0=73.40624999999972, w1=13.442927971971306\n",
      "Gradient Descent(9950/9999): loss=2.1032411571178464, w0=73.12499999999972, w1=13.340291363200318\n",
      "Gradient Descent(9951/9999): loss=2.2143854233880838, w0=73.34999999999971, w1=13.473832156786054\n",
      "Gradient Descent(9952/9999): loss=2.3419173841125938, w0=73.18124999999971, w1=13.513703191881179\n",
      "Gradient Descent(9953/9999): loss=2.395193021060535, w0=73.18124999999971, w1=13.478612314880746\n",
      "Gradient Descent(9954/9999): loss=1.905693754178229, w0=73.2937499999997, w1=13.785183286765943\n",
      "Gradient Descent(9955/9999): loss=2.2889752467553803, w0=73.4062499999997, w1=13.752200033957514\n",
      "Gradient Descent(9956/9999): loss=2.427845369497472, w0=73.46249999999971, w1=13.8373435343827\n",
      "Gradient Descent(9957/9999): loss=2.010729470943023, w0=73.46249999999971, w1=13.951416725982714\n",
      "Gradient Descent(9958/9999): loss=2.334102474566197, w0=73.46249999999971, w1=13.859394649788905\n",
      "Gradient Descent(9959/9999): loss=2.090724929634038, w0=73.34999999999971, w1=13.500618816789798\n",
      "Gradient Descent(9960/9999): loss=2.7219681001878024, w0=73.06874999999971, w1=13.4378932018504\n",
      "Gradient Descent(9961/9999): loss=2.8033788452068436, w0=72.95624999999971, w1=13.423966566830764\n",
      "Gradient Descent(9962/9999): loss=2.4365967910790056, w0=72.84374999999972, w1=13.446175918514609\n",
      "Gradient Descent(9963/9999): loss=2.5626745895104017, w0=73.06874999999971, w1=13.40769952531024\n",
      "Gradient Descent(9964/9999): loss=2.120958723601405, w0=73.18124999999971, w1=13.474610907963475\n",
      "Gradient Descent(9965/9999): loss=2.608782113687217, w0=73.34999999999971, w1=13.41679922046651\n",
      "Gradient Descent(9966/9999): loss=2.060214793589468, w0=73.34999999999971, w1=13.359386281148787\n",
      "Gradient Descent(9967/9999): loss=2.018982599359684, w0=73.51874999999971, w1=13.327861608431299\n",
      "Gradient Descent(9968/9999): loss=1.9650761519400202, w0=73.46249999999971, w1=13.482921341991592\n",
      "Gradient Descent(9969/9999): loss=2.2804281729554567, w0=73.5749999999997, w1=13.427688630644175\n",
      "Gradient Descent(9970/9999): loss=2.0308483507993778, w0=73.63124999999971, w1=13.393304479907146\n",
      "Gradient Descent(9971/9999): loss=2.359317377442668, w0=73.46249999999971, w1=13.41646136491954\n",
      "Gradient Descent(9972/9999): loss=2.443341150970438, w0=73.34999999999971, w1=13.39118275940297\n",
      "Gradient Descent(9973/9999): loss=1.7193723646279484, w0=73.23749999999971, w1=13.47960725766385\n",
      "Gradient Descent(9974/9999): loss=1.9157989200783891, w0=73.40624999999972, w1=13.599057655862438\n",
      "Gradient Descent(9975/9999): loss=2.324858242412077, w0=73.46249999999972, w1=13.663636147794033\n",
      "Gradient Descent(9976/9999): loss=2.2621392159077507, w0=73.34999999999972, w1=13.704612422580533\n",
      "Gradient Descent(9977/9999): loss=2.329119996702042, w0=73.34999999999972, w1=13.558872199030539\n",
      "Gradient Descent(9978/9999): loss=2.0967092967074503, w0=73.51874999999973, w1=13.408043313477863\n",
      "Gradient Descent(9979/9999): loss=2.1094524418407055, w0=73.34999999999972, w1=13.394120464565754\n",
      "Gradient Descent(9980/9999): loss=2.1425679032723615, w0=73.34999999999972, w1=13.413144705487028\n",
      "Gradient Descent(9981/9999): loss=2.1779177773423024, w0=73.40624999999973, w1=13.592201188919239\n",
      "Gradient Descent(9982/9999): loss=2.1265095350231045, w0=73.34999999999972, w1=13.338314288348041\n",
      "Gradient Descent(9983/9999): loss=1.9946864346380784, w0=73.51874999999973, w1=13.1965890891718\n",
      "Gradient Descent(9984/9999): loss=2.3721810910951824, w0=73.40624999999973, w1=13.516382896716294\n",
      "Gradient Descent(9985/9999): loss=1.8659096988615613, w0=73.40624999999973, w1=13.691313654630019\n",
      "Gradient Descent(9986/9999): loss=2.0741294093974814, w0=73.57499999999973, w1=13.534506619795593\n",
      "Gradient Descent(9987/9999): loss=1.8041183114274566, w0=73.23749999999973, w1=13.277503074333776\n",
      "Gradient Descent(9988/9999): loss=2.1024768351158696, w0=73.12499999999973, w1=13.262878261140509\n",
      "Gradient Descent(9989/9999): loss=1.829283191177251, w0=73.18124999999974, w1=13.18184797278145\n",
      "Gradient Descent(9990/9999): loss=2.404847612161757, w0=73.23749999999974, w1=13.305974321241914\n",
      "Gradient Descent(9991/9999): loss=3.0383037846884715, w0=73.40624999999974, w1=13.384852837110609\n",
      "Gradient Descent(9992/9999): loss=2.1937235826332153, w0=73.51874999999974, w1=13.662276450203331\n",
      "Gradient Descent(9993/9999): loss=2.206845933599208, w0=73.29374999999975, w1=13.676352783538976\n",
      "Gradient Descent(9994/9999): loss=2.3818174328220545, w0=73.40624999999974, w1=13.436211863579185\n",
      "Gradient Descent(9995/9999): loss=1.5904750049395138, w0=73.23749999999974, w1=13.221187983785676\n",
      "Gradient Descent(9996/9999): loss=1.978739318276448, w0=73.34999999999974, w1=13.107077084971356\n",
      "Gradient Descent(9997/9999): loss=2.7801786089832774, w0=73.23749999999974, w1=13.344260698156157\n",
      "Gradient Descent(9998/9999): loss=2.620631528348709, w0=73.23749999999974, w1=13.110974746424615\n",
      "Gradient Descent(9999/9999): loss=2.4911627629078943, w0=72.89999999999974, w1=13.226471977521664\n",
      "SGD: execution time=6.822 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10000\n",
    "gamma = 0.9\n",
    "batch_size = 32\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996060bc559344f0a6f6b1483efde59e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    dh = y - np.dot(tx,w)\n",
    "    dh[dh ==0] = random.uniform(-1,1)\n",
    "    dh[dh>0] = 1\n",
    "    dh[dh<0] = -1\n",
    "    Gd = [np.repeat(-1,tx.shape[0]),-tx[:,1]]\n",
    "    return 1/(len(y))*np.sum((dh*Gd), axis = 1)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma, num_batches):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        y_i = []\n",
    "        tx_i = []\n",
    "        G=[]\n",
    "        for minibatch_y, minibatch_tx in batch_iter(y, tx, batch_size, num_batches):\n",
    "            y_i.append(minibatch_y)\n",
    "            tx_i.append(minibatch_tx)\n",
    "        for i in range(num_batches):\n",
    "            G_i = compute_stoch_gradient(y_i[i], tx_i[i],w)\n",
    "            G.append(G_i)\n",
    "        g = (1/num_batches)*sum(G)\n",
    "        loss = compute_loss(1/num_batches*sum(y_i), 1/num_batches*sum(tx_i),w)\n",
    "        w = w-gamma*g\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/99): loss=36.32228060140201, w0=0.9, w1=-0.026481736578085762\n",
      "Gradient Descent(1/99): loss=36.37229827980046, w0=1.8, w1=0.014526678349382382\n",
      "Gradient Descent(2/99): loss=36.20068873347685, w0=2.7, w1=0.07293412678200661\n",
      "Gradient Descent(3/99): loss=35.20899146338639, w0=3.6, w1=0.0743503611863352\n",
      "Gradient Descent(4/99): loss=35.10620939747831, w0=4.5, w1=0.11734532276881551\n",
      "Gradient Descent(5/99): loss=34.844582314141206, w0=5.4, w1=0.17084319399263034\n",
      "Gradient Descent(6/99): loss=34.141544909347814, w0=6.300000000000001, w1=0.1966865569788776\n",
      "Gradient Descent(7/99): loss=33.55116625624175, w0=7.200000000000001, w1=0.19753666992564803\n",
      "Gradient Descent(8/99): loss=32.78970488410384, w0=8.100000000000001, w1=0.1386917531642744\n",
      "Gradient Descent(9/99): loss=32.76458625372445, w0=9.000000000000002, w1=0.1697759728868486\n",
      "Gradient Descent(10/99): loss=32.37195072472439, w0=9.900000000000002, w1=0.19797951885036597\n",
      "Gradient Descent(11/99): loss=32.024541191992064, w0=10.800000000000002, w1=0.2345987102743779\n",
      "Gradient Descent(12/99): loss=30.713471860710214, w0=11.700000000000003, w1=0.21018641113326847\n",
      "Gradient Descent(13/99): loss=30.76776473680556, w0=12.600000000000003, w1=0.19919982330260003\n",
      "Gradient Descent(14/99): loss=30.052913660324972, w0=13.500000000000004, w1=0.1595738240850355\n",
      "Gradient Descent(15/99): loss=29.34926174707703, w0=14.400000000000004, w1=0.10635245361331194\n",
      "Gradient Descent(16/99): loss=29.74483310868419, w0=15.300000000000004, w1=0.11999872115706142\n",
      "Gradient Descent(17/99): loss=29.163637218361796, w0=16.200000000000003, w1=0.1402122587419144\n",
      "Gradient Descent(18/99): loss=28.430338825725055, w0=17.1, w1=0.11621916634438252\n",
      "Gradient Descent(19/99): loss=27.716402058971717, w0=18.0, w1=0.08842504994312131\n",
      "Gradient Descent(20/99): loss=27.947158360504574, w0=18.9, w1=0.11091322446554999\n",
      "Gradient Descent(21/99): loss=27.45654654586563, w0=19.799999999999997, w1=0.17476911764812048\n",
      "Gradient Descent(22/99): loss=26.97533565899307, w0=20.699999999999996, w1=0.19816625398649462\n",
      "Gradient Descent(23/99): loss=26.328655836023554, w0=21.599999999999994, w1=0.19750602757019445\n",
      "Gradient Descent(24/99): loss=26.11903384787399, w0=22.499999999999993, w1=0.2456678144464166\n",
      "Gradient Descent(25/99): loss=25.550420343841047, w0=23.39999999999999, w1=0.2664766300504525\n",
      "Gradient Descent(26/99): loss=24.761726392884682, w0=24.29999999999999, w1=0.22692587765764938\n",
      "Gradient Descent(27/99): loss=24.77070685944154, w0=25.19999999999999, w1=0.2557938855400875\n",
      "Gradient Descent(28/99): loss=24.102864322863223, w0=26.099999999999987, w1=0.2815531909577455\n",
      "Gradient Descent(29/99): loss=24.03281217014661, w0=26.999999999999986, w1=0.3052912371000425\n",
      "Gradient Descent(30/99): loss=23.050249900586778, w0=27.899999999999984, w1=0.299946761367183\n",
      "Gradient Descent(31/99): loss=22.058246665080944, w0=28.799999999999983, w1=0.20281865423197987\n",
      "Gradient Descent(32/99): loss=22.510775546237827, w0=29.69999999999998, w1=0.2148856279248307\n",
      "Gradient Descent(33/99): loss=21.641328634149385, w0=30.59999999999998, w1=0.19333998778814437\n",
      "Gradient Descent(34/99): loss=21.59835963126634, w0=31.49999999999998, w1=0.2343018029097257\n",
      "Gradient Descent(35/99): loss=21.08196215371684, w0=32.39999999999998, w1=0.2466546731121812\n",
      "Gradient Descent(36/99): loss=20.187005839553926, w0=33.299999999999976, w1=0.2021573130087852\n",
      "Gradient Descent(37/99): loss=19.5808031309757, w0=34.199999999999974, w1=0.13396248362054175\n",
      "Gradient Descent(38/99): loss=19.634766974884926, w0=35.09999999999997, w1=0.14077069950000004\n",
      "Gradient Descent(39/99): loss=19.407225303975398, w0=35.99999999999997, w1=0.1888927612867205\n",
      "Gradient Descent(40/99): loss=18.88448612716341, w0=36.89999999999997, w1=0.2263883427164616\n",
      "Gradient Descent(41/99): loss=17.86062938960192, w0=37.79718749999997, w1=0.184203276526478\n",
      "Gradient Descent(42/99): loss=17.4386543524999, w0=38.69437499999997, w1=0.16039459530595468\n",
      "Gradient Descent(43/99): loss=17.46360477045973, w0=39.59437499999997, w1=0.16762479524840998\n",
      "Gradient Descent(44/99): loss=16.95674492749301, w0=40.48874999999997, w1=0.21918406837476778\n",
      "Gradient Descent(45/99): loss=16.001166146344538, w0=41.38593749999997, w1=0.17485843517876964\n",
      "Gradient Descent(46/99): loss=16.14216696881478, w0=42.28593749999997, w1=0.17801993605181596\n",
      "Gradient Descent(47/99): loss=15.494797322121812, w0=43.180312499999964, w1=0.17709288903847242\n",
      "Gradient Descent(48/99): loss=15.415234738158382, w0=44.08031249999996, w1=0.24684014132379822\n",
      "Gradient Descent(49/99): loss=14.721252371488928, w0=44.95781249999996, w1=0.2991945051151768\n",
      "Gradient Descent(50/99): loss=13.935864442972447, w0=45.84093749999996, w1=0.30439190758544266\n",
      "Gradient Descent(51/99): loss=13.586836090556055, w0=46.71843749999996, w1=0.3307360363629046\n",
      "Gradient Descent(52/99): loss=13.599694194586318, w0=47.59312499999996, w1=0.42877289639176325\n",
      "Gradient Descent(53/99): loss=12.894235969900096, w0=48.43687499999996, w1=0.5386652074497161\n",
      "Gradient Descent(54/99): loss=12.04296174585566, w0=49.29468749999996, w1=0.557018853557549\n",
      "Gradient Descent(55/99): loss=11.845139546237142, w0=50.11031249999996, w1=0.6885718630357044\n",
      "Gradient Descent(56/99): loss=11.370451597272032, w0=50.94562499999996, w1=0.7654381091125643\n",
      "Gradient Descent(57/99): loss=10.900534750486926, w0=51.74718749999996, w1=0.9016545961022131\n",
      "Gradient Descent(58/99): loss=10.703326096419774, w0=52.54312499999996, w1=1.0877671509936373\n",
      "Gradient Descent(59/99): loss=10.811518602187483, w0=53.36437499999996, w1=1.2610470657972637\n",
      "Gradient Descent(60/99): loss=9.876255101235612, w0=54.14343749999996, w1=1.4428070742372154\n",
      "Gradient Descent(61/99): loss=9.42839761799541, w0=54.93093749999996, w1=1.6198857640173092\n",
      "Gradient Descent(62/99): loss=9.120535994085708, w0=55.679062499999965, w1=1.8450656229632916\n",
      "Gradient Descent(63/99): loss=8.838084421744806, w0=56.40749999999996, w1=2.091283623486927\n",
      "Gradient Descent(64/99): loss=8.315029448202685, w0=57.093749999999964, w1=2.4099773799695465\n",
      "Gradient Descent(65/99): loss=7.8789233208355975, w0=57.774374999999964, w1=2.7073194541059484\n",
      "Gradient Descent(66/99): loss=7.62036630019139, w0=58.483124999999966, w1=2.954488236724563\n",
      "Gradient Descent(67/99): loss=7.449901417613884, w0=59.163749999999965, w1=3.260235679404999\n",
      "Gradient Descent(68/99): loss=7.4627943774370715, w0=59.875312499999964, w1=3.5840150331592104\n",
      "Gradient Descent(69/99): loss=6.987126442325495, w0=60.539062499999964, w1=3.9411555107214338\n",
      "Gradient Descent(70/99): loss=6.409481825134662, w0=61.20562499999996, w1=4.236980374009856\n",
      "Gradient Descent(71/99): loss=6.123208444362968, w0=61.85531249999996, w1=4.563513370134916\n",
      "Gradient Descent(72/99): loss=5.707730556185172, w0=62.47406249999996, w1=4.89975158434166\n",
      "Gradient Descent(73/99): loss=5.492469632880166, w0=63.07031249999996, w1=5.289563223220425\n",
      "Gradient Descent(74/99): loss=5.297405880152645, w0=63.64687499999996, w1=5.68557537208894\n",
      "Gradient Descent(75/99): loss=4.661661416594557, w0=64.21781249999997, w1=5.9849375779092515\n",
      "Gradient Descent(76/99): loss=4.684612559815765, w0=64.79437499999996, w1=6.353592427689124\n",
      "Gradient Descent(77/99): loss=4.206991121992216, w0=65.35968749999996, w1=6.71496078770896\n",
      "Gradient Descent(78/99): loss=3.6953768038526684, w0=65.84906249999996, w1=7.075427274325528\n",
      "Gradient Descent(79/99): loss=3.6810201820270403, w0=66.32718749999997, w1=7.478319214093867\n",
      "Gradient Descent(80/99): loss=3.6531925999897332, w0=66.82499999999996, w1=7.855318192945799\n",
      "Gradient Descent(81/99): loss=3.2134635851327413, w0=67.26374999999996, w1=8.30300255562633\n",
      "Gradient Descent(82/99): loss=3.3134060279610758, w0=67.74749999999996, w1=8.709937101242911\n",
      "Gradient Descent(83/99): loss=2.7376399096132458, w0=68.17218749999996, w1=9.097508356891105\n",
      "Gradient Descent(84/99): loss=2.6512685794851594, w0=68.60812499999996, w1=9.516679198012541\n",
      "Gradient Descent(85/99): loss=2.4325769824660934, w0=69.06093749999995, w1=9.849594750161224\n",
      "Gradient Descent(86/99): loss=2.3269851691084784, w0=69.48562499999996, w1=10.195189020479521\n",
      "Gradient Descent(87/99): loss=1.8502144420812503, w0=69.85406249999996, w1=10.56406677363419\n",
      "Gradient Descent(88/99): loss=1.748122968630537, w0=70.21406249999995, w1=10.882838756860483\n",
      "Gradient Descent(89/99): loss=1.431403686173619, w0=70.54312499999996, w1=11.204034706621085\n",
      "Gradient Descent(90/99): loss=1.4025037240181206, w0=70.81593749999996, w1=11.472423461655064\n",
      "Gradient Descent(91/99): loss=1.238777756733263, w0=71.09999999999997, w1=11.742866126709766\n",
      "Gradient Descent(92/99): loss=1.2365349614153496, w0=71.35031249999997, w1=12.001937204983488\n",
      "Gradient Descent(93/99): loss=0.9644550382306096, w0=71.58093749999998, w1=12.174806402441982\n",
      "Gradient Descent(94/99): loss=0.8357689409617617, w0=71.77781249999998, w1=12.296185774629553\n",
      "Gradient Descent(95/99): loss=0.7077209985043167, w0=71.91281249999999, w1=12.445327531303523\n",
      "Gradient Descent(96/99): loss=0.766451424285729, w0=72.04499999999999, w1=12.591434667628056\n",
      "Gradient Descent(97/99): loss=0.6948914886357631, w0=72.21374999999999, w1=12.761609534826505\n",
      "Gradient Descent(98/99): loss=0.6313081190281316, w0=72.37687499999998, w1=12.879059812787002\n",
      "Gradient Descent(99/99): loss=0.6654687065226537, w0=72.47531249999999, w1=12.982531076338303\n",
      "SGD: execution time=0.114 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.9\n",
    "batch_size = 32\n",
    "num_batches = 20\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma,num_batches)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734474329d454aff9b6d806137c59963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Effect of Outliers and MAE Cost Function, and Subgradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Load and plot data containing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, weight, gender = load_data(sub_sample=False, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=46.774801130735526, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.007 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0,6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
