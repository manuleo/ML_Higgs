{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from helpers import *\n",
    "from implementations import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = pd.DataFrame(tX)\n",
    "tX.where(tX!=-999, inplace=True)\n",
    "tX.fillna(tX.median(), inplace=True)    \n",
    "tX = tX.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_for_logistic(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=138629.43611198905\n",
      "Current iteration=1000, loss=106751.60151235267\n",
      "Current iteration=2000, loss=106408.02717390667\n",
      "Current iteration=3000, loss=106299.1933633236\n",
      "Current iteration=4000, loss=106253.50135402067\n",
      "Current iteration=5000, loss=106230.50607151032\n",
      "Current iteration=6000, loss=106216.27284014336\n",
      "Current iteration=7000, loss=106205.50922362538\n",
      "Current iteration=8000, loss=106196.14474365817\n",
      "Current iteration=9000, loss=106187.35953373656\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_tr_tot.shape[1])\n",
    "max_iters = 10000\n",
    "gamma = 1e-10\n",
    "w, loss = logistic_regression(y_tr_tot, x_tr_tot, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7362\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, x_te_tot, True)\n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = predict_labels(w, tX_test, True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=134001.19392646643\n",
      "Current iteration=1000, loss=106750.92508967363\n",
      "Current iteration=2000, loss=106407.85577669005\n",
      "Current iteration=3000, loss=106299.12698797951\n",
      "Current iteration=4000, loss=106253.47056667147\n",
      "Current iteration=5000, loss=106230.48878656604\n",
      "Current iteration=6000, loss=106216.26085869093\n",
      "Current iteration=7000, loss=106205.49936369108\n",
      "Current iteration=8000, loss=106196.13575003046\n",
      "Current iteration=9000, loss=106187.3509072507\n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_tr_tot.shape[1])\n",
    "lambda_ = 1e-8\n",
    "max_iters = 10000\n",
    "gamma = 1e-10\n",
    "w, loss = reg_logistic_regression(y_tr_tot, x_tr_tot, lambda_, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7362\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_labels(w, x_te_tot, True)\n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = predict_labels(w, tX_test, True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, gamma, acc = False, ls = False):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    accuracies = []\n",
    "    ws = []\n",
    "    initial_w = np.zeros(x.shape[1])\n",
    "\n",
    "    \n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        #print(index_te, index_tr)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "        \n",
    "        # form data with polynomial degree\n",
    "        x_te_poly = x_te\n",
    "        x_tr_poly = x_tr\n",
    "        \n",
    "        if (ls==False):\n",
    "            w, _ = reg_logistic_regression(y_tr, x_tr_poly, lambda_, initial_w, 1000, gamma)\n",
    "        else:\n",
    "            w = least_squares_lstsq_ver(y_tr, x_tr_poly)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        rmse_tr = calculate_loss(y_tr, x_tr_poly, w)\n",
    "        rmse_te = calculate_loss(y_te, x_te_poly, w)\n",
    "        #print(lambda_, rmse_te)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "        \n",
    "        y_pred = predict_labels(w, x_te_poly)\n",
    "        accuracies.append(accuracy(y_te, y_pred))\n",
    "        \n",
    "        ws.append(w)\n",
    "        \n",
    "    if acc==False:\n",
    "        loss_tr = np.median(losses_tr)\n",
    "        loss_te = np.median(losses_te)\n",
    "        return loss_tr, loss_te, np.mean(ws, axis=0)\n",
    "    else:\n",
    "        return np.mean(accuracies), np.mean(ws, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_hypers(y, x, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 100000, seed)\n",
    "    lambdas = np.logspace(-10, 0, 30)\n",
    "    lambdas = [1e-10]\n",
    "    gammas = np.logspace(-10, 0, 30)\n",
    "    #lambdas=[1e-06]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    gamma_star = 0\n",
    "    lambda_star = 0\n",
    "    w_star = 0\n",
    "    for gamma in gammas:\n",
    "        for lambda_ in lambdas:\n",
    "            #print(degree)\n",
    "            loss_tr, loss_te, w = cross_validation(y, x, k_indices, k_fold, lambda_, gamma)\n",
    "            #print(loss_te)\n",
    "            if loss_te < loss_min:\n",
    "                loss_min = loss_te\n",
    "                print(\"New loss: {}, gamma: {}, lambda: {}\".format(loss_te, gamma, lambda_))\n",
    "                gamma_star = gamma\n",
    "                lambda_star = lambda_\n",
    "                w_star = w\n",
    "    return gamma_star, lambda_star, loss_min, w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet 0: \n",
      "New loss: 1795.6522157325862, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 1781.2144753402767, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 1750.0401072214504, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 1684.6675541796776, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 1556.065366850918, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 1335.1674854273551, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 1043.929227467509, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 793.712284585019, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 668.980413708281, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 635.9759355542362, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 630.12605180581, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 623.4875319017765, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 610.2879758679297, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 586.2374445964485, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 546.7369998751694, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "New loss: 498.85033549008165, gamma: 1.4873521072935119e-05, lambda: 1e-10\n",
      "New loss: 463.841292986525, gamma: 3.290344562312671e-05, lambda: 1e-10\n",
      "New loss: 450.63995179435386, gamma: 7.27895384398316e-05, lambda: 1e-10\n",
      "New loss: 446.60582503784303, gamma: 0.00016102620275609426, lambda: 1e-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\EPFL\\ML\\github_mine\\ML\\Project_1\\final_zip\\implementation_helpers.py:102: RuntimeWarning: overflow encountered in log1p\n",
      "  \"\"\"\n",
      "D:\\EPFL\\ML\\github_mine\\ML\\Project_1\\final_zip\\implementation_helpers.py:102: RuntimeWarning: invalid value encountered in log1p\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet 0: best loss: 446.60582503784303, degree: 0.00016102620275609426, lambda: 1e-10\n",
      "jet 1: \n",
      "New loss: 5108.00889175473, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 5098.63216694286, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 5079.534651545113, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 5044.244638368951, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 4990.95863071256, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 4935.345306703182, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 4894.517106916253, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 4843.674440985561, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 4748.641892924933, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 4596.371974402499, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 4422.672104453381, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 4280.311450890442, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 4164.365690538143, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 4010.215528155093, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 3855.0453770351796, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 1: best loss: 3855.0453770351796, degree: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 2: \n",
      "New loss: 518.822058622105, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 517.5693567519788, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 514.8212111238638, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 508.8531840071796, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 496.17917100708314, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 470.5521078861734, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 423.9484633028354, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 355.7089210363779, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 287.86356986016995, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 249.43732794152183, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 238.57689395223764, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 236.22602866932266, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 233.21200822166168, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 227.6000488850215, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 218.71916952455808, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "New loss: 208.14807749031974, gamma: 1.4873521072935119e-05, lambda: 1e-10\n",
      "New loss: 201.24276726068302, gamma: 3.290344562312671e-05, lambda: 1e-10\n",
      "New loss: 196.25741635692415, gamma: 7.27895384398316e-05, lambda: 1e-10\n",
      "New loss: 192.54959168221578, gamma: 0.00016102620275609426, lambda: 1e-10\n",
      "jet 2: best loss: 192.54959168221578, degree: 0.00016102620275609426, lambda: 1e-10\n",
      "jet 3: \n",
      "New loss: 4842.796458158753, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 4834.4932136935995, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 4817.469382870694, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 4785.565051212227, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 4736.047170073563, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 4682.269441873012, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 4643.540680829991, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 4601.888944297538, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 4529.026810049196, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 4419.013507263706, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 4298.226946726979, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 4211.769739552097, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 4159.1979715259, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 4116.861962444976, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 4079.451632936986, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 3: best loss: 4079.451632936986, degree: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 4: \n",
      "New loss: 206.35483499383636, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 206.1095343759078, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 205.57001253216526, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 204.39167095048745, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 201.85768602623688, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 196.59104778132934, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 186.42715427847992, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 169.6399087933112, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 149.04133571656078, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 133.38233668328894, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 127.39407797208597, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 126.57807040738462, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 126.41722967212844, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 126.11104441652358, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 125.47301480631532, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "New loss: 124.53587152385768, gamma: 1.4873521072935119e-05, lambda: 1e-10\n",
      "New loss: 122.85139126983293, gamma: 3.290344562312671e-05, lambda: 1e-10\n",
      "New loss: 119.65212675561537, gamma: 7.27895384398316e-05, lambda: 1e-10\n",
      "New loss: 113.78082445251087, gamma: 0.00016102620275609426, lambda: 1e-10\n",
      "New loss: 110.29676816808399, gamma: 0.0003562247890262444, lambda: 1e-10\n",
      "jet 4: best loss: 110.29676816808399, degree: 0.0003562247890262444, lambda: 1e-10\n",
      "jet 5: \n",
      "New loss: 3286.990908425973, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 3286.2934879004297, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 3284.8854116070943, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 3282.3363301759787, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 3278.6856734843714, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 3275.455606609942, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 3274.2135302621728, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 3273.7050531534996, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 3272.749963003367, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 3270.7155109603254, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 3266.370874174668, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 3257.1269191137044, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 3237.8534684553592, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 3198.71927161551, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 3128.043070007697, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 5: best loss: 3128.043070007697, degree: 6.723357536499335e-06, lambda: 1e-10\n",
      "jet 6: \n",
      "New loss: 103.2037846519531, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 103.11284615321594, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 102.91226792316019, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 102.47145457601287, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 101.51036017319952, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 99.45143670104248, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 95.20785819784601, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 87.16247773983517, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 74.30549916222648, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 58.96921086580615, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 46.48009737140299, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 39.96194110393739, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 37.78822139313779, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 37.326018131811765, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 37.17866550265332, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "New loss: 36.93185645401856, gamma: 1.4873521072935119e-05, lambda: 1e-10\n",
      "New loss: 36.477802923470854, gamma: 3.290344562312671e-05, lambda: 1e-10\n",
      "New loss: 35.811893737160915, gamma: 7.27895384398316e-05, lambda: 1e-10\n",
      "New loss: 35.21977234799765, gamma: 0.00016102620275609426, lambda: 1e-10\n",
      "New loss: 34.974379217409265, gamma: 0.0003562247890262444, lambda: 1e-10\n",
      "New loss: 34.664598345529214, gamma: 0.0007880462815669921, lambda: 1e-10\n",
      "jet 6: best loss: 34.664598345529214, degree: 0.0007880462815669921, lambda: 1e-10\n",
      "jet 7: \n",
      "New loss: 1432.556776298973, gamma: 1e-10, lambda: 1e-10\n",
      "New loss: 1429.8716461161368, gamma: 2.2122162910704502e-10, lambda: 1e-10\n",
      "New loss: 1424.128012142225, gamma: 4.893900918477499e-10, lambda: 1e-10\n",
      "New loss: 1412.3296513617115, gamma: 1.0826367338740564e-09, lambda: 1e-10\n",
      "New loss: 1390.144143664117, gamma: 2.395026619987491e-09, lambda: 1e-10\n",
      "New loss: 1355.6064051860137, gamma: 5.298316906283702e-09, lambda: 1e-10\n",
      "New loss: 1318.870538836754, gamma: 1.1721022975334793e-08, lambda: 1e-10\n",
      "New loss: 1298.3152235680086, gamma: 2.592943797404667e-08, lambda: 1e-10\n",
      "New loss: 1288.9564581078573, gamma: 5.736152510448681e-08, lambda: 1e-10\n",
      "New loss: 1276.4652641836562, gamma: 1.2689610031679235e-07, lambda: 1e-10\n",
      "New loss: 1258.61595434207, gamma: 2.807216203941181e-07, lambda: 1e-10\n",
      "New loss: 1233.439781216397, gamma: 6.210169418915616e-07, lambda: 1e-10\n",
      "New loss: 1203.1489267329644, gamma: 1.3738237958832638e-06, lambda: 1e-10\n",
      "New loss: 1177.9892704771903, gamma: 3.039195382313201e-06, lambda: 1e-10\n",
      "New loss: 1161.2391196953713, gamma: 6.723357536499335e-06, lambda: 1e-10\n",
      "New loss: 1154.0383877999698, gamma: 1.4873521072935119e-05, lambda: 1e-10\n",
      "jet 7: best loss: 1154.0383877999698, degree: 1.4873521072935119e-05, lambda: 1e-10\n"
     ]
    }
   ],
   "source": [
    "k_fold = 8\n",
    "gammas_star=[]\n",
    "lambdas_star=[]\n",
    "w_star = []\n",
    "for jet in range(0, 8):\n",
    "    print(\"jet {}: \".format(jet))\n",
    "    gamma_star, lambda_star, loss, w = select_best_hypers(y_tr[jet], tX_tr[jet], k_fold, 1)\n",
    "    gammas_star.append(gamma_star)\n",
    "    lambdas_star.append(lambda_star)\n",
    "    w_star.append(w)\n",
    "    #print(\"jet {}: Best accuracy {}, degree: {},  lambda: {}\".format(jet, acc, degree_star, lambda_star))\n",
    "    print(\"jet {}: best loss: {}, degree: {}, lambda: {}\".format(jet, loss, gamma_star, lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(x_tr_tot, y_tr_tot)\n",
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(x_te_tot, y_te_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680.055481504468\n",
      "27764.72327117549\n",
      "1459.7004266660938\n",
      "31849.445585489553\n",
      "648.2671732761505\n",
      "21567.588545606493\n",
      "255.56508887001226\n",
      "8853.572060939954\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    initial_w = np.zeros(tX_tr[jet].shape[1])\n",
    "    w, loss = reg_logistic_regression(y_tr[jet], tX_tr[jet], 1e-10, initial_w, 100000, gammas_star[jet])\n",
    "    print(loss)\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75582\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10, 1e-10]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00016102620275609426,\n",
       " 6.723357536499335e-06,\n",
       " 0.00016102620275609426,\n",
       " 6.723357536499335e-06,\n",
       " 0.0003562247890262444,\n",
       " 6.723357536499335e-06,\n",
       " 0.0007880462815669921,\n",
       " 1.4873521072935119e-05]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gammas_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing(tX_test) #same function as train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, list_w, logistic=True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[2.848035868435799e-05,\n",
    " 3.5111917342151275e-06,\n",
    " 0.0002310129700083158,\n",
    " 3.5111917342151275e-06,\n",
    " 0.0002310129700083158,\n",
    " 3.5111917342151275e-06,\n",
    " 0.0002310129700083158,\n",
    " 2.848035868435799e-05]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
