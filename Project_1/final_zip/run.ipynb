{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from helpers import *\n",
    "from implementations import ridge_regression\n",
    "import numpy as np\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# original data loading\n",
    "DATA_TRAIN_PATH = 'train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 80/20 dataset\n",
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)\n",
    "\n",
    "# apply preprocessing to train and test sets\n",
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(x_tr_tot, y_tr_tot)\n",
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(x_te_tot, y_te_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet 0: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 1: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 2: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 3: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 4: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 5: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 6: best loss: inf, degree: 0, lambda: 0\n",
      "Jet 7: best loss: inf, degree: 0, lambda: 0\n",
      "0.6554\n"
     ]
    }
   ],
   "source": [
    "# split 80/20 dataset\n",
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)\n",
    "\n",
    "# apply preprocessing to train and test sets\n",
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(x_tr_tot, y_tr_tot)\n",
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(x_te_tot, y_te_tot)\n",
    "\n",
    "#hardcoded parameter from ridge regression\n",
    "degrees_star = [7, 5, 5, 9, 6, 9, 5, 8]\n",
    "lambdas_star = [2.8117686979742307e-08, 1.757510624854793e-08, 3.088843596477485e-06, 2.94705170255181e-07, 1e-10, 1e-10, 1.67683293681101e-09, 1.0481131341546874e-09]\n",
    "\n",
    "# compute parameters by using cross validation on train set\n",
    "# !NOTE: this function is commented because it needs more than 1 hour to run: the resulting parameters are hard coded for simplicity (uncomment if you wan to run it)\n",
    "#degrees_star, lambdas_star = select_best_hypers_ridge(y_tr, tX_tr, max_degree = 9, k_fold = 12, min_lambda_pow = -10, max_lambda_pow = 0)\n",
    "\n",
    "#computing w with tuned hyper-parameters\n",
    "w = []\n",
    "for jet in range(0,8):\n",
    "    x_tr_poly = build_poly(tX_tr[jet], degrees_star[jet])\n",
    "    w_star, loss = ridge_regression(y_tr[jet], x_tr_poly, lambdas_star[jet])\n",
    "    w.append(w_star)\n",
    "    \n",
    "#computing accuracy on our test set\n",
    "y_pred = build_predictions(tX_te, indexes_te, w, degrees_star)\n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)\n",
    "\n",
    "#preparing data for submission\n",
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing(tX_test) # same function as train, when run without y doesn't compute nothing on that\n",
    "\n",
    "#building submission\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, w, degrees_star)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet: 0\n",
      "New loss: 0.426764152294866, degree: 1, lambda: 1e-10\n",
      "New loss: 0.41724542341552934, degree: 2, lambda: 1e-10\n",
      "New loss: 0.4172453212102481, degree: 2, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.4172451574044353, degree: 2, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.4172448960913328, degree: 2, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.41724447966345796, degree: 2, lambda: 6.551285568595495e-10\n",
      "New loss: 0.41724381840953706, degree: 2, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.41724277252684433, degree: 2, lambda: 1.67683293681101e-09\n",
      "New loss: 0.4172411297364352, degree: 2, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.4172385775812469, degree: 2, lambda: 4.291934260128778e-09\n",
      "New loss: 0.4172346820013714, degree: 2, lambda: 6.866488450042998e-09\n",
      "New loss: 0.41722890113147787, degree: 2, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.4172207030897026, degree: 2, lambda: 1.757510624854793e-08\n",
      "New loss: 0.41720990645379025, degree: 2, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.4171973685796319, degree: 2, lambda: 4.498432668969444e-08\n",
      "New loss: 0.4171859558689865, degree: 2, lambda: 7.196856730011529e-08\n",
      "New loss: 0.4171812368497096, degree: 2, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.41295863008640804, degree: 3, lambda: 1e-10\n",
      "New loss: 0.412957791360908, degree: 3, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.4129568928976421, degree: 3, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.41295579886081984, degree: 3, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.4129543278615116, degree: 3, lambda: 6.551285568595495e-10\n",
      "New loss: 0.41295224030652083, degree: 3, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.41294927944777776, degree: 3, lambda: 1.67683293681101e-09\n",
      "New loss: 0.4129452995360703, degree: 3, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.41294065595826884, degree: 3, lambda: 4.291934260128778e-09\n",
      "New loss: 0.4129370154390846, degree: 3, lambda: 6.866488450042998e-09\n",
      "New loss: 0.41113785380768286, degree: 5, lambda: 1e-10\n",
      "New loss: 0.4108510110437655, degree: 5, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.4107121105843394, degree: 5, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.4106761921630633, degree: 5, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.4104022691681581, degree: 5, lambda: 6.866488450042998e-09\n",
      "New loss: 0.4099069067238486, degree: 5, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.4091821413074907, degree: 5, lambda: 1.757510624854793e-08\n",
      "New loss: 0.40843935025848116, degree: 5, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.408274802390598, degree: 7, lambda: 1.757510624854793e-08\n",
      "New loss: 0.40787495875618224, degree: 7, lambda: 2.8117686979742307e-08\n",
      "Jet 0 no mass -> best loss: 0.40787495875618224, degree: 7, lambda: 2.8117686979742307e-08\n",
      "jet: 1\n",
      "New loss: 0.7742719723174835, degree: 1, lambda: 1e-10\n",
      "New loss: 0.7676550543779278, degree: 2, lambda: 1e-10\n",
      "New loss: 0.7666091256628067, degree: 4, lambda: 1.757510624854793e-08\n",
      "New loss: 0.7599205086217579, degree: 5, lambda: 1.757510624854793e-08\n",
      "Jet 0 with mass -> best loss: 0.7599205086217579, degree: 5, lambda: 1.757510624854793e-08\n",
      "jet: 2\n",
      "New loss: 0.531597117813513, degree: 1, lambda: 1e-10\n",
      "New loss: 0.5315971177771243, degree: 1, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.5315971177189203, degree: 1, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.5315971176258235, degree: 1, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.5315971174769721, degree: 1, lambda: 6.551285568595495e-10\n",
      "New loss: 0.5315971172389274, degree: 1, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.5315971168584178, degree: 1, lambda: 1.67683293681101e-09\n",
      "New loss: 0.5315971162505143, degree: 1, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.5315971152801791, degree: 1, lambda: 4.291934260128778e-09\n",
      "New loss: 0.5315971137334012, degree: 1, lambda: 6.866488450042998e-09\n",
      "New loss: 0.5315971112732175, degree: 1, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.5315971073741227, degree: 1, lambda: 1.757510624854793e-08\n",
      "New loss: 0.5315971012305597, degree: 1, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.5315970916429281, degree: 1, lambda: 4.498432668969444e-08\n",
      "New loss: 0.5315970769202571, degree: 1, lambda: 7.196856730011529e-08\n",
      "New loss: 0.5315970549377099, degree: 1, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.5315970237696811, degree: 1, lambda: 1.8420699693267165e-07\n",
      "New loss: 0.5315969840571609, degree: 1, lambda: 2.94705170255181e-07\n",
      "New loss: 0.5315969461494835, degree: 1, lambda: 4.7148663634573897e-07\n",
      "New loss: 0.5214410208760581, degree: 2, lambda: 1e-10\n",
      "New loss: 0.5214410189032518, degree: 2, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.5214410135201727, degree: 2, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.5214410057169866, degree: 2, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.5214409925466367, degree: 2, lambda: 6.551285568595495e-10\n",
      "New loss: 0.5214409724376358, degree: 2, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.5214409399014143, degree: 2, lambda: 1.67683293681101e-09\n",
      "New loss: 0.5214408879234221, degree: 2, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.5214408047490195, degree: 2, lambda: 4.291934260128778e-09\n",
      "New loss: 0.5214406722567536, degree: 2, lambda: 6.866488450042998e-09\n",
      "New loss: 0.5214404613619527, degree: 2, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.5214401267225517, degree: 2, lambda: 1.757510624854793e-08\n",
      "New loss: 0.5214395983951136, degree: 2, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.5214387708100247, degree: 2, lambda: 4.498432668969444e-08\n",
      "New loss: 0.5214374905932696, degree: 2, lambda: 7.196856730011529e-08\n",
      "New loss: 0.5214355492152754, degree: 2, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.5214326965331797, degree: 2, lambda: 1.8420699693267165e-07\n",
      "New loss: 0.5214287088323802, degree: 2, lambda: 2.94705170255181e-07\n",
      "New loss: 0.5214235626669214, degree: 2, lambda: 4.7148663634573897e-07\n",
      "New loss: 0.5214177528996032, degree: 2, lambda: 7.543120063354623e-07\n",
      "New loss: 0.521412694742872, degree: 2, lambda: 1.2067926406393288e-06\n",
      "New loss: 0.5214109648786159, degree: 2, lambda: 1.9306977288832498e-06\n",
      "New loss: 0.5209106280649621, degree: 3, lambda: 1e-10\n",
      "New loss: 0.5209104341927393, degree: 3, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.5209096878957467, degree: 3, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.5209082949785112, degree: 3, lambda: 6.551285568595495e-10\n",
      "New loss: 0.5209059501067405, degree: 3, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.5209021617509791, degree: 3, lambda: 1.67683293681101e-09\n",
      "New loss: 0.5208961636957462, degree: 3, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.5208868230836633, degree: 3, lambda: 4.291934260128778e-09\n",
      "New loss: 0.5208725425122492, degree: 3, lambda: 6.866488450042998e-09\n",
      "New loss: 0.5208512350175646, degree: 3, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.5208204701937931, degree: 3, lambda: 1.757510624854793e-08\n",
      "New loss: 0.5207778994331645, degree: 3, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.5207219746661144, degree: 3, lambda: 4.498432668969444e-08\n",
      "New loss: 0.5206527146622415, degree: 3, lambda: 7.196856730011529e-08\n",
      "New loss: 0.5205719125888925, degree: 3, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.5204821157300593, degree: 3, lambda: 1.8420699693267165e-07\n",
      "New loss: 0.5203846541135225, degree: 3, lambda: 2.94705170255181e-07\n",
      "New loss: 0.5202784165819124, degree: 3, lambda: 4.7148663634573897e-07\n",
      "New loss: 0.5201609885800232, degree: 3, lambda: 7.543120063354623e-07\n",
      "New loss: 0.5200321652087401, degree: 3, lambda: 1.2067926406393288e-06\n",
      "New loss: 0.519898750050436, degree: 3, lambda: 1.9306977288832498e-06\n",
      "New loss: 0.5197793330535699, degree: 3, lambda: 3.088843596477485e-06\n",
      "New loss: 0.5197068725635857, degree: 3, lambda: 4.941713361323839e-06\n",
      "New loss: 0.5196319878289412, degree: 4, lambda: 8.286427728546843e-05\n",
      "New loss: 0.51903431294724, degree: 5, lambda: 1e-10\n",
      "New loss: 0.5181497861950609, degree: 5, lambda: 1.2067926406393288e-06\n",
      "New loss: 0.5172821882956996, degree: 5, lambda: 1.9306977288832498e-06\n",
      "New loss: 0.5169058851110625, degree: 5, lambda: 3.088843596477485e-06\n",
      "Jet 2 no mass -> best loss: 0.5169058851110625, degree: 5, lambda: 3.088843596477485e-06\n",
      "jet: 3\n",
      "New loss: 0.8851604133126557, degree: 1, lambda: 1e-10\n",
      "New loss: 0.834164699182527, degree: 2, lambda: 1e-10\n",
      "New loss: 0.8290998349717494, degree: 3, lambda: 1e-10\n",
      "New loss: 0.8110799014467981, degree: 4, lambda: 1e-10\n",
      "New loss: 0.8110794776738421, degree: 4, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.8110784808862901, degree: 4, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.8110767561355585, degree: 4, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.8110741150593214, degree: 4, lambda: 6.551285568595495e-10\n",
      "New loss: 0.8110704140509758, degree: 4, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.8110657055448934, degree: 4, lambda: 1.67683293681101e-09\n",
      "New loss: 0.8110604557492046, degree: 4, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.8110557372603021, degree: 4, lambda: 4.291934260128778e-09\n",
      "New loss: 0.811053272626055, degree: 4, lambda: 6.866488450042998e-09\n",
      "New loss: 0.8052051831819442, degree: 5, lambda: 1e-10\n",
      "New loss: 0.8052032111043258, degree: 5, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.8052017071318115, degree: 5, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.8052010430712624, degree: 5, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.8024542534395898, degree: 6, lambda: 1e-10\n",
      "New loss: 0.802419249299803, degree: 6, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.802374324155333, degree: 6, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.8023163012996734, degree: 6, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.8022439207261011, degree: 6, lambda: 6.551285568595495e-10\n",
      "New loss: 0.8021597527350641, degree: 6, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.8020716806037264, degree: 6, lambda: 1.67683293681101e-09\n",
      "New loss: 0.8019927109393947, degree: 6, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.8019381582750348, degree: 6, lambda: 4.291934260128778e-09\n",
      "New loss: 0.8019221646487168, degree: 6, lambda: 6.866488450042998e-09\n",
      "New loss: 0.8005554859923826, degree: 7, lambda: 1e-10\n",
      "New loss: 0.7994612500067443, degree: 7, lambda: 1.8420699693267165e-07\n",
      "New loss: 0.7985254322096643, degree: 7, lambda: 2.94705170255181e-07\n",
      "New loss: 0.7981928416420802, degree: 7, lambda: 4.7148663634573897e-07\n",
      "New loss: 0.7963747147428814, degree: 8, lambda: 1e-10\n",
      "New loss: 0.7962898283041246, degree: 8, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.7962014064577662, degree: 8, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.7961497726067185, degree: 8, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.796037455545422, degree: 8, lambda: 6.551285568595495e-10\n",
      "New loss: 0.7959530195060359, degree: 8, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.7948802455005491, degree: 9, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.7898065728892583, degree: 9, lambda: 1.8420699693267165e-07\n",
      "New loss: 0.7897383717211319, degree: 9, lambda: 2.94705170255181e-07\n",
      "Jet 2 with mass -> best loss: 0.7897383717211319, degree: 9, lambda: 2.94705170255181e-07\n",
      "jet: 4\n",
      "New loss: 0.5763360360649484, degree: 1, lambda: 1e-10\n",
      "New loss: 0.5763354397804501, degree: 1, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.5763344882184817, degree: 1, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.576332971990592, degree: 1, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.5763305618406199, degree: 1, lambda: 6.551285568595495e-10\n",
      "New loss: 0.5763267454425516, degree: 1, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.5763207391251923, degree: 1, lambda: 1.67683293681101e-09\n",
      "New loss: 0.5763113773685349, degree: 1, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.5762970066174607, degree: 1, lambda: 4.291934260128778e-09\n",
      "New loss: 0.5762754668636435, degree: 1, lambda: 6.866488450042998e-09\n",
      "New loss: 0.5762443518390047, degree: 1, lambda: 1.0985411419875573e-08\n",
      "New loss: 0.5762018723228579, degree: 1, lambda: 1.757510624854793e-08\n",
      "New loss: 0.576148646242354, degree: 1, lambda: 2.8117686979742307e-08\n",
      "New loss: 0.5760902616908651, degree: 1, lambda: 4.498432668969444e-08\n",
      "New loss: 0.5760393908657118, degree: 1, lambda: 7.196856730011529e-08\n",
      "New loss: 0.5760156014848288, degree: 1, lambda: 1.1513953993264481e-07\n",
      "New loss: 0.559500035191184, degree: 2, lambda: 1e-10\n",
      "New loss: 0.5545828851414689, degree: 3, lambda: 1e-10\n",
      "New loss: 0.5534220924181953, degree: 5, lambda: 1e-10\n",
      "New loss: 0.5532305313353277, degree: 6, lambda: 1e-10\n",
      "Jet 4 no mass -> best loss: 0.5532305313353277, degree: 6, lambda: 1e-10\n",
      "jet: 5\n",
      "New loss: 0.8410011247546906, degree: 1, lambda: 1e-10\n",
      "New loss: 0.8014434936564702, degree: 2, lambda: 1e-10\n",
      "New loss: 0.7903613472163592, degree: 3, lambda: 1e-10\n",
      "New loss: 0.7774172748164186, degree: 4, lambda: 1e-10\n",
      "New loss: 0.7743566890156992, degree: 5, lambda: 1e-10\n",
      "New loss: 0.7712676226559058, degree: 6, lambda: 1e-10\n",
      "New loss: 0.7564197991003053, degree: 7, lambda: 1e-10\n",
      "New loss: 0.7396287096427434, degree: 8, lambda: 1e-10\n",
      "New loss: 0.7330187659269308, degree: 9, lambda: 1e-10\n",
      "Jet 4 with mass -> best loss: 0.7330187659269308, degree: 9, lambda: 1e-10\n",
      "jet: 6\n",
      "New loss: 0.4904641511607557, degree: 1, lambda: 1e-10\n",
      "New loss: 0.48813703119826674, degree: 2, lambda: 1e-10\n",
      "New loss: 0.48812855382095127, degree: 2, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.48807244813898426, degree: 2, lambda: 6.551285568595495e-10\n",
      "New loss: 0.4880061216081148, degree: 2, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.4879513786553868, degree: 2, lambda: 1.67683293681101e-09\n",
      "New loss: 0.4879438920024231, degree: 2, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.48787121092491864, degree: 4, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.4875935869821442, degree: 4, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.487352906078457, degree: 4, lambda: 6.551285568595495e-10\n",
      "New loss: 0.4871552611927991, degree: 4, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.4870172774789576, degree: 4, lambda: 1.67683293681101e-09\n",
      "New loss: 0.48696627179067636, degree: 4, lambda: 2.6826957952797275e-09\n",
      "New loss: 0.4866692867360803, degree: 5, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.4863903768405337, degree: 5, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.48618210190163175, degree: 5, lambda: 6.551285568595495e-10\n",
      "New loss: 0.48604512803618877, degree: 5, lambda: 1.0481131341546874e-09\n",
      "New loss: 0.48598143431715485, degree: 5, lambda: 1.67683293681101e-09\n",
      "Jet 6 no mass -> best loss: 0.48598143431715485, degree: 5, lambda: 1.67683293681101e-09\n",
      "jet: 7\n",
      "New loss: 0.8559162975558924, degree: 1, lambda: 1e-10\n",
      "New loss: 0.811910488320556, degree: 2, lambda: 1e-10\n",
      "New loss: 0.8085381210090716, degree: 3, lambda: 1e-10\n",
      "New loss: 0.7864454901383221, degree: 4, lambda: 1e-10\n",
      "New loss: 0.7678551936959582, degree: 5, lambda: 1e-10\n",
      "New loss: 0.7678519107446329, degree: 5, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.7666594538551245, degree: 8, lambda: 1.5998587196060573e-10\n",
      "New loss: 0.7651128242760766, degree: 8, lambda: 2.5595479226995335e-10\n",
      "New loss: 0.7521512521846879, degree: 8, lambda: 4.0949150623804193e-10\n",
      "New loss: 0.7512362098880613, degree: 8, lambda: 6.551285568595495e-10\n",
      "New loss: 0.7510681898958587, degree: 8, lambda: 1.0481131341546874e-09\n",
      "Jet 6 with mass -> best loss: 0.7510681898958587, degree: 8, lambda: 1.0481131341546874e-09\n"
     ]
    }
   ],
   "source": [
    "degrees_star, lambdas_star = select_best_hypers_ridge(y_tr, tX_tr, max_degree = 9, k_fold = 12, min_lambda_pow = -10, max_lambda_pow = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_star = [7, 5, 5, 9, 6, 9, 5, 8]\n",
    "lambdas_star = [2.8117686979742307e-08, 1.757510624854793e-08, 3.088843596477485e-06, 2.94705170255181e-07, 1e-10, 1e-10, 1.67683293681101e-09, 1.0481131341546874e-09]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
