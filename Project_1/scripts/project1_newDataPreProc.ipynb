{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , deal with outliers, adding log colm (?)\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra preprocessing for logistic (run only if you wanna run that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_for_logistic(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(y_tr_tot, x_tr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(y_te_tot, x_te_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All already done: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0       138.470                       51.655        97.827    27.980   \n",
       "1       160.937                       68.768       103.235    48.146   \n",
       "2      -999.000                      162.172       125.953    35.635   \n",
       "3       143.905                       81.417        80.943     0.414   \n",
       "4       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  PRI_met_phi  \\\n",
       "0               3.064      41.928     197.760  ...       -0.277   \n",
       "1               3.473       2.078     125.157  ...       -1.916   \n",
       "2               3.148       9.336     197.814  ...       -2.186   \n",
       "3               3.310       0.414      75.968  ...        0.060   \n",
       "4               3.891      16.405      57.983  ...       -0.871   \n",
       "\n",
       "   PRI_met_sumet  PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  \\\n",
       "0        258.733            2              67.435                2.150   \n",
       "1        164.546            1              46.226                0.725   \n",
       "2        260.414            1              44.251                2.053   \n",
       "3         86.062            0            -999.000             -999.000   \n",
       "4         53.131            0            -999.000             -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "0                  -2.475         113.497  \n",
       "1                -999.000          46.226  \n",
       "2                -999.000          44.251  \n",
       "3                -999.000           0.000  \n",
       "4                -999.000           0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd = pd.read_csv('../data/train.csv')\n",
    "tX_pd.drop(labels=['Id', 'Prediction'], axis=1, inplace=True)\n",
    "tX_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pd0 = tX_pd[tX_pd['PRI_jet_num']==0]\n",
    "tX_pd1 = tX_pd[tX_pd['PRI_jet_num']==1]\n",
    "tX_pd2 = tX_pd[tX_pd['PRI_jet_num']==2]\n",
    "tX_pd3 = tX_pd[tX_pd['PRI_jet_num']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pds = [tX_pd0, tX_pd1, tX_pd2, tX_pd3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet: 0\n",
      "26123 0 DER_mass_MMC\n",
      "99913 4 DER_deltaeta_jet_jet\n",
      "99913 5 DER_mass_jet_jet\n",
      "99913 6 DER_prodeta_jet_jet\n",
      "99913 12 DER_lep_eta_centrality\n",
      "99913 23 PRI_jet_leading_pt\n",
      "99913 24 PRI_jet_leading_eta\n",
      "99913 25 PRI_jet_leading_phi\n",
      "99913 26 PRI_jet_subleading_pt\n",
      "99913 27 PRI_jet_subleading_eta\n",
      "99913 28 PRI_jet_subleading_phi\n",
      "jet: 1\n",
      "7562 0 DER_mass_MMC\n",
      "77544 4 DER_deltaeta_jet_jet\n",
      "77544 5 DER_mass_jet_jet\n",
      "77544 6 DER_prodeta_jet_jet\n",
      "77544 12 DER_lep_eta_centrality\n",
      "77544 26 PRI_jet_subleading_pt\n",
      "77544 27 PRI_jet_subleading_eta\n",
      "77544 28 PRI_jet_subleading_phi\n",
      "jet: 2\n",
      "2952 0 DER_mass_MMC\n",
      "jet: 3\n",
      "1477 0 DER_mass_MMC\n"
     ]
    }
   ],
   "source": [
    "#searching for nulls:\n",
    "for jet in range (0, 4):\n",
    "    print (\"jet: {}\".format(jet))\n",
    "    i = 0\n",
    "    for c in tX_pds[jet]:\n",
    "        s = tx_pds[jet][tx_pds[jet][c] == -999].index.size\n",
    "        if s > 0:\n",
    "            print(s, i, c)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet: 0\n",
      "99913 22 PRI_jet_num\n",
      "99913 29 PRI_jet_all_pt\n",
      "jet: 1\n",
      "jet: 2\n",
      "9925 12 DER_lep_eta_centrality\n",
      "jet: 3\n",
      "5827 12 DER_lep_eta_centrality\n"
     ]
    }
   ],
   "source": [
    "#searching for zeros:\n",
    "for jet in range (0, 4):\n",
    "    print (\"jet: {}\".format(jet))\n",
    "    i = 0\n",
    "    for c in tX_pds[jet]:\n",
    "        s = tx_pds[jet][tx_pds[jet][c] == 0].index.size\n",
    "        if s > 1000:\n",
    "            print(s, i, c)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dealing with outliers\n",
    "# for jet in range(0, 4):\n",
    "#     for i in range(0, tX_new[jet].shape[1]):\n",
    "#         tX_act = tX_new[jet]\n",
    "#         q1 = np.quantile(tX_act[:,i], 0.25)\n",
    "#         q3 = np.quantile(tX_act[:,i], 0.75)\n",
    "#         iqr = q3 - q1\n",
    "#         max_val = q3 + 1.5*iqr \n",
    "#         indexs_out = tX_act[:,i] > max_val\n",
    "#         #print(tX_act[indexs_out, :].shape)\n",
    "#         tX_act[indexs_out, :] = max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Main Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, acc = False, ls = False):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    accuracies = []\n",
    "    ws = []\n",
    "    \n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        #print(index_te, index_tr)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "        \n",
    "        # form data with polynomial degree\n",
    "        x_te_poly = build_poly(x_te, degree)\n",
    "        x_tr_poly = build_poly(x_tr, degree)\n",
    "        \n",
    "        if (ls==False):\n",
    "            w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "        else:\n",
    "            w = least_squares_lstsq_ver(y_tr, x_tr_poly)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "        rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "        #print(lambda_, rmse_te)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "        \n",
    "        y_pred = predict_labels(w, x_te_poly)\n",
    "        accuracies.append(accuracy(y_te, y_pred))\n",
    "        \n",
    "        ws.append(w)\n",
    "        \n",
    "    if acc==False:\n",
    "        loss_tr = np.median(losses_tr)\n",
    "        loss_te = np.median(losses_te)\n",
    "        return loss_tr, loss_te, np.mean(ws, axis=0)\n",
    "    else:\n",
    "        return np.mean(accuracies), np.mean(ws, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    degree = 3\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-8, 1, 50)\n",
    "#     for d in range (1, degree+1):\n",
    "#         for i in range (0, 3):\n",
    "    #seed = i\n",
    "    # split data in k fold\n",
    "    #y_sub, x_sub = get_subsample(y_tr, x_tr, 10000, seed)\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y_tr[0], k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr, loss_te = cross_validation(y_tr[0], x_tr[0], k_indices, k_fold, lambda_, degree)\n",
    "        #print(loss_te)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, degree)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree(y, x, max_degree, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 100000, seed)\n",
    "    lambdas = np.logspace(-10, 0, 5)\n",
    "    #lambdas=[1e-06]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    w_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        for lambda_ in lambdas:\n",
    "            #print(degree)\n",
    "            loss_tr, loss_te, w = cross_validation(y, x, k_indices, k_fold, lambda_, degree)\n",
    "            #print(loss_te)\n",
    "            if loss_te < loss_min:\n",
    "                loss_min = loss_te\n",
    "                print(\"New loss: {}, degree: {}, lambda: {}\".format(loss_te, degree, lambda_))\n",
    "                degree_star = degree\n",
    "                lambda_star = lambda_\n",
    "                w_star = w\n",
    "    return degree_star, lambda_star, loss_min, w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree_accuracy(y, x, max_degree, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 100000, seed)\n",
    "    lambdas = np.logspace(-10, 0, 5)\n",
    "    #lambdas=[1e-06]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    acc_max = 0\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    w_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        for lambda_ in lambdas:\n",
    "            #print(degree)\n",
    "            accuracy, w = cross_validation(y, x, k_indices, k_fold, lambda_, degree, acc=True)\n",
    "            #print(loss_te)\n",
    "            if accuracy > acc_max:\n",
    "                acc_max = accuracy\n",
    "                print(\"New accuracy: {}, degree: {}, lambda: {}\".format(accuracy, degree, lambda_))\n",
    "                degree_star = degree\n",
    "                lambda_star = lambda_\n",
    "                w_star = w\n",
    "    return degree_star, lambda_star, acc_max, w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet 0: \n",
      "New loss: 0.4249167699506766, degree: 1, lambda: 1e-10\n",
      "New loss: 0.4249160599803238, degree: 1, lambda: 3.162277660168379e-08\n",
      "New loss: 0.42290159516796066, degree: 2, lambda: 1e-10\n",
      "New loss: 0.41893315322898217, degree: 4, lambda: 1e-10\n"
     ]
    }
   ],
   "source": [
    "max_degree = 8\n",
    "k_fold = 8\n",
    "degrees_star=[]\n",
    "lambdas_star=[]\n",
    "w_star = []\n",
    "for jet in range(0, 8):\n",
    "    print(\"jet {}: \".format(jet))\n",
    "    degree_star, lambda_star, loss, w = select_best_degree(y_tr[jet], tX_tr[jet], max_degree, k_fold, 1)\n",
    "    degrees_star.append(degree_star)\n",
    "    lambdas_star.append(lambda_star)\n",
    "    w_star.append(w)\n",
    "    #print(\"jet {}: Best accuracy {}, degree: {},  lambda: {}\".format(jet, acc, degree_star, lambda_star))\n",
    "    print(\"jet{}: best loss: {}, degree: {}, lambda: {}\".format(jet, loss, degree_star, lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "degrees = [5, 8, 8, 8]\n",
    "lambdas = [2.807216203941181e-07, 2.807216203941181e-07, 1e-10, 2.2122162910704502e-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree_ls(y, x, max_degree, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 50000, seed)\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        rmse_te = []\n",
    "        loss_tr, loss_te = cross_validation(y, x, k_indices, k_fold, 0, degree, mean=True, ls=True)\n",
    "        #print(degree, lambda_, loss_te)\n",
    "        if loss_te < loss_min:\n",
    "            loss_min = loss_te\n",
    "            print(\"New loss: {}, degree: {}\".format(loss_te, degree))\n",
    "            degree_star = degree\n",
    "    return degree_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.7361479935065579, degree: 1\n",
      "New loss: 0.7081331175137364, degree: 2\n",
      "New loss: 0.7042588060195737, degree: 3\n",
      "New loss: 0.7031084041385761, degree: 4\n",
      "Best degree for least squares: 4\n",
      "New loss: 0.8642239524433158, degree: 1\n",
      "New loss: 0.8280068635705337, degree: 2\n",
      "New loss: 0.8255413920585553, degree: 3\n",
      "New loss: 0.8214537575154983, degree: 5\n",
      "Best degree for least squares: 5\n",
      "New loss: 0.8417973669655965, degree: 1\n",
      "New loss: 0.8074770514863915, degree: 2\n",
      "New loss: 0.7993306312028321, degree: 4\n",
      "Best degree for least squares: 4\n",
      "New loss: 0.8454785517535026, degree: 1\n",
      "New loss: 0.8173852852871264, degree: 2\n",
      "New loss: 0.8039465316482824, degree: 5\n",
      "Best degree for least squares: 5\n"
     ]
    }
   ],
   "source": [
    "max_degree = 13\n",
    "k_fold = 8\n",
    "degrees_star_ls = []\n",
    "for jet in range(0, 4):\n",
    "    degree_star_ls = select_best_degree_ls(y_tr[jet], tX_tr[jet], max_degree, k_fold, 1)\n",
    "    degrees_star_ls.append(degree_star_ls)\n",
    "    print(\"Best degree for jet {} for least squares: {}\".format(jet, degree_star_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc 0.7888 ridge (0.798 on real test)\n",
    "lambdas = [1e-09, 1.1288378916846883e-08, 0.00018329807108324338, 1e-09]\n",
    "degrees = [3, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc 0.79634 ridge (0.785 on real test)\n",
    "lambdas = [1e-07, 1e-07, 1e-08, 1e-08]\n",
    "degrees = [5, 5, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc 0.79636\n",
    "lambdas = [2.807216203941181e-07, 2.807216203941181e-07, 1e-10, 2.2122162910704502e-10]\n",
    "degrees = [5, 5, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#good 0.79 acc least square (lstsq ver)\n",
    "degrees_ls = [4, 5, 4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=0.9198964473372747, loss=0.49991543478685996, w0=-3.1381702249274604e-05, w1=-1.870332018899931e-05\n",
      "Gradient Descent(1/199): ||gradient||=0.9186848945865361, loss=0.4997636980245322, w0=-8.78042153832626e-05, w1=-5.236507656267239e-05\n",
      "Gradient Descent(2/199): ||gradient||=0.9165086790795948, loss=0.4994921215723027, w0=-0.00018915591588298592, w1=-0.00011294257728487969\n",
      "Gradient Descent(3/199): ||gradient||=0.9126063083581455, loss=0.4990082616199879, w0=-0.000370914263529803, w1=-0.0002219378872869192\n",
      "Gradient Descent(4/199): ||gradient||=0.9056299309807633, loss=0.49815319084166065, w0=-0.0006959041130019329, w1=-0.00041798666987893115\n",
      "Gradient Descent(5/199): ||gradient||=0.8932269289430593, loss=0.49666407351067016, w0=-0.0012739014639035493, w1=-0.0007704142328603178\n",
      "Gradient Descent(6/199): ||gradient||=0.8713980371643467, loss=0.49413764114400255, w0=-0.002292032255140569, w1=-0.0014033062129404496\n",
      "Gradient Descent(7/199): ||gradient||=0.8336914854870378, loss=0.49004496000892644, w0=-0.004054621864207936, w1=-0.0025378020652307696\n",
      "Gradient Descent(8/199): ||gradient||=0.7708210576370715, loss=0.48392417182397535, w0=-0.007012143781032614, w1=-0.0045650758007922156\n",
      "Gradient Descent(9/199): ||gradient||=0.6730593787686956, loss=0.4758746897312079, w0=-0.011703585013512112, w1=-0.008168591631344086\n",
      "Gradient Descent(10/199): ||gradient||=0.5420469001358224, loss=0.4668314646613726, w0=-0.018440239398187373, w1=-0.014520131670101091\n",
      "Gradient Descent(11/199): ||gradient||=0.417585276699318, loss=0.45684219087207145, w0=-0.0266511349012321, w1=-0.02557881435937765\n",
      "Gradient Descent(12/199): ||gradient||=0.34951950406858123, loss=0.4441675056514142, w0=-0.03507313894297936, w1=-0.044530615642647674\n",
      "Gradient Descent(13/199): ||gradient||=0.2802581748299309, loss=0.4305010778316688, w0=-0.044834692707357696, w1=-0.07630731446605377\n",
      "Gradient Descent(14/199): ||gradient||=0.19933661937191804, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(15/199): ||gradient||=0.2126484850736893, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(16/199): ||gradient||=0.2126484850736893, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(17/199): ||gradient||=0.41005196201736743, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(18/199): ||gradient||=0.41005196201736743, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(19/199): ||gradient||=0.3562131324819593, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(20/199): ||gradient||=0.3562131324819593, loss=0.4119259421855884, w0=-0.03309942831257582, w1=-0.16844869171449792\n",
      "Gradient Descent(21/199): ||gradient||=0.12649027507232175, loss=0.41025030347357094, w0=-0.029575140439970622, w1=-0.17946578929166102\n",
      "Gradient Descent(22/199): ||gradient||=0.12292495119423105, loss=0.40743339281067326, w0=-0.023442494523926122, w1=-0.19876354675726643\n",
      "Gradient Descent(23/199): ||gradient||=0.11680132506627634, loss=0.4029471327448956, w0=-0.01278468241212367, w1=-0.23182458833403902\n",
      "Gradient Descent(24/199): ||gradient||=0.10674668689952199, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(25/199): ||gradient||=0.11037853434564666, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(26/199): ||gradient||=0.11037853434564666, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(27/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(28/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(29/199): ||gradient||=0.30949111243344546, loss=0.3934972452046127, w0=0.012645111284258674, w1=-0.3149399904670085\n",
      "Gradient Descent(30/199): ||gradient||=0.08518921631593725, loss=0.3928422773768506, w0=0.01390331604737596, w1=-0.3215594212339835\n",
      "Gradient Descent(31/199): ||gradient||=0.08143325683778938, loss=0.39176458577149614, w0=0.01769426852847459, w1=-0.333137608291744\n",
      "Gradient Descent(32/199): ||gradient||=0.08261374022271092, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(33/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(34/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(35/199): ||gradient||=0.15244231978895387, loss=0.38970194899647265, w0=0.023115513292314566, w1=-0.35613896221101615\n",
      "Gradient Descent(36/199): ||gradient||=0.07231686370955237, loss=0.38924702709110287, w0=0.024867215085094988, w1=-0.36144019878751626\n",
      "Gradient Descent(37/199): ||gradient||=0.0693800954973171, loss=0.3884863243303348, w0=0.026866841411884168, w1=-0.3708048228655269\n",
      "Gradient Descent(38/199): ||gradient||=0.06849694468173842, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(39/199): ||gradient||=0.09598461127388905, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(40/199): ||gradient||=0.09598461127388905, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(41/199): ||gradient||=0.1410714760751551, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(42/199): ||gradient||=0.1410714760751551, loss=0.3864215399793017, w0=0.0350727177951097, w1=-0.39921296841299686\n",
      "Gradient Descent(43/199): ||gradient||=0.08456831532148283, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(44/199): ||gradient||=0.11635687063747818, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(45/199): ||gradient||=0.11635687063747818, loss=0.3855801463907678, w0=0.037564023270261745, w1=-0.4104639083541116\n",
      "Gradient Descent(46/199): ||gradient||=0.07098725686300805, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(47/199): ||gradient||=0.08865280508918541, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(48/199): ||gradient||=0.08865280508918541, loss=0.38485406384997894, w0=0.0397988301803087, w1=-0.4207754388932331\n",
      "Gradient Descent(49/199): ||gradient||=0.05956788893440248, loss=0.38448192908571455, w0=0.0402881948329062, w1=-0.4270292492982512\n",
      "Gradient Descent(50/199): ||gradient||=0.06633820597340725, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(51/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(52/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(53/199): ||gradient||=0.15761065003057645, loss=0.38370536779400194, w0=0.044360770783240404, w1=-0.4394259860437441\n",
      "Gradient Descent(54/199): ||gradient||=0.06222099110230357, loss=0.38348800700174634, w0=0.044301326033474996, w1=-0.44236759427566763\n",
      "Gradient Descent(55/199): ||gradient||=0.04860668539425892, loss=0.383189073110083, w0=0.04591786180813736, w1=-0.4475058466448936\n",
      "Gradient Descent(56/199): ||gradient||=0.04885484002764845, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(57/199): ||gradient||=0.07218281893711131, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(58/199): ||gradient||=0.07218281893711131, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(59/199): ||gradient||=0.0865949367220621, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(60/199): ||gradient||=0.0865949367220621, loss=0.38229970311486866, w0=0.04905354484341635, w1=-0.4634154097612027\n",
      "Gradient Descent(61/199): ||gradient||=0.045554523575472565, loss=0.38208015281211827, w0=0.05059222337498455, w1=-0.46760109124772614\n",
      "Gradient Descent(62/199): ||gradient||=0.046707216525688364, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(63/199): ||gradient||=0.08067338628463173, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(64/199): ||gradient||=0.08067338628463173, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(65/199): ||gradient||=0.09181351733579902, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(66/199): ||gradient||=0.09181351733579902, loss=0.38140101837396745, w0=0.052993763875687355, w1=-0.48064650178588253\n",
      "Gradient Descent(67/199): ||gradient||=0.040172136024456635, loss=0.38123091391261776, w0=0.05420974414245746, w1=-0.48409726949739024\n",
      "Gradient Descent(68/199): ||gradient||=0.04011639835467928, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(69/199): ||gradient||=0.0587199843578713, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(70/199): ||gradient||=0.0587199843578713, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(71/199): ||gradient||=0.06065901120024391, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(72/199): ||gradient||=0.06065901120024391, loss=0.38070675484703786, w0=0.05629675863399411, w1=-0.4948880697081792\n",
      "Gradient Descent(73/199): ||gradient||=0.03437710169565648, loss=0.3805724543076369, w0=0.05709337174035709, w1=-0.4977650896919669\n",
      "Gradient Descent(74/199): ||gradient||=0.03371872029255384, loss=0.380347504802962, w0=0.057923161002275445, w1=-0.502821417967265\n",
      "Gradient Descent(75/199): ||gradient||=0.0353360741618754, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(76/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(77/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(78/199): ||gradient||=0.0868549972476703, loss=0.37990446055546606, w0=0.06025622386799663, w1=-0.512732449768847\n",
      "Gradient Descent(79/199): ||gradient||=0.03013797122742819, loss=0.3798059156030959, w0=0.06086479596353714, w1=-0.5149947769707354\n",
      "Gradient Descent(80/199): ||gradient||=0.029582646754048075, loss=0.3796377325021035, w0=0.06155759036056807, w1=-0.5189768598962371\n",
      "Gradient Descent(81/199): ||gradient||=0.02988603821652102, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(82/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(83/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(84/199): ||gradient||=0.05385331845836831, loss=0.37930748989144936, w0=0.06340685859282029, w1=-0.5268228016896819\n",
      "Gradient Descent(85/199): ||gradient||=0.026807087764073437, loss=0.379232343212017, w0=0.06379460699418472, w1=-0.5286305900589766\n",
      "Gradient Descent(86/199): ||gradient||=0.026429755894928714, loss=0.37910158102876307, w0=0.06449936504431049, w1=-0.5318081753946006\n",
      "Gradient Descent(87/199): ||gradient||=0.025779232451409895, loss=0.37887998603582906, w0=0.06565536813188565, w1=-0.5372914115868667\n",
      "Gradient Descent(88/199): ||gradient||=0.02479156357937694, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(89/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(90/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(91/199): ||gradient||=0.03258988562878005, loss=0.37847536538605114, w0=0.06774134339187216, w1=-0.5477141177050802\n",
      "Gradient Descent(92/199): ||gradient||=0.027738362406501706, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(93/199): ||gradient||=0.03951405284074884, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(94/199): ||gradient||=0.03951405284074884, loss=0.37834206727202646, w0=0.06842349019205861, w1=-0.5511561942990433\n",
      "Gradient Descent(95/199): ||gradient||=0.0304225158131295, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(96/199): ||gradient||=0.046421773810511664, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(97/199): ||gradient||=0.046421773810511664, loss=0.3782192101919261, w0=0.06906497913554238, w1=-0.5543145897328704\n",
      "Gradient Descent(98/199): ||gradient||=0.03256348800089378, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(99/199): ||gradient||=0.05053199788292059, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(100/199): ||gradient||=0.05053199788292059, loss=0.37810265907289226, w0=0.06968905597825942, w1=-0.5572191567845676\n",
      "Gradient Descent(101/199): ||gradient||=0.032866606098295975, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(102/199): ||gradient||=0.049904511233416445, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(103/199): ||gradient||=0.049904511233416445, loss=0.37799031022315804, w0=0.07030697705295337, w1=-0.5598962692587653\n",
      "Gradient Descent(104/199): ||gradient||=0.030864528540993398, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(105/199): ||gradient||=0.044474850521653266, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(106/199): ||gradient||=0.044474850521653266, loss=0.3778831177632533, w0=0.0709143275158598, w1=-0.5623694962267904\n",
      "Gradient Descent(107/199): ||gradient||=0.02725555397893965, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(108/199): ||gradient||=0.036174660635675905, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(109/199): ||gradient||=0.036174660635675905, loss=0.3777832329470302, w0=0.07149562027576611, w1=-0.5646599120664837\n",
      "Gradient Descent(110/199): ||gradient||=0.023491643257352396, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(111/199): ||gradient||=0.02798541531932825, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(112/199): ||gradient||=0.02798541531932825, loss=0.37769151450075156, w0=0.07203435972166758, w1=-0.5667861513379303\n",
      "Gradient Descent(113/199): ||gradient||=0.020796448379199806, loss=0.3776393572545536, w0=0.07258948801176464, w1=-0.568075243409437\n",
      "Gradient Descent(114/199): ||gradient||=0.02232622559275806, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(115/199): ||gradient||=0.044569960234299495, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(116/199): ||gradient||=0.044569960234299495, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(117/199): ||gradient||=0.06049611094726945, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(118/199): ||gradient||=0.06049611094726945, loss=0.3774734104704165, w0=0.07307154647503165, w1=-0.5721438415511358\n",
      "Gradient Descent(119/199): ||gradient||=0.02560307349675981, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(120/199): ||gradient||=0.030235195825791275, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(121/199): ||gradient||=0.030235195825791275, loss=0.3773936500349805, w0=0.0735942921530556, w1=-0.5738034701868857\n",
      "Gradient Descent(122/199): ||gradient||=0.01930338988021067, loss=0.3773502513239982, w0=0.07402822791850343, w1=-0.5748167485368086\n",
      "Gradient Descent(123/199): ||gradient||=0.019883352701387243, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(124/199): ||gradient||=0.03180432497976459, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(125/199): ||gradient||=0.03180432497976459, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(126/199): ||gradient||=0.0368369701461698, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(127/199): ||gradient||=0.0368369701461698, loss=0.3772087103489772, w0=0.07452601579779915, w1=-0.5780354108422809\n",
      "Gradient Descent(128/199): ||gradient||=0.018672376155633576, loss=0.3771703336844059, w0=0.07491495902096323, w1=-0.5789005133364988\n",
      "Gradient Descent(129/199): ||gradient||=0.01894887411582462, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(130/199): ||gradient||=0.027883181430691767, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(131/199): ||gradient||=0.027883181430691767, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(132/199): ||gradient||=0.029631455078381125, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(133/199): ||gradient||=0.029631455078381125, loss=0.37704490546502617, w0=0.07536964604927561, w1=-0.5816628169500092\n",
      "Gradient Descent(134/199): ||gradient||=0.01724997691647665, loss=0.3770105401718543, w0=0.07562881667941775, w1=-0.5824123704142081\n",
      "Gradient Descent(135/199): ||gradient||=0.017165877881501656, loss=0.37695177088913756, w0=0.07574484702850508, w1=-0.5837476546228534\n",
      "Gradient Descent(136/199): ||gradient||=0.01899421714878402, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(137/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(138/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(139/199): ||gradient||=0.054220090727918, loss=0.37682644524415465, w0=0.07640556694575028, w1=-0.5864102724253792\n",
      "Gradient Descent(140/199): ||gradient||=0.016650580776069446, loss=0.3767965645111003, w0=0.07664467148671401, w1=-0.5870356600147364\n",
      "Gradient Descent(141/199): ||gradient||=0.016501858917438078, loss=0.376745477937799, w0=0.07671846419935624, w1=-0.5881547717783094\n",
      "Gradient Descent(142/199): ||gradient||=0.01816804164140217, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(143/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(144/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(145/199): ||gradient||=0.04825610442092132, loss=0.3766356523318643, w0=0.07732128008506954, w1=-0.5903961372206211\n",
      "Gradient Descent(146/199): ||gradient||=0.01563931340254658, loss=0.3766094801974734, w0=0.07745336774852776, w1=-0.5909299030113423\n",
      "Gradient Descent(147/199): ||gradient||=0.01555626373968682, loss=0.37656297807301164, w0=0.07763617808606241, w1=-0.5918788948154293\n",
      "Gradient Descent(148/199): ||gradient||=0.015451985463614959, loss=0.3764819326011553, w0=0.07809347822581313, w1=-0.5935397672728017\n",
      "Gradient Descent(149/199): ||gradient||=0.016210974608243976, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(150/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(151/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(152/199): ||gradient||=0.0572020732177142, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(153/199): ||gradient||=0.04260833075213723, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(154/199): ||gradient||=0.04260833075213723, loss=0.37630531695231634, w0=0.0788118352881565, w1=-0.5970633924846425\n",
      "Gradient Descent(155/199): ||gradient||=0.014908211767569006, loss=0.3762840666377174, w0=0.0788347473226203, w1=-0.5974755644658697\n",
      "Gradient Descent(156/199): ||gradient||=0.014708048925549026, loss=0.3762467257517887, w0=0.07906897984336392, w1=-0.5982031234845887\n",
      "Gradient Descent(157/199): ||gradient||=0.014963885063396522, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(158/199): ||gradient||=0.021972572186543523, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(159/199): ||gradient||=0.021972572186543523, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(160/199): ||gradient||=0.03521861893866122, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(161/199): ||gradient||=0.03521861893866122, loss=0.3761362201008082, w0=0.07924611088708965, w1=-0.6005349176900081\n",
      "Gradient Descent(162/199): ||gradient||=0.02351622446106723, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(163/199): ||gradient||=0.03747859183540602, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(164/199): ||gradient||=0.03747859183540602, loss=0.3760848467338232, w0=0.07945208937262828, w1=-0.6015121960900223\n",
      "Gradient Descent(165/199): ||gradient||=0.023370968435216157, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(166/199): ||gradient||=0.03591853609344955, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(167/199): ||gradient||=0.03591853609344955, loss=0.37603359618573673, w0=0.07967763478331942, w1=-0.6024388052305133\n",
      "Gradient Descent(168/199): ||gradient||=0.021481809200450213, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(169/199): ||gradient||=0.030995904539739747, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(170/199): ||gradient||=0.030995904539739747, loss=0.3759837423493009, w0=0.07991361406706003, w1=-0.6033190135612887\n",
      "Gradient Descent(171/199): ||gradient||=0.018635502628599573, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(172/199): ||gradient||=0.0245163904254113, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(173/199): ||gradient||=0.0245163904254113, loss=0.3759366863297597, w0=0.08014500940912765, w1=-0.6041568478334536\n",
      "Gradient Descent(174/199): ||gradient||=0.01599958312904941, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(175/199): ||gradient||=0.018751655043872835, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(176/199): ||gradient||=0.018751655043872835, loss=0.3758928304184189, w0=0.0803586612622116, w1=-0.6049558909348881\n",
      "Gradient Descent(177/199): ||gradient||=0.014338863123069666, loss=0.3758671970659703, w0=0.08062719990283188, w1=-0.6054485791833413\n",
      "Gradient Descent(178/199): ||gradient||=0.015200543414685935, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(179/199): ||gradient||=0.027969392552206655, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(180/199): ||gradient||=0.027969392552206655, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(181/199): ||gradient||=0.036968997621708515, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(182/199): ||gradient||=0.036968997621708515, loss=0.375783531496386, w0=0.08070975465583485, w1=-0.6070644065150398\n",
      "Gradient Descent(183/199): ||gradient||=0.0167182053280009, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(184/199): ||gradient||=0.018986866136306145, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(185/199): ||gradient||=0.018986866136306145, loss=0.3757441578195452, w0=0.08094097589901549, w1=-0.6077511395045695\n",
      "Gradient Descent(186/199): ||gradient||=0.013572183029778981, loss=0.37572153696885513, w0=0.08114010120247711, w1=-0.6081779108393255\n",
      "Gradient Descent(187/199): ||gradient||=0.013800818079967348, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(188/199): ||gradient||=0.019086297838013175, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(189/199): ||gradient||=0.019086297838013175, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(190/199): ||gradient||=0.02127896202154825, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(191/199): ||gradient||=0.02127896202154825, loss=0.3756473456718056, w0=0.08130696324519089, w1=-0.6095769393921033\n",
      "Gradient Descent(192/199): ||gradient||=0.013232837805350147, loss=0.3756267043461724, w0=0.08148075967979389, w1=-0.6099657476358593\n",
      "Gradient Descent(193/199): ||gradient||=0.013302247084608905, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(194/199): ||gradient||=0.016545311030577412, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(195/199): ||gradient||=0.016545311030577412, loss=0.3755736084999899, w0=0.08179841617225363, w1=-0.6110356901740174\n",
      "Gradient Descent(196/199): ||gradient||=0.017100034533504855, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(197/199): ||gradient||=0.03414575672935005, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(198/199): ||gradient||=0.03414575672935005, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n",
      "Gradient Descent(199/199): ||gradient||=0.03491003301262125, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=3.819894318273993, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(1/99): |gradient|=3.313465869115332, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(2/99): |gradient|=2.7465500598415247, loss=0.5231169592050283, w0=0.006188461831573778, w1=-0.003969062074416842\n",
      "SGD(3/99): |gradient|=3.2750457421264016, loss=0.49026883819335143, w0=-0.0036366473237117534, w1=-0.008645873148768043\n",
      "SGD(4/99): |gradient|=3.648974628785057, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(5/99): |gradient|=3.4494660744980905, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(6/99): |gradient|=3.143628555043909, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(7/99): |gradient|=2.8915114659527927, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(8/99): |gradient|=2.897412774883511, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(9/99): |gradient|=2.1737890066966687, loss=0.4714443822232635, w0=-0.019904464199047903, w1=-0.02634329478613387\n",
      "SGD(10/99): |gradient|=5.937595790584886, loss=0.4709996675291747, w0=-0.019421647326106496, w1=-0.026244718166632628\n",
      "SGD(11/99): |gradient|=2.677548444145319, loss=0.47095357270473925, w0=-0.02081102663905905, w1=-0.026713517117417356\n",
      "SGD(12/99): |gradient|=12.171538978449524, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(13/99): |gradient|=6.554489780166215, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(14/99): |gradient|=3.1860265668993417, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(15/99): |gradient|=4.980416065195217, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(16/99): |gradient|=8.601456112840077, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(17/99): |gradient|=2.507560742564985, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(18/99): |gradient|=3.39722270862085, loss=0.46639024949469604, w0=-0.0204532765809344, w1=-0.027407190321897496\n",
      "SGD(19/99): |gradient|=5.7959604991936695, loss=0.46636056728990233, w0=-0.019964622091377646, w1=-0.02757084427383505\n",
      "SGD(20/99): |gradient|=2.6503861820488868, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(21/99): |gradient|=4.766341850026974, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(22/99): |gradient|=8.113459704551065, loss=0.4660904073374325, w0=-0.019901578391751246, w1=-0.027387751546646452\n",
      "SGD(23/99): |gradient|=2.1523951322923573, loss=0.4655962637317894, w0=-0.020503431962578023, w1=-0.02763510127265685\n",
      "SGD(24/99): |gradient|=2.125400495030948, loss=0.465050896308293, w0=-0.021228871282967347, w1=-0.02694446888606359\n",
      "SGD(25/99): |gradient|=2.90920195097991, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(26/99): |gradient|=3.3299678307457063, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(27/99): |gradient|=2.035550059511512, loss=0.46382531406925614, w0=-0.024134478424571018, w1=-0.028252426454319492\n",
      "SGD(28/99): |gradient|=2.3580415986033985, loss=0.463454498002539, w0=-0.025559249743730812, w1=-0.0282759613139578\n",
      "SGD(29/99): |gradient|=2.032813924807822, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(30/99): |gradient|=2.2382793772999583, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(31/99): |gradient|=1.8764337853530073, loss=0.4623414591560108, w0=-0.03057723228144025, w1=-0.030403073754884043\n",
      "SGD(32/99): |gradient|=1.8232787854563, loss=0.46205157824875615, w0=-0.03371482023092263, w1=-0.0319979397946591\n",
      "SGD(33/99): |gradient|=3.47991815484829, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(34/99): |gradient|=2.6051466504383227, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(35/99): |gradient|=3.122047619325702, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(36/99): |gradient|=2.2102969803941135, loss=0.46103141880159887, w0=-0.023194632035417127, w1=-0.036577706438919055\n",
      "SGD(37/99): |gradient|=1.874048874756718, loss=0.46038466659937205, w0=-0.02427213706531704, w1=-0.03684812713307184\n",
      "SGD(38/99): |gradient|=2.0605893824283457, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(39/99): |gradient|=4.445874973352969, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(40/99): |gradient|=2.9986231336102747, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(41/99): |gradient|=12.892590418104017, loss=0.4580599239196462, w0=-0.026000009060535678, w1=-0.03758569839571127\n",
      "SGD(42/99): |gradient|=4.933085208595288, loss=0.4580198866671206, w0=-0.024411383252656044, w1=-0.03756723828049817\n",
      "SGD(43/99): |gradient|=2.093934690011671, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(44/99): |gradient|=2.9960753128358437, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(45/99): |gradient|=2.438209540121207, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(46/99): |gradient|=4.548454818977723, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(47/99): |gradient|=2.492690399903696, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(48/99): |gradient|=5.610456340644089, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(49/99): |gradient|=3.318079123588936, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(50/99): |gradient|=4.566336903746895, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(51/99): |gradient|=2.6302184968784394, loss=0.45700336773487327, w0=-0.02627204818351755, w1=-0.0380443223126299\n",
      "SGD(52/99): |gradient|=4.585911442587165, loss=0.45698730432309054, w0=-0.026215919321215708, w1=-0.038060274350766095\n",
      "SGD(53/99): |gradient|=2.211072273144503, loss=0.45696654859836666, w0=-0.026284231954664526, w1=-0.03811421256649752\n",
      "SGD(54/99): |gradient|=1.9795518859859038, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(55/99): |gradient|=2.556518850535039, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(56/99): |gradient|=5.226010984055924, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(57/99): |gradient|=4.042771629866015, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(58/99): |gradient|=3.3067416727703223, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(59/99): |gradient|=1.8323943777272642, loss=0.45692388503402925, w0=-0.0263770162421353, w1=-0.03811226028038737\n",
      "SGD(60/99): |gradient|=11.470974679830231, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(61/99): |gradient|=3.0144147497867375, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(62/99): |gradient|=3.6818876157772396, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(63/99): |gradient|=2.163572003684967, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(64/99): |gradient|=4.289954026547787, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(65/99): |gradient|=2.714025703840808, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(66/99): |gradient|=6.1144593103535225, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(67/99): |gradient|=5.47835855595733, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(68/99): |gradient|=2.3926333231147896, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(69/99): |gradient|=3.578895865252755, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(70/99): |gradient|=1.9919780690927347, loss=0.45691478898804516, w0=-0.026372087220421152, w1=-0.03811124869164356\n",
      "SGD(71/99): |gradient|=2.987998220725619, loss=0.456914772142296, w0=-0.026372194256254262, w1=-0.03811126143458942\n",
      "SGD(72/99): |gradient|=2.6470399520361894, loss=0.456914747739093, w0=-0.026372328513162742, w1=-0.03811127085851918\n",
      "SGD(73/99): |gradient|=2.6022737766902844, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(74/99): |gradient|=4.187131906617635, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(75/99): |gradient|=2.015576101474963, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(76/99): |gradient|=2.860467748289693, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(77/99): |gradient|=4.06415115563897, loss=0.45691465332672976, w0=-0.02637251625322853, w1=-0.03811148912062281\n",
      "SGD(78/99): |gradient|=2.4510072413307586, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(79/99): |gradient|=3.826423961329138, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(80/99): |gradient|=2.74372652321718, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(81/99): |gradient|=5.885837597513265, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(82/99): |gradient|=4.653513935748076, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(83/99): |gradient|=2.989275111376364, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(84/99): |gradient|=1.938495142487275, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(85/99): |gradient|=4.135621499992417, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(86/99): |gradient|=8.998813146377053, loss=0.45691462517922826, w0=-0.02637258430471322, w1=-0.03811154343460073\n",
      "SGD(87/99): |gradient|=6.338983621184608, loss=0.4569146227568537, w0=-0.026372581103101932, w1=-0.03811154506135068\n",
      "SGD(88/99): |gradient|=24.4524925542933, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(89/99): |gradient|=3.656262524412902, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(90/99): |gradient|=2.87088407305221, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(91/99): |gradient|=7.309979979904143, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(92/99): |gradient|=3.2762100650218438, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(93/99): |gradient|=3.1031068732380334, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(94/99): |gradient|=3.610075038327429, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(95/99): |gradient|=3.3793881254517495, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(96/99): |gradient|=2.942350120659295, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(97/99): |gradient|=5.18818024609198, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(98/99): |gradient|=4.0703048291822475, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(99/99): |gradient|=2.903848600170023, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.solve(G,tx.T.dot(y))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares_lstsq_ver(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25108353401177774\n",
      "2.302894037351024\n",
      "0.32681724036218224\n",
      "0.47402537599287914\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "for jet in range(0,4):\n",
    "    x_tr_poly = build_poly(tX_tr[jet], degrees_star_ls[jet])\n",
    "    w.append(least_squares_lstsq_ver(y_tr[jet], x_tr_poly))\n",
    "    print(compute_loss(y_te[jet], build_poly(tX_te[jet], degrees_star_ls[jet]), w[jet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3427725541434394\n",
      "105844.23254608287\n",
      "7373.296251656729\n",
      "3027.1645594095685\n"
     ]
    }
   ],
   "source": [
    "#degrees star version\n",
    "w = []\n",
    "for jet in range(0,4):\n",
    "    x_tr_poly = build_poly(tX_tr[jet], degrees_star[jet])\n",
    "    w.append(ridge_regression(y_tr[jet], x_tr_poly, lambdas_star[jet]))\n",
    "    print(compute_loss(y_te[jet], build_poly(tX_te[jet], degrees_star[jet]), w[jet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3427725541434394\n",
      "2.1826414026231085\n",
      "0.5535167673565986\n",
      "2.296580118917571\n"
     ]
    }
   ],
   "source": [
    "#degrees saved version\n",
    "w = []\n",
    "for jet in range(0,4):\n",
    "    x_tr_poly = build_poly(tX_tr[jet], degrees[jet])\n",
    "    w.append(ridge_regression(y_tr[jet], x_tr_poly, lambdas[jet]))\n",
    "    print(compute_loss(y_te[jet], build_poly(tX_te[jet], degrees[jet]), w[jet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = build_predictions(tX_te, indexes_te, w_star, degrees_star) #leave degree blank if it's not polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, w, degrees, logistic=True) #leave degree blank if it's not polynomial, add logistic flag to build [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6597\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing([], tX_test, test=True) #same function as train, test=True is to avoid computation on the y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for normal regression\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, w, degrees)\n",
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, w, degrees, logistic=True)\n",
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
