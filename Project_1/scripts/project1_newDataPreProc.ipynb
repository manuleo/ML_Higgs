{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , deal with outliers, adding log colm (?)\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(y_tr_tot, x_tr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(y_te_tot, x_te_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All already done: skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0       138.470                       51.655        97.827    27.980   \n",
       "1       160.937                       68.768       103.235    48.146   \n",
       "2      -999.000                      162.172       125.953    35.635   \n",
       "3       143.905                       81.417        80.943     0.414   \n",
       "4       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  PRI_met_phi  \\\n",
       "0               3.064      41.928     197.760  ...       -0.277   \n",
       "1               3.473       2.078     125.157  ...       -1.916   \n",
       "2               3.148       9.336     197.814  ...       -2.186   \n",
       "3               3.310       0.414      75.968  ...        0.060   \n",
       "4               3.891      16.405      57.983  ...       -0.871   \n",
       "\n",
       "   PRI_met_sumet  PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  \\\n",
       "0        258.733            2              67.435                2.150   \n",
       "1        164.546            1              46.226                0.725   \n",
       "2        260.414            1              44.251                2.053   \n",
       "3         86.062            0            -999.000             -999.000   \n",
       "4         53.131            0            -999.000             -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "0                  -2.475         113.497  \n",
       "1                -999.000          46.226  \n",
       "2                -999.000          44.251  \n",
       "3                -999.000           0.000  \n",
       "4                -999.000           0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd = pd.read_csv('../data/train.csv')\n",
    "tX_pd.drop(labels=['Id', 'Prediction'], axis=1, inplace=True)\n",
    "tX_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pd0 = tX_pd[tX_pd['PRI_jet_num']==0]\n",
    "tX_pd1 = tX_pd[tX_pd['PRI_jet_num']==1]\n",
    "tX_pd2 = tX_pd[tX_pd['PRI_jet_num']==2]\n",
    "tX_pd3 = tX_pd[tX_pd['PRI_jet_num']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pds = [tX_pd0, tX_pd1, tX_pd2, tX_pd3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet: 0\n",
      "26123 0 DER_mass_MMC\n",
      "99913 4 DER_deltaeta_jet_jet\n",
      "99913 5 DER_mass_jet_jet\n",
      "99913 6 DER_prodeta_jet_jet\n",
      "99913 12 DER_lep_eta_centrality\n",
      "99913 23 PRI_jet_leading_pt\n",
      "99913 24 PRI_jet_leading_eta\n",
      "99913 25 PRI_jet_leading_phi\n",
      "99913 26 PRI_jet_subleading_pt\n",
      "99913 27 PRI_jet_subleading_eta\n",
      "99913 28 PRI_jet_subleading_phi\n",
      "jet: 1\n",
      "7562 0 DER_mass_MMC\n",
      "77544 4 DER_deltaeta_jet_jet\n",
      "77544 5 DER_mass_jet_jet\n",
      "77544 6 DER_prodeta_jet_jet\n",
      "77544 12 DER_lep_eta_centrality\n",
      "77544 26 PRI_jet_subleading_pt\n",
      "77544 27 PRI_jet_subleading_eta\n",
      "77544 28 PRI_jet_subleading_phi\n",
      "jet: 2\n",
      "2952 0 DER_mass_MMC\n",
      "jet: 3\n",
      "1477 0 DER_mass_MMC\n"
     ]
    }
   ],
   "source": [
    "#searching for nulls:\n",
    "for jet in range (0, 4):\n",
    "    print (\"jet: {}\".format(jet))\n",
    "    i = 0\n",
    "    for c in tX_pds[jet]:\n",
    "        s = tx_pds[jet][tx_pds[jet][c] == -999].index.size\n",
    "        if s > 0:\n",
    "            print(s, i, c)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet: 0\n",
      "99913 22 PRI_jet_num\n",
      "99913 29 PRI_jet_all_pt\n",
      "jet: 1\n",
      "jet: 2\n",
      "9925 12 DER_lep_eta_centrality\n",
      "jet: 3\n",
      "5827 12 DER_lep_eta_centrality\n"
     ]
    }
   ],
   "source": [
    "#searching for zeros:\n",
    "for jet in range (0, 4):\n",
    "    print (\"jet: {}\".format(jet))\n",
    "    i = 0\n",
    "    for c in tX_pds[jet]:\n",
    "        s = tx_pds[jet][tx_pds[jet][c] == 0].index.size\n",
    "        if s > 1000:\n",
    "            print(s, i, c)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dealing with outliers\n",
    "# for jet in range(0, 4):\n",
    "#     for i in range(0, tX_new[jet].shape[1]):\n",
    "#         tX_act = tX_new[jet]\n",
    "#         q1 = np.quantile(tX_act[:,i], 0.25)\n",
    "#         q3 = np.quantile(tX_act[:,i], 0.75)\n",
    "#         iqr = q3 - q1\n",
    "#         max_val = q3 + 1.5*iqr \n",
    "#         indexs_out = tX_act[:,i] > max_val\n",
    "#         #print(tX_act[indexs_out, :].shape)\n",
    "#         tX_act[indexs_out, :] = max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop Main Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, mean=True):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        #print(index_te, index_tr)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "        \n",
    "        # form data with polynomial degree\n",
    "        x_te_poly = build_poly(x_te, degree)\n",
    "        x_tr_poly = build_poly(x_tr, degree)\n",
    "        \n",
    "        w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "        #w = least_squares(y_tr, x_tr_poly)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "        rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "        #print(lambda_, rmse_te)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "        \n",
    "    if mean==True:\n",
    "        loss_tr = np.median(losses_tr)\n",
    "        loss_te = np.median(losses_te)\n",
    "    else:\n",
    "        loss_tr = losses_tr\n",
    "        loss_te = losses_te\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81653618460762, 0.8230270384484322)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_indices = build_k_indices(y_tr[3], 8, 264)\n",
    "cross_validation(y_tr[3], x_tr[3], k_indices, 8, 0.0001, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-737ca643b82e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcross_validation_visualization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrmse_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-737ca643b82e>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mrmse_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;31m#print(loss_te)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrmse_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-fc94f8d05394>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, mean)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# form data with polynomial degree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mx_te_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mx_tr_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\EPFL\\ML\\github_mine\\ML\\Project_1\\scripts\\my_helpers.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mpsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    degree = 3\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-8, 1, 50)\n",
    "#     for d in range (1, degree+1):\n",
    "#         for i in range (0, 3):\n",
    "    #seed = i\n",
    "    # split data in k fold\n",
    "    #y_sub, x_sub = get_subsample(y_tr, x_tr, 10000, seed)\n",
    "    seed = 1\n",
    "    k_indices = build_k_indices(y_tr[0], k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr, loss_te = cross_validation(y_tr[0], x_tr[0], k_indices, k_fold, lambda_, degree)\n",
    "        #print(loss_te)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te, degree)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree(y, x, max_degree, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 100000, seed)\n",
    "    lambdas = np.logspace(-8, 0, 20)\n",
    "    lambdas=[1e-08]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        for lambda_ in lambdas:\n",
    "            #print(degree)\n",
    "            loss_tr, loss_te = cross_validation(y, x, k_indices, k_fold, lambda_, degree)\n",
    "            #print(loss_te)\n",
    "            if loss_te < loss_min:\n",
    "                loss_min = loss_te\n",
    "                print(\"New loss: {}, degree: {}\".format(loss_te, degree))\n",
    "                degree_star = degree\n",
    "                lambda_star = lambda_\n",
    "    return degree_star, lambda_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.746624407452123, degree: 1\n",
      "New loss: 0.7144596490854669, degree: 2\n",
      "New loss: 0.703377138042603, degree: 3\n",
      "New loss: 0.7026347376891526, degree: 4\n",
      "New loss: 0.7015496722502343, degree: 5\n",
      "Best degree for jet 0: 5 with lambda = 1e-08\n",
      "New loss: 0.8781075352063104, degree: 1\n",
      "New loss: 0.8510822241635114, degree: 2\n",
      "New loss: 0.8297812483756203, degree: 3\n",
      "New loss: 0.8277301763931495, degree: 4\n",
      "New loss: 0.8234771397544294, degree: 5\n",
      "New loss: 0.822279383545697, degree: 6\n",
      "New loss: 0.8214666797494583, degree: 7\n",
      "New loss: 0.8210525542348139, degree: 8\n",
      "Best degree for jet 1: 8 with lambda = 1e-08\n",
      "New loss: 0.884493498924857, degree: 1\n",
      "New loss: 0.862033361457138, degree: 2\n",
      "New loss: 0.8389246935729879, degree: 3\n",
      "New loss: 0.8374912057128021, degree: 4\n",
      "New loss: 0.8348363755534025, degree: 5\n",
      "New loss: 0.8344311019179658, degree: 6\n",
      "New loss: 0.8336978082876696, degree: 7\n",
      "New loss: 0.8333553869088407, degree: 8\n",
      "Best degree for jet 2: 8 with lambda = 1e-08\n",
      "New loss: 0.8552289516162845, degree: 1\n",
      "New loss: 0.8433039686591286, degree: 2\n",
      "New loss: 0.818410480074236, degree: 3\n",
      "New loss: 0.8176933710477967, degree: 4\n",
      "New loss: 0.8130811262501993, degree: 5\n",
      "New loss: 0.8121343889981323, degree: 6\n",
      "Best degree for jet 3: 6 with lambda = 1e-08\n"
     ]
    }
   ],
   "source": [
    "max_degree = 8\n",
    "k_fold = 4\n",
    "degrees_star=[]\n",
    "lambdas_star=[]\n",
    "for jet in range(0, 4):\n",
    "    degree_star, lambda_star = select_best_degree(y_tr[jet], x_tr[jet], max_degree, k_fold, 1)\n",
    "    degrees_star.append(degree_star)\n",
    "    lambdas_star.append(lambda_star)\n",
    "    print(\"Best degree for jet {}: {} with lambda = {}\".format(jet, degree_star, lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree_ls(y, x, max_degree, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 50000, seed)\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        rmse_te = []\n",
    "        loss_tr, loss_te = cross_validation(y, x, k_indices, k_fold, 0, degree)\n",
    "        #print(degree, lambda_, loss_te)\n",
    "        if loss_te < loss_min:\n",
    "            loss_min = loss_te\n",
    "            print(\"New loss: {}, degree: {}\".format(loss_te, degree))\n",
    "            degree_star = degree\n",
    "    return degree_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New loss: 0.8352554444438645, degree: 1\n",
      "New loss: 0.8074198480959829, degree: 2\n",
      "New loss: 0.7970505329743582, degree: 3\n",
      "New loss: 0.7967003079401784, degree: 4\n",
      "Best degree for least squares: 4\n"
     ]
    }
   ],
   "source": [
    "max_degree = 10\n",
    "k_fold = 4\n",
    "degree_star_ls = select_best_degree_ls(y_tr, x_tr, max_degree, k_fold, 264)\n",
    "print(\"Best degree for least squares: {}\".format(degree_star_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [1e-09, 1.1288378916846883e-08, 0.00018329807108324338, 1e-09]\n",
    "degrees = [3, 5, 5, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=0.9198964473372747, loss=0.49991543478685996, w0=-3.1381702249274604e-05, w1=-1.870332018899931e-05\n",
      "Gradient Descent(1/199): ||gradient||=0.9186848945865361, loss=0.4997636980245322, w0=-8.78042153832626e-05, w1=-5.236507656267239e-05\n",
      "Gradient Descent(2/199): ||gradient||=0.9165086790795948, loss=0.4994921215723027, w0=-0.00018915591588298592, w1=-0.00011294257728487969\n",
      "Gradient Descent(3/199): ||gradient||=0.9126063083581455, loss=0.4990082616199879, w0=-0.000370914263529803, w1=-0.0002219378872869192\n",
      "Gradient Descent(4/199): ||gradient||=0.9056299309807633, loss=0.49815319084166065, w0=-0.0006959041130019329, w1=-0.00041798666987893115\n",
      "Gradient Descent(5/199): ||gradient||=0.8932269289430593, loss=0.49666407351067016, w0=-0.0012739014639035493, w1=-0.0007704142328603178\n",
      "Gradient Descent(6/199): ||gradient||=0.8713980371643467, loss=0.49413764114400255, w0=-0.002292032255140569, w1=-0.0014033062129404496\n",
      "Gradient Descent(7/199): ||gradient||=0.8336914854870378, loss=0.49004496000892644, w0=-0.004054621864207936, w1=-0.0025378020652307696\n",
      "Gradient Descent(8/199): ||gradient||=0.7708210576370715, loss=0.48392417182397535, w0=-0.007012143781032614, w1=-0.0045650758007922156\n",
      "Gradient Descent(9/199): ||gradient||=0.6730593787686956, loss=0.4758746897312079, w0=-0.011703585013512112, w1=-0.008168591631344086\n",
      "Gradient Descent(10/199): ||gradient||=0.5420469001358224, loss=0.4668314646613726, w0=-0.018440239398187373, w1=-0.014520131670101091\n",
      "Gradient Descent(11/199): ||gradient||=0.417585276699318, loss=0.45684219087207145, w0=-0.0266511349012321, w1=-0.02557881435937765\n",
      "Gradient Descent(12/199): ||gradient||=0.34951950406858123, loss=0.4441675056514142, w0=-0.03507313894297936, w1=-0.044530615642647674\n",
      "Gradient Descent(13/199): ||gradient||=0.2802581748299309, loss=0.4305010778316688, w0=-0.044834692707357696, w1=-0.07630731446605377\n",
      "Gradient Descent(14/199): ||gradient||=0.19933661937191804, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(15/199): ||gradient||=0.2126484850736893, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(16/199): ||gradient||=0.2126484850736893, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(17/199): ||gradient||=0.41005196201736743, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(18/199): ||gradient||=0.41005196201736743, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(19/199): ||gradient||=0.3562131324819593, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(20/199): ||gradient||=0.3562131324819593, loss=0.4119259421855884, w0=-0.03309942831257582, w1=-0.16844869171449792\n",
      "Gradient Descent(21/199): ||gradient||=0.12649027507232175, loss=0.41025030347357094, w0=-0.029575140439970622, w1=-0.17946578929166102\n",
      "Gradient Descent(22/199): ||gradient||=0.12292495119423105, loss=0.40743339281067326, w0=-0.023442494523926122, w1=-0.19876354675726643\n",
      "Gradient Descent(23/199): ||gradient||=0.11680132506627634, loss=0.4029471327448956, w0=-0.01278468241212367, w1=-0.23182458833403902\n",
      "Gradient Descent(24/199): ||gradient||=0.10674668689952199, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(25/199): ||gradient||=0.11037853434564666, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(26/199): ||gradient||=0.11037853434564666, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(27/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(28/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(29/199): ||gradient||=0.30949111243344546, loss=0.3934972452046127, w0=0.012645111284258674, w1=-0.3149399904670085\n",
      "Gradient Descent(30/199): ||gradient||=0.08518921631593725, loss=0.3928422773768506, w0=0.01390331604737596, w1=-0.3215594212339835\n",
      "Gradient Descent(31/199): ||gradient||=0.08143325683778938, loss=0.39176458577149614, w0=0.01769426852847459, w1=-0.333137608291744\n",
      "Gradient Descent(32/199): ||gradient||=0.08261374022271092, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(33/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(34/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(35/199): ||gradient||=0.15244231978895387, loss=0.38970194899647265, w0=0.023115513292314566, w1=-0.35613896221101615\n",
      "Gradient Descent(36/199): ||gradient||=0.07231686370955237, loss=0.38924702709110287, w0=0.024867215085094988, w1=-0.36144019878751626\n",
      "Gradient Descent(37/199): ||gradient||=0.0693800954973171, loss=0.3884863243303348, w0=0.026866841411884168, w1=-0.3708048228655269\n",
      "Gradient Descent(38/199): ||gradient||=0.06849694468173842, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(39/199): ||gradient||=0.09598461127388905, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(40/199): ||gradient||=0.09598461127388905, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(41/199): ||gradient||=0.1410714760751551, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(42/199): ||gradient||=0.1410714760751551, loss=0.3864215399793017, w0=0.0350727177951097, w1=-0.39921296841299686\n",
      "Gradient Descent(43/199): ||gradient||=0.08456831532148283, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(44/199): ||gradient||=0.11635687063747818, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(45/199): ||gradient||=0.11635687063747818, loss=0.3855801463907678, w0=0.037564023270261745, w1=-0.4104639083541116\n",
      "Gradient Descent(46/199): ||gradient||=0.07098725686300805, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(47/199): ||gradient||=0.08865280508918541, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(48/199): ||gradient||=0.08865280508918541, loss=0.38485406384997894, w0=0.0397988301803087, w1=-0.4207754388932331\n",
      "Gradient Descent(49/199): ||gradient||=0.05956788893440248, loss=0.38448192908571455, w0=0.0402881948329062, w1=-0.4270292492982512\n",
      "Gradient Descent(50/199): ||gradient||=0.06633820597340725, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(51/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(52/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(53/199): ||gradient||=0.15761065003057645, loss=0.38370536779400194, w0=0.044360770783240404, w1=-0.4394259860437441\n",
      "Gradient Descent(54/199): ||gradient||=0.06222099110230357, loss=0.38348800700174634, w0=0.044301326033474996, w1=-0.44236759427566763\n",
      "Gradient Descent(55/199): ||gradient||=0.04860668539425892, loss=0.383189073110083, w0=0.04591786180813736, w1=-0.4475058466448936\n",
      "Gradient Descent(56/199): ||gradient||=0.04885484002764845, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(57/199): ||gradient||=0.07218281893711131, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(58/199): ||gradient||=0.07218281893711131, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(59/199): ||gradient||=0.0865949367220621, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(60/199): ||gradient||=0.0865949367220621, loss=0.38229970311486866, w0=0.04905354484341635, w1=-0.4634154097612027\n",
      "Gradient Descent(61/199): ||gradient||=0.045554523575472565, loss=0.38208015281211827, w0=0.05059222337498455, w1=-0.46760109124772614\n",
      "Gradient Descent(62/199): ||gradient||=0.046707216525688364, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(63/199): ||gradient||=0.08067338628463173, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(64/199): ||gradient||=0.08067338628463173, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(65/199): ||gradient||=0.09181351733579902, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(66/199): ||gradient||=0.09181351733579902, loss=0.38140101837396745, w0=0.052993763875687355, w1=-0.48064650178588253\n",
      "Gradient Descent(67/199): ||gradient||=0.040172136024456635, loss=0.38123091391261776, w0=0.05420974414245746, w1=-0.48409726949739024\n",
      "Gradient Descent(68/199): ||gradient||=0.04011639835467928, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(69/199): ||gradient||=0.0587199843578713, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(70/199): ||gradient||=0.0587199843578713, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(71/199): ||gradient||=0.06065901120024391, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(72/199): ||gradient||=0.06065901120024391, loss=0.38070675484703786, w0=0.05629675863399411, w1=-0.4948880697081792\n",
      "Gradient Descent(73/199): ||gradient||=0.03437710169565648, loss=0.3805724543076369, w0=0.05709337174035709, w1=-0.4977650896919669\n",
      "Gradient Descent(74/199): ||gradient||=0.03371872029255384, loss=0.380347504802962, w0=0.057923161002275445, w1=-0.502821417967265\n",
      "Gradient Descent(75/199): ||gradient||=0.0353360741618754, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(76/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(77/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(78/199): ||gradient||=0.0868549972476703, loss=0.37990446055546606, w0=0.06025622386799663, w1=-0.512732449768847\n",
      "Gradient Descent(79/199): ||gradient||=0.03013797122742819, loss=0.3798059156030959, w0=0.06086479596353714, w1=-0.5149947769707354\n",
      "Gradient Descent(80/199): ||gradient||=0.029582646754048075, loss=0.3796377325021035, w0=0.06155759036056807, w1=-0.5189768598962371\n",
      "Gradient Descent(81/199): ||gradient||=0.02988603821652102, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(82/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(83/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(84/199): ||gradient||=0.05385331845836831, loss=0.37930748989144936, w0=0.06340685859282029, w1=-0.5268228016896819\n",
      "Gradient Descent(85/199): ||gradient||=0.026807087764073437, loss=0.379232343212017, w0=0.06379460699418472, w1=-0.5286305900589766\n",
      "Gradient Descent(86/199): ||gradient||=0.026429755894928714, loss=0.37910158102876307, w0=0.06449936504431049, w1=-0.5318081753946006\n",
      "Gradient Descent(87/199): ||gradient||=0.025779232451409895, loss=0.37887998603582906, w0=0.06565536813188565, w1=-0.5372914115868667\n",
      "Gradient Descent(88/199): ||gradient||=0.02479156357937694, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(89/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(90/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(91/199): ||gradient||=0.03258988562878005, loss=0.37847536538605114, w0=0.06774134339187216, w1=-0.5477141177050802\n",
      "Gradient Descent(92/199): ||gradient||=0.027738362406501706, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(93/199): ||gradient||=0.03951405284074884, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(94/199): ||gradient||=0.03951405284074884, loss=0.37834206727202646, w0=0.06842349019205861, w1=-0.5511561942990433\n",
      "Gradient Descent(95/199): ||gradient||=0.0304225158131295, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(96/199): ||gradient||=0.046421773810511664, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(97/199): ||gradient||=0.046421773810511664, loss=0.3782192101919261, w0=0.06906497913554238, w1=-0.5543145897328704\n",
      "Gradient Descent(98/199): ||gradient||=0.03256348800089378, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(99/199): ||gradient||=0.05053199788292059, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(100/199): ||gradient||=0.05053199788292059, loss=0.37810265907289226, w0=0.06968905597825942, w1=-0.5572191567845676\n",
      "Gradient Descent(101/199): ||gradient||=0.032866606098295975, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(102/199): ||gradient||=0.049904511233416445, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(103/199): ||gradient||=0.049904511233416445, loss=0.37799031022315804, w0=0.07030697705295337, w1=-0.5598962692587653\n",
      "Gradient Descent(104/199): ||gradient||=0.030864528540993398, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(105/199): ||gradient||=0.044474850521653266, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(106/199): ||gradient||=0.044474850521653266, loss=0.3778831177632533, w0=0.0709143275158598, w1=-0.5623694962267904\n",
      "Gradient Descent(107/199): ||gradient||=0.02725555397893965, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(108/199): ||gradient||=0.036174660635675905, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(109/199): ||gradient||=0.036174660635675905, loss=0.3777832329470302, w0=0.07149562027576611, w1=-0.5646599120664837\n",
      "Gradient Descent(110/199): ||gradient||=0.023491643257352396, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(111/199): ||gradient||=0.02798541531932825, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(112/199): ||gradient||=0.02798541531932825, loss=0.37769151450075156, w0=0.07203435972166758, w1=-0.5667861513379303\n",
      "Gradient Descent(113/199): ||gradient||=0.020796448379199806, loss=0.3776393572545536, w0=0.07258948801176464, w1=-0.568075243409437\n",
      "Gradient Descent(114/199): ||gradient||=0.02232622559275806, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(115/199): ||gradient||=0.044569960234299495, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(116/199): ||gradient||=0.044569960234299495, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(117/199): ||gradient||=0.06049611094726945, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(118/199): ||gradient||=0.06049611094726945, loss=0.3774734104704165, w0=0.07307154647503165, w1=-0.5721438415511358\n",
      "Gradient Descent(119/199): ||gradient||=0.02560307349675981, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(120/199): ||gradient||=0.030235195825791275, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(121/199): ||gradient||=0.030235195825791275, loss=0.3773936500349805, w0=0.0735942921530556, w1=-0.5738034701868857\n",
      "Gradient Descent(122/199): ||gradient||=0.01930338988021067, loss=0.3773502513239982, w0=0.07402822791850343, w1=-0.5748167485368086\n",
      "Gradient Descent(123/199): ||gradient||=0.019883352701387243, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(124/199): ||gradient||=0.03180432497976459, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(125/199): ||gradient||=0.03180432497976459, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(126/199): ||gradient||=0.0368369701461698, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(127/199): ||gradient||=0.0368369701461698, loss=0.3772087103489772, w0=0.07452601579779915, w1=-0.5780354108422809\n",
      "Gradient Descent(128/199): ||gradient||=0.018672376155633576, loss=0.3771703336844059, w0=0.07491495902096323, w1=-0.5789005133364988\n",
      "Gradient Descent(129/199): ||gradient||=0.01894887411582462, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(130/199): ||gradient||=0.027883181430691767, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(131/199): ||gradient||=0.027883181430691767, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(132/199): ||gradient||=0.029631455078381125, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(133/199): ||gradient||=0.029631455078381125, loss=0.37704490546502617, w0=0.07536964604927561, w1=-0.5816628169500092\n",
      "Gradient Descent(134/199): ||gradient||=0.01724997691647665, loss=0.3770105401718543, w0=0.07562881667941775, w1=-0.5824123704142081\n",
      "Gradient Descent(135/199): ||gradient||=0.017165877881501656, loss=0.37695177088913756, w0=0.07574484702850508, w1=-0.5837476546228534\n",
      "Gradient Descent(136/199): ||gradient||=0.01899421714878402, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(137/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(138/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(139/199): ||gradient||=0.054220090727918, loss=0.37682644524415465, w0=0.07640556694575028, w1=-0.5864102724253792\n",
      "Gradient Descent(140/199): ||gradient||=0.016650580776069446, loss=0.3767965645111003, w0=0.07664467148671401, w1=-0.5870356600147364\n",
      "Gradient Descent(141/199): ||gradient||=0.016501858917438078, loss=0.376745477937799, w0=0.07671846419935624, w1=-0.5881547717783094\n",
      "Gradient Descent(142/199): ||gradient||=0.01816804164140217, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(143/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(144/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(145/199): ||gradient||=0.04825610442092132, loss=0.3766356523318643, w0=0.07732128008506954, w1=-0.5903961372206211\n",
      "Gradient Descent(146/199): ||gradient||=0.01563931340254658, loss=0.3766094801974734, w0=0.07745336774852776, w1=-0.5909299030113423\n",
      "Gradient Descent(147/199): ||gradient||=0.01555626373968682, loss=0.37656297807301164, w0=0.07763617808606241, w1=-0.5918788948154293\n",
      "Gradient Descent(148/199): ||gradient||=0.015451985463614959, loss=0.3764819326011553, w0=0.07809347822581313, w1=-0.5935397672728017\n",
      "Gradient Descent(149/199): ||gradient||=0.016210974608243976, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(150/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(151/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(152/199): ||gradient||=0.0572020732177142, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(153/199): ||gradient||=0.04260833075213723, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(154/199): ||gradient||=0.04260833075213723, loss=0.37630531695231634, w0=0.0788118352881565, w1=-0.5970633924846425\n",
      "Gradient Descent(155/199): ||gradient||=0.014908211767569006, loss=0.3762840666377174, w0=0.0788347473226203, w1=-0.5974755644658697\n",
      "Gradient Descent(156/199): ||gradient||=0.014708048925549026, loss=0.3762467257517887, w0=0.07906897984336392, w1=-0.5982031234845887\n",
      "Gradient Descent(157/199): ||gradient||=0.014963885063396522, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(158/199): ||gradient||=0.021972572186543523, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(159/199): ||gradient||=0.021972572186543523, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(160/199): ||gradient||=0.03521861893866122, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(161/199): ||gradient||=0.03521861893866122, loss=0.3761362201008082, w0=0.07924611088708965, w1=-0.6005349176900081\n",
      "Gradient Descent(162/199): ||gradient||=0.02351622446106723, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(163/199): ||gradient||=0.03747859183540602, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(164/199): ||gradient||=0.03747859183540602, loss=0.3760848467338232, w0=0.07945208937262828, w1=-0.6015121960900223\n",
      "Gradient Descent(165/199): ||gradient||=0.023370968435216157, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(166/199): ||gradient||=0.03591853609344955, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(167/199): ||gradient||=0.03591853609344955, loss=0.37603359618573673, w0=0.07967763478331942, w1=-0.6024388052305133\n",
      "Gradient Descent(168/199): ||gradient||=0.021481809200450213, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(169/199): ||gradient||=0.030995904539739747, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(170/199): ||gradient||=0.030995904539739747, loss=0.3759837423493009, w0=0.07991361406706003, w1=-0.6033190135612887\n",
      "Gradient Descent(171/199): ||gradient||=0.018635502628599573, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(172/199): ||gradient||=0.0245163904254113, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(173/199): ||gradient||=0.0245163904254113, loss=0.3759366863297597, w0=0.08014500940912765, w1=-0.6041568478334536\n",
      "Gradient Descent(174/199): ||gradient||=0.01599958312904941, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(175/199): ||gradient||=0.018751655043872835, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(176/199): ||gradient||=0.018751655043872835, loss=0.3758928304184189, w0=0.0803586612622116, w1=-0.6049558909348881\n",
      "Gradient Descent(177/199): ||gradient||=0.014338863123069666, loss=0.3758671970659703, w0=0.08062719990283188, w1=-0.6054485791833413\n",
      "Gradient Descent(178/199): ||gradient||=0.015200543414685935, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(179/199): ||gradient||=0.027969392552206655, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(180/199): ||gradient||=0.027969392552206655, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(181/199): ||gradient||=0.036968997621708515, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(182/199): ||gradient||=0.036968997621708515, loss=0.375783531496386, w0=0.08070975465583485, w1=-0.6070644065150398\n",
      "Gradient Descent(183/199): ||gradient||=0.0167182053280009, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(184/199): ||gradient||=0.018986866136306145, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(185/199): ||gradient||=0.018986866136306145, loss=0.3757441578195452, w0=0.08094097589901549, w1=-0.6077511395045695\n",
      "Gradient Descent(186/199): ||gradient||=0.013572183029778981, loss=0.37572153696885513, w0=0.08114010120247711, w1=-0.6081779108393255\n",
      "Gradient Descent(187/199): ||gradient||=0.013800818079967348, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(188/199): ||gradient||=0.019086297838013175, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(189/199): ||gradient||=0.019086297838013175, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(190/199): ||gradient||=0.02127896202154825, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(191/199): ||gradient||=0.02127896202154825, loss=0.3756473456718056, w0=0.08130696324519089, w1=-0.6095769393921033\n",
      "Gradient Descent(192/199): ||gradient||=0.013232837805350147, loss=0.3756267043461724, w0=0.08148075967979389, w1=-0.6099657476358593\n",
      "Gradient Descent(193/199): ||gradient||=0.013302247084608905, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(194/199): ||gradient||=0.016545311030577412, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(195/199): ||gradient||=0.016545311030577412, loss=0.3755736084999899, w0=0.08179841617225363, w1=-0.6110356901740174\n",
      "Gradient Descent(196/199): ||gradient||=0.017100034533504855, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(197/199): ||gradient||=0.03414575672935005, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(198/199): ||gradient||=0.03414575672935005, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n",
      "Gradient Descent(199/199): ||gradient||=0.03491003301262125, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=3.819894318273993, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(1/99): |gradient|=3.313465869115332, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(2/99): |gradient|=2.7465500598415247, loss=0.5231169592050283, w0=0.006188461831573778, w1=-0.003969062074416842\n",
      "SGD(3/99): |gradient|=3.2750457421264016, loss=0.49026883819335143, w0=-0.0036366473237117534, w1=-0.008645873148768043\n",
      "SGD(4/99): |gradient|=3.648974628785057, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(5/99): |gradient|=3.4494660744980905, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(6/99): |gradient|=3.143628555043909, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(7/99): |gradient|=2.8915114659527927, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(8/99): |gradient|=2.897412774883511, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(9/99): |gradient|=2.1737890066966687, loss=0.4714443822232635, w0=-0.019904464199047903, w1=-0.02634329478613387\n",
      "SGD(10/99): |gradient|=5.937595790584886, loss=0.4709996675291747, w0=-0.019421647326106496, w1=-0.026244718166632628\n",
      "SGD(11/99): |gradient|=2.677548444145319, loss=0.47095357270473925, w0=-0.02081102663905905, w1=-0.026713517117417356\n",
      "SGD(12/99): |gradient|=12.171538978449524, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(13/99): |gradient|=6.554489780166215, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(14/99): |gradient|=3.1860265668993417, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(15/99): |gradient|=4.980416065195217, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(16/99): |gradient|=8.601456112840077, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(17/99): |gradient|=2.507560742564985, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(18/99): |gradient|=3.39722270862085, loss=0.46639024949469604, w0=-0.0204532765809344, w1=-0.027407190321897496\n",
      "SGD(19/99): |gradient|=5.7959604991936695, loss=0.46636056728990233, w0=-0.019964622091377646, w1=-0.02757084427383505\n",
      "SGD(20/99): |gradient|=2.6503861820488868, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(21/99): |gradient|=4.766341850026974, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(22/99): |gradient|=8.113459704551065, loss=0.4660904073374325, w0=-0.019901578391751246, w1=-0.027387751546646452\n",
      "SGD(23/99): |gradient|=2.1523951322923573, loss=0.4655962637317894, w0=-0.020503431962578023, w1=-0.02763510127265685\n",
      "SGD(24/99): |gradient|=2.125400495030948, loss=0.465050896308293, w0=-0.021228871282967347, w1=-0.02694446888606359\n",
      "SGD(25/99): |gradient|=2.90920195097991, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(26/99): |gradient|=3.3299678307457063, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(27/99): |gradient|=2.035550059511512, loss=0.46382531406925614, w0=-0.024134478424571018, w1=-0.028252426454319492\n",
      "SGD(28/99): |gradient|=2.3580415986033985, loss=0.463454498002539, w0=-0.025559249743730812, w1=-0.0282759613139578\n",
      "SGD(29/99): |gradient|=2.032813924807822, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(30/99): |gradient|=2.2382793772999583, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(31/99): |gradient|=1.8764337853530073, loss=0.4623414591560108, w0=-0.03057723228144025, w1=-0.030403073754884043\n",
      "SGD(32/99): |gradient|=1.8232787854563, loss=0.46205157824875615, w0=-0.03371482023092263, w1=-0.0319979397946591\n",
      "SGD(33/99): |gradient|=3.47991815484829, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(34/99): |gradient|=2.6051466504383227, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(35/99): |gradient|=3.122047619325702, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(36/99): |gradient|=2.2102969803941135, loss=0.46103141880159887, w0=-0.023194632035417127, w1=-0.036577706438919055\n",
      "SGD(37/99): |gradient|=1.874048874756718, loss=0.46038466659937205, w0=-0.02427213706531704, w1=-0.03684812713307184\n",
      "SGD(38/99): |gradient|=2.0605893824283457, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(39/99): |gradient|=4.445874973352969, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(40/99): |gradient|=2.9986231336102747, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(41/99): |gradient|=12.892590418104017, loss=0.4580599239196462, w0=-0.026000009060535678, w1=-0.03758569839571127\n",
      "SGD(42/99): |gradient|=4.933085208595288, loss=0.4580198866671206, w0=-0.024411383252656044, w1=-0.03756723828049817\n",
      "SGD(43/99): |gradient|=2.093934690011671, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(44/99): |gradient|=2.9960753128358437, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(45/99): |gradient|=2.438209540121207, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(46/99): |gradient|=4.548454818977723, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(47/99): |gradient|=2.492690399903696, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(48/99): |gradient|=5.610456340644089, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(49/99): |gradient|=3.318079123588936, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(50/99): |gradient|=4.566336903746895, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(51/99): |gradient|=2.6302184968784394, loss=0.45700336773487327, w0=-0.02627204818351755, w1=-0.0380443223126299\n",
      "SGD(52/99): |gradient|=4.585911442587165, loss=0.45698730432309054, w0=-0.026215919321215708, w1=-0.038060274350766095\n",
      "SGD(53/99): |gradient|=2.211072273144503, loss=0.45696654859836666, w0=-0.026284231954664526, w1=-0.03811421256649752\n",
      "SGD(54/99): |gradient|=1.9795518859859038, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(55/99): |gradient|=2.556518850535039, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(56/99): |gradient|=5.226010984055924, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(57/99): |gradient|=4.042771629866015, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(58/99): |gradient|=3.3067416727703223, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(59/99): |gradient|=1.8323943777272642, loss=0.45692388503402925, w0=-0.0263770162421353, w1=-0.03811226028038737\n",
      "SGD(60/99): |gradient|=11.470974679830231, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(61/99): |gradient|=3.0144147497867375, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(62/99): |gradient|=3.6818876157772396, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(63/99): |gradient|=2.163572003684967, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(64/99): |gradient|=4.289954026547787, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(65/99): |gradient|=2.714025703840808, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(66/99): |gradient|=6.1144593103535225, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(67/99): |gradient|=5.47835855595733, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(68/99): |gradient|=2.3926333231147896, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(69/99): |gradient|=3.578895865252755, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(70/99): |gradient|=1.9919780690927347, loss=0.45691478898804516, w0=-0.026372087220421152, w1=-0.03811124869164356\n",
      "SGD(71/99): |gradient|=2.987998220725619, loss=0.456914772142296, w0=-0.026372194256254262, w1=-0.03811126143458942\n",
      "SGD(72/99): |gradient|=2.6470399520361894, loss=0.456914747739093, w0=-0.026372328513162742, w1=-0.03811127085851918\n",
      "SGD(73/99): |gradient|=2.6022737766902844, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(74/99): |gradient|=4.187131906617635, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(75/99): |gradient|=2.015576101474963, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(76/99): |gradient|=2.860467748289693, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(77/99): |gradient|=4.06415115563897, loss=0.45691465332672976, w0=-0.02637251625322853, w1=-0.03811148912062281\n",
      "SGD(78/99): |gradient|=2.4510072413307586, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(79/99): |gradient|=3.826423961329138, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(80/99): |gradient|=2.74372652321718, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(81/99): |gradient|=5.885837597513265, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(82/99): |gradient|=4.653513935748076, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(83/99): |gradient|=2.989275111376364, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(84/99): |gradient|=1.938495142487275, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(85/99): |gradient|=4.135621499992417, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(86/99): |gradient|=8.998813146377053, loss=0.45691462517922826, w0=-0.02637258430471322, w1=-0.03811154343460073\n",
      "SGD(87/99): |gradient|=6.338983621184608, loss=0.4569146227568537, w0=-0.026372581103101932, w1=-0.03811154506135068\n",
      "SGD(88/99): |gradient|=24.4524925542933, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(89/99): |gradient|=3.656262524412902, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(90/99): |gradient|=2.87088407305221, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(91/99): |gradient|=7.309979979904143, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(92/99): |gradient|=3.2762100650218438, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(93/99): |gradient|=3.1031068732380334, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(94/99): |gradient|=3.610075038327429, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(95/99): |gradient|=3.3793881254517495, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(96/99): |gradient|=2.942350120659295, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(97/99): |gradient|=5.18818024609198, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(98/99): |gradient|=4.0703048291822475, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(99/99): |gradient|=2.903848600170023, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34673788750013346"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls = least_squares(y, tX)\n",
    "loss = compute_loss(y, tX, w_ls)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3135299879473527 0.31508110848941073 0.7918711864278845 0.7938275738337774\n"
     ]
    }
   ],
   "source": [
    "x_tr_poly = build_poly(x_tr, 4)\n",
    "x_te_poly = build_poly(x_te, 4)\n",
    "w_ls = least_squares(y_tr, x_tr_poly)\n",
    "loss_tr = compute_loss(y_tr, x_tr_poly, w_ls)\n",
    "loss_te = compute_loss(y_te, x_te_poly, w_ls)\n",
    "rmse_te = compute_rmse(y_te, x_te_poly, w_ls)\n",
    "rmse_tr = compute_rmse(y_tr, x_tr_poly, w_ls)\n",
    "print(loss_tr, loss_te, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2519236360137322\n",
      "2.107687580005468\n",
      "0.596520466605012\n",
      "0.4495880577116992\n"
     ]
    }
   ],
   "source": [
    "w = []\n",
    "for jet in range(0,4):\n",
    "    x_tr_poly = build_poly(tX_tr[jet], degrees[jet])\n",
    "    w.append(ridge_regression(y_tr[jet], x_tr_poly, lambdas[jet]))\n",
    "    print(compute_loss(y_te[jet], build_poly(tX_te[jet], degrees[jet]), w[jet]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24510851416621188 1026200624535.4009 0.7001550030760502 1432620.4134629667\n"
     ]
    }
   ],
   "source": [
    "x_tr_poly = build_poly(x_tr, 7)\n",
    "x_te_poly = build_poly(x_te, 7)\n",
    "lambda_ = 0.00001\n",
    "w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "loss_tr = compute_loss(y_tr, x_tr_poly, w)\n",
    "loss_te = compute_loss(y_te, x_te_poly, w)\n",
    "rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "print(loss_tr, loss_te, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2891950223111849 0.2942724952626915 0.7605195885855733 0.7671668596370564\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "w = ridge_regression(y_tr, x_tr, lambda_)\n",
    "loss_tr = compute_loss(y_tr, x_tr, w)\n",
    "loss_te = compute_loss(y_te, x_te, w)\n",
    "rmse_te = compute_rmse(y_te, x_te, w)\n",
    "rmse_tr = compute_rmse(y_tr, x_tr, w)\n",
    "print(loss_tr, loss_te, rmse_tr, rmse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = build_predictions(tX_te, indexes_te, w, degrees) #leave degree blank if it's not polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79358\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing([], tX_test, test=True) #same function as train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, w, degrees)\n",
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
