{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from costs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(y, x, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # split the data based on the given ratio: TODO\n",
    "    # ***************************************************\n",
    "    \n",
    "    train_size = round(x.shape[0] * ratio)\n",
    "    indexes = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    \n",
    "    train_indexes = indexes[:train_size]\n",
    "    test_indexes = np.setdiff1d(indexes, train_indexes)    \n",
    "    x_train = x[train_indexes,:]\n",
    "    y_train = y[train_indexes]\n",
    "    \n",
    "    x_test = x[test_indexes,:]\n",
    "    y_test = y[test_indexes]\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9   ...     20       21   22       23       24       25       26  \\\n",
       "0  197.760  ... -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157  ... -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814  ... -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968  ...  0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983  ... -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tX_pd = pd.DataFrame(tX)\n",
    "tX_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38114\n",
      "4 177457\n",
      "5 177457\n",
      "6 177457\n",
      "12 177457\n",
      "23 99913\n",
      "24 99913\n",
      "25 99913\n",
      "26 177457\n",
      "27 177457\n",
      "28 177457\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30):\n",
    "    s = tX_pd[tX_pd[i]==-999].index.size\n",
    "    if s > 900:\n",
    "        print(i, s)\n",
    "        #if (i!=4):\n",
    "            #tx_test = tX_pd.drop(i, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = np.where(tX==-999,1,0)\n",
    "s = np.sum(uno,axis=0)\n",
    "index = np.argwhere(s > 100000)\n",
    "new = np.delete(tX,index[1:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = np.where(new==-999,1,0)\n",
    "s = np.sum(uno,axis=0)\n",
    "index = np.argwhere((s>40000)&(s < 100000))\n",
    "new = np.delete(new,index[1:],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = np.where(new==-999,1,0)\n",
    "s = np.sum(uno,axis=0)\n",
    "index = np.argwhere((s<40000)&(s>1))\n",
    "prima = new[:,index]\n",
    "index_ = np.argwhere(prima==-999)\n",
    "prima_new = np.delete(prima, index_)\n",
    "media = np.median(prima_new)\n",
    "prima[prima==-999] = media\n",
    "new[:,index] = prima\n",
    "\n",
    "uno = np.where(new==-999,1,0)\n",
    "s = np.sum(uno,axis=0)\n",
    "index = np.argwhere((s>40000)&(s < 100000))\n",
    "prima = new[:,index]\n",
    "index_ = np.argwhere(prima==-999)\n",
    "prima_new = np.delete(prima, index_)\n",
    "media = np.median(prima_new)\n",
    "prima[prima==-999] = media\n",
    "new[:,index] = prima\n",
    "\n",
    "uno = np.where(new==-999,1,0)\n",
    "s = np.sum(uno,axis=0)\n",
    "index = np.argwhere(s > 100000)\n",
    "prima = new[:,index]\n",
    "index_ = np.argwhere(prima==-999)\n",
    "prima_new = np.delete(prima, index_)\n",
    "media = np.median(prima_new)\n",
    "prima[prima==-999] = media\n",
    "new[:,index] = prima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [5,6,12,24,25,26,27,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pd.drop(drops, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 3\n",
      "2 0\n",
      "3 41\n",
      "4 6\n",
      "5 0\n",
      "6 58\n",
      "7 0\n",
      "8 39\n",
      "9 0\n",
      "10 0\n",
      "11 53\n",
      "12 15752\n",
      "13 0\n",
      "14 0\n",
      "15 32\n",
      "16 0\n",
      "17 35\n",
      "18 33\n",
      "19 0\n",
      "20 44\n",
      "21 0\n",
      "22 99913\n",
      "23 0\n",
      "24 26\n",
      "25 19\n",
      "26 0\n",
      "27 9\n",
      "28 10\n",
      "29 99913\n"
     ]
    }
   ],
   "source": [
    "tx_test=tX_pd\n",
    "for col in tX_pd.columns:\n",
    "    s = tX_pd[tX_pd[col]==0].index.size\n",
    "    if s >= 0:\n",
    "        print(col, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "      <td>250000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-49.023079</td>\n",
       "      <td>49.239819</td>\n",
       "      <td>81.181982</td>\n",
       "      <td>57.895962</td>\n",
       "      <td>-708.420675</td>\n",
       "      <td>2.373100</td>\n",
       "      <td>18.917332</td>\n",
       "      <td>158.432217</td>\n",
       "      <td>1.437609</td>\n",
       "      <td>-0.128305</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008171</td>\n",
       "      <td>46.660207</td>\n",
       "      <td>-0.019507</td>\n",
       "      <td>0.043543</td>\n",
       "      <td>41.717235</td>\n",
       "      <td>-0.010119</td>\n",
       "      <td>209.797178</td>\n",
       "      <td>0.979176</td>\n",
       "      <td>-348.329567</td>\n",
       "      <td>73.064591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>406.345647</td>\n",
       "      <td>35.344886</td>\n",
       "      <td>40.828691</td>\n",
       "      <td>63.655682</td>\n",
       "      <td>454.480565</td>\n",
       "      <td>0.782911</td>\n",
       "      <td>22.273494</td>\n",
       "      <td>115.706115</td>\n",
       "      <td>0.844743</td>\n",
       "      <td>1.193585</td>\n",
       "      <td>...</td>\n",
       "      <td>1.816763</td>\n",
       "      <td>22.064922</td>\n",
       "      <td>1.264982</td>\n",
       "      <td>1.816611</td>\n",
       "      <td>32.894693</td>\n",
       "      <td>1.812223</td>\n",
       "      <td>126.499506</td>\n",
       "      <td>0.977426</td>\n",
       "      <td>532.962789</td>\n",
       "      <td>98.015662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.329000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.104000</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>-1.414000</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>-2.505000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>-3.142000</td>\n",
       "      <td>13.678000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>78.100750</td>\n",
       "      <td>19.241000</td>\n",
       "      <td>59.388750</td>\n",
       "      <td>14.068750</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>1.810000</td>\n",
       "      <td>2.841000</td>\n",
       "      <td>77.550000</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>-1.371000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>32.375000</td>\n",
       "      <td>-1.014000</td>\n",
       "      <td>-1.522000</td>\n",
       "      <td>21.398000</td>\n",
       "      <td>-1.575000</td>\n",
       "      <td>123.017500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>105.012000</td>\n",
       "      <td>46.524000</td>\n",
       "      <td>73.752000</td>\n",
       "      <td>38.467500</td>\n",
       "      <td>-999.000000</td>\n",
       "      <td>2.491500</td>\n",
       "      <td>12.315500</td>\n",
       "      <td>120.664500</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>-0.356000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033000</td>\n",
       "      <td>40.516000</td>\n",
       "      <td>-0.045000</td>\n",
       "      <td>0.086000</td>\n",
       "      <td>34.802000</td>\n",
       "      <td>-0.024000</td>\n",
       "      <td>179.739000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>38.960000</td>\n",
       "      <td>40.512500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>130.606250</td>\n",
       "      <td>73.598000</td>\n",
       "      <td>92.259000</td>\n",
       "      <td>79.169000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>2.961000</td>\n",
       "      <td>27.591000</td>\n",
       "      <td>200.478250</td>\n",
       "      <td>1.777000</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.565000</td>\n",
       "      <td>53.390000</td>\n",
       "      <td>0.959000</td>\n",
       "      <td>1.618000</td>\n",
       "      <td>51.895000</td>\n",
       "      <td>1.561000</td>\n",
       "      <td>263.379250</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>75.349000</td>\n",
       "      <td>109.933750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1192.026000</td>\n",
       "      <td>690.075000</td>\n",
       "      <td>1349.351000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>8.503000</td>\n",
       "      <td>5.684000</td>\n",
       "      <td>2834.999000</td>\n",
       "      <td>1852.462000</td>\n",
       "      <td>19.773000</td>\n",
       "      <td>1.414000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>560.271000</td>\n",
       "      <td>2.503000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2842.617000</td>\n",
       "      <td>3.142000</td>\n",
       "      <td>2003.976000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1120.573000</td>\n",
       "      <td>1633.433000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean      -49.023079      49.239819      81.181982      57.895962   \n",
       "std       406.345647      35.344886      40.828691      63.655682   \n",
       "min      -999.000000       0.000000       6.329000       0.000000   \n",
       "25%        78.100750      19.241000      59.388750      14.068750   \n",
       "50%       105.012000      46.524000      73.752000      38.467500   \n",
       "75%       130.606250      73.598000      92.259000      79.169000   \n",
       "max      1192.026000     690.075000    1349.351000    2834.999000   \n",
       "\n",
       "                  4              7              8              9   \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean     -708.420675       2.373100      18.917332     158.432217   \n",
       "std       454.480565       0.782911      22.273494     115.706115   \n",
       "min      -999.000000       0.208000       0.000000      46.104000   \n",
       "25%      -999.000000       1.810000       2.841000      77.550000   \n",
       "50%      -999.000000       2.491500      12.315500     120.664500   \n",
       "75%         0.490000       2.961000      27.591000     200.478250   \n",
       "max         8.503000       5.684000    2834.999000    1852.462000   \n",
       "\n",
       "                  10             11  ...             15             16  \\\n",
       "count  250000.000000  250000.000000  ...  250000.000000  250000.000000   \n",
       "mean        1.437609      -0.128305  ...      -0.008171      46.660207   \n",
       "std         0.844743       1.193585  ...       1.816763      22.064922   \n",
       "min         0.047000      -1.414000  ...      -3.142000      26.000000   \n",
       "25%         0.883000      -1.371000  ...      -1.575000      32.375000   \n",
       "50%         1.280000      -0.356000  ...      -0.033000      40.516000   \n",
       "75%         1.777000       1.225000  ...       1.565000      53.390000   \n",
       "max        19.773000       1.414000  ...       3.142000     560.271000   \n",
       "\n",
       "                  17             18             19             20  \\\n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000   \n",
       "mean       -0.019507       0.043543      41.717235      -0.010119   \n",
       "std         1.264982       1.816611      32.894693       1.812223   \n",
       "min        -2.505000      -3.142000       0.109000      -3.142000   \n",
       "25%        -1.014000      -1.522000      21.398000      -1.575000   \n",
       "50%        -0.045000       0.086000      34.802000      -0.024000   \n",
       "75%         0.959000       1.618000      51.895000       1.561000   \n",
       "max         2.503000       3.142000    2842.617000       3.142000   \n",
       "\n",
       "                  21             22             23             29  \n",
       "count  250000.000000  250000.000000  250000.000000  250000.000000  \n",
       "mean      209.797178       0.979176    -348.329567      73.064591  \n",
       "std       126.499506       0.977426     532.962789      98.015662  \n",
       "min        13.678000       0.000000    -999.000000       0.000000  \n",
       "25%       123.017500       0.000000    -999.000000       0.000000  \n",
       "50%       179.739000       1.000000      38.960000      40.512500  \n",
       "75%       263.379250       2.000000      75.349000     109.933750  \n",
       "max      2003.976000       3.000000    1120.573000    1633.433000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 21)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_clean = tX_pd.values\n",
    "tX_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "from helpers import *\n",
    "tX, mean_x, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61210652,  0.27263912,  0.45318267, ...,  0.07906273,\n",
       "         0.07239186,  0.51445612],\n",
       "       [ 0.69995786,  0.33955504,  0.47432925, ...,  0.07349064,\n",
       "         0.07518377,  0.25141043],\n",
       "       [-3.83567311,  0.70478701,  0.56316206, ...,  0.07868344,\n",
       "         0.06272575,  0.24368771],\n",
       "       ...,\n",
       "       [ 0.4830178 ,  0.30732685,  0.36720434, ...,  0.07769415,\n",
       "         0.07000662,  0.23485448],\n",
       "       [ 0.44193683,  0.14636577,  0.33972709, ..., -3.83567311,\n",
       "        -3.83567311,  0.07065572],\n",
       "       [-3.83567311,  0.35514907,  0.34762186, ..., -3.83567311,\n",
       "        -3.83567311,  0.07065572]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial basis\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    for d in range (0, degree+1):\n",
    "        if d==0:\n",
    "            psi=np.power(x,d)\n",
    "        else:\n",
    "            psi = np.hstack((psi, np.power(x,d)))\n",
    "    return psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_poly_2(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    d = np.arange(0, degree+1).repeat(x.shape[1])\n",
    "    psi = np.tile(x, degree+1)\n",
    "    psi = np.power(psi, d)\n",
    "    return psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 69)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpoly = build_poly_2(tX,2)\n",
    "xpoly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval]\n",
    "                 for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costs import *\n",
    "def compute_rmse(y, x, w):\n",
    "    l = compute_loss(y, x, w)\n",
    "    return np.math.sqrt(2*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degree, mean=True):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "        x_te_poly = build_poly(x_te, degree)\n",
    "        x_tr_poly = build_poly(x_tr, degree)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form data with polynomial degree: TODO\n",
    "    # ***************************************************\n",
    "        w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "        rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "        rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    if mean==True:\n",
    "        loss_tr = np.mean(losses_tr)\n",
    "        loss_te = np.mean(losses_te)\n",
    "    else:\n",
    "        loss_tr = losses_tr\n",
    "        loss_te = losses_te\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3yUVfb48c9JTEJChwDSBBRkAQVUREcUA0pTf1iwYS+7yNp1sa2gqKu46+raddXliwVF115wRQgRwShFUCnSQUKQ3gIkIcn5/XFnyCTMJDMhk0ky5/16Pa9knjZnLuGeee69z31EVTHGGGNKi4t2AMYYY6onSxDGGGMCsgRhjDEmIEsQxhhjArIEYYwxJiBLEMYYYwKyBGFMFIjIGhE50/v7X0XktVD2rcD7nCYiSysap4lth0U7AGNinao+VlnnEhEFOqnqCu+5vwU6V9b5TWyxKwhTK4mIffkx5hBZgjA1ioi0FZEPRWSziGwVkee9668RkVki8i8R2QaMFZE4ERktImtFZJOIvCEiDb371xGRt7zn2CEic0Skhd+5VonIbhFZLSKXB4ijlYjsE5EmfuuOE5EtIpIgIkeJSLr3/FtEZKKINArymcaKyFt+r6/0xrxVRO4vtW9vEcn0xrxBRJ4XkUTvthne3X4SkRwRuURE0kQky+/4LiKS4T1+kYgM9ds2QUReEJEvvJ/9BxE5Kvx/JVNbWIIwNYaIxAOfA2uB9kBrYJLfLicBq4DmwKPANd6lH3AkUA943rvv1UBDoC3QFBgJ7BORusCzwBBVrQ+cAiwoHYuqZgOZwDC/1ZcB76vqfkCAcUAroIv3fcaG8Bm7Ai8BV3qPbQq08dulELgDSAU8wBnAjd6Y+nr36aGq9VT13VLnTgA+A6Z4y+gWYKKI+DdBDQceAhoDK3DlaGKUJQhTk/TGVZp3qeoeVc1V1Zl+27NV9TlVLVDVfcDlwFOqukpVc4D7gEu9zU/7cZVvR1UtVNV5qrrLe54i4BgRSVbVDaq6KEg8b+MqVEREgEu961DVFar6tarmqepm4Cng9BA+44XA56o6Q1XzgDHeePCed56qfu/9jGuAf4d4XoCTcUnycVXNV9V0XMId7rfPh6o6W1ULgIlAzxDPbWohSxCmJmkLrPVWXoGsK/W6Fe5qw2ctbmBGC+BN4Ctgkohki8g/RCRBVfcAl+CuKDZ4m1v+EOT93gc8ItIK6Aso8C2AiDQXkUkisl5EdgFv4b71l6eV/+fwxrPV91pEjhaRz0Xkd+95HwvxvAfOrapFfuvW4q7EfH73+30vLqGYGGUJwtQk64AjyuiALj01cTbQzu/1EUABsFFV96vqQ6raFdeMdA5wFYCqfqWqA4CWwK/AqwHfTHUHrrnmYlzz0jtaPD3yOG883VW1AXAFrtmpPBtwiRAAEUnBXen4vOSNqZP3vH8N8bzgyqOtiPj/vz8CWB/i8SbGWIIwNclsXAX6uIjU9XY09ylj/3eAO0Skg4jUw33bfldVC0Skn4gc6+3X2IVrcioUkRYiMtTbF5EH5ODa/YN5G5dYhnl/96nvPXaHiLQG7grxM74PnCMip3o7nx+m5P/T+t54c7xXNn8udfxGXH9LID8Ae4C7vR3pacD/o2Q/jjEHWIIwNYaqFuIqtI7Ab0AWrjkomPG4pqQZwGogF9cxC3A4rjLeBSwBvsE1A8UBf8F9296Ga9+/sYz3+BTohLsq+clv/UPA8cBO4AvgwxA/4yLgJlyy2QBs935On1G4q5XduCubd0udYizwuneU0sWlzp0PDAWGAFuAF4GrVPXXUGIzsUfsgUHGGGMCsSsIY4wxAVmCMMYYE5AlCGOMMQFZgjDGGBOQJQhjjDEB1aoZL1NTU7V9+/YVOnbPnj3UrVu3cgOqxay8wmPlFR4rr/AcSnnNmzdvi6o2C7StViWI9u3bM3fu3Aodm5GRQVpaWuUGVItZeYXHyis8Vl7hOZTyEpG1wbZFrIlJRMZ7p1heGGT7H7zTFueJyKhS29aIyC8iskBEKlbjG2OMOSSR7IOYAAwuY/s24Fbgn0G291PVnqraq7IDM8YYU76IJQhVnYFLAsG2b1LVObg5cIwxxlQz1bUPQoEp4p6v+29VfSXYjiIyAhgB0KJFCzIyMkpvp27dusTHx5f5hg0aNGD+/PmHGnfMqFOnDt988w02VUtocnJyDvrbNMFZeYUnUuVVXRNEH1XNFpHmwNci8qv3iuQg3uTxCkCvXr20dEfN6tWrqV+/Pk2bNsU90yWw3bt3U79+/Ur7ALWZqpKVlUVBQQEdOnSIdjg1gnW6hsfKKzyRKq9qeR+E93GOqOom4CPck8QqJDc3t9zkEFU5ObBhg/tZQ4gIDRs2JDc3N9qhGGMiqNolCO88//V9vwMDgYAjocI4Z2WEVvlycmDpUli/HpYtq3FJwhhTu0VymOs7uIe6dxaRLBG5XkRGishI7/bDRSQLuBMY7d2nAe5xkDNF5CfcA2K+UNX/RSrOSNuxYwcvvvhi8Yr8fNi6FdasgeXLwdeGX1QEu3eXOPass85ix44dVResMcb4iVgfhKoOL2f770CbAJt2AT0iElRVysmB3bvZsXkzLz73HDeefbZLAL5mmfh4CpOSiN+3rzhJlOrwnTx5cqWHVVBQwGGHHRb0dTCFhYXldvQbY2qX6tpJHVWZmZCRAWlp4PFU4AS+piNV7v3rX1m5ejU9Bw5kwGmncfZZZ/HQM8/QsnVrFixYwOLZsznvwgtZt24dubm53HbLLYy4/Xag+M7wnJwchgwZwqmnnsp3331H69at+eSTT0hOTi7xtps3b2bkyJH89ttvADz99NP06dOHsWPHkp2dzZo1a0hNTWXgwIF88cUX5ObmsmfPHqZNm8bdd9/Nl19+iYgwevRoLrnkEjIyMnjooYdo2bKli3Xx4kMrWGNMjRJTCeL222HBgsDbCguTiY+HnTvh559di09cHHTvDg0bBj9nz57w9NOlVm7ZcuBq4PGbb2bh2rUsWLIE4uLIyMhg9ty5LJww4cAIoPFvv02Thg3Z99NPnHjRRQw77zyalppTavny5bzzzju8+uqrXHzxxXzwwQdcccUVJfa57bbbuOOOOzj11FP57bffGDRoEEuWLAFg3rx5zJw5k+TkZCZMmEBmZiY///wzTZo04YMPPmDBggX89NNPbNmyhRNPPJG+ffsCMHv2bBYuXGijlYyJQTGVIEKxc6dLDuB+7txZdoI4iCrs3Vv8Oi4O4uPdT6/evXuXqHCfffZZPvroI1Bl3aZNLP/mG5qmppY4bYcOHejZsycAJ5xwAmvWrDnoradOnVriW/6uXbvY7e3XGDp0aIkrjgEDBtCkSRMAZs6cyfDhw4mPj6dFixacfvrpzJkzhwYNGhwUqzEmdsRUgjjom76f3bv3Ub9+fTIz4YwzXF9yYiJMnBhmM9Pu3S5BtGgBhx0GycklkgNQYtbFjIwMpk6dSmZmJikpKaSdfjq5RUUlO7CBpKSkA7/Hx8ezb9++g966qKiIzMzMg5qeSr9n6ddl3exmM2oaE7uq3TDXaPN4YNo0eOQR9zOs5KDqhqwmJkLr1tCyJfUPP/zAt/hAdu7cSePGjUlJSeHXX3/l+x9+gDZt3FXH/v0QIBEEM3DgQJ5//vkDrxcEa08rpW/fvrz77rsUFhayefNmZsyYQe/eFb71xBhTS1iCCMDjgfvuq0AH9a5dsGcPtGx54KqhadOm9OnTh2OOOYa77rrroEMGDx5MQUEB3bt3Z8yYMZx88smQkACdO7sdVq6EvLyQ3v7ZZ59l7ty5dO/ena5du/Lyyy+HdNz5559P9+7d6dGjB/379+cf//gHhx9+eGif2RhTa0ltmkunV69eWvp5EEuWLKFLly7lHnvIU22owpIlUFgI3bod1KxUIfv2udFQcXEuYfg1M0Xb7t27ycrKCqlsjU0dES4rr/Ac4vMg5gWbNduuICrLjh2u78Hv6uGQJSfD0Ue7pLNsmXuPGjYthzGm5rIEURlUITsb6tSBpk0r99wpKdCpk+uPWLGiRk7LYYypmSxBVIbt211zUMuWEIk5iurVA++QVCDgtBzGGFPZLEEcKv+rB/9KvLKlphYnHxGwqcmNMRFmCeJQbdvm5ldq1SoyVw8+9epBx47uPVJS3GtjjIkgSxCHwnf1kJwMjRtH/v0aNnT3V+zZY01MxpiIswRxKLZudfcotG4d9OrhoOm+w/T000+z13/qjmbN3H0S69cfNPurMcZUJksQFVVU5K4eUlLKnKyp0hNEfLzrDM/JcRNFlaOgoKDM16EeZ4yJPTE1F1PIQpnve+tWN2FTu3Zl9j3ce++9rFy5kp49ezJgwACeeOIJnnjiCd577z3y8vI4//zzeeihh9izZw8XX3wxWVlZFBYWMmbMGDZu3Eh2djb9+vUjNTWV6dOnu5OmpsLGjcybMoU7X3iBnJwcUlNTmTBhAi1btiQtLY1TTjmFWbNmMXToUH755ReaNGnC/PnzOf7447n//vu57rrrWLVqFSkpKbzyyit07979oGnB33777UovWmNMzRFbCaKM+b6TCwsJa77vPXtcYvB44Jlngr7l448/zsKFCw/MizRlyhSWL1/O7NmzUVWGDh3KjBkz2Lx5M61ateKLL74A3BxNDRs25KmnnmL69Omk+s/uGhfH/mbNuOXyy/nkv/+lWefOvPvuu9x///2MHz8ecFcu33zzDQDXXHMNy5YtY+rUqcTHx3PLLbdw3HHH8fHHH5Oens5VV111ID7/acGNMbEtthJEKEKZ7zs/321LSQl75NKUKVOYMmUKxx13HAA5OTksX76c0047jVGjRnHPPfdwzjnncNppp5V5nqWbN7Nw1SoGnHsu1KlDYWEhLVu2PLD9kksuKbH/RRdddOCJcDNnzuSDDz4AoH///mzdupWd3uaq0tOCG2NiV2wliDLm+97nm4upvPm+Cwth4UJ338PRR4edIFSV++67jxtuuOGgbfPmzWPy5Mncd999DBw4kAceeCD4eYBuXbuS+eKLrpmrWbMS28Od3lu8n8Om9zbG+FgndWnlzfe9ebOb9iLE+x7q169fYrrvQYMGMX78eHK8U2WsX7+eTZs2kZ2dTUpKCldccQWjRo3ixx9/DHi8T+fOndm8bRuZy5dDdjb7c3NZtGhRSB+xb9++TJw4EXCTfKWmptKgQYOQjjXGxI7YuoIIlccTuHN61y43cqlu3ZDvZPaf7nvIkCE88cQTLFmyBI/3/PXq1eOtt95ixYoV3HXXXcTFxZGQkMBLL70EwIgRIxgyZAgtW7Ys7qQGEhMTef/997n1ppvYuXkzBSLcPmoU3bp1KzemsWPHcu2119K9e3dSUlJ4/fXXQ/osxpjYYtN9e5U73XdOjpt6W9VdOXTuXH3uZl6+3MV37LHuKXZVwKb7Do9NXx0eK6/w1LjpvkVkvIhsEpGFQbb/QUQyRSRPREaV2jZYRJaKyAoRuTdSMYZl9+7iG9NUq9edzK1bu76RjRujHYkxphaJZB/EBGBwGdu3AbcC//RfKSLxwAvAEKArMFxEukYoxtD5Xy3ExVWvyfJSUtxUHxs3uv4RY4ypBBFLEKo6A5cEgm3fpKpzgNI1Wm9ghaquUtV8YBJwbqTiDJmvQ7pJEzd6qbo0L/m0bu2G3m7YEO1IjDG1RHUcxdQaWOf3Osu7rsIqpZ/F16TUtm31Sw7ght2mprpRViE+w/pQ1Ka+K2NMYNVxFFOgsaNBayMRGQGMAGjRogUZGRkltterV4+srCwaNmx4YKx/IIWFhQGHk/okb9+OJCWxNzfXTe9dDUmDBtTdupWCtWvJ9btprrKpKtu3b2fPnj0HlbcJLCcnx8oqDFZe4YlUeVXHBJEFtPV73QbIDrazqr4CvAJuFFPpnvz9+/eTlZXF+vXry3zT3Nxc6tSpE+xNYN06d+WQlRXCR4iivDx3FbF5sxuOm5QUkbfZs2cPPXr0ICEhISLnr21sVE54rLzCE6nyqo4JYg7QSUQ6AOuBS4HLKnqyhIQEOnToUO5+GRkZB6a/CLARBg+Gzz6DPn0qGkrV+PJLOOss93tycuCb/SpBRkaGJQdjarmIJQgReQdIA1JFJAt4EEgAUNWXReRwYC7QACgSkduBrqq6S0RuBr4C4oHxqhraLcKRkp7uJvLr2zeqYYRkwQLXoa7qpgvJyIhIgjDG1H4RSxCqOryc7b/jmo8CbZsMTI5EXBUybRr06gU1YTqKtDTXrOTrJ7HLdGNMBVXHUUzVy+7dMHs29O8f7UhC4/G4K55evdxVT+fO0Y7IGFNDWYIoz8yZUFDgZnitKTweGD/eNTG9+mq0ozHG1FCWIMqTnu6m/T7llGhHEp5jj3VXPS+84BKcMcaEyRJEedLTXXKoiQ/Rue02Nzz3o4+iHYkxpgayBFGWbdtg/vya0/9Q2tlnw5FHlvlIVGOMCcYSRFm++cYNF62pCSI+Hm65BWbNglLToBtjTHksQZQlPd3djXziidGOpOKuvdbdAW5XEcaYMFmCKEt6Opx2muukrqkaNnRJ4t13baZXY0xYLEEEs2EDLF5cc5uX/N1yixvJ9PLL0Y7EGFODWIIIxvf859qQIDp1ch3WL79cJVOBG2NqB0sQwaSnQ6NG0LNntCOpHLfdBps2waRJ0Y7EGFNDWIIIJj3dzWMUHx/tSCrHGWdAt27w9NPFz9Y2xpgyWIIIZPVqt9SG5iUfEbj1Vjfb67ffRjsaY0wNYAkiEF//Q02afykUV1zhnqltQ16NMSGwBBFIejq0aAFdukQ7ksqVkgJ/+hN8/DGsWRPtaIwx1ZwliNJUXYLo3981y9Q2N93kPtcLL0Q7EmNMNWcJorSlS909ELWp/8Ff27YwbBi89hrk5EQ7GmNMNWYJorRp09zP2pogwA153bED3ngj2pEYY6oxSxClpadDu3bQoUO0I4kcj8c9ce7ZZ6GoKNrRGGOqKUsQ/oqK3Aim2tr/4CPiriKWLoUpU6IdjTGmmrIE4e+nn2D79trdvORz8cVw+OEwdiyMGweZmdGOyBhTzRwW7QCqlfR097Nfv+jGURUSE+Gcc1xn9Zw5kJTk+l88nmhHZoypJuwKwl96OnTuDK1bRzuSqtG8uftZVAT5+ZCREdVwjDHVS8QShIiMF5FNIrIwyHYRkWdFZIWI/Cwix/ttKxSRBd7l00jFWCKeggKYMSM2mpd8zjmneK6pxEQ395QxxnhF8gpiAjC4jO1DgE7eZQTwkt+2fara07sMjVyIxeovXeruC6ht02uUxeOBV15xv99wgzUvGWNKiFiCUNUZwLYydjkXeEOd74FGItIyUvGUp9H8+e6XWPsWfd11cNJJ8MUXNuTVGFNCNPsgWgPr/F5nedcB1BGRuSLyvYicVxXBNP7xR/fsh6ZNq+LtqpdbboHly+Hrr6MdiTGmGonmKKZANxr4HlRwhKpmi8iRQLqI/KKqKwOeRGQEromKFi1akFGBjta4vDz6LFzIuvPOY2UMdtRK8+Z4Gjdm99ix/JKUFNIxOTk5FSrrWGXlFR4rr/BErLxUNWIL0B5YGGTbv4Hhfq+XAi0D7DcBuDCU9zvhhBO0QqZNUwXVzz+v2PG1wZgxqiKqK1aEtPv06dMjG08tY+UVHiuv8BxKeQFzNUidGs0mpk+Bq7yjmU4GdqrqBhFpLCJJACKSCvQBFkc0kjfeQEXcvQCxauRIN6LpxRejHYkxppqI5DDXd4BMoLOIZInI9SIyUkRGeneZDKwCVgCvAjd613cB5orIT8B04HFVjVyCyMyEN99003wPHRq7dxS3auVmeR0/HvbsiXY0xphqIGJ9EKo6vJztCtwUYP13wLGRiusgU6dCUZHrEPHdLBarwz1vuQXefRcmToQRI6IdjTEmyuxO6jPPhORkiuLi7GaxU05xI7mee85dURljYpolCI8Hpk1jzXXX2VxEIu4qYuFC+OabaEdjjIkySxAAHg+/XX55bCcHn+HDoUkTeP75aEdijIkySxCmpORk+OMf4eOPYd268vc3xtRaliDMwW680fVBvPRS+fsaY2otSxDmYO3auSG/r74KubnRjsYYEyWWIExgN98MW7a4Ya/GmJhkCcIE1r8/dO1qQ16NiWGWIExgIu4qYt48+OGHaEdjjIkCSxAmuCuvhAYN3FWEMSbmWIIwwdWrB9deC//9L/z+e7SjMcZUMUsQpmw33QT79xc/mtQYEzMsQZiydeoEQ4bAyy+7yQyNMTHDEoQp3803w4YNcNVVsTsdujExyBKEKV/Dhm5U07vvwhlnWJIwJkZYgjDlmzGj+HffMzOMMbWeJQhTvrQ0qFOn5GtjTK1nCcKUz/vMDNLSoLAQUlOjHZExpgpYgjCh8Xhg0iRISoInn4x2NMaYKmAJwoSuRQu45hqYMIGEbduiHY0xJsIsQZjw/OUvkJ9Pm48+inYkxpgIswRhwtOpE1xwAa0+/hhycqIdjTEmgixBmPDddRcJOTnw2mvRjsQYE0ERTRAiMl5ENonIwiDbRUSeFZEVIvKziBzvt+1qEVnuXa6OZJwmTCedxI4ePeCpp9w8TcaYWimkBOGtyK8QkQe8r48Qkd4hHDoBGFzG9iFAJ+8yAnjJe/4mwIPASUBv4EERaRxKrKZq/HbppbBunT1xzphaLNQriBcBDzDc+3o38EJ5B6nqDKCs4S7nAm+o8z3QSERaAoOAr1V1m6puB76m7ERjqti2k06CY46Bf/zDnjhnTC11WIj7naSqx4vIfABV3S4iiZXw/q2BdX6vs7zrgq0/iIiMwF190KJFCzIqOA1ETk5OhY+NRTl79rDknHPo8vjj/PzEE2zrHcoFZeyyv6/wWHmFJ1LlFWqC2C8i8YACiEgzoKgS3l8CrNMy1h+8UvUV4BWAXr16aVoFp4HIyMigosfGooyMDLo89BC89Rbd//c/uPvuaIdUrdnfV3isvMITqfIKtYnpWeAjoLmIPArMBB6rhPfPAtr6vW4DZJex3lQniYlw++0wfTrMmRPtaIwxlSykBKGqE4G7gXHABuA8Vf1vJbz/p8BV3k7wk4GdqroB+AoYKCKNvZ3TA73rTHXzpz+56cCfeCLakRhjKllITUwichSwWlVfEJE0YICIbFDVHeUc9w6QBqSKSBZuZFICgKq+DEwGzgJWAHuBa73btonII4Dva+nDqmpzO1RHDRrAn//sOqtXrICOHaMdkTGmkoTaB/EB0EtEOgKvAZ8Bb+Mq96BUdXg52xW4Kci28cD4EOMz0XTrre6eiKeeghdfjHY0xphKEmofRJGqFgAXAM+o6h1Ay8iFZWqUli3d40j/7/9g06ZoR2OMqSShJoj9IjIcuAr43LsuITIhmRpp1CjIy4Pnn492JMaYShJqgrgWd6Pco6q6WkQ6AG9FLixT43TuDOee6xKETeJnTK0Q6iimxap6q6q+4329WlUfj2xopsa5+27Yvh3GW9eRMbVBqKOYzgEeAdp5jxFcH3ODCMZmahqPB049FR57DHbtgjPOcOuMMTVSqE1MTwNXA01VtYGq1rfkYAIaOhQ2boQHH3QJIjMz2hEZYyoo1ASxDljoHZZqTHC+6b+LiiA/H2w+HWNqrFDvg7gbmCwi3wB5vpWq+lREojI1V79+bgqO/HyIiwObT8eYGivUK4hHcXc61wHq+y3GlOTxuLmZOnSAevXg2GOjHZExpoJCvYJooqoDIxqJqT1OOQUmTYKTTnJ3Vz/wQLQjMsZUQKhXEFNFxBKECV3v3jBsmJvEz+6uNqZGKjdBiIjg+iD+JyL7RGSXiOwWkV2RD8/UaI8+Cvv2wd/+Fu1IjDEVUG6C8I5cWqCqcaqabMNcTcg6d4Y//hFefhlWrox2NMaYMIXaxJQpIidGNBJTOz34ICQkwJgx0Y7EGBOmUBNEP+B7EVkpIj+LyC8i8nMkAzO1RMuWcMcd8M478OOP0Y7GGBOGUEcxDYloFKZ2u+su18x0770wZUq0ozHGhCjUyfrWBloiHZypJRo2hNGj4euv3WKMqRFCbWIy5tD8+c/Qrp27iigqinY0xpgQWIIwVSMpyQ13/fFHeO+9aEdjjAmBJQhTdS67DHr0gPvvd3M1GWOqNUsQpurExcHjj8OqVfDKK9GOxhhTDksQpmoNGuRmfH34Ydi9O9rRGGPKYAnCVC0R+PvfYfNmePLJaEdjjClDRBOEiAwWkaUiskJE7g2wvZ2ITPPefJchIm38thWKyALv8mkk4zRV7MQT4aKLXKK4/3576pwx1VTEEoSIxAMv4G6y6woMF5GupXb7J/CGqnYHHgbG+W3bp6o9vcvQSMVpouTCCyE3F8aNs0eTGlNNRfIKojewQlVXqWo+MAk4t9Q+XYFp3t+nB9huaquVK11zk6o9mtSYairUqTYqojXuWdY+WcBJpfb5CRgGPAOcD9QXkaaquhWoIyJzgQLgcVX9ONCbiMgIYARAixYtyKhgRZOTk1PhY2PRoZZXgwYN6JGQQFx+PhQVsSA5mZ21uPzt7ys8Vl7hiVR5RTJBSIB1Wur1KOB5EbkGmAGsxyUEgCNUNVtEjgTSReQXVT1ozmhVfQV4BaBXr16aVsFnIGdkZFDRY2PRIZdXWhocf7ybo+mNNzhuy5Za/fxq+/sKj5VXeCJVXpFMEFlAW7/XbYBs/x1UNRu4AEBE6gHDVHWn3zZUdZWIZADHAfZQgdrE43GL7/6I886DXr2iHZUxxiuSfRBzgE4i0kFEEoFLgRKjkUQkVUR8MdwHjPeubywiSb59gD7A4gjGaqLpX/+Cww+Ha66BvLxoR2OM8YpYglDVAuBm4CtgCfCeqi4SkYdFxDcqKQ1YKiLLgBbAo971XYC5IvITrvP6cVW1BFFbNWoEr74KixbBQw9FOxpjjFckm5hQ1cnA5FLrHvD7/X3g/QDHfQccG8nYTDUzZAhcf727N+K886B372hHZEzMszupTfXx5JPQqhVcfbW7R8IYE1WWIEz10bAh/Oc/8Ouv7lnWxpiosgRhqpeBA2HECPjnP+3uamOizBKEqX6eeALatHGjmvbti3Y0xsQsSxCm+mnQwDU1LVsGY8ZEOxpjYpYlCFM9nXkmjBwJTz0Fs2ZFOxpjYpIlCLg34TwAABrQSURBVFN9/eMf0K4dXHst7N0b7WiMiTmWIEz1Vb8+jB8Py5e7JDFunHVcG1OFInqjnDGHrF8/GDYM3nsP3n8fkpJg2jQ3h5MxJqLsCsJUf8cc434WFdmzI4ypQpYgTPU3aBDUqeN+LyqCbt2iG48xMcIShKn+PB5IT4ebb3aJYswY2Lkz2lEZU+tZgjA1g8cDzz0Hn3wCixfD+efb1ODGRJglCFOzDBgA//d/MH26m9SvqCjaERlTa9koJlPzXHEFZGfDPfdA69ZuFlhjYlVmJkdMnOhG+FXy6D5LEKZmuusuWL/e3WndujXceWe0IzKmfJmZbhReWlr5lbn/viee6OYl27u35DJnDkW330n7/fkUvjmR+OmVOwTcEoSpmURccsjOhr/8xT1H4tJLox2VqQlCraT99zvpJNfnVXrJzXU/582DOXOgSxc48ki33rfN9/uKFRRNeB0pLIS4OGTQQHcz6L59xUtuLrpvH7ptO/L7hgOhSBkfx9dPsD8vn6w3MmhnCcIYID4e3nwTNm+Gq66C5s2hf/9oR2UqU6iVeUYGfPUV9OgBnTrB7t2Qk1P80/f70qXupsuCAvf34/FASoqrwL0VNLm5sGOH+7uqZOJdtKiQnKmZ7E45nL0ks6+oDjmFyewuaMTu/XXooCvpwe/EoRQipHMG/2Mw+0gmLy4FqZtCfP0Umu1Zzeidd3EYBewnkW9I46pKjNcShKnZ6tSBjz+GU091jyr99ltXSZgaTfLz4b//hSuvhP37XWV+1VWuMt+ypXjZuhU2bgx9RNthh7nkAFBYCCtWuPm+6tSBpk3dz+RkWLYM3bwFQVERpH9/N4FkUhIF8Unsyktix74ktu9Ngs8/o+eiicRTRAHxvN/8Rt5pfBO/b08ie1sddhfUIZc6HM88pjKABPLZTyKDCyezKsVD06YctMzMyKTz92cc2HfhRQ9z+5MeGjeGunXdBTS4/Dk4rRen7J/Odwn9GHeV9UEYU1KjRvC//7lvg2ecAddd54bB2nQc1VNmJnz5JXTs6P7tfvsN1q4t/rl2Laf//nvJY4qK3BTwjRpBaqqrRVu1gu7dYeVKN+OvKsTFweWXu7m76tVzTTj16hUvc+ZQ2O8Md0d+YiLxH3544O8kJ8e1WK5fDyvfyuSyed4KWhO5cd0jzH7Dw++/w/btJUM7mTZM4/0DlfkkhqNHd+aYZtCvGTTzLlu39mHIPdM4tSCDWQlp/GO6h1NOCVZEHs5Km0af/W7fcXd4aNv24P08HhiX4WH8+BaMu+7ISv+TtwRhaoc2beDxx90IpyeecPdMpKdbkqhKgZqDtm6FRYuKl1mzYMGCg49NSoIjjnDLWWexuqiIDq1buycLFhRAQgJMmQKnnRb4fc8orvT5859L/Luruvsq1y6CKTM9fFowjdM0gxn708i71cOePS4p7Nrlf1IP/2EaaWSQQRob8jz06uWmBjv8cLe0aOF+ZmV5OGu4X2X+sSfon93JJ3vIyPAwLq3sP01fxR/qvnl5v+HxHBl8pwqyBGFqj99+c98gi4pcO/Izz1iCqCoZGTB4sKuk4+NdM19Wlmv+8alf310BiBR/27/hBvf88WbN3GuvtRkZdEhLg7PPLrcPQk/2MOvhaWz+bwbbjk1jywwPa98seVGye7f/ER5m4oEiaLsRevd2rUetW7ulVSvYtAmuu87DnHwPiYkw7Z3gf0q9e0PLMCrzUP8kw9k3UixBmNojLc19E83PdxXQu+9Ckybwr3+59aZyqLpmne+/hx9+cD9//LH4psWCApcYzjrLzZvlW9q0cfv6f9u/8kr3VTyITDxk4KFvERyxznUZ+C8rV7p+59xcD+CB2e64Jk3cxchRR7lv/e3aude7drkZW/bvd2//7rvBK+F27UIfkVodKvNIiGiCEJHBwDNAPPCaqj5eans7YDzQDNgGXKGqWd5tVwOjvbv+TVVfj2SsphbweNxU4BkZrtP6s89cc9Ps2a7Ds0OHaEdY8/j6C5o0cV/DfUlh61a3vW5d9xX68stdbVtY6Gre994LXGP6/xsFqHn37YNff4Wvv27O+PEwcWLgm+UTE91o0qOOciFkZrq8FR8Po0fD2LHBP1LXrqFV/LW10g9HxBKEiMQDLwADgCxgjoh8qqqL/Xb7J/CGqr4uIv2BccCVItIEeBDoBSgwz3tsqe4hY0rx/1992mkuUVx9NRx/PLz+OgwdGt34aoLt2+Gbb+Dtt90zOFSLt3XrBueeCyef7JauXV2tDK7tP4SaNxMPX+/3cMRSSFhV3D2xeDGsWuVLCF0PtBaCa5U691z37b9jR3cx4nvb0l0QgwaV/fGs4g9dJK8gegMrVHUVgIhMAs4F/BNEV+AO7+/TgY+9vw8CvlbVbd5jvwYGA+9EMF5TGw0d6po/LrrI1TCjRsFjj7lOz1jm36HctasbHjx9ulsWLHBJ4bDDipNDXJz7av7QQ8HPGaTm3bUL5s93/wz/+x98/XXJnHPYYXD00dCzp7sQ6dYNcnJmc9RRvQ90ayQmwt13V+iixBwCUf9/qco8sciFwGBV/aP39ZXASap6s98+bwM/qOozInIB8AGQClwL1FHVv3n3GwPsU9V/BnifEcAIgBYtWpwwadKkCsWbk5NDvXr1KnRsLKpp5SX5+XR88UVaf/IJO449lsVjxpDfrFmVvX91Kq+G8+fT4557kP37DwyoF1WKEhLY1bUr2487jh09e4Iq3e+9F9m/H01I4Kcnn2RXGc/iWLSoAbNnN6Fhw3z2749n2bJ6LFtWn6yslAP71K27nz17DgMEEeWCC7K44YZVJCSUrId85bVoUQMWLGhEz5476NZtFyawQ/n76tev3zxV7RVwo6pGZAEuwvU7+F5fCTxXap9WwIfAfFxfRRbQELgLGO233xjgL+W95wknnKAVNX369AofG4tqbHm9/bZq3bqqzZqpPvOM6mOPqX73XcTfNqrlVVCg+sMPqo8+qpqWphofr+q+xLulXz/VadNU9+49+NjvvgtaRoWFqgsXqv7736pDhqiKlDxtu3aq55+v+re/qX75perGje40yckuhOTk4EVfY/++ouRQyguYq0Hq1Eg2MWUB/rd2tAGy/XdQ1WzgAgARqQcMU9WdIpIFpJU6NiOCsZpYMXw4HHecG2Fz221uXVKSa1qp6W0Tvmaj0093w0anTnVLerqbOgJcO84ll8AHH7jRRomJ8OijQT+7bxRRGtBzn+vvnzXLLZmZxTeN1a1bsjXqvvvgb387+HzNm1tzUE0SyQQxB+gkIh2A9cClwGX+O4hIKrBNVYuA+3AjmgC+Ah4Tkcbe1wO92405dH/4g+u4fughV6vl5cFll7lpw889t7j3syb55BO4+GI3fhOKa+sjjoBhw9xA//79XQ0Nrre3nFp66lR3G4KvJUrEDVICNyfdsGFuDECfPu6+gTPPLO4vOPvs4KFaJ3HNEbEEoaoFInIzrrKPB8ar6iIReRh3SfMp7iphnIgoMAO4yXvsNhF5BJdkAB5Wb4e1MZVi4ED4+99djRYX526sGzbMDX6/+Wa4/npo3Lj880SDKqxe7UYazZjhfq5eXXKfoUPdXcgdOxZP3OMvQC2dl+dGsU6b5pLD998X5xlVd2EyapQ7rGnTkqfr2NGuDGqjiN4HoaqTgcml1j3g9/v7wPtBjh1P8RWFMZWr9NCXE0+ETz91d1/fdZe7u/fqq+HWW107SjRrvu++c8NNExLc3ckzZrif4Grqvn1dQnj55eJmo3vvdbOaBpGZ6VrVWrd2c95NnepOu3evy5cnnujmxps0qfiU48bZfQOxxu6kNrGrdI12wQVuWbDAJYr//Adeeql4CojERDdG89RTIxdTTo67KeCXX9wya5Z71oBPkyauLef0093SpUtxfJdcUm4i27IFnn3WjfT1NReBO8111xWfulEjt/6GG+yqIJZZgjCmtJ493XOv//53NzB/6lS3PjfX1ZTdurka1X85+mjX2V3e8wt8fR6+5xc0aeJe//ILLFzo7hTzSUlx231zF8XHu4cj/fWvgeMO8BVeFX76Cb74Aj7/3N0E7T+yPS7O3V8wblzIpzQxxBKEMcE0bw4PP+y+xefluQr6oovc1KBz5rjpJPyH7rRsCRs2uNt/4+JcIomLgz178Gzb5np79+w5eO6IuDjo3Bl69XLTVB9zDBx7rJsa5IcfSt4m3K9f0HB9uemkk9yFyOefw+TJbqZScM1GDz4Ibdu6bhbfKe3mchOMJQhjylLWbbr79sGyZbBkiVs+/LC4Ni4qcg363bpB3bps3bWLVp06ufGgc+YU304cH+9q7TFjwn9/P59+ChdeWDyICdzkqQMHuhFFQ4a4qal9unSxpiNTPksQxpQnWDtLcrKb1tr3BLvBg0t+23/zzQPHLcvIoFVamtsvM9NNbeHb78wzK/T+S5fCRx+5B+r98EPxehHXn/Dii+704XwkY/xZgjCmsoQ6KVCYkwf53/+WkOCSwkcfuVlPwTUd3XCDm4vQN4319dcHTw7GhMoShDGVKdSv5iHu9+23MGCAu9iA4laptDS46SZ3X5/vUZRXX23NRqZyWYIwpprJz3ezY7z/Przzjusf97nwQvj3v93gptKs2chUNksQxlQDubnukcsffOA6nHfscJ3Mp57qrgp8z+G5887AycGYSLAEYUwUZGa6hJCQ4G6B+PxzNzS1USM47zw368eAAaHdWmFMpFiCMKYK7d7tHpH98MPFdzI3bOgmmR02zN3mULpz2ZqOTLRYgjAmwnbsgK++asG//uVunvbvU4iLc1M/3X9/9OIzJhhLEMZUIl9zUM+ekJ3t+hSmToX9+7vQpg2MHOlmG7/zzuLbIPr3j3bUxgRmCcKYSvLZZ66ZyP9u5g4d3HOJOnSYx8iRJxyYV69HD+tXMNWfJQhjDsHy5cU3rn3/ffF6EbjxRnjuOfd7RsbuA8kBrF/B1Axx5e9ijPFRdbNvjx7tplk6+mi45x7XXPSnP0GdOu5Gtjp13ESwgZ7VY0xNYVcQxpTj22/hjTdcZ/MPP8C6da5zuW9fN8XFeee5J3uCm4zVmo5MbWEJwpgAtmyBL7908xtNm1a8/rTT3BDVc86B1NSDj7OmI1ObWIIwhuAP1qlXr+TzeoYMgWuuiXa0xlQNSxAmZqWnw4QJ7vk/8+Yd/GCds8929yz4JstLTHRNR8bECksQJmYUFMDcue6+hA8/hPnzi7elpcEjjxz8YB0Ia2ZuY2oVSxCmVvGft+jkk90zE6ZOdZX89Omwa5fbr2XLkk1HAwe6DuZArF/BxKqIJggRGQw8A8QDr6nq46W2HwG8DjTy7nOvqk4WkfbAEmCpd9fvVXVkJGM1NV9mprsrOT/fVf6NG7vOZoAjj4RLL3UPb+vXz92/4P/wN2s6MuZgEUsQIhIPvAAMALKAOSLyqaou9tttNPCeqr4kIl2ByUB777aVqtozUvGZmq+wEH7+GWbNgpkz3aij3Nzi7S1bwmOPuURw5JElj01NtaYjY8oTySuI3sAKVV0FICKTgHMB/wShQAPv7w2B7AjGY2qwzEw30V3TprBtm0sK33/vZkcFaNPGdS7PmAFFRe6q4N//Lrvit6YjY8oWyQTRGljn9zoLOKnUPmOBKSJyC1AX8H96ewcRmQ/sAkar6rcRjNVUM7m57urgxx9h8mQ39FS1eHuPHnDlldCnj3uoju9GNXt2gjGVR9T/f11lnljkImCQqv7R+/pKoLeq3uK3z53eGJ4UEQ/wH+AYIAGop6pbReQE4GOgm6ruCvA+I4ARAC1atDhh0qRJFYo3JyeHevXqVejYWFQZ5bVoUQMWLGhE1647SUxUli2rx7Jl9Vm+vD5r1qRQWOhmgklKKiQvLw4QRJQrr1zDtdeurYRPUXXs7ys8Vl7hOZTy6tev3zxV7RVwo6pGZAE8wFd+r+8D7iu1zyKgrd/rVUDzAOfKAHqV954nnHCCVtT06dMrfGwsqkh5FRSoLl2q+tFHqiNGqMbHq7rrguIlNVV10CDVv/5V9YMPVFevVp01SzU52e2fnKz63XeV/nEizv6+wmPlFZ5DKS9grgapUyPZxDQH6CQiHYD1wKXAZaX2+Q04A5ggIl2AOsBmEWkGbFPVQhE5EujkTR6mBvj2W/j4Y2jWzN17sHgxLFoES5eWfFiOj4ib2O6xx1xfQukJ7tq3tw5lY6IhYglCVQtE5GbgK9wQ1vGqukhEHsZlrE+BvwCvisgduA7ra1RVRaQv8LCIFACFwEhV3RapWE35Srft5+cLv/4KK1fCihXFPxcudJPZ+WvXzs18OnCg+9m1q7sfYejQ4mGmN94IbdsGf3/rUDam6kX0PghVnYwbuuq/7gG/3xcDfQIc9wHwQSRjM06wTt39+93UE7/95r69jxvnrgZE3JXBpk19S3Qa168PRx3lnq+clVV8A9ro0TB2bOD3tqsCY6o3u5O6FipvJE9+Pmzc6IaN3nSTSwbx8W766r173RVAdnbJUUM+qm4qisGD13LGGe3p2NElhmbNXPLIzCx5A9qgQcHjtKsCY6o3SxA1SFkVv6prtvnqK7jqquJKf/hw9+yCDRvckp1dfHexv4ICN6y0Rw/XFHTEEW5p2xa2boXrriuu9F96CfLy1pCW1v6g83g8dmVgTG1hCSLKglX6qm6W0S1bXAX97bdw//3FFf+AAW6/jRth0ya35OeXPHdREbz5JrRu7e4qbt8eTjnF/d6qlXsAzujRLjkkJsKnnwav0Nu1KxlnRkbwz2RXBsbUDpYgIsRX8Z9+uuuY3b7d3QHs/3P+fHjtNTdlhAgce6yr5LdudUthYeBzFxTAd99Bp06usu/RA5o3d8vOnfD3v7t9EhLcRHV9DurlKdanT2jf9q3SNyb2WILAVeYTJx5BUtLBlWBBgZvOYedO9y1+5kxXMR9+uPsG7r/s3Ol+ZmW5IZ3h3IPoayI6/ng3T1BqqptWwvdzwwa4+WZ3BZGY6OYdClZhDx4cehOPVfzGmGBiPkFkZrrHSBYWdmD8eOjc2TXN7NrlKvy9e8s/R0oKNGpUvBQVFScHEfeMgQsvhCZN3AyjjRu735ctcw+l8bXtT5xYdmXdtat92zfGVJ2YTxAZGb6mHKGoyF0xHH88NGhQvDRs6K4ePvrIVf7x8XD77XDvvW5bQkLJc5YeyTN6dOAKu3Xr8Dp0reI3xlSlmE8QaWmQnAx5eUUkJcXx+uuBK+Hevd2kcb5Kf9iwwA+th/BG8lilb4yprmI+Qfgq8/Hj13DddUcGrazDHb5pFb8xpqaL+QQBriLPy/sNj+fIcvezSt8YEyvioh2AMcaY6skShDHGmIAsQRhjjAnIEoQxxpiALEEYY4wJyBKEMcaYgETDmTComhORzYDvafYNgZ1+m8t7nQoEmAi7UpR+r8o8pqz9gm0LtL68dVZe4a2z8gp/nf9rK6+qK692qtos4JZgD6uu6QvwSpivgz64u7Jjqcxjytov2LZA68tbZ+Vl5RXJ8gpQflZe1aC8anMT02dhvo6kirxXqMeUtV+wbYHWl7fOyiu8dVZe4a+rqjKz8gpRrWpiOhQiMldVe0U7jprCyis8Vl7hsfIKT6TKqzZfQYTrlWgHUMNYeYXHyis8Vl7hiUh52RWEMcaYgOwKwhhjTECWIIwxxgRkCcIYY0xAliBCICJ1RWSeiJwT7VhqAhHpIiIvi8j7IvLnaMdT3YnIeSLyqoh8IiIDox1PdSciR4rIf0Tk/WjHUl1566zXvX9Xl1f0PLU6QYjIeBHZJCILS60fLCJLRWSFiNwbwqnuAd6LTJTVS2WUmaouUdWRwMVArR6qWEnl9bGq/gm4BrgkguFGXSWV1ypVvT6ykVY/YZbdBcD73r+roRV9z1qdIIAJwGD/FSISD7wADAG6AsNFpKuIHCsin5damovImcBiYGNVBx8lEzjEMvMeMxSYCUyr2vCr3AQqoby8RnuPq80mUHnlFWsmEGLZAW2Add7dCiv6hrX6kaOqOkNE2pda3RtYoaqrAERkEnCuqo4DDmpCEpF+QF1c4e8TkcmqWhTRwKOoMsrMe55PgU9F5Avg7chFHF2V9DcmwOPAl6r6Y2Qjjq7K+vuKReGUHZCFSxILOIQLgVqdIIJoTXFmBVeQJwXbWVXvBxCRa4AttTk5lCGsMhORNNwlbhIwOaKRVU9hlRdwC3Am0FBEOqrqy5EMrhoK9++rKfAocJyI3OdNJLEqWNk9CzwvImdzCFNyxGKCkADryr1bUFUnVH4oNUZYZaaqGUBGpIKpAcItr2dx/6FjVbjltRUYGblwapSAZaeqe4BrD/Xktb0PIpAsoK3f6zZAdpRiqSmszMJj5RUeK6+Ki2jZxWKCmAN0EpEOIpIIXAp8GuWYqjsrs/BYeYXHyqviIlp2tTpBiMg7QCbQWUSyROR6VS0Abga+ApYA76nqomjGWZ1YmYXHyis8Vl4VF42ys8n6jDHGBFSrryCMMcZUnCUIY4wxAVmCMMYYE5AlCGOMMQFZgjDGGBOQJQhjjDEBWYIwpgwiklNJ5xkrIqNC2G+CiFxYGe9pzKGyBGGMMSYgSxDGhEBE6onINBH5UUR+EZFzvevbi8ivIvKaiCwUkYkicqaIzBKR5SLS2+80PUQk3bv+T97jRUSeF5HF3qnRm/u95wMiMsd73le804IbU2UsQRgTmlzgfFU9HugHPOlXYXcEngG6A38ALgNOBUYBf/U7R3fgbMADPCAirYDzgc7AscCfgFP89n9eVU9U1WOAZOzZCKaKxeJ038ZUhACPiUhfoAg3D38L77bVqvoLgIgsAqapqorIL0B7v3N8oqr7cA+emo572Etf4B1VLQSyRSTdb/9+InI3kAI0ARZxCHP7GxMuSxDGhOZyoBlwgqruF5E1QB3vtjy//Yr8XhdR8v9Y6YnPNMh6RKQO8CLQS1XXichYv/czpkpYE5MxoWkIbPImh35Auwqc41wRqeN9IloabqrmGcClIhIvIi1xzVdQnAy2iEg9wEY2mSpnVxDGhGYi8JmIzMU95/fXCpxjNvAFcATwiKpmi8hHQH/gF2AZ8A2Aqu4QkVe969fgkokxVcqm+zbGGBOQNTEZY4wJyBKEMcaYgCxBGGOMCcgShDHGmIAsQRhjjAnIEoQxxpiALEEYY4wJyBKEMcaYgP4/q0KRGyo03RsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots_lab4 import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 3\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    #lambdas =[0.1, 0.01, 1]\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr, loss_te = cross_validation(y, tX, k_indices, k_fold, lambda_, degree)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    e = y - np.dot(tx,w)\n",
    "    N = len(y)\n",
    "    L=e.T.dot(e)/(2*N)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-tx.dot(w)\n",
    "    N=len(y)\n",
    "    return -(np.transpose(tx).dot(e))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=841.1056518611562, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(1/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(2/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(3/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(4/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(5/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(6/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(7/199): ||gradient||=478994.9203081395, loss=1232.2010112889718, w0=0.008269730909798007, w1=-0.001553582148011996\n",
      "Gradient Descent(8/199): ||gradient||=118568.00691101696, loss=159.89405725657957, w0=0.006937361606668593, w1=-0.0010253834130051697\n",
      "Gradient Descent(9/199): ||gradient||=42065.194171113806, loss=159.89405725657957, w0=0.006937361606668593, w1=-0.0010253834130051697\n",
      "Gradient Descent(10/199): ||gradient||=42065.194171113806, loss=16.598703741111052, w0=0.006984806580487647, w1=-0.0011219266681309632\n",
      "Gradient Descent(11/199): ||gradient||=11418.199437661786, loss=5.950786020203283, w0=0.006816388320450447, w1=-0.001164331416586919\n",
      "Gradient Descent(12/199): ||gradient||=3904.1504718876095, loss=5.950786020203283, w0=0.006816388320450447, w1=-0.001164331416586919\n",
      "Gradient Descent(13/199): ||gradient||=3904.1504718876095, loss=4.630922230979405, w0=0.006664272917044151, w1=-0.0011530231322829329\n",
      "Gradient Descent(14/199): ||gradient||=1821.3598227515126, loss=4.063655499189869, w0=0.006417859929509145, w1=-0.0011433458051179073\n",
      "Gradient Descent(15/199): ||gradient||=1407.5813616770226, loss=3.3609862366259597, w0=0.006013971851757816, w1=-0.0011361840232744203\n",
      "Gradient Descent(16/199): ||gradient||=1242.4394319351404, loss=2.5784178478809316, w0=0.005329811953847738, w1=-0.0011154739433217621\n",
      "Gradient Descent(17/199): ||gradient||=1495.6757099253277, loss=2.5784178478809316, w0=0.005329811953847738, w1=-0.0011154739433217621\n",
      "Gradient Descent(18/199): ||gradient||=1495.6757099253277, loss=2.3360718435619545, w0=0.005035309006382242, w1=-0.0011167029191523192\n",
      "Gradient Descent(19/199): ||gradient||=1679.6115700045566, loss=2.3360718435619545, w0=0.005035309006382242, w1=-0.0011167029191523192\n",
      "Gradient Descent(20/199): ||gradient||=1679.6115700045566, loss=2.0196671641951807, w0=0.004867894005331114, w1=-0.0011072228131857624\n",
      "Gradient Descent(21/199): ||gradient||=831.7994786420545, loss=1.806456389215204, w0=0.004602335792168594, w1=-0.0011029635991958479\n",
      "Gradient Descent(22/199): ||gradient||=790.3184232081487, loss=1.580089992226833, w0=0.004140839049776025, w1=-0.0010876000213891261\n",
      "Gradient Descent(23/199): ||gradient||=1211.2280502079714, loss=1.580089992226833, w0=0.004140839049776025, w1=-0.0010876000213891261\n",
      "Gradient Descent(24/199): ||gradient||=1211.2280502079714, loss=1.476313896701144, w0=0.003940589090300368, w1=-0.0010897626629984534\n",
      "Gradient Descent(25/199): ||gradient||=1299.2581487965722, loss=1.476313896701144, w0=0.003940589090300368, w1=-0.0010897626629984534\n",
      "Gradient Descent(26/199): ||gradient||=1299.2581487965722, loss=1.2981317647361224, w0=0.00382335927897803, w1=-0.001082858873988922\n",
      "Gradient Descent(27/199): ||gradient||=560.5508418255569, loss=1.199130739004287, w0=0.0036367496574774734, w1=-0.0010795757983016144\n",
      "Gradient Descent(28/199): ||gradient||=530.5282717185291, loss=1.0638625968316067, w0=0.0033131358208093203, w1=-0.0010701042558196269\n",
      "Gradient Descent(29/199): ||gradient||=651.6549717847764, loss=1.0638625968316067, w0=0.0033131358208093203, w1=-0.0010701042558196269\n",
      "Gradient Descent(30/199): ||gradient||=651.6549717847764, loss=0.9969989122758439, w0=0.003165325998968148, w1=-0.001069758359215855\n",
      "Gradient Descent(31/199): ||gradient||=633.9320454555267, loss=0.9969989122758439, w0=0.003165325998968148, w1=-0.001069758359215855\n",
      "Gradient Descent(32/199): ||gradient||=633.9320454555267, loss=0.9448852909795803, w0=0.0030835949931374454, w1=-0.0010663709369441476\n",
      "Gradient Descent(33/199): ||gradient||=407.6002690107199, loss=0.89060337264951, w0=0.002947223730772726, w1=-0.0010636459545625646\n",
      "Gradient Descent(34/199): ||gradient||=385.33268660962045, loss=0.8057813099381063, w0=0.002713558401606673, w1=-0.0010583185787098551\n",
      "Gradient Descent(35/199): ||gradient||=357.745021204037, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(36/199): ||gradient||=550.7643230600471, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(37/199): ||gradient||=550.7643230600471, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(38/199): ||gradient||=550.7643230600471, loss=0.6700932631084654, w0=0.0022852703249467417, w1=-0.0010500894478139076\n",
      "Gradient Descent(39/199): ||gradient||=279.86689246631073, loss=0.6456612960339795, w0=0.0021968120214067664, w1=-0.0010486233030475218\n",
      "Gradient Descent(40/199): ||gradient||=266.19696712398223, loss=0.6066809062647454, w0=0.0020451673247723536, w1=-0.0010460683623724886\n",
      "Gradient Descent(41/199): ||gradient||=243.48264526239504, loss=0.5509669495097792, w0=0.0017975715949004, w1=-0.0010427846763206166\n",
      "Gradient Descent(42/199): ||gradient||=222.88729642038425, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(43/199): ||gradient||=864.2461870052957, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(44/199): ||gradient||=864.2461870052957, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(45/199): ||gradient||=864.2461870052957, loss=0.5104715274703092, w0=0.0013824207927842131, w1=-0.0010404203982392693\n",
      "Gradient Descent(46/199): ||gradient||=640.7748902665478, loss=0.5104715274703092, w0=0.0013824207927842131, w1=-0.0010404203982392693\n",
      "Gradient Descent(47/199): ||gradient||=640.7748902665478, loss=0.4733280099273123, w0=0.0013562993181995152, w1=-0.0010385363047634207\n",
      "Gradient Descent(48/199): ||gradient||=143.6317127949208, loss=0.46777660023407364, w0=0.0013166486498825316, w1=-0.0010383874141367158\n",
      "Gradient Descent(49/199): ||gradient||=134.88873573030858, loss=0.4591023106420811, w0=0.0012497431357828233, w1=-0.0010388994293573973\n",
      "Gradient Descent(50/199): ||gradient||=133.9827691559406, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(51/199): ||gradient||=268.83503963227184, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(52/199): ||gradient||=268.83503963227184, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(53/199): ||gradient||=268.83503963227184, loss=0.4440206822362742, w0=0.0011220882067207187, w1=-0.0010396663749367005\n",
      "Gradient Descent(54/199): ||gradient||=106.85862932854249, loss=0.441087804650226, w0=0.001095181208044561, w1=-0.0010402559859381706\n",
      "Gradient Descent(55/199): ||gradient||=100.37086761597108, loss=0.436429244025444, w0=0.0010479658842144333, w1=-0.001040975121600948\n",
      "Gradient Descent(56/199): ||gradient||=97.25171282189831, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(57/199): ||gradient||=153.9418813700377, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(58/199): ||gradient||=153.9418813700377, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(59/199): ||gradient||=153.9418813700377, loss=0.428310518048747, w0=0.0009593473729321971, w1=-0.0010433433372556838\n",
      "Gradient Descent(60/199): ||gradient||=81.81785619294817, loss=0.42667499930067726, w0=0.0009399098162095435, w1=-0.0010439131500585407\n",
      "Gradient Descent(61/199): ||gradient||=76.8995847584856, loss=0.42404499762822095, w0=0.0009068590596733773, w1=-0.0010452800073509068\n",
      "Gradient Descent(62/199): ||gradient||=73.68191481768591, loss=0.4203877155740092, w0=0.0008507023004211215, w1=-0.0010474493889117858\n",
      "Gradient Descent(63/199): ||gradient||=95.82030970867986, loss=0.4203877155740092, w0=0.0008507023004211215, w1=-0.0010474493889117858\n",
      "Gradient Descent(64/199): ||gradient||=95.82030970867986, loss=0.4193517184686776, w0=0.0008257588349145832, w1=-0.001049554179079764\n",
      "Gradient Descent(65/199): ||gradient||=131.64220978288836, loss=0.4193517184686776, w0=0.0008257588349145832, w1=-0.001049554179079764\n",
      "Gradient Descent(66/199): ||gradient||=131.64220978288836, loss=0.41749698342216157, w0=0.000811527038902468, w1=-0.0010498577851861919\n",
      "Gradient Descent(67/199): ||gradient||=76.03159656835628, loss=0.416355457241538, w0=0.0007895482182202901, w1=-0.0010518288779811152\n",
      "Gradient Descent(68/199): ||gradient||=94.42838831122786, loss=0.416355457241538, w0=0.0007895482182202901, w1=-0.0010518288779811152\n",
      "Gradient Descent(69/199): ||gradient||=94.42838831122786, loss=0.415206620798497, w0=0.0007773751836812311, w1=-0.0010523605972406881\n",
      "Gradient Descent(70/199): ||gradient||=60.56468066094686, loss=0.4141142723888324, w0=0.0007579211978855824, w1=-0.0010542105916125043\n",
      "Gradient Descent(71/199): ||gradient||=66.56481698823032, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(72/199): ||gradient||=158.71954548090878, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(73/199): ||gradient||=158.71954548090878, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(74/199): ||gradient||=158.71954548090878, loss=0.41196340313164553, w0=0.0007190353875549346, w1=-0.0010574509610657668\n",
      "Gradient Descent(75/199): ||gradient||=61.377035443875045, loss=0.41137113203435366, w0=0.0007109148605399808, w1=-0.0010585615971054028\n",
      "Gradient Descent(76/199): ||gradient||=45.63878276113564, loss=0.4106138204606555, w0=0.0006959889005565438, w1=-0.0010601676356890902\n",
      "Gradient Descent(77/199): ||gradient||=45.257296616742465, loss=0.40961645684083975, w0=0.0006715627973583006, w1=-0.001063543455741474\n",
      "Gradient Descent(78/199): ||gradient||=67.6721273575576, loss=0.40961645684083975, w0=0.0006715627973583006, w1=-0.001063543455741474\n",
      "Gradient Descent(79/199): ||gradient||=67.6721273575576, loss=0.40920981282345964, w0=0.0006587773637357621, w1=-0.0010648933888183813\n",
      "Gradient Descent(80/199): ||gradient||=80.40583747361563, loss=0.40920981282345964, w0=0.0006587773637357621, w1=-0.0010648933888183813\n",
      "Gradient Descent(81/199): ||gradient||=80.40583747361563, loss=0.40849786180283865, w0=0.0006532671580679312, w1=-0.0010661487591727267\n",
      "Gradient Descent(82/199): ||gradient||=39.909526494015665, loss=0.40800980841890905, w0=0.0006424367528909528, w1=-0.0010677899856662385\n",
      "Gradient Descent(83/199): ||gradient||=40.4038686992513, loss=0.40747360604225796, w0=0.0006249911773239704, w1=-0.0010712752086044432\n",
      "Gradient Descent(84/199): ||gradient||=69.0501365443513, loss=0.40747360604225796, w0=0.0006249911773239704, w1=-0.0010712752086044432\n",
      "Gradient Descent(85/199): ||gradient||=69.0501365443513, loss=0.40718743210513636, w0=0.0006154428300717527, w1=-0.0010726359562995918\n",
      "Gradient Descent(86/199): ||gradient||=77.1851484859161, loss=0.40718743210513636, w0=0.0006154428300717527, w1=-0.0010726359562995918\n",
      "Gradient Descent(87/199): ||gradient||=77.1851484859161, loss=0.4065653755182817, w0=0.000611535572639877, w1=-0.0010738800916445367\n",
      "Gradient Descent(88/199): ||gradient||=33.2081917631457, loss=0.40621921875323563, w0=0.0006036311575470258, w1=-0.0010755721862546522\n",
      "Gradient Descent(89/199): ||gradient||=32.765077917974125, loss=0.4057302975625341, w0=0.0005907502636212691, w1=-0.001078953673013926\n",
      "Gradient Descent(90/199): ||gradient||=45.52864370203942, loss=0.4057302975625341, w0=0.0005907502636212691, w1=-0.001078953673013926\n",
      "Gradient Descent(91/199): ||gradient||=45.52864370203942, loss=0.4054512097810763, w0=0.0005838517095477515, w1=-0.0010804946772405885\n",
      "Gradient Descent(92/199): ||gradient||=46.143764278291535, loss=0.4054512097810763, w0=0.0005838517095477515, w1=-0.0010804946772405885\n",
      "Gradient Descent(93/199): ||gradient||=46.143764278291535, loss=0.40518626398133334, w0=0.0005808209163562352, w1=-0.0010815939636243535\n",
      "Gradient Descent(94/199): ||gradient||=27.764192696205644, loss=0.40492775098249656, w0=0.0005750145562394795, w1=-0.0010833205569521543\n",
      "Gradient Descent(95/199): ||gradient||=27.108373987715456, loss=0.40449467744114004, w0=0.0005652572369224834, w1=-0.0010865400448348355\n",
      "Gradient Descent(96/199): ||gradient||=27.40807391920005, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(97/199): ||gradient||=56.00624880043475, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(98/199): ||gradient||=56.00624880043475, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(99/199): ||gradient||=56.00624880043475, loss=0.4036654007873637, w0=0.0005469258806137134, w1=-0.0010932795715305585\n",
      "Gradient Descent(100/199): ||gradient||=23.898010339003022, loss=0.4034818064204777, w0=0.0005430761649937927, w1=-0.0010950041471024215\n",
      "Gradient Descent(101/199): ||gradient||=23.473449839811174, loss=0.4031659139240469, w0=0.0005365544640254464, w1=-0.0010981673046472082\n",
      "Gradient Descent(102/199): ||gradient||=23.104049044878185, loss=0.4026710746450444, w0=0.0005255846971985537, w1=-0.0011037979419484484\n",
      "Gradient Descent(103/199): ||gradient||=30.270162082523083, loss=0.4026710746450444, w0=0.0005255846971985537, w1=-0.0011037979419484484\n",
      "Gradient Descent(104/199): ||gradient||=30.270162082523083, loss=0.40257065314461793, w0=0.0005210856049048208, w1=-0.0011071571661627096\n",
      "Gradient Descent(105/199): ||gradient||=52.622046775103776, loss=0.40257065314461793, w0=0.0005210856049048208, w1=-0.0011071571661627096\n",
      "Gradient Descent(106/199): ||gradient||=52.622046775103776, loss=0.4023510785248375, w0=0.0005180812922385379, w1=-0.0011085847160732644\n",
      "Gradient Descent(107/199): ||gradient||=42.519481602846696, loss=0.4023510785248375, w0=0.0005180812922385379, w1=-0.0011085847160732644\n",
      "Gradient Descent(108/199): ||gradient||=42.519481602846696, loss=0.4021593747389557, w0=0.0005170428466753712, w1=-0.0011096234802934438\n",
      "Gradient Descent(109/199): ||gradient||=20.799479865540864, loss=0.4020311171034005, w0=0.0005148345875917532, w1=-0.001111298712501439\n",
      "Gradient Descent(110/199): ||gradient||=20.537946985794182, loss=0.4018069155811977, w0=0.0005109725314723883, w1=-0.0011142952404748173\n",
      "Gradient Descent(111/199): ||gradient||=20.26410867813398, loss=0.4014316096113674, w0=0.0005046801030705092, w1=-0.0011197868448744488\n",
      "Gradient Descent(112/199): ||gradient||=22.577092404779297, loss=0.4014316096113674, w0=0.0005046801030705092, w1=-0.0011197868448744488\n",
      "Gradient Descent(113/199): ||gradient||=22.577092404779297, loss=0.40126942307151714, w0=0.000501480890933531, w1=-0.0011226437836218731\n",
      "Gradient Descent(114/199): ||gradient||=29.92345925844856, loss=0.40126942307151714, w0=0.000501480890933531, w1=-0.0011226437836218731\n",
      "Gradient Descent(115/199): ||gradient||=29.92345925844856, loss=0.4011379622475513, w0=0.0005002319011027359, w1=-0.0011243755612254597\n",
      "Gradient Descent(116/199): ||gradient||=23.797046609339215, loss=0.40100427229386004, w0=0.000497383176281982, w1=-0.0011271443560776648\n",
      "Gradient Descent(117/199): ||gradient||=33.352840755589405, loss=0.40100427229386004, w0=0.000497383176281982, w1=-0.0011271443560776648\n",
      "Gradient Descent(118/199): ||gradient||=33.352840755589405, loss=0.40086417810374664, w0=0.000496370539494106, w1=-0.0011288609606941267\n",
      "Gradient Descent(119/199): ||gradient||=24.48836679696838, loss=0.4007480808194793, w0=0.0004938352183256815, w1=-0.001131553790938264\n",
      "Gradient Descent(120/199): ||gradient||=34.66320413814315, loss=0.4007480808194793, w0=0.0004938352183256815, w1=-0.001131553790938264\n",
      "Gradient Descent(121/199): ||gradient||=34.66320413814315, loss=0.400604816236632, w0=0.0004929980657027706, w1=-0.001133239641636779\n",
      "Gradient Descent(122/199): ||gradient||=24.036441270040765, loss=0.4004922620641316, w0=0.0004907588958166496, w1=-0.0011358735005002477\n",
      "Gradient Descent(123/199): ||gradient||=33.027044179287294, loss=0.4004922620641316, w0=0.0004907588958166496, w1=-0.0011358735005002477\n",
      "Gradient Descent(124/199): ||gradient||=33.027044179287294, loss=0.40035797770013426, w0=0.0004900371273011674, w1=-0.0011375109339503778\n",
      "Gradient Descent(125/199): ||gradient||=22.437272143488766, loss=0.40023992555026505, w0=0.00048808275181782884, w1=-0.0011401016117084072\n",
      "Gradient Descent(126/199): ||gradient||=28.94193850096423, loss=0.40023992555026505, w0=0.00048808275181782884, w1=-0.0011401016117084072\n",
      "Gradient Descent(127/199): ||gradient||=28.94193850096423, loss=0.4001250530936252, w0=0.00048742795359510964, w1=-0.0011416768443916418\n",
      "Gradient Descent(128/199): ||gradient||=20.32560636654322, loss=0.40000117469639157, w0=0.0004857405374685825, w1=-0.0011442334904676343\n",
      "Gradient Descent(129/199): ||gradient||=24.004347300886447, loss=0.40000117469639157, w0=0.0004857405374685825, w1=-0.0011442334904676343\n",
      "Gradient Descent(130/199): ||gradient||=24.004347300886447, loss=0.39990765971996656, w0=0.0004851236622090328, w1=-0.0011457408555677984\n",
      "Gradient Descent(131/199): ||gradient||=18.509078339791653, loss=0.3997827720012965, w0=0.00048367384950193153, w1=-0.0011482640677273114\n",
      "Gradient Descent(132/199): ||gradient||=19.988067941177746, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(133/199): ||gradient||=40.8594844996685, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(134/199): ||gradient||=40.8594844996685, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(135/199): ||gradient||=40.8594844996685, loss=0.39951644668045294, w0=0.00048120362717750246, w1=-0.0011537383048700303\n",
      "Gradient Descent(136/199): ||gradient||=19.29804770527485, loss=0.39944470630032447, w0=0.00048053585066380375, w1=-0.001155058457834156\n",
      "Gradient Descent(137/199): ||gradient||=16.90376041435936, loss=0.3993310017480798, w0=0.0004795647308737723, w1=-0.0011575375939189103\n",
      "Gradient Descent(138/199): ||gradient||=17.046308730480778, loss=0.39914679922510793, w0=0.00047769863669953617, w1=-0.0011618990727273817\n",
      "Gradient Descent(139/199): ||gradient||=21.983624528925695, loss=0.39914679922510793, w0=0.00047769863669953617, w1=-0.0011618990727273817\n",
      "Gradient Descent(140/199): ||gradient||=21.983624528925695, loss=0.3990557393783822, w0=0.00047707057642832793, w1=-0.0011644074431182482\n",
      "Gradient Descent(141/199): ||gradient||=25.471873584796384, loss=0.3990557393783822, w0=0.00047707057642832793, w1=-0.0011644074431182482\n",
      "Gradient Descent(142/199): ||gradient||=25.471873584796384, loss=0.39896755725642086, w0=0.0004764726345688402, w1=-0.0011656219950594306\n",
      "Gradient Descent(143/199): ||gradient||=16.76449265268181, loss=0.398868637543409, w0=0.0004757811681859825, w1=-0.0011679928929621307\n",
      "Gradient Descent(144/199): ||gradient||=17.10276348705993, loss=0.39872230069744297, w0=0.00047429795526507764, w1=-0.0011721125777152547\n",
      "Gradient Descent(145/199): ||gradient||=25.24421536915001, loss=0.39872230069744297, w0=0.00047429795526507764, w1=-0.0011721125777152547\n",
      "Gradient Descent(146/199): ||gradient||=25.24421536915001, loss=0.39864428326858853, w0=0.00047393065205735474, w1=-0.0011745298772846784\n",
      "Gradient Descent(147/199): ||gradient||=28.4158439291318, loss=0.39864428326858853, w0=0.00047393065205735474, w1=-0.0011745298772846784\n",
      "Gradient Descent(148/199): ||gradient||=28.4158439291318, loss=0.39854692520639107, w0=0.00047341288182837853, w1=-0.0011756678772910558\n",
      "Gradient Descent(149/199): ||gradient||=16.069014117513397, loss=0.39845896827966887, w0=0.0004728863835142045, w1=-0.001177914209968948\n",
      "Gradient Descent(150/199): ||gradient||=16.14752739919354, loss=0.39831871797552754, w0=0.0004717450874911485, w1=-0.0011818411430173962\n",
      "Gradient Descent(151/199): ||gradient||=20.998885100349224, loss=0.39831871797552754, w0=0.0004717450874911485, w1=-0.0011818411430173962\n",
      "Gradient Descent(152/199): ||gradient||=20.998885100349224, loss=0.3982391963263021, w0=0.0004714346866097339, w1=-0.0011840981024987649\n",
      "Gradient Descent(153/199): ||gradient||=21.74412746575503, loss=0.39823372304957266, w0=0.0004701798559676789, w1=-0.001187795131262214\n",
      "Gradient Descent(154/199): ||gradient||=45.61884600353725, loss=0.39823372304957266, w0=0.0004701798559676789, w1=-0.001187795131262214\n",
      "Gradient Descent(155/199): ||gradient||=45.61884600353725, loss=0.39816340099144476, w0=0.000470304215030966, w1=-0.0011901809683076568\n",
      "Gradient Descent(156/199): ||gradient||=46.44739933010738, loss=0.39816340099144476, w0=0.000470304215030966, w1=-0.0011901809683076568\n",
      "Gradient Descent(157/199): ||gradient||=46.44739933010738, loss=0.3979544133588752, w0=0.0004697656450188587, w1=-0.0011911608794574534\n",
      "Gradient Descent(158/199): ||gradient||=15.21938578085237, loss=0.39788075002709017, w0=0.0004694143206910901, w1=-0.0011932322945997875\n",
      "Gradient Descent(159/199): ||gradient||=15.092458100700574, loss=0.3977574651828803, w0=0.00046862710557291656, w1=-0.0011968722400272664\n",
      "Gradient Descent(160/199): ||gradient||=17.459674823159773, loss=0.3977574651828803, w0=0.00046862710557291656, w1=-0.0011968722400272664\n",
      "Gradient Descent(161/199): ||gradient||=17.459674823159773, loss=0.39768697550058824, w0=0.0004683950381288716, w1=-0.0011989250529045087\n",
      "Gradient Descent(162/199): ||gradient||=16.899351382688632, loss=0.3975990223545446, w0=0.00046758113456258314, w1=-0.0012024120750347663\n",
      "Gradient Descent(163/199): ||gradient||=26.019833941819268, loss=0.3975990223545446, w0=0.00046758113456258314, w1=-0.0012024120750347663\n",
      "Gradient Descent(164/199): ||gradient||=26.019833941819268, loss=0.39752304948421946, w0=0.0004675299356815308, w1=-0.0012044823759427019\n",
      "Gradient Descent(165/199): ||gradient||=23.398773921717257, loss=0.39752304948421946, w0=0.0004675299356815308, w1=-0.0012044823759427019\n",
      "Gradient Descent(166/199): ||gradient||=23.398773921717257, loss=0.3974579612179805, w0=0.00046724538003479273, w1=-0.0012054689435615793\n",
      "Gradient Descent(167/199): ||gradient||=14.054105190616793, loss=0.3973959980173014, w0=0.00046695606675141964, w1=-0.0012073549207443715\n",
      "Gradient Descent(168/199): ||gradient||=13.976326987591387, loss=0.39728593725673683, w0=0.00046644893306398925, w1=-0.0012107502800225937\n",
      "Gradient Descent(169/199): ||gradient||=13.838335237006905, loss=0.3970924901945808, w0=0.00046558726733264826, w1=-0.0012168683324334482\n",
      "Gradient Descent(170/199): ||gradient||=13.600084522199227, loss=0.39675971276928534, w0=0.0004641465511188998, w1=-0.0012278788154433118\n",
      "Gradient Descent(171/199): ||gradient||=13.626603342993453, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(172/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(173/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(174/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(175/199): ||gradient||=63.583777857176486, loss=0.39617803328070517, w0=0.00046196131315609463, w1=-0.001248752797916121\n",
      "Gradient Descent(176/199): ||gradient||=13.399282050703505, loss=0.39613216067418866, w0=0.0004617658421167397, w1=-0.001250469495285852\n",
      "Gradient Descent(177/199): ||gradient||=12.812854237603393, loss=0.39605534488491967, w0=0.0004615761827407514, w1=-0.0012536395893912686\n",
      "Gradient Descent(178/199): ||gradient||=13.888507181794518, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(179/199): ||gradient||=30.713651396605982, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(180/199): ||gradient||=30.713651396605982, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(181/199): ||gradient||=30.713651396605982, loss=0.39589014412267565, w0=0.0004609996667137755, w1=-0.0012601726719174173\n",
      "Gradient Descent(182/199): ||gradient||=12.643616971766718, loss=0.3958494855462032, w0=0.00046091445594507026, w1=-0.0012618396212008233\n",
      "Gradient Descent(183/199): ||gradient||=12.226074899651255, loss=0.3957792989537911, w0=0.00046065886446540575, w1=-0.0012647875917508652\n",
      "Gradient Descent(184/199): ||gradient||=12.54535219479914, loss=0.3956729809015767, w0=0.00046039898889128325, w1=-0.0012701911503392297\n",
      "Gradient Descent(185/199): ||gradient||=19.402694715336576, loss=0.3956729809015767, w0=0.00046039898889128325, w1=-0.0012701911503392297\n",
      "Gradient Descent(186/199): ||gradient||=19.402694715336576, loss=0.39564837778976664, w0=0.0004599269998368624, w1=-0.0012729389741762073\n",
      "Gradient Descent(187/199): ||gradient||=29.220539831279712, loss=0.39564837778976664, w0=0.0004599269998368624, w1=-0.0012729389741762073\n",
      "Gradient Descent(188/199): ||gradient||=29.220539831279712, loss=0.3955648208324623, w0=0.00046007668369990953, w1=-0.001274625475802047\n",
      "Gradient Descent(189/199): ||gradient||=17.344152656382057, loss=0.39552681996328626, w0=0.0004596675395918377, w1=-0.0012773189769549578\n",
      "Gradient Descent(190/199): ||gradient||=24.254833014732103, loss=0.39552681996328626, w0=0.0004596675395918377, w1=-0.0012773189769549578\n",
      "Gradient Descent(191/199): ||gradient||=24.254833014732103, loss=0.39546109845038896, w0=0.00045976489295202096, w1=-0.0012789328426391418\n",
      "Gradient Descent(192/199): ||gradient||=14.856010281550212, loss=0.39541233533958825, w0=0.00045943023002053933, w1=-0.0012815805525175875\n",
      "Gradient Descent(193/199): ||gradient||=18.73111992964592, loss=0.39541233533958825, w0=0.00045943023002053933, w1=-0.0012815805525175875\n",
      "Gradient Descent(194/199): ||gradient||=18.73111992964592, loss=0.3953641016538838, w0=0.0004594683830141113, w1=-0.0012831197554240823\n",
      "Gradient Descent(195/199): ||gradient||=12.8810290100354, loss=0.39531051427753455, w0=0.00045920293374423165, w1=-0.0012857215827570562\n",
      "Gradient Descent(196/199): ||gradient||=14.456989818135845, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(197/199): ||gradient||=33.33018268942083, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(198/199): ||gradient||=33.33018268942083, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(199/199): ||gradient||=33.33018268942083, loss=0.3951914510670374, w0=0.0004589646275692114, w1=-0.0012913200860782174\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e=y-tx.dot(w)\n",
    "    N=len(y)\n",
    "    return -(np.transpose(tx).dot(e))/N\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=5.606944888677391, loss=0.6114707782168463, w0=0.006394104193840289, w1=0.005450759883688027\n",
      "SGD(1/99): |gradient|=7.570340290732537, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(2/99): |gradient|=3.573906656201934, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(3/99): |gradient|=3.3552360099930136, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(4/99): |gradient|=4.283991744813342, loss=0.43571977288915875, w0=0.049524230918568043, w1=-0.009027933449343517\n",
      "SGD(5/99): |gradient|=3.214881116998168, loss=0.43571977288915875, w0=0.049524230918568043, w1=-0.009027933449343517\n",
      "SGD(6/99): |gradient|=3.9249474696806628, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(7/99): |gradient|=3.203119350249999, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(8/99): |gradient|=3.2516736215578637, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(9/99): |gradient|=3.9203972908220814, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(10/99): |gradient|=2.877050071545468, loss=0.4354068526349247, w0=0.04852514287605632, w1=-0.009795561038324215\n",
      "SGD(11/99): |gradient|=3.237847604866295, loss=0.4354068526349247, w0=0.04852514287605632, w1=-0.009795561038324215\n",
      "SGD(12/99): |gradient|=3.2135569327736273, loss=0.4354065352520262, w0=0.04856341179440249, w1=-0.009807263876546889\n",
      "SGD(13/99): |gradient|=3.6318714658786204, loss=0.43539765852075396, w0=0.04851269682408544, w1=-0.009846730088762143\n",
      "SGD(14/99): |gradient|=8.922838090507645, loss=0.4353512563362487, w0=0.04864559566537896, w1=-0.009733761714443675\n",
      "SGD(15/99): |gradient|=3.257847145379344, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(16/99): |gradient|=4.02164685942485, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(17/99): |gradient|=3.2572148658644138, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(18/99): |gradient|=4.452631520185705, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(19/99): |gradient|=3.6487893078723563, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(20/99): |gradient|=3.2821648805273687, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(21/99): |gradient|=7.797940164477905, loss=0.4353263030355192, w0=0.048840038730257815, w1=-0.009811107123552357\n",
      "SGD(22/99): |gradient|=3.992349023741367, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(23/99): |gradient|=3.2527088858245445, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(24/99): |gradient|=3.8963899595362563, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(25/99): |gradient|=3.90756824641425, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(26/99): |gradient|=2.984587021256584, loss=0.4353246339526049, w0=0.04883281169827094, w1=-0.00981652705700677\n",
      "SGD(27/99): |gradient|=3.375345091019779, loss=0.4353245174533584, w0=0.04883217576962479, w1=-0.009816980862667569\n",
      "SGD(28/99): |gradient|=8.882359251533417, loss=0.43532387343956525, w0=0.04883393639448352, w1=-0.009815558363804608\n",
      "SGD(29/99): |gradient|=7.8314175762930915, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(30/99): |gradient|=3.216640749684483, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(31/99): |gradient|=2.6789071208412274, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(32/99): |gradient|=8.896513586862488, loss=0.4353208107866267, w0=0.04883722044527872, w1=-0.009812787566203993\n",
      "SGD(33/99): |gradient|=7.917345311733529, loss=0.4353199667044906, w0=0.04883801861744155, w1=-0.009812159224872807\n",
      "SGD(34/99): |gradient|=4.054883017006384, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(35/99): |gradient|=3.8583111607911094, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(36/99): |gradient|=4.221725188458153, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(37/99): |gradient|=8.825453051084411, loss=0.43531965968590314, w0=0.048837275711854845, w1=-0.009812751475599317\n",
      "SGD(38/99): |gradient|=7.955222080995427, loss=0.43531921172232746, w0=0.04883782974631502, w1=-0.009812268704276081\n",
      "SGD(39/99): |gradient|=5.317256180130434, loss=0.4353190913739208, w0=0.04883697165385909, w1=-0.009812832952753986\n",
      "SGD(40/99): |gradient|=3.2040442264536786, loss=0.4353190913739208, w0=0.04883697165385909, w1=-0.009812832952753986\n",
      "SGD(41/99): |gradient|=3.0295458149027654, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(42/99): |gradient|=3.8980707909256775, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(43/99): |gradient|=3.8848078489302518, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(44/99): |gradient|=3.228463734076844, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(45/99): |gradient|=3.8675611019024707, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(46/99): |gradient|=3.89859151939232, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(47/99): |gradient|=3.263934119713472, loss=0.43531904516928305, w0=0.04883663042453911, w1=-0.009813078489623567\n",
      "SGD(48/99): |gradient|=3.204213295092851, loss=0.43531904516928305, w0=0.04883663042453911, w1=-0.009813078489623567\n",
      "SGD(49/99): |gradient|=3.4394804493736415, loss=0.4353190450410245, w0=0.0488366297167548, w1=-0.009813079081019476\n",
      "SGD(50/99): |gradient|=2.9024717295998896, loss=0.4353190450410245, w0=0.0488366297167548, w1=-0.009813079081019476\n",
      "SGD(51/99): |gradient|=3.093680470224432, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(52/99): |gradient|=3.9533018211314523, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(53/99): |gradient|=2.87744668988226, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(54/99): |gradient|=3.3318373430046258, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(55/99): |gradient|=3.3158506822067055, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(56/99): |gradient|=3.8960808365785815, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(57/99): |gradient|=7.872778330565903, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(58/99): |gradient|=2.6798366821690047, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(59/99): |gradient|=3.234114564601285, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(60/99): |gradient|=3.919351282881189, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(61/99): |gradient|=3.3549198934655897, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(62/99): |gradient|=3.2154106441270565, loss=0.43531904498240476, w0=0.04883662907767243, w1=-0.009813079374175495\n",
      "SGD(63/99): |gradient|=8.970449265864339, loss=0.43531904498226326, w0=0.04883662907805685, w1=-0.009813079373855126\n",
      "SGD(64/99): |gradient|=2.826122756568962, loss=0.43531904498226326, w0=0.04883662907805685, w1=-0.009813079373855126\n",
      "SGD(65/99): |gradient|=3.2546919408212385, loss=0.4353190449822374, w0=0.04883662907791595, w1=-0.009813079373965809\n",
      "SGD(66/99): |gradient|=3.2273199101846566, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(67/99): |gradient|=2.6770668351032825, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(68/99): |gradient|=3.8524543787623844, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(69/99): |gradient|=3.8998391272758015, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(70/99): |gradient|=2.9297591762273685, loss=0.4353190449822166, w0=0.04883662907828266, w1=-0.009813079374098813\n",
      "SGD(71/99): |gradient|=7.81078644316268, loss=0.4353190449821845, w0=0.04883662907831475, w1=-0.00981307937407468\n",
      "SGD(72/99): |gradient|=3.2576130548889224, loss=0.4353190449821845, w0=0.04883662907831475, w1=-0.00981307937407468\n",
      "SGD(73/99): |gradient|=3.2226760275492636, loss=0.4353190449821835, w0=0.04883662907833267, w1=-0.009813079374079893\n",
      "SGD(74/99): |gradient|=2.9304946257411397, loss=0.4353190449821805, w0=0.04883662907830929, w1=-0.009813079374094853\n",
      "SGD(75/99): |gradient|=3.0316166379110445, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(76/99): |gradient|=3.87860594604992, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(77/99): |gradient|=2.6722369316023546, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(78/99): |gradient|=8.79213351440848, loss=0.43531904498217067, w0=0.048836629078279854, w1=-0.00981307937411849\n",
      "SGD(79/99): |gradient|=2.982150379967762, loss=0.43531904498217067, w0=0.048836629078279854, w1=-0.00981307937411849\n",
      "SGD(80/99): |gradient|=7.860694834409841, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(81/99): |gradient|=3.9345760088688193, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(82/99): |gradient|=3.2447582177109853, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(83/99): |gradient|=3.35655313002987, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(84/99): |gradient|=3.887554000656264, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(85/99): |gradient|=3.2517879873166664, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(86/99): |gradient|=8.814821572997982, loss=0.43531904498216567, w0=0.048836629078285, w1=-0.009813079374115328\n",
      "SGD(87/99): |gradient|=3.6438796881947124, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(88/99): |gradient|=3.8980277591024244, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(89/99): |gradient|=3.8388791919466243, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(90/99): |gradient|=3.288654918945359, loss=0.43531904498216567, w0=0.04883662907828516, w1=-0.009813079374115236\n",
      "SGD(91/99): |gradient|=2.6772538871447735, loss=0.43531904498216567, w0=0.04883662907828516, w1=-0.009813079374115236\n",
      "SGD(92/99): |gradient|=8.878001694603418, loss=0.43531904498216567, w0=0.04883662907828518, w1=-0.00981307937411522\n",
      "SGD(93/99): |gradient|=8.858379756186846, loss=0.43531904498216567, w0=0.048836629078285225, w1=-0.009813079374115196\n",
      "SGD(94/99): |gradient|=3.2182852932164865, loss=0.43531904498216567, w0=0.048836629078285294, w1=-0.009813079374115219\n",
      "SGD(95/99): |gradient|=3.906587326476464, loss=0.43531904498216567, w0=0.04883662907828524, w1=-0.00981307937411526\n",
      "SGD(96/99): |gradient|=2.8424234277980114, loss=0.43531904498216567, w0=0.0488366290782854, w1=-0.009813079374115165\n",
      "SGD(97/99): |gradient|=2.685708557567156, loss=0.43531904498216567, w0=0.048836629078285135, w1=-0.009813079374115375\n",
      "SGD(98/99): |gradient|=9.793714709632297, loss=0.435319044982165, w0=0.04883662907828585, w1=-0.009813079374114915\n",
      "SGD(99/99): |gradient|=3.318574468833573, loss=0.4353190449821648, w0=0.04883662907828498, w1=-0.009813079374115529\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35214980423406766"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls = least_squares(y, tX)\n",
    "loss = compute_loss(y, tX, w_ls)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w_star = least_squares(y,xpoly)\n",
    "w0 = least_squares(y,xpoly[:,:,0])\n",
    "w1 = least_squares(y,xpoly[:,:,1])\n",
    "w2 = least_squares(y,xpoly[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w0, w1, w2]\n",
    "yp = np.zeros((250000,3))\n",
    "yp[:,0] = xpoly[:,:,0].dot(w0)\n",
    "yp[:,1] = xpoly[:,:,1].dot(w1)\n",
    "yp[:,2] = xpoly[:,:,2].dot(w2)\n",
    "compute_loss(yp, xpoly, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoly = build_poly(tX,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37063036405320426 0.37169429181471236\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "w = ridge_regression(y_tr, x_tr, lambda_)\n",
    "loss_tr = compute_loss(y_tr, x_tr, w)\n",
    "loss_te = compute_loss(y_te, x_te, w)\n",
    "print(loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.24104142e-04, -8.68280397e-03, -2.68859353e-03, -2.22470926e-03,\n",
       "       -5.48363092e-03,  5.00183793e-04, -1.72285231e-02,  6.05199540e-02,\n",
       "        1.57457370e-05,  3.49522005e-03, -5.39186900e-02,  6.84999623e-02,\n",
       "        2.19603319e-02,  5.70895097e-03, -4.23536533e-04, -1.28871636e-03,\n",
       "        3.30152909e-03, -5.20781620e-04,  9.12327144e-04,  4.86026909e-03,\n",
       "        4.62605701e-04, -7.55206472e-04, -5.59287480e-02,  1.20180623e-03,\n",
       "       -6.34079801e-04, -4.00651108e-04,  5.73198500e-05,  1.25042999e-03,\n",
       "       -8.49579237e-04, -5.51323579e-03])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = w\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "tX_test_s, mean_tes, std_test = standardize(tX_test)\n",
    "y_pred = predict_labels(weights, tX_test_s)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4940938727534078"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = compute_loss(y_pred, tX_test_s, w)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.96637287, -1.59557484],\n",
       "       [-0.96443888,  2.30659937],\n",
       "       [-2.7925803 , -1.26619016],\n",
       "       ...,\n",
       "       [-2.78555006, -1.2651345 ],\n",
       "       [-0.97573941,  2.28919741],\n",
       "       [-2.80136474, -1.27570913]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
