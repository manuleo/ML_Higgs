{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize data\n",
    "tX, mean_x, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial basis\n",
    "def build_poly(x, degree):\n",
    "    \"\"\"polynomial basis functions for input data x, for j=0 up to j=degree.\"\"\"\n",
    "    degrees = np.arange(0, degree+1)\n",
    "    psi = np.power(x[:, :, np.newaxis], degrees)\n",
    "    return psi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30, 3)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xpoly = build_poly(tX,2)\n",
    "xpoly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    e = y - np.dot(tx,w)\n",
    "    N = len(y)\n",
    "    L=e.T.dot(e)/(2*N)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Compute the gradient.\"\"\"\n",
    "    e=y-tx.dot(w)\n",
    "    N=len(y)\n",
    "    return -(np.transpose(tx).dot(e))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=841.1056518611562, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(1/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(2/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(3/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(4/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(5/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(6/199): ||gradient||=478994.9203081395, loss=20011.433828797766, w0=0.010766649510400012, w1=-0.0027284198603999992\n",
      "Gradient Descent(7/199): ||gradient||=478994.9203081395, loss=1232.2010112889718, w0=0.008269730909798007, w1=-0.001553582148011996\n",
      "Gradient Descent(8/199): ||gradient||=118568.00691101696, loss=159.89405725657957, w0=0.006937361606668593, w1=-0.0010253834130051697\n",
      "Gradient Descent(9/199): ||gradient||=42065.194171113806, loss=159.89405725657957, w0=0.006937361606668593, w1=-0.0010253834130051697\n",
      "Gradient Descent(10/199): ||gradient||=42065.194171113806, loss=16.598703741111052, w0=0.006984806580487647, w1=-0.0011219266681309632\n",
      "Gradient Descent(11/199): ||gradient||=11418.199437661786, loss=5.950786020203283, w0=0.006816388320450447, w1=-0.001164331416586919\n",
      "Gradient Descent(12/199): ||gradient||=3904.1504718876095, loss=5.950786020203283, w0=0.006816388320450447, w1=-0.001164331416586919\n",
      "Gradient Descent(13/199): ||gradient||=3904.1504718876095, loss=4.630922230979405, w0=0.006664272917044151, w1=-0.0011530231322829329\n",
      "Gradient Descent(14/199): ||gradient||=1821.3598227515126, loss=4.063655499189869, w0=0.006417859929509145, w1=-0.0011433458051179073\n",
      "Gradient Descent(15/199): ||gradient||=1407.5813616770226, loss=3.3609862366259597, w0=0.006013971851757816, w1=-0.0011361840232744203\n",
      "Gradient Descent(16/199): ||gradient||=1242.4394319351404, loss=2.5784178478809316, w0=0.005329811953847738, w1=-0.0011154739433217621\n",
      "Gradient Descent(17/199): ||gradient||=1495.6757099253277, loss=2.5784178478809316, w0=0.005329811953847738, w1=-0.0011154739433217621\n",
      "Gradient Descent(18/199): ||gradient||=1495.6757099253277, loss=2.3360718435619545, w0=0.005035309006382242, w1=-0.0011167029191523192\n",
      "Gradient Descent(19/199): ||gradient||=1679.6115700045566, loss=2.3360718435619545, w0=0.005035309006382242, w1=-0.0011167029191523192\n",
      "Gradient Descent(20/199): ||gradient||=1679.6115700045566, loss=2.0196671641951807, w0=0.004867894005331114, w1=-0.0011072228131857624\n",
      "Gradient Descent(21/199): ||gradient||=831.7994786420545, loss=1.806456389215204, w0=0.004602335792168594, w1=-0.0011029635991958479\n",
      "Gradient Descent(22/199): ||gradient||=790.3184232081487, loss=1.580089992226833, w0=0.004140839049776025, w1=-0.0010876000213891261\n",
      "Gradient Descent(23/199): ||gradient||=1211.2280502079714, loss=1.580089992226833, w0=0.004140839049776025, w1=-0.0010876000213891261\n",
      "Gradient Descent(24/199): ||gradient||=1211.2280502079714, loss=1.476313896701144, w0=0.003940589090300368, w1=-0.0010897626629984534\n",
      "Gradient Descent(25/199): ||gradient||=1299.2581487965722, loss=1.476313896701144, w0=0.003940589090300368, w1=-0.0010897626629984534\n",
      "Gradient Descent(26/199): ||gradient||=1299.2581487965722, loss=1.2981317647361224, w0=0.00382335927897803, w1=-0.001082858873988922\n",
      "Gradient Descent(27/199): ||gradient||=560.5508418255569, loss=1.199130739004287, w0=0.0036367496574774734, w1=-0.0010795757983016144\n",
      "Gradient Descent(28/199): ||gradient||=530.5282717185291, loss=1.0638625968316067, w0=0.0033131358208093203, w1=-0.0010701042558196269\n",
      "Gradient Descent(29/199): ||gradient||=651.6549717847764, loss=1.0638625968316067, w0=0.0033131358208093203, w1=-0.0010701042558196269\n",
      "Gradient Descent(30/199): ||gradient||=651.6549717847764, loss=0.9969989122758439, w0=0.003165325998968148, w1=-0.001069758359215855\n",
      "Gradient Descent(31/199): ||gradient||=633.9320454555267, loss=0.9969989122758439, w0=0.003165325998968148, w1=-0.001069758359215855\n",
      "Gradient Descent(32/199): ||gradient||=633.9320454555267, loss=0.9448852909795803, w0=0.0030835949931374454, w1=-0.0010663709369441476\n",
      "Gradient Descent(33/199): ||gradient||=407.6002690107199, loss=0.89060337264951, w0=0.002947223730772726, w1=-0.0010636459545625646\n",
      "Gradient Descent(34/199): ||gradient||=385.33268660962045, loss=0.8057813099381063, w0=0.002713558401606673, w1=-0.0010583185787098551\n",
      "Gradient Descent(35/199): ||gradient||=357.745021204037, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(36/199): ||gradient||=550.7643230600471, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(37/199): ||gradient||=550.7643230600471, loss=0.7038250132281689, w0=0.002338970470520786, w1=-0.0010525375790369697\n",
      "Gradient Descent(38/199): ||gradient||=550.7643230600471, loss=0.6700932631084654, w0=0.0022852703249467417, w1=-0.0010500894478139076\n",
      "Gradient Descent(39/199): ||gradient||=279.86689246631073, loss=0.6456612960339795, w0=0.0021968120214067664, w1=-0.0010486233030475218\n",
      "Gradient Descent(40/199): ||gradient||=266.19696712398223, loss=0.6066809062647454, w0=0.0020451673247723536, w1=-0.0010460683623724886\n",
      "Gradient Descent(41/199): ||gradient||=243.48264526239504, loss=0.5509669495097792, w0=0.0017975715949004, w1=-0.0010427846763206166\n",
      "Gradient Descent(42/199): ||gradient||=222.88729642038425, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(43/199): ||gradient||=864.2461870052957, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(44/199): ||gradient||=864.2461870052957, loss=0.5461013880734272, w0=0.0014162528822008447, w1=-0.0010356652364586023\n",
      "Gradient Descent(45/199): ||gradient||=864.2461870052957, loss=0.5104715274703092, w0=0.0013824207927842131, w1=-0.0010404203982392693\n",
      "Gradient Descent(46/199): ||gradient||=640.7748902665478, loss=0.5104715274703092, w0=0.0013824207927842131, w1=-0.0010404203982392693\n",
      "Gradient Descent(47/199): ||gradient||=640.7748902665478, loss=0.4733280099273123, w0=0.0013562993181995152, w1=-0.0010385363047634207\n",
      "Gradient Descent(48/199): ||gradient||=143.6317127949208, loss=0.46777660023407364, w0=0.0013166486498825316, w1=-0.0010383874141367158\n",
      "Gradient Descent(49/199): ||gradient||=134.88873573030858, loss=0.4591023106420811, w0=0.0012497431357828233, w1=-0.0010388994293573973\n",
      "Gradient Descent(50/199): ||gradient||=133.9827691559406, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(51/199): ||gradient||=268.83503963227184, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(52/199): ||gradient||=268.83503963227184, loss=0.4509758522363563, w0=0.0011361477440612645, w1=-0.0010387239653848785\n",
      "Gradient Descent(53/199): ||gradient||=268.83503963227184, loss=0.4440206822362742, w0=0.0011220882067207187, w1=-0.0010396663749367005\n",
      "Gradient Descent(54/199): ||gradient||=106.85862932854249, loss=0.441087804650226, w0=0.001095181208044561, w1=-0.0010402559859381706\n",
      "Gradient Descent(55/199): ||gradient||=100.37086761597108, loss=0.436429244025444, w0=0.0010479658842144333, w1=-0.001040975121600948\n",
      "Gradient Descent(56/199): ||gradient||=97.25171282189831, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(57/199): ||gradient||=153.9418813700377, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(58/199): ||gradient||=153.9418813700377, loss=0.4307066274888843, w0=0.0009709782085313263, w1=-0.0010433343785092224\n",
      "Gradient Descent(59/199): ||gradient||=153.9418813700377, loss=0.428310518048747, w0=0.0009593473729321971, w1=-0.0010433433372556838\n",
      "Gradient Descent(60/199): ||gradient||=81.81785619294817, loss=0.42667499930067726, w0=0.0009399098162095435, w1=-0.0010439131500585407\n",
      "Gradient Descent(61/199): ||gradient||=76.8995847584856, loss=0.42404499762822095, w0=0.0009068590596733773, w1=-0.0010452800073509068\n",
      "Gradient Descent(62/199): ||gradient||=73.68191481768591, loss=0.4203877155740092, w0=0.0008507023004211215, w1=-0.0010474493889117858\n",
      "Gradient Descent(63/199): ||gradient||=95.82030970867986, loss=0.4203877155740092, w0=0.0008507023004211215, w1=-0.0010474493889117858\n",
      "Gradient Descent(64/199): ||gradient||=95.82030970867986, loss=0.4193517184686776, w0=0.0008257588349145832, w1=-0.001049554179079764\n",
      "Gradient Descent(65/199): ||gradient||=131.64220978288836, loss=0.4193517184686776, w0=0.0008257588349145832, w1=-0.001049554179079764\n",
      "Gradient Descent(66/199): ||gradient||=131.64220978288836, loss=0.41749698342216157, w0=0.000811527038902468, w1=-0.0010498577851861919\n",
      "Gradient Descent(67/199): ||gradient||=76.03159656835628, loss=0.416355457241538, w0=0.0007895482182202901, w1=-0.0010518288779811152\n",
      "Gradient Descent(68/199): ||gradient||=94.42838831122786, loss=0.416355457241538, w0=0.0007895482182202901, w1=-0.0010518288779811152\n",
      "Gradient Descent(69/199): ||gradient||=94.42838831122786, loss=0.415206620798497, w0=0.0007773751836812311, w1=-0.0010523605972406881\n",
      "Gradient Descent(70/199): ||gradient||=60.56468066094686, loss=0.4141142723888324, w0=0.0007579211978855824, w1=-0.0010542105916125043\n",
      "Gradient Descent(71/199): ||gradient||=66.56481698823032, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(72/199): ||gradient||=158.71954548090878, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(73/199): ||gradient||=158.71954548090878, loss=0.41408746210179187, w0=0.0007231211350569051, w1=-0.0010565782308964663\n",
      "Gradient Descent(74/199): ||gradient||=158.71954548090878, loss=0.41196340313164553, w0=0.0007190353875549346, w1=-0.0010574509610657668\n",
      "Gradient Descent(75/199): ||gradient||=61.377035443875045, loss=0.41137113203435366, w0=0.0007109148605399808, w1=-0.0010585615971054028\n",
      "Gradient Descent(76/199): ||gradient||=45.63878276113564, loss=0.4106138204606555, w0=0.0006959889005565438, w1=-0.0010601676356890902\n",
      "Gradient Descent(77/199): ||gradient||=45.257296616742465, loss=0.40961645684083975, w0=0.0006715627973583006, w1=-0.001063543455741474\n",
      "Gradient Descent(78/199): ||gradient||=67.6721273575576, loss=0.40961645684083975, w0=0.0006715627973583006, w1=-0.001063543455741474\n",
      "Gradient Descent(79/199): ||gradient||=67.6721273575576, loss=0.40920981282345964, w0=0.0006587773637357621, w1=-0.0010648933888183813\n",
      "Gradient Descent(80/199): ||gradient||=80.40583747361563, loss=0.40920981282345964, w0=0.0006587773637357621, w1=-0.0010648933888183813\n",
      "Gradient Descent(81/199): ||gradient||=80.40583747361563, loss=0.40849786180283865, w0=0.0006532671580679312, w1=-0.0010661487591727267\n",
      "Gradient Descent(82/199): ||gradient||=39.909526494015665, loss=0.40800980841890905, w0=0.0006424367528909528, w1=-0.0010677899856662385\n",
      "Gradient Descent(83/199): ||gradient||=40.4038686992513, loss=0.40747360604225796, w0=0.0006249911773239704, w1=-0.0010712752086044432\n",
      "Gradient Descent(84/199): ||gradient||=69.0501365443513, loss=0.40747360604225796, w0=0.0006249911773239704, w1=-0.0010712752086044432\n",
      "Gradient Descent(85/199): ||gradient||=69.0501365443513, loss=0.40718743210513636, w0=0.0006154428300717527, w1=-0.0010726359562995918\n",
      "Gradient Descent(86/199): ||gradient||=77.1851484859161, loss=0.40718743210513636, w0=0.0006154428300717527, w1=-0.0010726359562995918\n",
      "Gradient Descent(87/199): ||gradient||=77.1851484859161, loss=0.4065653755182817, w0=0.000611535572639877, w1=-0.0010738800916445367\n",
      "Gradient Descent(88/199): ||gradient||=33.2081917631457, loss=0.40621921875323563, w0=0.0006036311575470258, w1=-0.0010755721862546522\n",
      "Gradient Descent(89/199): ||gradient||=32.765077917974125, loss=0.4057302975625341, w0=0.0005907502636212691, w1=-0.001078953673013926\n",
      "Gradient Descent(90/199): ||gradient||=45.52864370203942, loss=0.4057302975625341, w0=0.0005907502636212691, w1=-0.001078953673013926\n",
      "Gradient Descent(91/199): ||gradient||=45.52864370203942, loss=0.4054512097810763, w0=0.0005838517095477515, w1=-0.0010804946772405885\n",
      "Gradient Descent(92/199): ||gradient||=46.143764278291535, loss=0.4054512097810763, w0=0.0005838517095477515, w1=-0.0010804946772405885\n",
      "Gradient Descent(93/199): ||gradient||=46.143764278291535, loss=0.40518626398133334, w0=0.0005808209163562352, w1=-0.0010815939636243535\n",
      "Gradient Descent(94/199): ||gradient||=27.764192696205644, loss=0.40492775098249656, w0=0.0005750145562394795, w1=-0.0010833205569521543\n",
      "Gradient Descent(95/199): ||gradient||=27.108373987715456, loss=0.40449467744114004, w0=0.0005652572369224834, w1=-0.0010865400448348355\n",
      "Gradient Descent(96/199): ||gradient||=27.40807391920005, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(97/199): ||gradient||=56.00624880043475, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(98/199): ||gradient||=56.00624880043475, loss=0.40399246518405085, w0=0.0005487686174389897, w1=-0.0010921467617485317\n",
      "Gradient Descent(99/199): ||gradient||=56.00624880043475, loss=0.4036654007873637, w0=0.0005469258806137134, w1=-0.0010932795715305585\n",
      "Gradient Descent(100/199): ||gradient||=23.898010339003022, loss=0.4034818064204777, w0=0.0005430761649937927, w1=-0.0010950041471024215\n",
      "Gradient Descent(101/199): ||gradient||=23.473449839811174, loss=0.4031659139240469, w0=0.0005365544640254464, w1=-0.0010981673046472082\n",
      "Gradient Descent(102/199): ||gradient||=23.104049044878185, loss=0.4026710746450444, w0=0.0005255846971985537, w1=-0.0011037979419484484\n",
      "Gradient Descent(103/199): ||gradient||=30.270162082523083, loss=0.4026710746450444, w0=0.0005255846971985537, w1=-0.0011037979419484484\n",
      "Gradient Descent(104/199): ||gradient||=30.270162082523083, loss=0.40257065314461793, w0=0.0005210856049048208, w1=-0.0011071571661627096\n",
      "Gradient Descent(105/199): ||gradient||=52.622046775103776, loss=0.40257065314461793, w0=0.0005210856049048208, w1=-0.0011071571661627096\n",
      "Gradient Descent(106/199): ||gradient||=52.622046775103776, loss=0.4023510785248375, w0=0.0005180812922385379, w1=-0.0011085847160732644\n",
      "Gradient Descent(107/199): ||gradient||=42.519481602846696, loss=0.4023510785248375, w0=0.0005180812922385379, w1=-0.0011085847160732644\n",
      "Gradient Descent(108/199): ||gradient||=42.519481602846696, loss=0.4021593747389557, w0=0.0005170428466753712, w1=-0.0011096234802934438\n",
      "Gradient Descent(109/199): ||gradient||=20.799479865540864, loss=0.4020311171034005, w0=0.0005148345875917532, w1=-0.001111298712501439\n",
      "Gradient Descent(110/199): ||gradient||=20.537946985794182, loss=0.4018069155811977, w0=0.0005109725314723883, w1=-0.0011142952404748173\n",
      "Gradient Descent(111/199): ||gradient||=20.26410867813398, loss=0.4014316096113674, w0=0.0005046801030705092, w1=-0.0011197868448744488\n",
      "Gradient Descent(112/199): ||gradient||=22.577092404779297, loss=0.4014316096113674, w0=0.0005046801030705092, w1=-0.0011197868448744488\n",
      "Gradient Descent(113/199): ||gradient||=22.577092404779297, loss=0.40126942307151714, w0=0.000501480890933531, w1=-0.0011226437836218731\n",
      "Gradient Descent(114/199): ||gradient||=29.92345925844856, loss=0.40126942307151714, w0=0.000501480890933531, w1=-0.0011226437836218731\n",
      "Gradient Descent(115/199): ||gradient||=29.92345925844856, loss=0.4011379622475513, w0=0.0005002319011027359, w1=-0.0011243755612254597\n",
      "Gradient Descent(116/199): ||gradient||=23.797046609339215, loss=0.40100427229386004, w0=0.000497383176281982, w1=-0.0011271443560776648\n",
      "Gradient Descent(117/199): ||gradient||=33.352840755589405, loss=0.40100427229386004, w0=0.000497383176281982, w1=-0.0011271443560776648\n",
      "Gradient Descent(118/199): ||gradient||=33.352840755589405, loss=0.40086417810374664, w0=0.000496370539494106, w1=-0.0011288609606941267\n",
      "Gradient Descent(119/199): ||gradient||=24.48836679696838, loss=0.4007480808194793, w0=0.0004938352183256815, w1=-0.001131553790938264\n",
      "Gradient Descent(120/199): ||gradient||=34.66320413814315, loss=0.4007480808194793, w0=0.0004938352183256815, w1=-0.001131553790938264\n",
      "Gradient Descent(121/199): ||gradient||=34.66320413814315, loss=0.400604816236632, w0=0.0004929980657027706, w1=-0.001133239641636779\n",
      "Gradient Descent(122/199): ||gradient||=24.036441270040765, loss=0.4004922620641316, w0=0.0004907588958166496, w1=-0.0011358735005002477\n",
      "Gradient Descent(123/199): ||gradient||=33.027044179287294, loss=0.4004922620641316, w0=0.0004907588958166496, w1=-0.0011358735005002477\n",
      "Gradient Descent(124/199): ||gradient||=33.027044179287294, loss=0.40035797770013426, w0=0.0004900371273011674, w1=-0.0011375109339503778\n",
      "Gradient Descent(125/199): ||gradient||=22.437272143488766, loss=0.40023992555026505, w0=0.00048808275181782884, w1=-0.0011401016117084072\n",
      "Gradient Descent(126/199): ||gradient||=28.94193850096423, loss=0.40023992555026505, w0=0.00048808275181782884, w1=-0.0011401016117084072\n",
      "Gradient Descent(127/199): ||gradient||=28.94193850096423, loss=0.4001250530936252, w0=0.00048742795359510964, w1=-0.0011416768443916418\n",
      "Gradient Descent(128/199): ||gradient||=20.32560636654322, loss=0.40000117469639157, w0=0.0004857405374685825, w1=-0.0011442334904676343\n",
      "Gradient Descent(129/199): ||gradient||=24.004347300886447, loss=0.40000117469639157, w0=0.0004857405374685825, w1=-0.0011442334904676343\n",
      "Gradient Descent(130/199): ||gradient||=24.004347300886447, loss=0.39990765971996656, w0=0.0004851236622090328, w1=-0.0011457408555677984\n",
      "Gradient Descent(131/199): ||gradient||=18.509078339791653, loss=0.3997827720012965, w0=0.00048367384950193153, w1=-0.0011482640677273114\n",
      "Gradient Descent(132/199): ||gradient||=19.988067941177746, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(133/199): ||gradient||=40.8594844996685, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(134/199): ||gradient||=40.8594844996685, loss=0.3996654402926818, w0=0.0004817117332992698, w1=-0.0011530715473632364\n",
      "Gradient Descent(135/199): ||gradient||=40.8594844996685, loss=0.39951644668045294, w0=0.00048120362717750246, w1=-0.0011537383048700303\n",
      "Gradient Descent(136/199): ||gradient||=19.29804770527485, loss=0.39944470630032447, w0=0.00048053585066380375, w1=-0.001155058457834156\n",
      "Gradient Descent(137/199): ||gradient||=16.90376041435936, loss=0.3993310017480798, w0=0.0004795647308737723, w1=-0.0011575375939189103\n",
      "Gradient Descent(138/199): ||gradient||=17.046308730480778, loss=0.39914679922510793, w0=0.00047769863669953617, w1=-0.0011618990727273817\n",
      "Gradient Descent(139/199): ||gradient||=21.983624528925695, loss=0.39914679922510793, w0=0.00047769863669953617, w1=-0.0011618990727273817\n",
      "Gradient Descent(140/199): ||gradient||=21.983624528925695, loss=0.3990557393783822, w0=0.00047707057642832793, w1=-0.0011644074431182482\n",
      "Gradient Descent(141/199): ||gradient||=25.471873584796384, loss=0.3990557393783822, w0=0.00047707057642832793, w1=-0.0011644074431182482\n",
      "Gradient Descent(142/199): ||gradient||=25.471873584796384, loss=0.39896755725642086, w0=0.0004764726345688402, w1=-0.0011656219950594306\n",
      "Gradient Descent(143/199): ||gradient||=16.76449265268181, loss=0.398868637543409, w0=0.0004757811681859825, w1=-0.0011679928929621307\n",
      "Gradient Descent(144/199): ||gradient||=17.10276348705993, loss=0.39872230069744297, w0=0.00047429795526507764, w1=-0.0011721125777152547\n",
      "Gradient Descent(145/199): ||gradient||=25.24421536915001, loss=0.39872230069744297, w0=0.00047429795526507764, w1=-0.0011721125777152547\n",
      "Gradient Descent(146/199): ||gradient||=25.24421536915001, loss=0.39864428326858853, w0=0.00047393065205735474, w1=-0.0011745298772846784\n",
      "Gradient Descent(147/199): ||gradient||=28.4158439291318, loss=0.39864428326858853, w0=0.00047393065205735474, w1=-0.0011745298772846784\n",
      "Gradient Descent(148/199): ||gradient||=28.4158439291318, loss=0.39854692520639107, w0=0.00047341288182837853, w1=-0.0011756678772910558\n",
      "Gradient Descent(149/199): ||gradient||=16.069014117513397, loss=0.39845896827966887, w0=0.0004728863835142045, w1=-0.001177914209968948\n",
      "Gradient Descent(150/199): ||gradient||=16.14752739919354, loss=0.39831871797552754, w0=0.0004717450874911485, w1=-0.0011818411430173962\n",
      "Gradient Descent(151/199): ||gradient||=20.998885100349224, loss=0.39831871797552754, w0=0.0004717450874911485, w1=-0.0011818411430173962\n",
      "Gradient Descent(152/199): ||gradient||=20.998885100349224, loss=0.3982391963263021, w0=0.0004714346866097339, w1=-0.0011840981024987649\n",
      "Gradient Descent(153/199): ||gradient||=21.74412746575503, loss=0.39823372304957266, w0=0.0004701798559676789, w1=-0.001187795131262214\n",
      "Gradient Descent(154/199): ||gradient||=45.61884600353725, loss=0.39823372304957266, w0=0.0004701798559676789, w1=-0.001187795131262214\n",
      "Gradient Descent(155/199): ||gradient||=45.61884600353725, loss=0.39816340099144476, w0=0.000470304215030966, w1=-0.0011901809683076568\n",
      "Gradient Descent(156/199): ||gradient||=46.44739933010738, loss=0.39816340099144476, w0=0.000470304215030966, w1=-0.0011901809683076568\n",
      "Gradient Descent(157/199): ||gradient||=46.44739933010738, loss=0.3979544133588752, w0=0.0004697656450188587, w1=-0.0011911608794574534\n",
      "Gradient Descent(158/199): ||gradient||=15.21938578085237, loss=0.39788075002709017, w0=0.0004694143206910901, w1=-0.0011932322945997875\n",
      "Gradient Descent(159/199): ||gradient||=15.092458100700574, loss=0.3977574651828803, w0=0.00046862710557291656, w1=-0.0011968722400272664\n",
      "Gradient Descent(160/199): ||gradient||=17.459674823159773, loss=0.3977574651828803, w0=0.00046862710557291656, w1=-0.0011968722400272664\n",
      "Gradient Descent(161/199): ||gradient||=17.459674823159773, loss=0.39768697550058824, w0=0.0004683950381288716, w1=-0.0011989250529045087\n",
      "Gradient Descent(162/199): ||gradient||=16.899351382688632, loss=0.3975990223545446, w0=0.00046758113456258314, w1=-0.0012024120750347663\n",
      "Gradient Descent(163/199): ||gradient||=26.019833941819268, loss=0.3975990223545446, w0=0.00046758113456258314, w1=-0.0012024120750347663\n",
      "Gradient Descent(164/199): ||gradient||=26.019833941819268, loss=0.39752304948421946, w0=0.0004675299356815308, w1=-0.0012044823759427019\n",
      "Gradient Descent(165/199): ||gradient||=23.398773921717257, loss=0.39752304948421946, w0=0.0004675299356815308, w1=-0.0012044823759427019\n",
      "Gradient Descent(166/199): ||gradient||=23.398773921717257, loss=0.3974579612179805, w0=0.00046724538003479273, w1=-0.0012054689435615793\n",
      "Gradient Descent(167/199): ||gradient||=14.054105190616793, loss=0.3973959980173014, w0=0.00046695606675141964, w1=-0.0012073549207443715\n",
      "Gradient Descent(168/199): ||gradient||=13.976326987591387, loss=0.39728593725673683, w0=0.00046644893306398925, w1=-0.0012107502800225937\n",
      "Gradient Descent(169/199): ||gradient||=13.838335237006905, loss=0.3970924901945808, w0=0.00046558726733264826, w1=-0.0012168683324334482\n",
      "Gradient Descent(170/199): ||gradient||=13.600084522199227, loss=0.39675971276928534, w0=0.0004641465511188998, w1=-0.0012278788154433118\n",
      "Gradient Descent(171/199): ||gradient||=13.626603342993453, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(172/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(173/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(174/199): ||gradient||=63.583777857176486, loss=0.396539763378423, w0=0.0004624116752919005, w1=-0.0012479707521078397\n",
      "Gradient Descent(175/199): ||gradient||=63.583777857176486, loss=0.39617803328070517, w0=0.00046196131315609463, w1=-0.001248752797916121\n",
      "Gradient Descent(176/199): ||gradient||=13.399282050703505, loss=0.39613216067418866, w0=0.0004617658421167397, w1=-0.001250469495285852\n",
      "Gradient Descent(177/199): ||gradient||=12.812854237603393, loss=0.39605534488491967, w0=0.0004615761827407514, w1=-0.0012536395893912686\n",
      "Gradient Descent(178/199): ||gradient||=13.888507181794518, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(179/199): ||gradient||=30.713651396605982, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(180/199): ||gradient||=30.713651396605982, loss=0.39598065726146364, w0=0.0004609096537485329, w1=-0.0012591772910870323\n",
      "Gradient Descent(181/199): ||gradient||=30.713651396605982, loss=0.39589014412267565, w0=0.0004609996667137755, w1=-0.0012601726719174173\n",
      "Gradient Descent(182/199): ||gradient||=12.643616971766718, loss=0.3958494855462032, w0=0.00046091445594507026, w1=-0.0012618396212008233\n",
      "Gradient Descent(183/199): ||gradient||=12.226074899651255, loss=0.3957792989537911, w0=0.00046065886446540575, w1=-0.0012647875917508652\n",
      "Gradient Descent(184/199): ||gradient||=12.54535219479914, loss=0.3956729809015767, w0=0.00046039898889128325, w1=-0.0012701911503392297\n",
      "Gradient Descent(185/199): ||gradient||=19.402694715336576, loss=0.3956729809015767, w0=0.00046039898889128325, w1=-0.0012701911503392297\n",
      "Gradient Descent(186/199): ||gradient||=19.402694715336576, loss=0.39564837778976664, w0=0.0004599269998368624, w1=-0.0012729389741762073\n",
      "Gradient Descent(187/199): ||gradient||=29.220539831279712, loss=0.39564837778976664, w0=0.0004599269998368624, w1=-0.0012729389741762073\n",
      "Gradient Descent(188/199): ||gradient||=29.220539831279712, loss=0.3955648208324623, w0=0.00046007668369990953, w1=-0.001274625475802047\n",
      "Gradient Descent(189/199): ||gradient||=17.344152656382057, loss=0.39552681996328626, w0=0.0004596675395918377, w1=-0.0012773189769549578\n",
      "Gradient Descent(190/199): ||gradient||=24.254833014732103, loss=0.39552681996328626, w0=0.0004596675395918377, w1=-0.0012773189769549578\n",
      "Gradient Descent(191/199): ||gradient||=24.254833014732103, loss=0.39546109845038896, w0=0.00045976489295202096, w1=-0.0012789328426391418\n",
      "Gradient Descent(192/199): ||gradient||=14.856010281550212, loss=0.39541233533958825, w0=0.00045943023002053933, w1=-0.0012815805525175875\n",
      "Gradient Descent(193/199): ||gradient||=18.73111992964592, loss=0.39541233533958825, w0=0.00045943023002053933, w1=-0.0012815805525175875\n",
      "Gradient Descent(194/199): ||gradient||=18.73111992964592, loss=0.3953641016538838, w0=0.0004594683830141113, w1=-0.0012831197554240823\n",
      "Gradient Descent(195/199): ||gradient||=12.8810290100354, loss=0.39531051427753455, w0=0.00045920293374423165, w1=-0.0012857215827570562\n",
      "Gradient Descent(196/199): ||gradient||=14.456989818135845, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(197/199): ||gradient||=33.33018268942083, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(198/199): ||gradient||=33.33018268942083, loss=0.3952873732686723, w0=0.0004591609374763892, w1=-0.001290622114083704\n",
      "Gradient Descent(199/199): ||gradient||=33.33018268942083, loss=0.3951914510670374, w0=0.0004589646275692114, w1=-0.0012913200860782174\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\n",
    "    e=y-tx.dot(w)\n",
    "    N=len(y)\n",
    "    return -(np.transpose(tx).dot(e))/N\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=5.606944888677391, loss=0.6114707782168463, w0=0.006394104193840289, w1=0.005450759883688027\n",
      "SGD(1/99): |gradient|=7.570340290732537, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(2/99): |gradient|=3.573906656201934, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(3/99): |gradient|=3.3552360099930136, loss=0.4362372886140286, w0=0.0507102059786907, w1=-0.008130332381347041\n",
      "SGD(4/99): |gradient|=4.283991744813342, loss=0.43571977288915875, w0=0.049524230918568043, w1=-0.009027933449343517\n",
      "SGD(5/99): |gradient|=3.214881116998168, loss=0.43571977288915875, w0=0.049524230918568043, w1=-0.009027933449343517\n",
      "SGD(6/99): |gradient|=3.9249474696806628, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(7/99): |gradient|=3.203119350249999, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(8/99): |gradient|=3.2516736215578637, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(9/99): |gradient|=3.9203972908220814, loss=0.4354151761726099, w0=0.04857157669861133, w1=-0.00976244417735909\n",
      "SGD(10/99): |gradient|=2.877050071545468, loss=0.4354068526349247, w0=0.04852514287605632, w1=-0.009795561038324215\n",
      "SGD(11/99): |gradient|=3.237847604866295, loss=0.4354068526349247, w0=0.04852514287605632, w1=-0.009795561038324215\n",
      "SGD(12/99): |gradient|=3.2135569327736273, loss=0.4354065352520262, w0=0.04856341179440249, w1=-0.009807263876546889\n",
      "SGD(13/99): |gradient|=3.6318714658786204, loss=0.43539765852075396, w0=0.04851269682408544, w1=-0.009846730088762143\n",
      "SGD(14/99): |gradient|=8.922838090507645, loss=0.4353512563362487, w0=0.04864559566537896, w1=-0.009733761714443675\n",
      "SGD(15/99): |gradient|=3.257847145379344, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(16/99): |gradient|=4.02164685942485, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(17/99): |gradient|=3.2572148658644138, loss=0.4353365323409414, w0=0.048871751453989774, w1=-0.009798370839382307\n",
      "SGD(18/99): |gradient|=4.452631520185705, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(19/99): |gradient|=3.6487893078723563, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(20/99): |gradient|=3.2821648805273687, loss=0.4353320852832367, w0=0.04883431442664927, w1=-0.009814802576834605\n",
      "SGD(21/99): |gradient|=7.797940164477905, loss=0.4353263030355192, w0=0.048840038730257815, w1=-0.009811107123552357\n",
      "SGD(22/99): |gradient|=3.992349023741367, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(23/99): |gradient|=3.2527088858245445, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(24/99): |gradient|=3.8963899595362563, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(25/99): |gradient|=3.90756824641425, loss=0.4353246950206849, w0=0.048833117212861106, w1=-0.009816222087608698\n",
      "SGD(26/99): |gradient|=2.984587021256584, loss=0.4353246339526049, w0=0.04883281169827094, w1=-0.00981652705700677\n",
      "SGD(27/99): |gradient|=3.375345091019779, loss=0.4353245174533584, w0=0.04883217576962479, w1=-0.009816980862667569\n",
      "SGD(28/99): |gradient|=8.882359251533417, loss=0.43532387343956525, w0=0.04883393639448352, w1=-0.009815558363804608\n",
      "SGD(29/99): |gradient|=7.8314175762930915, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(30/99): |gradient|=3.216640749684483, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(31/99): |gradient|=2.6789071208412274, loss=0.435320985788389, w0=0.04883678615528364, w1=-0.009813182452047693\n",
      "SGD(32/99): |gradient|=8.896513586862488, loss=0.4353208107866267, w0=0.04883722044527872, w1=-0.009812787566203993\n",
      "SGD(33/99): |gradient|=7.917345311733529, loss=0.4353199667044906, w0=0.04883801861744155, w1=-0.009812159224872807\n",
      "SGD(34/99): |gradient|=4.054883017006384, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(35/99): |gradient|=3.8583111607911094, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(36/99): |gradient|=4.221725188458153, loss=0.435319755720812, w0=0.04883702767400843, w1=-0.009812936289430104\n",
      "SGD(37/99): |gradient|=8.825453051084411, loss=0.43531965968590314, w0=0.048837275711854845, w1=-0.009812751475599317\n",
      "SGD(38/99): |gradient|=7.955222080995427, loss=0.43531921172232746, w0=0.04883782974631502, w1=-0.009812268704276081\n",
      "SGD(39/99): |gradient|=5.317256180130434, loss=0.4353190913739208, w0=0.04883697165385909, w1=-0.009812832952753986\n",
      "SGD(40/99): |gradient|=3.2040442264536786, loss=0.4353190913739208, w0=0.04883697165385909, w1=-0.009812832952753986\n",
      "SGD(41/99): |gradient|=3.0295458149027654, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(42/99): |gradient|=3.8980707909256775, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(43/99): |gradient|=3.8848078489302518, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(44/99): |gradient|=3.228463734076844, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(45/99): |gradient|=3.8675611019024707, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(46/99): |gradient|=3.89859151939232, loss=0.4353190454115933, w0=0.0488366317706634, w1=-0.00981307739865317\n",
      "SGD(47/99): |gradient|=3.263934119713472, loss=0.43531904516928305, w0=0.04883663042453911, w1=-0.009813078489623567\n",
      "SGD(48/99): |gradient|=3.204213295092851, loss=0.43531904516928305, w0=0.04883663042453911, w1=-0.009813078489623567\n",
      "SGD(49/99): |gradient|=3.4394804493736415, loss=0.4353190450410245, w0=0.0488366297167548, w1=-0.009813079081019476\n",
      "SGD(50/99): |gradient|=2.9024717295998896, loss=0.4353190450410245, w0=0.0488366297167548, w1=-0.009813079081019476\n",
      "SGD(51/99): |gradient|=3.093680470224432, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(52/99): |gradient|=3.9533018211314523, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(53/99): |gradient|=2.87744668988226, loss=0.4353190450075203, w0=0.048836629120268815, w1=-0.009813079341295607\n",
      "SGD(54/99): |gradient|=3.3318373430046258, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(55/99): |gradient|=3.3158506822067055, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(56/99): |gradient|=3.8960808365785815, loss=0.4353190449968911, w0=0.048836629060965184, w1=-0.00981307938779756\n",
      "SGD(57/99): |gradient|=7.872778330565903, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(58/99): |gradient|=2.6798366821690047, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(59/99): |gradient|=3.234114564601285, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(60/99): |gradient|=3.919351282881189, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(61/99): |gradient|=3.3549198934655897, loss=0.43531904498241625, w0=0.04883662907745726, w1=-0.009813079374112224\n",
      "SGD(62/99): |gradient|=3.2154106441270565, loss=0.43531904498240476, w0=0.04883662907767243, w1=-0.009813079374175495\n",
      "SGD(63/99): |gradient|=8.970449265864339, loss=0.43531904498226326, w0=0.04883662907805685, w1=-0.009813079373855126\n",
      "SGD(64/99): |gradient|=2.826122756568962, loss=0.43531904498226326, w0=0.04883662907805685, w1=-0.009813079373855126\n",
      "SGD(65/99): |gradient|=3.2546919408212385, loss=0.4353190449822374, w0=0.04883662907791595, w1=-0.009813079373965809\n",
      "SGD(66/99): |gradient|=3.2273199101846566, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(67/99): |gradient|=2.6770668351032825, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(68/99): |gradient|=3.8524543787623844, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(69/99): |gradient|=3.8998391272758015, loss=0.4353190449822183, w0=0.04883662907829651, w1=-0.00981307937408899\n",
      "SGD(70/99): |gradient|=2.9297591762273685, loss=0.4353190449822166, w0=0.04883662907828266, w1=-0.009813079374098813\n",
      "SGD(71/99): |gradient|=7.81078644316268, loss=0.4353190449821845, w0=0.04883662907831475, w1=-0.00981307937407468\n",
      "SGD(72/99): |gradient|=3.2576130548889224, loss=0.4353190449821845, w0=0.04883662907831475, w1=-0.00981307937407468\n",
      "SGD(73/99): |gradient|=3.2226760275492636, loss=0.4353190449821835, w0=0.04883662907833267, w1=-0.009813079374079893\n",
      "SGD(74/99): |gradient|=2.9304946257411397, loss=0.4353190449821805, w0=0.04883662907830929, w1=-0.009813079374094853\n",
      "SGD(75/99): |gradient|=3.0316166379110445, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(76/99): |gradient|=3.87860594604992, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(77/99): |gradient|=2.6722369316023546, loss=0.4353190449821743, w0=0.04883662907827043, w1=-0.009813079374124192\n",
      "SGD(78/99): |gradient|=8.79213351440848, loss=0.43531904498217067, w0=0.048836629078279854, w1=-0.00981307937411849\n",
      "SGD(79/99): |gradient|=2.982150379967762, loss=0.43531904498217067, w0=0.048836629078279854, w1=-0.00981307937411849\n",
      "SGD(80/99): |gradient|=7.860694834409841, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(81/99): |gradient|=3.9345760088688193, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(82/99): |gradient|=3.2447582177109853, loss=0.43531904498216584, w0=0.04883662907828543, w1=-0.009813079374115012\n",
      "SGD(83/99): |gradient|=3.35655313002987, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(84/99): |gradient|=3.887554000656264, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(85/99): |gradient|=3.2517879873166664, loss=0.4353190449821658, w0=0.04883662907828489, w1=-0.009813079374115413\n",
      "SGD(86/99): |gradient|=8.814821572997982, loss=0.43531904498216567, w0=0.048836629078285, w1=-0.009813079374115328\n",
      "SGD(87/99): |gradient|=3.6438796881947124, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(88/99): |gradient|=3.8980277591024244, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(89/99): |gradient|=3.8388791919466243, loss=0.43531904498216567, w0=0.048836629078285176, w1=-0.009813079374115224\n",
      "SGD(90/99): |gradient|=3.288654918945359, loss=0.43531904498216567, w0=0.04883662907828516, w1=-0.009813079374115236\n",
      "SGD(91/99): |gradient|=2.6772538871447735, loss=0.43531904498216567, w0=0.04883662907828516, w1=-0.009813079374115236\n",
      "SGD(92/99): |gradient|=8.878001694603418, loss=0.43531904498216567, w0=0.04883662907828518, w1=-0.00981307937411522\n",
      "SGD(93/99): |gradient|=8.858379756186846, loss=0.43531904498216567, w0=0.048836629078285225, w1=-0.009813079374115196\n",
      "SGD(94/99): |gradient|=3.2182852932164865, loss=0.43531904498216567, w0=0.048836629078285294, w1=-0.009813079374115219\n",
      "SGD(95/99): |gradient|=3.906587326476464, loss=0.43531904498216567, w0=0.04883662907828524, w1=-0.00981307937411526\n",
      "SGD(96/99): |gradient|=2.8424234277980114, loss=0.43531904498216567, w0=0.0488366290782854, w1=-0.009813079374115165\n",
      "SGD(97/99): |gradient|=2.685708557567156, loss=0.43531904498216567, w0=0.048836629078285135, w1=-0.009813079374115375\n",
      "SGD(98/99): |gradient|=9.793714709632297, loss=0.435319044982165, w0=0.04883662907828585, w1=-0.009813079374114915\n",
      "SGD(99/99): |gradient|=3.318574468833573, loss=0.4353190449821648, w0=0.04883662907828498, w1=-0.009813079374115529\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 30, 3)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w_star = least_squares(y,xpoly)\n",
    "xpoly[:,:,0]\n",
    "w0 = least_squares(y,xpoly[:,:,0])\n",
    "w1 = least_squares(y,xpoly[:,:,1])\n",
    "w2 = least_squares(y,xpoly[:,:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [w0, w1, w2]\n",
    "yp = np.zeros((250000,3))\n",
    "yp[:,0] = xpoly[:,:,0].dot(w0)\n",
    "yp[:,1] = xpoly[:,:,1].dot(w1)\n",
    "yp[:,2] = xpoly[:,:,2].dot(w2)\n",
    "compute_loss(yp, xpoly, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.24104142e-04 -8.68280397e-03 -2.68859353e-03 -2.22470926e-03\n",
      " -5.48363092e-03  5.00183793e-04 -1.72285231e-02  6.05199540e-02\n",
      "  1.57457370e-05  3.49522005e-03 -5.39186900e-02  6.84999623e-02\n",
      "  2.19603319e-02  5.70895097e-03 -4.23536533e-04 -1.28871636e-03\n",
      "  3.30152909e-03 -5.20781620e-04  9.12327144e-04  4.86026909e-03\n",
      "  4.62605701e-04 -7.55206472e-04 -5.59287480e-02  1.20180623e-03\n",
      " -6.34079801e-04 -4.00651108e-04  5.73198500e-05  1.25042999e-03\n",
      " -8.49579237e-04 -5.51323579e-03] 0.3483563858251794\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.2\n",
    "w = ridge_regression(y, tX, lambda_)\n",
    "loss = compute_loss(y, tX, w)\n",
    "print(w, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.58964628e-04, -1.29132009e-03, -4.71611097e-04,  4.35105239e-04,\n",
       "       -9.51234626e-05,  8.76108257e-04, -1.17527898e-04, -7.17490384e-06,\n",
       "       -2.25082512e-04, -3.08541973e-04, -2.94446989e-05,  2.20370090e-05,\n",
       "       -9.60401499e-05,  4.88644294e-04, -4.09816793e-07, -9.94246210e-07,\n",
       "       -3.66728587e-04, -4.37996069e-07,  9.71191920e-07, -1.02244569e-05,\n",
       "        1.36577033e-06, -6.03655237e-04, -1.23027882e-05,  1.17095806e-04,\n",
       "        3.21211363e-05,  3.21108916e-05, -3.45141914e-04, -9.77186517e-05,\n",
       "       -9.84173092e-05, -4.30457476e-04])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = gradient_w\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "tX_test_s, _, _ = standardize(tX_test)\n",
    "y_pred = predict_labels(weights, tX_test_s)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.505418514821007"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = compute_loss(y_pred, tX_test_s, w)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
