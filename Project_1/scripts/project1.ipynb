{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , ?\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2       3       4        5        6      7       8   \\\n",
       "0  138.470   51.655   97.827  27.980    0.91  124.711    2.666  3.064  41.928   \n",
       "1  160.937   68.768  103.235  48.146 -999.00 -999.000 -999.000  3.473   2.078   \n",
       "2 -999.000  162.172  125.953  35.635 -999.00 -999.000 -999.000  3.148   9.336   \n",
       "3  143.905   81.417   80.943   0.414 -999.00 -999.000 -999.000  3.310   0.414   \n",
       "4  175.864   16.915  134.805  16.405 -999.00 -999.000 -999.000  3.891  16.405   \n",
       "\n",
       "        9   ...     20       21   22       23       24       25       26  \\\n",
       "0  197.760  ... -0.277  258.733  2.0   67.435    2.150    0.444   46.062   \n",
       "1  125.157  ... -1.916  164.546  1.0   46.226    0.725    1.158 -999.000   \n",
       "2  197.814  ... -2.186  260.414  1.0   44.251    2.053   -2.028 -999.000   \n",
       "3   75.968  ...  0.060   86.062  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "4   57.983  ... -0.871   53.131  0.0 -999.000 -999.000 -999.000 -999.000   \n",
       "\n",
       "       27       28       29  \n",
       "0    1.24   -2.475  113.497  \n",
       "1 -999.00 -999.000   46.226  \n",
       "2 -999.00 -999.000   44.251  \n",
       "3 -999.00 -999.000    0.000  \n",
       "4 -999.00 -999.000    0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd = pd.DataFrame(tX)\n",
    "tX_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation graph needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38114\n",
      "4 177457\n",
      "5 177457\n",
      "6 177457\n",
      "12 177457\n",
      "23 99913\n",
      "24 99913\n",
      "25 99913\n",
      "26 177457\n",
      "27 177457\n",
      "28 177457\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30):\n",
    "    s = tX_pd[tX_pd[i]==-999].index.size\n",
    "    if s > 900:\n",
    "        print(i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione di Marco per cancellare tutto con numpy e mettere la media: da scrivere e usare al posto di pandas alla fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uno = np.where(tX==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere(s > 100000)\n",
    "# new = np.delete(tX,index[1:],axis=1)\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s>40000)&(s < 100000))\n",
    "# new = np.delete(new,index[1:],axis=1)\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s<40000)&(s>1))\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima\n",
    "\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s>40000)&(s < 100000))\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima\n",
    "\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere(s > 100000)\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop PRI with a lot of null + 29 (29 is a Pri corr with 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [5,6,12,24,25,26,27,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pd.drop(drops, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with median\n",
    "tX_pd.where(tX_pd!=-999, inplace=True)\n",
    "tX_pd.fillna(tX_pd.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_pd.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, mean_x, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, mean=True):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        x_te = x[index_te,:]\n",
    "        x_tr = x[index_tr,:]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    # ***************************************************\n",
    "        x_te_poly = build_poly_2(x_te, degree)\n",
    "        x_tr_poly = build_poly_2(x_tr, degree)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # form data with polynomial degree: TODO\n",
    "    # ***************************************************\n",
    "        w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # ridge regression: TODO\n",
    "    # ***************************************************\n",
    "        rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "        rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate the loss for train and test data: TODO\n",
    "    # ***************************************************\n",
    "    if mean==True:\n",
    "        loss_tr = np.mean(losses_tr)\n",
    "        loss_te = np.mean(losses_te)\n",
    "    else:\n",
    "        loss_tr = losses_tr\n",
    "        loss_te = losses_te\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEaCAYAAADQVmpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZxT9b3/8dcnmX0JuyMoFVSuRQRRkEKtOGjF5bba1qvW1rX9CdbWbldbra2112q99ba21GpLW4pe96u12ooVRAYEUTaRVQUFZWRfk1kyM0k+vz/OyRiGzJBkksmB+Twfj3lM8s1ZvvlOJu98v+fkfEVVMcYYYzLhy3cFjDHGHLosRIwxxmTMQsQYY0zGLESMMcZkzELEGGNMxixEjDHGZMxCxBiPEpGNIvJZ9/aPROTPqSybwX7OEJF3Mq2n6d4K8l0BY8zBqerd2dqWiCgwRFXXu9t+FTghW9s33Yv1REy3JSL2IcqYTrIQMYcdERkoIn8TkR0isktE7nfLrxGRBSJyn4jsBu4QEZ+I/FhEPhCR7SLysIj0cJcvEZFH3G3sFZHFIlKVsK33RSQkIhtE5KtJ6jFARBpFpHdC2SkislNECkXkOBF5xd3+ThF5VER6tvOc7hCRRxLuX+nWeZeI3NZm2TEistCt8xYRuV9EitzH5rmLvSUidSJymYhUi0htwvpDRaTGXX+1iFyY8Nh0Efm9iLzgPvc3ROS49P9K5nBhIWIOKyLiB/4JfAAMAo4CnkhY5FPA+8ARwF3ANe7PBOBYoAK43132aqAHMBDoA1wPNIpIOTAFOF9VK4FPA8vb1kVVNwMLgYsTir8CPK2qLYAAvwAGAEPd/dyRwnM8EXgQuNJdtw9wdMIiUeB7QF9gHHA2cINbp/HuMieraoWqPtlm24XAP4CZbhvdCDwqIonDXZcDPwN6Aetx2tF0UxYi5nAzBueN9WZVrVfVsKrOT3h8s6r+TlUjqtoIfBX4taq+r6p1wK3Al92hrhacN+jjVTWqqktVNehuJwacJCKlqrpFVVe3U5/HcN50EREBvuyWoarrVXWWqjap6g7g18CZKTzH/wD+qarzVLUJ+IlbH9ztLlXV193nuBH4Y4rbBRiLE6T3qGqzqr6CE8qXJyzzN1VdpKoR4FFgZIrbNochCxFzuBkIfOC+wSWzqc39ATi9lrgPcE44qQL+F3gJeEJENovIL0WkUFXrgctweiZb3KGdT7azv6eBcSIyABgPKPAqgIgcISJPiMhHIhIEHsHpPRzMgMTn4dZnV/y+iPybiPxTRLa62707xe22bltVYwllH+D06OK2JtxuwAkd001ZiJjDzSbgEx0cNG972erNwDEJ9z8BRIBtqtqiqj9T1RNxhqw+B1wFoKovqeo5QH/gbeBPSXemuhdnaOhSnKGsx/XjS2f/wq3PCFUNAFfgDHEdzBacsARARMpwekxxD7p1GuJu90cpbhec9hgoIonvDZ8APkpxfdPNWIiYw80inDfZe0Sk3D04fnoHyz8OfE9EBotIBc6n9idVNSIiE0RkuHucJYgzvBUVkSoRudA9NtIE1OEch2jPYzjhc7F7O67SXXeviBwF3Jzic3wa+JyIfMY9YP5f7P+/XOnWt87tIX2jzfrbcI7/JPMGUA/8wD34Xw18nv2PKxnTykLEHFZUNYrzpnc88CFQizP01J5pOMNW84ANQBjnYDLAkThv2EFgLTAXZ8jJB/wnzqf23TjHG27oYB/PA0NwejdvJZT/DDgV2Ae8APwtxee4GvgmTiBtAfa4zzPuJpxeTwinh/Rkm03cATzknn11aZttNwMXAucDO4EHgKtU9e1U6ma6H7FJqYwxxmTKeiLGGGMyZiFijDEmYxYixhhjMmYhYowxJmMWIsYYYzLW7a5i2rdvXx00aFBG69bX11NeXp7dCh3GrL3SY+2VHmuv9HS2vZYuXbpTVfu1Le92ITJo0CCWLFmS0bo1NTVUV1dnt0KHMWuv9Fh7pcfaKz2dbS8R+SBZuQ1nGWOMyZiFiDHGmIxZiBhjjMlYtzsmkkxLSwu1tbWEw+EOl+vRowdr167tolod+ioqKmhpaaGwsDDfVTHG5IiFCFBbW0tlZSWDBg3CmTcouVAoRGVlZRfW7NClqtTW1lJbW8vgwYPzXR1jTI7YcBYQDofp06dPhwFi0iMi9OjR46C9O2OSWrgQfvEL57fxNOuJuCxAss/a1GRk4UI4+2xoaoLiYpg9G8aNy3etTDusJ+IBe/fu5YEHHsho3QsuuIC9e/dmuUbG5FFNDRoOQyyGNjVDTU2+a2Q6YCHiAR2FSDTa0YR5MGPGDHr27JnV+kQikQ7vt+dgdTUmFSv7VBNRPwDhWCEr+1Tnt0KmQxYiGcrmkO0tt9zCe++9x8iRI7n55pupqalhwoQJfOUrX2H48OEAfOELX2DUqFEMGzaMqVOntq47aNAgdu7cycaNGxk6dCjXXXcdw4YNY+LEiTQ2Nh6wrx07dnDxxRdz2mmncdppp7FgwQIA7rjjDiZNmsTEiRO56qqrmD59Opdccgmf//znmThxIqrKzTffzEknncTw4cN58klnsrxkdTWmM/65axxP4Uy4eL1M5Z+7bCjLy+yYSBvf/S4sX578sWi0FL8f9u2DFSsgFgOfD0aMgB492t/myJHwm9+0//g999zDqlWrWO7uuKamhkWLFrFq1arWM5umTZtG7969aWxs5LTTTuPiiy+mT58++21n3bp1PP744/zpT3/i0ksv5ZlnnuGKK67Yb5nvfOc7fO973+Mzn/kMH374Ieeee27ractLly5l/vz5lJaWMn36dBYuXMiKFSvo3bs3zzzzDMuXL+ett95i586dnHbaaYwfPx7ggLoa0xnV1bBKKkChtnAw11fnu0amIxYiGdi3zwkQcH7v29dxiGRizJgx+70pT5kyhWeffRaATZs2sW7dugNCZPDgwYwcORKAUaNGsXHjxgO2+/LLL7NmzZrW+8FgkFAoBMCFF15IaWlp62PnnHMOvXv3BmD+/Plcfvnl+P1+qqqqOPPMM1m8eDGBQOCAuhrTGePGQezoIGyCH94QsmPqHmch0kZHPYZQqJHKysrWk0eam6GoCB59NPsnjyRebbOmpoaXX36ZhQsXUlZWRnV1ddJTZ4uLi1tv+/3+pMNZsViMhQsX7hcWyfbZ9r6qplRXY7KhXJ0PNkdVBvNcE3MwdkwkA+PGOWcd3nlnds4+rKysbO0NJLNv3z569epFWVkZb7/9Nq+//nrG+5o4cSL3339/6/3l7Y3dtTF+/HiefPJJotEoO3bsYN68eYwZMybjehjTkaKwEx4tuyxEvC5nISIiA0VkjoisFZHVIvIdt7y3iMwSkXXu715uuYjIFBFZLyIrROTUhG1d7S6/TkSuTigfJSIr3XWmSBd+MWHcOLj11uz0QPr06cPpp5/OSSedxM0333zA4+eddx6RSIQRI0bwk5/8hLFjx2a8rylTprBkyRJGjBjBiSeeyB/+8IeU1vviF7/IiBEjOPnkkznrrLP45S9/yZFHHplxPYzpSLEbIpE9FiKep6o5+QH6A6e6tyuBd4ETgV8Ct7jltwD/7d6+AHgREGAs8IZb3ht43/3dy73dy31sETDOXedF4PyD1WvUqFHa1po1aw4oSyYYDKa0nHEEg8GU29aozpkzJ99V8IxNxceqgi46//Z2l7H2Sk9n2wtYokneU3PWE1HVLaq6zL0dAtYCRwEXAQ+5iz0EfMG9fRHwsFvf14GeItIfOBeYpaq7VXUPMAs4z30soKoL3Sf4cMK2jDGHsLKI0wPRoPVEvK5LjomIyCDgFOANoEpVt4ATNMAR7mJHAZsSVqt1yzoqr01Sbow5xJVHnfCQkIWI1+X87CwRqQCeAb6rqsEODlske0AzKE9Wh0nAJICqqipq2lxGoUePHh0e2I6LRqMpLWcc0WiUcDh8QHub5Orq6qytAGlu5kyaAWjeua3dNrH2Sk+u2iunISIihTgB8qiq/s0t3iYi/VV1izsktd0trwUGJqx+NLDZLa9uU17jlh+dZPkDqOpUYCrA6NGjte08w2vXrk3pEu92Kfj0hEIhSkpKOOWUU/JdlUOCzRnuiG3f2Xq7UlsY0U6bWHulJ1ftlcuzswT4C7BWVX+d8NDzQPwMq6uB5xLKr3LP0hoL7HOHu14CJopIL/dMronAS+5jIREZ6+7rqoRtGWMOUQ1bPx7CKgpbz9/rctkTOR24ElgpIvEvI/wIuAd4SkS+DnwIXOI+NgPnDK31QANwLYCq7haRO4HF7nL/paq73dvfAKYDpThnZ72Yw+djjOkC9dtCVAAtFFDcZMdEvC5nIaKq80l+3ALg7CTLK/DNdrY1DZiWpHwJcFInqukJe/fu5bHHHuOGG27IaP3f/OY3TJo0ibKysizXzJiuF97mBMcW+lPaYiHidfaNdQ/ozHwi4IRIQ0NDxutneun3VJczJh3h7U5wbPUfTVnUQsTr7NpZmVq40Jksp7q6019bT7wU/DnnnMO9997Lvffey1NPPUVTUxNf/OIX+dnPfkZ9fT2XXnoptbW1RKNRfvKTn7Bt2zY2b97MhAkT6Nu3L3PmzNlv20uXLuX73/8+dXV19O3bl+nTp9O/f3+qq6v59Kc/zYIFC7jwwgtZuXIlvXv35s033+TUU0/ltttu42tf+xrvv/8+ZWVlTJ06lREjRnDHHXewefNmNm7cSN++fXnsscc69dyNaat5l3McZHf50VQEXwdVsFkyPctCpK0OrgVfGo2Si2vBt70U/MyZM1m3bh2LFi1CVbnwwguZN28eO3bsYMCAAbzwwguAc02tHj168Otf/5o5c+bQt2/f/bbb0tLCjTfeyHPPPUe/fv148sknue2225g2zRkZ3Lt3L3PnzgXgmmuu4d133+Xll1/G7/dz4403csopp/D3v/+dV155hauuuqq1fomXjDcm2+LXy2rodRS+oEJ9PVRU5LlWpj0WIpnI8bXgZ86cycyZM1tPja2rq2PdunWcccYZ3HTTTfzwhz/kc5/7HGeccUaH23nnnXdYtWoV55xzDuB8b6N///6tj1922WX7LX/JJZfg9zszys2fP59nnnkGgLPOOotdu3axb98+4MBLxhuTTbG97nWzqo6CD6B5Z5AiCxHPshBpq4MeQ2P8eyI5vha8qnLrrbcyefLkAx5bunQpM2bM4NZbb2XixIncfvvtHW5n2LBhLGxn+sV0L/0e/6KoXfrd5FJsX4gYgv8o5wNP/dYgRYMG5LlWpj12YD0TWb4WfNtLwZ977rlMmzaNuro6AD766CO2b9/O5s2bKSsr44orruCmm25i2bJlSdePO+GEE9ixY0driLS0tLB69eqU6jR+/HgeffRRwPmSUt++fQkEAp16nsakQoNB6qig/KiewP7fGzHeYz2RTI0bl7XeR+Kl4M8//3zuvfde1q5dyzh3+xUVFTzyyCOsX7+em2++GZ/PR2FhIQ8++CAAkyZN4vzzz6d///77HVgvKiri6aef5tvf/jb79u0jEonw3e9+l2HDhh20TnfccQfXXnstI0aMoKysjIceeuig6xiTDb5QkCAByo50PrSEt9sXDr1Mkg1bHM5Gjx6tS5Ys2a9s7dq1DB069KDr2mVP0hMKhaitrU2pbY1dxiNu6XGXUvHBKvY+8DifmjySNT//Gyfe9sUDlrP2Sk9n20tElqrq6LblNpxljPGUwsYgjQWVlBzh9ESabXZDT7MQMcZ4SmE4SLjo4+GsyG4LES+zEDHGeEpxc4im4gAV/Z2h45hNketpFiKu7nZsqCtYm5pMlDYHiZRUEuhbRJhim93Q4yxEgJKSEnbt2mVvelmkquzbt4+SkpJ8V8UcYsqiQSJlAcrKIEgAsRDxNDvFFzj66KOpra1lx44dHS4XDoftTTEN9fX1nHzyyfmuhjmUqFIeCxGrCCACdb4AvnoLES+zEAEKCwsZPHjwQZerqamxWfrSUFNTQ2FhYb6rYQ4ljY0UEEUrnOMhDf4ABRYinmbDWcYYz2je6QZGD+fMrMbCAAU2u6GnWYgYYzyjfqsTGP6eTog0FQdsdkOPsxAxxnhGozurYUEvZziruSRASbOFiJdZiBhjPCMeIoV93C8algUoi1iIeJmFiDHGM5p2OsNZRX2dEImWByi3KXI9zULEGOMZ8QPrJf2c4SytrKSEJmhqyme1TAcsRIwxnhF1L3ESv26WuHPYRPfaGVpeZSFijPGMeFiU93fCw+eepVVvE1N5loWIMcYzYvuCRPBT2c+5MoS/lxMiDVssRLzKQsQY4xkSdGY1rKgU4OOztBptdkPPshAxxniG1IUISQCf+84UP0srvN16Il5lIWKM8YyChiCN/o+noG6d3XCnhYhXWYgYYzyjoDFIQ2Gg9b7Nbuh9FiLGGM8oagoRLvo4ROJnaUVtdkPPshAxxnhGSXOQluKPh7Mqq8qI4iO2z0LEqyxEjDGeUdoSpKX0455IRaUQotJmN/QwCxFjjGeURUNEyz8OEb8fQhJA6ixEvMpCxBjjDbEYlRoiVl65X3G9P4DfZjf0LAsRY4wnaKjOuREI7FfeWBigoNG+bOhVFiLGGE+IfytdeuwfIuGiAMVh64l4lYWIMcYT6t3rY/l77j+c1VwcoNhmN/QsCxFjjCe0ndUwrqU0QFmLhYhXWYgYYzwhPpzVNkSi5QHKbHZDz8pZiIjINBHZLiKrEsruEJGPRGS5+3NBwmO3ish6EXlHRM5NKD/PLVsvIrcklA8WkTdEZJ2IPCkiRbl6LsaY3ItfH6u47/7DWVpRSaWGIBbLR7XMQeSyJzIdOC9J+X2qOtL9mQEgIicCXwaGues8ICJ+EfEDvwfOB04ELneXBfhvd1tDgD3A13P4XIwxOdayywmR0qr9eyLqnq3VevaW8ZSchYiqzgN2p7j4RcATqtqkqhuA9cAY92e9qr6vqs3AE8BFIiLAWcDT7voPAV/I6hMwxnSpyB5nOCt+0cU4n3u2VvyYifGWfBwT+ZaIrHCHu3q5ZUcBmxKWqXXL2ivvA+xV1UibcmPMISq21wmJ8iP3H86Kz25Yb7MbelJBF+/vQeBOQN3fvwK+BkiSZZXkIacdLJ+UiEwCJgFUVVVRU1OTVqXj6urqMl63O7L2Sk93b6/whx8Sppjla16naP3Hxz821+0FYPErCynT7a3l3b290pWr9urSEFHVbfHbIvIn4J/u3VpgYMKiRwOb3dvJyncCPUWkwO2NJC6fbL9TgakAo0eP1urq6ozqX1NTQ6brdkfWXunp7u31uv8pggQ455zxSMLHxNcWF8ITcFy/T3BCQvt09/ZKV67aq0uHs0Skf8LdLwLxM7eeB74sIsUiMhgYAiwCFgND3DOxinAOvj+vqgrMAf7DXf9q4LmueA7GmNzw1Qep91XuFyAAxf2c4aymHTac5UU564mIyONANdBXRGqBnwLVIjISZ+hpIzAZQFVXi8hTwBogAnxTVaPudr4FvAT4gWmqutrdxQ+BJ0Tk58CbwF9y9VyMMblX0BCkoSBwQHn8bK342VvGW3IWIqp6eZLidt/oVfUu4K4k5TOAGUnK38c5e8sYcxgoDIdoLDwwRGx2Q2+zb6wbYzyhOBykuajygPKK/k5Z/Owt4y0WIsYYTyhpCdJccmBPpLJXAQ2Uoja7oSdZiBhjPKE0EiJSdmCIFBVBkAC+kIWIF1mIGGM8oSIaJFp+4HAWOLMb+mx2Q0+yEDHG5F8kQimNaMWBPRGAhoIABQ02u6EXWYgYY/Iuft2stlPjxoULAxTa7IaeZCFijMm7+HWxfD2TD2c1FQcobrIQ8SILEWNM3jVsjU+Nm7wn0lIaoNRmN/QkCxFjTN41tDOrYVykLEBZxELEiyxEjDF517TdCYiiPsmHs2LllVTEgqDtXqzb5ImFiDEm7+JT45YckbwnooEARbRAU1NXVsukwELEGJN3LfFZDauS90QkYFfy9SoLEWNM3kV3J59fPc7nzm5Yt8W+K+I1FiLGmLzTfU6IVA5I3hMpcEMkfhaX8Q4LEWNM3mkwRD1lVPb0J328qK8TIuHtFiJeYyFijMk7X12QkATwJ88Qimx2Q8+yEDHG5J2/PkidL/nxEIBS96yt+FlcxjssRIwxeedvDBEuSH48BKDsSCdEIrstRLzGQsQYk3dF4SCNRe33RMqPdAImarMbeo6FiDEm70qagjQXtx8igapSIvhbz+Iy3mEhYozJu5JIiJaS9oezSkqFIAHEpsj1HAsRY0zelUWCSafGjROBOl8AX7192dBrLESMMXlXEQsSa2dWw7gGfwB/g/VEvMZCxBiTVxpuoogWtLL94SyAxsIAhY0WIl5jIWKMyav4Fwilnalx48LFAYptilzPsRAxxuTVx1PjdhwizSUBSmx2Q89JKUTEcYWI3O7e/4SIjMlt1Ywx3UH9VudgeUGvjoezIqUByixEPCfVnsgDwDjgcvd+CPh9TmpkjOlW4hdVbG9q3LhYeSXlMQsRrylIcblPqeqpIvImgKruEZGiHNbLGNNNxI+JFPfrOES0MkC51kM0SrtXajRdLtWeSIuI+AEFEJF+QCxntTLGdBvNu5zhrJIjOh7Owj3wHtlj3xXxklRDZArwLHCEiNwFzAfuzlmtjDHdRvyiiqXtzK8eFz/wbrMbektKw1mq+qiILAXOBgT4gqquzWnNjDHdQsy9qGLFgI5DxJ8wu2HP4TmvlklRqmdnHQdsUNXfA6uAc0SkZ05rZozpFmL7QsQQKo8s73C5+IH3xm12cN1LUh3OegaIisjxwJ+BwcBjOauVMabbkFCQEJWUlkmHy9kUud6UaojEVDUCfAn4rap+D+ifu2oZY7oLX12QOgkgHWcIJTa7oSelc3bW5cBVwD/dssLcVMkY0534G0I0+A9yZhZQWuWESMsuCxEvSTVErsX5suFdqrpBRAYDj+SuWsaY7qKgMUhDYccH1SFhdsM9FiJekurZWWuAbyfc3wDck6tKGWO6j+JwkLoOpsaNqxzghIjNbugtqZ6d9TkReVNEdotIUERCItLhX1JEponIdhFZlVDWW0Rmicg693cvt1xEZIqIrBeRFSJyasI6V7vLrxORqxPKR4nISnedKSIHG1E1xnhRcUuI5uKDD2eVB/zUUQ42u6GnpDqc9RvgaqCPqgZUtVJVD/bRYTpwXpuyW4DZqjoEmO3eBzgfGOL+TAIeBCd0gJ8CnwLGAD+NB4+7zKSE9druyxhzCChtCdLSwayGcT4fhCSA1NmXDb0k1RDZBKxSVU11w6o6D9jdpvgi4CH39kPAFxLKH1bH60BPEekPnAvMUtXdqroHmAWc5z4WUNWFbp0eTtiWMeYQUh4NEk0hRADq/QH89dYT8ZJUL8D4A2CGiMwFmuKFqvrrNPdXpapb3HW3iMgRbvlROEEVV+uWdVRem6TcGHMoUaVCQ2jFwYezABoLAhTY7IaekmqI3AXUASVALq7em+x4hmZQnnzjIpNwhr6oqqqipqYmgypCXV1dxut2R9Ze6emW7VXfSDUxdkcjKT33Yn8ZpXW7qamp6Z7t1Qm5aq9UQ6S3qk7Mwv62iUh/txfSH9jultcCAxOWOxrY7JZXtymvccuPTrJ8Uqo6FZgKMHr0aK2urm5v0Q7V1NSQ6brdkbVXerpje4Xe3QJA72MGpfTc55f3pqJuPSOrq7tle3VGrtor1WMiL4tINkLkeZwD9Li/n0sov8o9S2sssM8d9noJmCgivdwD6hOBl9zHQiIy1j0r66qEbRljDhHxWQ39B5nVMC5SGqDUZjf0lIP2RNw36R8APxCRJqAFZzhJOzpDS0Qex+lF9BWRWpyzrO4BnhKRrwMfApe4i88ALgDWAw04X25EVXeLyJ3AYne5/1LV+MH6b+CcAVYKvOj+GGMOIQ1b3VkNe6d2YD1aVkl5xELESw4aIqqqIrJcVU892LJt1ru8nYfOTrYP4JvtbGcaMC1J+RLgpHTqZIzxlvjFFOMXVzyYWEWACg1C6ieKmhxLdThroYicltOaGGO6nfishkV9UhvOIhCggCix+sYc1sqkI9UD6xOA60VkI1DPx8NZI3JVMWPM4S9+McX4xRUPRnq4E1Ntsy8cekWqIXJ+TmthjOmW4lPjlh2ZWoj43NkN6zbbcRGvSPUCjB/kuiLGmO4nts/pUVT0T204K34AvnFbEPrmrFomDakeEzHGmOwLBmmhgMp+JSktbrMbeo+FiDEmf0JBggQoLErtItzF/ZwQadphIeIVFiLGmLzx1Yeo96V4ZhY2u6EXWYgYY/KmoCFIQ0FqB9UByqqcwIkfkDf5ZyFijMmbosYg4RSmxo2rGOAsG9trIeIVFiLGmLwpag7RVJT6cFZl32KaKbTZDT3EQsQYkzclzUGaS1PviRQUCkECELIvG3qFhYgxJm/KIkEiaYQIOLMb+mx2Q8+wEDHG5E15LESsPPXhLICGggAFDRYiXmEhYozJj2iUCq1DK9PriTQWBigKW4h4hYWIMSYvmnbVAaCB9EKkuThAcZOFiFdYiBhj8qJ1VsMe6Q1nNZcGKG22EPEKCxFjTF7Ub3GCwN8rvZ5ItLSSMpvd0DMsRIwxedG4zZ0at0+aIVIRoDxmIeIVFiLGmLxo2ukMZxWmOqthXGWAMhohEs1BrUy6LESMMXnRvNPpTZT0S68ngnsgPro3nO0qmQxYiBhj8iJ+Jd5UZzWMi89u2LzTQsQLLESMMXkR3esMZ8WvzJuqAjdEonssRLzAQsQYkxe6z+mJVA5IL0TiB+Ijuy1EvMBCxBiTH8EgjZRQ3qsordXisxvqvsZc1MqkyULEGJMXUheiTiqR1GbGbVVyhIWIl1iIGGPywlcfpN6X5plZJBxDCVmIeIGFiDEmLwobgjSkMathXHx2Q1+oIdtVMhmwEDHG5EVhU4hwYZpfNAQq+1cA4K+3EPECCxFjTF6UNAVpLk6/J1Jc6iNIJf4GCxEvsBAxxuRFaUuQljRnNYyr8wUobKzPco1MJixEjDF5URoNESlLfzgLnClyi5osRLzAQsQYkxcVsSCx8sx6Io2FAYqb6rJcI5MJCxFjTJfT5hZKCac9NW5cU1GA0mYLES+wEDHGdLn4rG45WQEAABm0SURBVIaS5qyGcc0lAUojFiJeYCFijOlyrbMa9sysJ9JSFqA8GspmlUyGLESMMV2uYasbIr0zC5FYWSUVFiKeYCFijOly4R3urIa9MhvO0soAFYRANZvVMhmwEDHGdLmmHU5PpDjdWQ3jAgH8xGjaY184zLe8hIiIbBSRlSKyXESWuGW9RWSWiKxzf/dyy0VEpojIehFZISKnJmznanf5dSJydT6eizEmfa1T4x6RWYiIeyylbnMwa3UymclnT2SCqo5U1dHu/VuA2ao6BJjt3gc4Hxji/kwCHgQndICfAp8CxgA/jQePMcbbInsym9UwLj67YfzYiskfLw1nXQQ85N5+CPhCQvnD6ngd6Cki/YFzgVmqultV9wCzgPO6utLGmPTF9jpv/uX9M+uJFLgH5Bu2WYjkW75CRIGZIrJURCa5ZVWqugXA/X2EW34UsClh3Vq3rL1yY4zHtU6N616RN13xYylN2y1E8q0gT/s9XVU3i8gRwCwRebuDZZPNe6YdlB+4ASeoJgFUVVVRU1OTZnUddXV1Ga/bHVl7pac7tVf9Rx9RRzlLFr2a0fo7dm5hFLBu6Up21/izW7nDVK5eX3kJEVXd7P7eLiLP4hzT2CYi/VV1iztctd1dvBYYmLD60cBmt7y6TXlNO/ubCkwFGD16tFZXVydb7KBqamrIdN3uyNorPd2pveb7HqXOF8j4+b4f3QB3wYCKXozrJm3WWbl6fXX5cJaIlItIZfw2MBFYBTwPxM+wuhp4zr39PHCVe5bWWGCfO9z1EjBRRHq5B9QnumXGGI8rqA9S78/w9F4+PiAf3WPDWfmWj55IFfCsiMT3/5iq/ktEFgNPicjXgQ+BS9zlZwAXAOuBBuBaAFXdLSJ3Aovd5f5LVXd33dMwxmSqIBwiXJDZmVkAlQOcdWP7LETyrctDRFXfB05OUr4LODtJuQLfbGdb04Bp2a6jMSa3isNBwkWd6In0KiZMMQTt0if55qVTfI0x3URJc5DmksxDRARCEkDqrCeSbxYixpguVxIJESnNfDgLoM5Xib/eQiTfLESMMV2uPBokUpZ5TwSg3l9JYYOFSL5ZiBhjupYqlRokluGshnGNBRUUhS1E8s1CxBjTpVrqmigkglR2bjirsaiC4mYLkXyzEDHGdKn4lXelR+d6Ik1FFZS2WIjkm4WIMaZLtU6N26tzIdJSUkZ51EIk3yxEjDFdqnG7892OggxnNYxrKS2nPGbfE8k3CxFjTJcKu1feLerbuZ5IpKycUsJEG5uzUS2TIQsRY0yXis9qWNy3cz2RaHkZAHVbrDeSTxYixpgu1bzLedMvrepcTyRW4YaITZGbVxYixpguFb/ybmdDhEApAI02u2FeWYgYY7pUfGrciv6dG86ih4WIF1iIGGO6lAZDxBAqqso7tR1/TydE4sdYTH5YiBhjupSEggQJ4C9INsN16nw9SwBo3mUhkk8WIsaYLuWvD9Lg6+RQFlDYuxiA6G4LkXyyEDHGdCl/Y4j6gk4eVAcK+7ghstdO8c0nCxFjTJcqagwSLux8iBT1KiKGQNB6IvlkIWKM6VLFTUGaijs/nOUvEIIEkJCFSD5ZiBhjulRxS4iWTkyNm6jeH8BnU+TmlYWIMaZLlUWCtJRmJ0Qa/AEKbHbDvLIQMcZ0qYpokFh554ezABqLAhTa7IZ5ZSFijOkyGlMqCKGdnBo3rqk4QHGThUg+WYgYY7pM4856fCgEshMiLSUBm90wzyxEjDFdJn7FXV+P7AxnRcoqKYtYiOSThYgxpss0bHO+GNjZqXHjYuUByqL2ZcODWfXgq4RvfISVUxdmfdsFWd+iMca0I37F3cI+2QkRrQxQSQiNxhC/fSZOZuXUhXzyhrMYRoTw5EdZySsMnzQua9u3VjfGdJmmHe7UuH2yM5xFIIAPpX57fXa2dxhqmPJnCoggQAEt7HqmJqvbtxAxxnSZpp3O0FPJEdnpifh6Otux2Q2Ta6rdwZC1f0cRWvDTQhF9Lq7O6j5sOMsY02Uiu7M0q6GroLeznYatQeCorGzzsKHK+jOu4bhYPTMufYjm91cw5LovZXUoCyxEjDFdKD41bllVdoaz4sdWbHbDA628bgrDN87g6er7+Y8nr6Smpobh1dkNELDhLGNMF4oFneGsyqOy0xMp7udsJ36spautnLqQmnN/kZOznjpjy4w3+be//ICaHhfy+RdvyOm+rCdiTBasnLqQXc/U0Ofi6qwPFxxWgkGaKaSkR3FWNhc/tpKPKXJXTl3I8ZPP4kSaaJpZwkpme+Jv37KnjpaLv8xO6ccnZk2juKRzM0gejPVEjOmkBd9+gqGTz+DMmT/ihMlnsvS/Z3V5Hbz6ibgtXyhInVQivuy8scWHxSJdPLuhKuz56X2UEMaPUkoju+58gGhEu7QeybxV/R2ODq/j7dse4djT+uR8fxYixmRo9WNvsWDgZYz73eX4iSJAES2ceMvnmV/9Y/a9t7NL6rHij68xZPIEPjPzxxw3+WxPB4m/IUS9LztDWQDl/Z1tdeXshns3N/DK4K8zfuv/EcNHBB+KUF37CK/3+XfeemZ9l9WlrRU/eoLRK6bxwsjbOPvO6i7Zp4WI8TSvfcJWhcW/W8jrfT/HsK+OZHjti7x27JWEKaEFP2GKWRMYx6fn3k3B8cewYOx/snv1lpzVZ+kflxD45pWU0EQBMUpoZPt9j6D5/0CcVEFDkIYszGoYV9Hf6YmUz/lHl7xG1jy9hu2DxjDhg7/y+tm3sfr3Ncyf+HNW/34uy668jxGh+XzyP4bxz1N+ws4PG3Jen0Q7Fm1g0D2TebN0HGfP/WnX7VhVu9XPqFGjNFNz5sw56DKrfjtL3xj3HV3xwLyM9+M1K/74ms6ZeLeu+ONraa2XSnu1p6VF9dFvLtBGijWCaCPF+qdrXtWdOzPeZKdEWmI69/aXdVHlBFXQXdJHXz33Tt33wR5VPbCN1jy9Wms+cYW24NdGinXByG/o9kUbOtxHOu317ovrde6AL6uC7qGHNlGoEURjTs7poopqnfODGdrSHMv0KacsndfHsp4T9K3K07Oy3zlz5uiKP76mMdAYaJhifev3ufm/i8VUX77ir1pPqe7w9dNV981Mulzwnc26ZOgVqqAf+I7RFyc/q5GW3P8NouFmXd1jrO4loO+8tCHpMp35f1RVBZZokvfUvL+pd/VPVkMkFtOmdzfqe3c+qivO/KZuKhvS+k8cA337iM/o2humaP2SNc6r8BATfG+7vnrez7WZAo0i2kShLph4h37w2HwNr9+kGonst3zbN5NMXrTr3o3p7762TKdU3Ko76e28RN2fBor1H3xO7x90rz70rUW6+q2WjJo1rTe9+xfonE9cqW8XnKgKutXfXxde8isN7wyltK93ZqzX2cddp00UajMF+vrQa3TRTU8k3X8q7bV15XadPexGbaJQ6ynV+RNu04at+1qf0/JfzdKFl/5aNxccrQq6tnC4zrzyYa3b05xSfRMla6f6D3fq+48v1OU3/a8uvuB2fevIc1rDqwW/Lhr9Dd308Csa3bMv6TZXl43WRf3OT7suycyZM0fnTLxbo0jrayRIhb46+rv67hNLs/Y/t++jkNYMukoVdHnvCbpz5eaDrrPhobn6XvlJqqCvBs7XN/9vnb72murdd6u+lt5nsZTMP/NHqqCzJz3R7jIWIu38AOcB7wDrgVsOtnymIbLij6/p30Z9Txde92dd9JX7dOlxl+i2wgGtL94Q5bpejmt9QUeR/d4EtxcO0CUnXaVv/ufDGnxnc+s2M/mEnyt7NuzRN370d517yrf13dLhrXWPJbyRJ/40Uai1Jcfq6qoJuvyoC7SZgtZew+pfPKdzXnklpf3W18X0H/+1TP934K36LsergkbEr7VHj9EmCrUFv4Yp0g0jL9Qdvf+tdf/7qNSa0vP0+U//Qhf9ZoG+OWXux+0ZiWjT9r26661N+uG/Vus7D7+ub/3PTJ17/t3aRJFG3FCcd+xV+srwb+uc476m8/pfqq/1ukAXl4/XFUWn6ocMbH3uUURf+8zNGqlrzKht35u7SWcO/baGKWr95NxMgb468We64433VGOxDv/JQ1vrdPaEO3UflRrBpwuGXac7V3zU7vLRxiZd+p2H9L3SYaqgm3wD9cVz79PX73456WuupUV16/v1uvZfG3XRA4v1pXP/R8MUaRTRFvy6rmio7pZe+/39o4juoqdGk7xOoohuLPukLhp6lb5x1e/0/Sfe0Eh9WDf5B+rbxcOz8pqP90TqKdVm9zWypPJMDVOkCrq+eKjOmXiXbnp1Q8b7ePupt3R94Sc1iuirZ/9Uo82Rg6/kijU169Ir79OgVGqYIv0r1+j9fFOv8T+sT/x0ja742zpd+6+N+t6rH+mmZdt129u7dW9tSJfdV6OvnHNXym208rezNYro7GO/3mFu5ipExHns0CQifuBd4BygFlgMXK6qa9pbZ/To0bpkyZK09rNy6kJOnPwZfMSIn1PyoXyCd/udTt3wT1M+8XSGfGk4wVcWc/zksymkmRaKWP3b2TT2OJKdT75Mj0Uvc/Ku2fRll7N+wWD6RzbhI0qUAl4dfxuFp47AV1qMlJbgKyvBX16Cr7SYPa+tQZcsxX/Gpxnw+dH4Cnz4CwR/gSA+ab3t8wsbn1lCw6wFVF4wnuOvHIf4ffgKfB//9jnLrZ22kN1PvASVAWTrFo5cO4dPNi7Dh9JAKWt6ns7eU89C+vVl3JPfaX1OC679M9KrF+F3PkA+2EjJtg/ose8DhjSvIkCIxHNuGqSMbWXHsrfPcTQNPA7fvx1P+fDj6Df2OLa+uZndf3iK2J59HLN5IcfF1hPBz4fHn0WvSZfS69ovQN++yU+d3bKFXc/OY+uTc6lYNpdj6pw/d+IrOdVzf2IIIQnQ4KsgXFBBc2EFzcUVREsqKN+zieMaVuJDacHPgol3Uv3SrWm9dtqaPfY2JrzxC3womlDPXb6+vNtjJJFTP0PFWWM49rLT+HD2OnY+9QqEggxd8r8cGdvC4gEXccRffsEx5w1NbYeqrPnVi0Tv+SXDd81FAUWI4WNl2Vh8sRZ6NO+gb2w7FSS//pQCHxQcx8YhE4kMOp7CE4dQeeoQqsYOZtesZRx//cev+QXf/xvRGLS8tphe7y7i+L2LOZJtALTgp4AoihCmhPf+2LlTYmtqaqiurj7gNbLz3d2suuNper7wCCODrwKwPHAGu08YhxQU0POycxl54xkdniG24o+v0fCzezllywz2+nqz7VePMuK7Z2VUz7p1W1j1qWsZu+ellNeJ/53Wlo1m1zGnEBt0LMUnDKbHKcfS/9OD6Xlsb8QnLL7zRU64/TKC/l4ENq0h0L+83W3G2ytTIrJUVUcfWFkP9CYy/QHGAS8l3L8VuLWjdTLpiSR2mSP4dNaoH7YdyWnVUe+iPhTVN/6wTF86+7/1/YLj2/2En+ufaJtPjU0U6PKe43VO9R365m/nauPecMrPKW7Z751PhE6voVhfGPp9/b+Bk3Vuzwv1bf+J2kDJfnXYb9gvMFrfvmmqxrbvSPtvo6pav3G7Lhj8lf16gYt7nKWzzvsfnf3lqVpz/eO64LYXdMl987Tmaw9pAyXajF/rKdW3/rCg3e0mfsqtpzQrn57bbvPlK/+qsy5+UOcMvlbXFgzdb2gmmnCM4+3i4bryD/M7te9Xh39jv17DVn9/fbNqor4x5Kv62tjv6Wufv1sXTf6zrvj5c7rw61P3a6eOnntHr4+W5pi+PetDnfOtp3VZYHzr/pvx65yJd3fq+aTyyfrDeRt0zjl36Sb/Mfu9/iKIhijXHdJXa/0D9f2if9O3S0/WlZVjdU3ZqRrB1/r//sbt/+xUPVVVN15/t7YkbPPtkZfpsu89rIuu/4u+fu0fdMFXfqevfunXuqjfeRpJeB1/5Dtad0jfA/6H91GpGwqOb32NNFJ80Ndnrnoih/qXDY8CNiXcrwU+le2d9Lm4mvDMktZPW1WTLsLvT77s8EnjoJ1PV2UVPsZMPgUmn8LKqWfQ6PZaIhTy1nf/ypHVQ4nUhYk1NhGtDxNrCBP60+OctuEp/MSI4mPZURci552LRmPEYgoxJRZTNKoU1szk1G0z8KNEEd7sdy7hMWeisRgSi4H7U7FsHiP21CAoEXy8NuHHVL/S/tkcHT2nuFNuGMfKgtmtnwgvmDSOmpoaxruffBrqYry7eAu7F60n9pspjN36rLt/P1vGfonqe69L6W+RtF2P6UflLd8iPPnZ1r9R8S9/zmeT1vkMVn5qSGs9R3TwvIZPGsdKZmf1S4Rtt3l26zavp6amhqpjR/HeU0vRe/6bUbv+hQARfGwe/2UmTD69U/vu8a0rCU+e3tpG2x94hpEdPKeVY05K6bl39PooKBRO+OxATvjsQFYOH0A4oaee7QsBJjPwjEEMnPkj5kyEI2f9hAJiRBFW9DyT4HGnII2N+Joa8TU34m9upKC5kaqGdfiIAU5PteH1FcC/d6oex1xVTfSvxUSbm6GoiBMe+A6MO7DNVk4dS9Pkua1ttOvBpxg+aRzBj0JseW0De5ZtILx2A2x4n4FrZyEoAviJOFfnzcOXHQ/14axLgHNV9f+5968ExqjqjW2WmwRMAqiqqhr1xBNPpL2vnf/YiM55C5lwMn0/P6jTdY9v0zdvNbHxw9rd5s5/bOSCX9/Q+qKa8f0HOr1sOtvsjLq6OioqKrps/6m0p5clttfh2kbZ3H97r6/29pvt/6N0BVavpufy5ewdOZLgsGEd1jWVNkq3num0VzITJkyw4SzNcDgrrrPdwUylcwA+1WW74qB+R+3ltZMKvKBte1kbdSzd/8dc/B/lWzr1tAPrSYhIAc6B9bOBj3AOrH9FVVe3t04mB9bjOntgqrux9kqPtVd6rL3Sk6sD64f0MRFVjYjIt4CXAD8wraMAMcYYk12HdIgAqOoMYEa+62GMMd2RXTvLGGNMxixEjDHGZMxCxBhjTMYsRIwxxmTskD7FNxMisgP4wL3bA9jXZpHEsraP9wVyNdNQsrpka52OlmvvsYO1TXtlifetvay9rL3SW87L7XWMqvY7oDTZl0e6yw8wtaOyto/TzpdtclWXbK3T0XLtPXawtumgjRLbz9rL2sva6zBvr+4+nPWPg5QlezxXMtlXqut0tFx7jx2sbdor66o2s/ZKj7VXeqy9UtTthrM6Q0SWaLJrx5ikrL3SY+2VHmuv9OSqvbp7TyRdU/NdgUOMtVd6rL3SY+2Vnpy0l/VEjDHGZMx6IsYYYzJmIWKMMSZjFiLGGGMyZiGSRSJSLiJLReRz+a6L14nIUBH5g4g8LSLfyHd9vE5EviAifxKR50RkYr7r43UicqyI/EVEns53XbzKfb96yH1dfTXT7ViIACIyTUS2i8iqNuXnicg7IrJeRG5JYVM/BJ7KTS29IxvtpaprVfV64FLgsD5NM0vt9XdVvQ64Brgsh9XNuyy11/uq+vXc1tR70my7LwFPu6+rCzPdp4WIYzpwXmKBiPiB3wPnAycCl4vIiSIyXET+2ebnCBH5LLAG2NbVlc+D6XSyvdx1LgTmA7O7tvpdbjpZaC/Xj931DmfTyV57dTfTSbHtgKOBTe5i0Ux3eMhPSpUNqjpPRAa1KR4DrFfV9wFE5AngIlX9BXDAcJWITADKcf5IjSIyQ1VjOa14nmSjvdztPA88LyIvAI/lrsb5laXXlwD3AC+q6rLc1ji/svX66o7SaTugFidIltOJDoWFSPuO4uOUBqfBP9Xewqp6G4CIXAPsPFwDpANptZeIVON0p4vpnjNTptVewI3AZ4EeInK8qv4hl5XzoHRfX32Au4BTRORWN2y6q/babgpwv4j8O524PIqFSPskSdlBv5mpqtOzX5VDQlrtpao1QE2uKnMISLe9puD803dX6bbXLuD63FXnkJK07VS1Hri2sxu3YyLtqwUGJtw/Gticp7ocCqy90mPtlR5rr8zltO0sRNq3GBgiIoNFpAj4MvB8nuvkZdZe6bH2So+1V+Zy2nYWIoCIPA4sBE4QkVoR+bqqRoBvAS8Ba4GnVHV1PuvpFdZe6bH2So+1V+by0XZ2AUZjjDEZs56IMcaYjFmIGGOMyZiFiDHGmIxZiBhjjMmYhYgxxpiMWYgYY4zJmIWIMZ0kInVZ2s4dInJTCstNF5H/yMY+jeksCxFjjDEZsxAxJktEpEJEZovIMhFZKSIXueWDRORtEfmziKwSkUdF5LMiskBE1onImITNnCwir7jl17nri4jcLyJr3MvmH5Gwz9tFZLG73anuJeON6TIWIsZkTxj4oqqeCkwAfpXwpn488FtgBPBJ4CvAZ4CbgB8lbGME8O/AOOB2ERkAfBE4ARgOXAd8OmH5+1X1NFU9CSjF5tYwXcwuBW9M9ghwt4iMB2I48zhUuY9tUNWVACKyGpitqioiK4FBCdt4TlUbcSY2m4MzodB44HFVjQKbReSVhOUniMgPgDKgN7CaTswNYUy6LESMyZ6vAv2AUaraIiIbgRL3saaE5WIJ92Ps/3/Y9mJ22k45IlICPACMVtVNInJHwv6M6RI2nGVM9vQAtrsBMgE4JoNtXCQiJe7MfNU4l/GeB3xZRPwi0h9nqAw+DoydIlIB2BlbpstZT8SY7HkU+IeILMGZt/rtDLaxCHgB+ARwp6puFpFngbOAlcC7wFwAVd0rIn9yyzfiBI4xXcouBW+MMSZjNpxljDEmYxYixhhjMmYhYowxJmMWIsYYYzJmIWKMMSZjFiLGGGMyZiFijDEmYxYixhhjMvb/AcmNFiwK6+kHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "sub_size = 5000\n",
    "indexes = np.arange(tX.shape[0])\n",
    "np.random.shuffle(indexes)  \n",
    "sub_index = indexes[:sub_size] \n",
    "x_sub = tX[sub_index,:]\n",
    "y_sub = y[sub_index]\n",
    "\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 1\n",
    "    degree = 6\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 0, 30)\n",
    "    #lambdas =[0.1, 0.01, 1]\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y_sub, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        loss_tr, loss_te = cross_validation(y_sub, x_sub, k_indices, k_fold, lambda_, degree)\n",
    "        rmse_tr.append(loss_tr)\n",
    "        rmse_te.append(loss_te)\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # cross validation: TODO\n",
    "    # ***************************************************    \n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=0.9198964473372747, loss=0.49991543478685996, w0=-3.1381702249274604e-05, w1=-1.870332018899931e-05\n",
      "Gradient Descent(1/199): ||gradient||=0.9186848945865361, loss=0.4997636980245322, w0=-8.78042153832626e-05, w1=-5.236507656267239e-05\n",
      "Gradient Descent(2/199): ||gradient||=0.9165086790795948, loss=0.4994921215723027, w0=-0.00018915591588298592, w1=-0.00011294257728487969\n",
      "Gradient Descent(3/199): ||gradient||=0.9126063083581455, loss=0.4990082616199879, w0=-0.000370914263529803, w1=-0.0002219378872869192\n",
      "Gradient Descent(4/199): ||gradient||=0.9056299309807633, loss=0.49815319084166065, w0=-0.0006959041130019329, w1=-0.00041798666987893115\n",
      "Gradient Descent(5/199): ||gradient||=0.8932269289430593, loss=0.49666407351067016, w0=-0.0012739014639035493, w1=-0.0007704142328603178\n",
      "Gradient Descent(6/199): ||gradient||=0.8713980371643467, loss=0.49413764114400255, w0=-0.002292032255140569, w1=-0.0014033062129404496\n",
      "Gradient Descent(7/199): ||gradient||=0.8336914854870378, loss=0.49004496000892644, w0=-0.004054621864207936, w1=-0.0025378020652307696\n",
      "Gradient Descent(8/199): ||gradient||=0.7708210576370715, loss=0.48392417182397535, w0=-0.007012143781032614, w1=-0.0045650758007922156\n",
      "Gradient Descent(9/199): ||gradient||=0.6730593787686956, loss=0.4758746897312079, w0=-0.011703585013512112, w1=-0.008168591631344086\n",
      "Gradient Descent(10/199): ||gradient||=0.5420469001358224, loss=0.4668314646613726, w0=-0.018440239398187373, w1=-0.014520131670101091\n",
      "Gradient Descent(11/199): ||gradient||=0.417585276699318, loss=0.45684219087207145, w0=-0.0266511349012321, w1=-0.02557881435937765\n",
      "Gradient Descent(12/199): ||gradient||=0.34951950406858123, loss=0.4441675056514142, w0=-0.03507313894297936, w1=-0.044530615642647674\n",
      "Gradient Descent(13/199): ||gradient||=0.2802581748299309, loss=0.4305010778316688, w0=-0.044834692707357696, w1=-0.07630731446605377\n",
      "Gradient Descent(14/199): ||gradient||=0.19933661937191804, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(15/199): ||gradient||=0.2126484850736893, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(16/199): ||gradient||=0.2126484850736893, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(17/199): ||gradient||=0.41005196201736743, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(18/199): ||gradient||=0.41005196201736743, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(19/199): ||gradient||=0.3562131324819593, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(20/199): ||gradient||=0.3562131324819593, loss=0.4119259421855884, w0=-0.03309942831257582, w1=-0.16844869171449792\n",
      "Gradient Descent(21/199): ||gradient||=0.12649027507232175, loss=0.41025030347357094, w0=-0.029575140439970622, w1=-0.17946578929166102\n",
      "Gradient Descent(22/199): ||gradient||=0.12292495119423105, loss=0.40743339281067326, w0=-0.023442494523926122, w1=-0.19876354675726643\n",
      "Gradient Descent(23/199): ||gradient||=0.11680132506627634, loss=0.4029471327448956, w0=-0.01278468241212367, w1=-0.23182458833403902\n",
      "Gradient Descent(24/199): ||gradient||=0.10674668689952199, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(25/199): ||gradient||=0.11037853434564666, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(26/199): ||gradient||=0.11037853434564666, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(27/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(28/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(29/199): ||gradient||=0.30949111243344546, loss=0.3934972452046127, w0=0.012645111284258674, w1=-0.3149399904670085\n",
      "Gradient Descent(30/199): ||gradient||=0.08518921631593725, loss=0.3928422773768506, w0=0.01390331604737596, w1=-0.3215594212339835\n",
      "Gradient Descent(31/199): ||gradient||=0.08143325683778938, loss=0.39176458577149614, w0=0.01769426852847459, w1=-0.333137608291744\n",
      "Gradient Descent(32/199): ||gradient||=0.08261374022271092, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(33/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(34/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(35/199): ||gradient||=0.15244231978895387, loss=0.38970194899647265, w0=0.023115513292314566, w1=-0.35613896221101615\n",
      "Gradient Descent(36/199): ||gradient||=0.07231686370955237, loss=0.38924702709110287, w0=0.024867215085094988, w1=-0.36144019878751626\n",
      "Gradient Descent(37/199): ||gradient||=0.0693800954973171, loss=0.3884863243303348, w0=0.026866841411884168, w1=-0.3708048228655269\n",
      "Gradient Descent(38/199): ||gradient||=0.06849694468173842, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(39/199): ||gradient||=0.09598461127388905, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(40/199): ||gradient||=0.09598461127388905, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(41/199): ||gradient||=0.1410714760751551, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(42/199): ||gradient||=0.1410714760751551, loss=0.3864215399793017, w0=0.0350727177951097, w1=-0.39921296841299686\n",
      "Gradient Descent(43/199): ||gradient||=0.08456831532148283, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(44/199): ||gradient||=0.11635687063747818, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(45/199): ||gradient||=0.11635687063747818, loss=0.3855801463907678, w0=0.037564023270261745, w1=-0.4104639083541116\n",
      "Gradient Descent(46/199): ||gradient||=0.07098725686300805, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(47/199): ||gradient||=0.08865280508918541, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(48/199): ||gradient||=0.08865280508918541, loss=0.38485406384997894, w0=0.0397988301803087, w1=-0.4207754388932331\n",
      "Gradient Descent(49/199): ||gradient||=0.05956788893440248, loss=0.38448192908571455, w0=0.0402881948329062, w1=-0.4270292492982512\n",
      "Gradient Descent(50/199): ||gradient||=0.06633820597340725, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(51/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(52/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(53/199): ||gradient||=0.15761065003057645, loss=0.38370536779400194, w0=0.044360770783240404, w1=-0.4394259860437441\n",
      "Gradient Descent(54/199): ||gradient||=0.06222099110230357, loss=0.38348800700174634, w0=0.044301326033474996, w1=-0.44236759427566763\n",
      "Gradient Descent(55/199): ||gradient||=0.04860668539425892, loss=0.383189073110083, w0=0.04591786180813736, w1=-0.4475058466448936\n",
      "Gradient Descent(56/199): ||gradient||=0.04885484002764845, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(57/199): ||gradient||=0.07218281893711131, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(58/199): ||gradient||=0.07218281893711131, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(59/199): ||gradient||=0.0865949367220621, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(60/199): ||gradient||=0.0865949367220621, loss=0.38229970311486866, w0=0.04905354484341635, w1=-0.4634154097612027\n",
      "Gradient Descent(61/199): ||gradient||=0.045554523575472565, loss=0.38208015281211827, w0=0.05059222337498455, w1=-0.46760109124772614\n",
      "Gradient Descent(62/199): ||gradient||=0.046707216525688364, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(63/199): ||gradient||=0.08067338628463173, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(64/199): ||gradient||=0.08067338628463173, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(65/199): ||gradient||=0.09181351733579902, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(66/199): ||gradient||=0.09181351733579902, loss=0.38140101837396745, w0=0.052993763875687355, w1=-0.48064650178588253\n",
      "Gradient Descent(67/199): ||gradient||=0.040172136024456635, loss=0.38123091391261776, w0=0.05420974414245746, w1=-0.48409726949739024\n",
      "Gradient Descent(68/199): ||gradient||=0.04011639835467928, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(69/199): ||gradient||=0.0587199843578713, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(70/199): ||gradient||=0.0587199843578713, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(71/199): ||gradient||=0.06065901120024391, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(72/199): ||gradient||=0.06065901120024391, loss=0.38070675484703786, w0=0.05629675863399411, w1=-0.4948880697081792\n",
      "Gradient Descent(73/199): ||gradient||=0.03437710169565648, loss=0.3805724543076369, w0=0.05709337174035709, w1=-0.4977650896919669\n",
      "Gradient Descent(74/199): ||gradient||=0.03371872029255384, loss=0.380347504802962, w0=0.057923161002275445, w1=-0.502821417967265\n",
      "Gradient Descent(75/199): ||gradient||=0.0353360741618754, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(76/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(77/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(78/199): ||gradient||=0.0868549972476703, loss=0.37990446055546606, w0=0.06025622386799663, w1=-0.512732449768847\n",
      "Gradient Descent(79/199): ||gradient||=0.03013797122742819, loss=0.3798059156030959, w0=0.06086479596353714, w1=-0.5149947769707354\n",
      "Gradient Descent(80/199): ||gradient||=0.029582646754048075, loss=0.3796377325021035, w0=0.06155759036056807, w1=-0.5189768598962371\n",
      "Gradient Descent(81/199): ||gradient||=0.02988603821652102, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(82/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(83/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(84/199): ||gradient||=0.05385331845836831, loss=0.37930748989144936, w0=0.06340685859282029, w1=-0.5268228016896819\n",
      "Gradient Descent(85/199): ||gradient||=0.026807087764073437, loss=0.379232343212017, w0=0.06379460699418472, w1=-0.5286305900589766\n",
      "Gradient Descent(86/199): ||gradient||=0.026429755894928714, loss=0.37910158102876307, w0=0.06449936504431049, w1=-0.5318081753946006\n",
      "Gradient Descent(87/199): ||gradient||=0.025779232451409895, loss=0.37887998603582906, w0=0.06565536813188565, w1=-0.5372914115868667\n",
      "Gradient Descent(88/199): ||gradient||=0.02479156357937694, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(89/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(90/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(91/199): ||gradient||=0.03258988562878005, loss=0.37847536538605114, w0=0.06774134339187216, w1=-0.5477141177050802\n",
      "Gradient Descent(92/199): ||gradient||=0.027738362406501706, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(93/199): ||gradient||=0.03951405284074884, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(94/199): ||gradient||=0.03951405284074884, loss=0.37834206727202646, w0=0.06842349019205861, w1=-0.5511561942990433\n",
      "Gradient Descent(95/199): ||gradient||=0.0304225158131295, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(96/199): ||gradient||=0.046421773810511664, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(97/199): ||gradient||=0.046421773810511664, loss=0.3782192101919261, w0=0.06906497913554238, w1=-0.5543145897328704\n",
      "Gradient Descent(98/199): ||gradient||=0.03256348800089378, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(99/199): ||gradient||=0.05053199788292059, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(100/199): ||gradient||=0.05053199788292059, loss=0.37810265907289226, w0=0.06968905597825942, w1=-0.5572191567845676\n",
      "Gradient Descent(101/199): ||gradient||=0.032866606098295975, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(102/199): ||gradient||=0.049904511233416445, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(103/199): ||gradient||=0.049904511233416445, loss=0.37799031022315804, w0=0.07030697705295337, w1=-0.5598962692587653\n",
      "Gradient Descent(104/199): ||gradient||=0.030864528540993398, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(105/199): ||gradient||=0.044474850521653266, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(106/199): ||gradient||=0.044474850521653266, loss=0.3778831177632533, w0=0.0709143275158598, w1=-0.5623694962267904\n",
      "Gradient Descent(107/199): ||gradient||=0.02725555397893965, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(108/199): ||gradient||=0.036174660635675905, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(109/199): ||gradient||=0.036174660635675905, loss=0.3777832329470302, w0=0.07149562027576611, w1=-0.5646599120664837\n",
      "Gradient Descent(110/199): ||gradient||=0.023491643257352396, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(111/199): ||gradient||=0.02798541531932825, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(112/199): ||gradient||=0.02798541531932825, loss=0.37769151450075156, w0=0.07203435972166758, w1=-0.5667861513379303\n",
      "Gradient Descent(113/199): ||gradient||=0.020796448379199806, loss=0.3776393572545536, w0=0.07258948801176464, w1=-0.568075243409437\n",
      "Gradient Descent(114/199): ||gradient||=0.02232622559275806, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(115/199): ||gradient||=0.044569960234299495, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(116/199): ||gradient||=0.044569960234299495, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(117/199): ||gradient||=0.06049611094726945, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(118/199): ||gradient||=0.06049611094726945, loss=0.3774734104704165, w0=0.07307154647503165, w1=-0.5721438415511358\n",
      "Gradient Descent(119/199): ||gradient||=0.02560307349675981, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(120/199): ||gradient||=0.030235195825791275, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(121/199): ||gradient||=0.030235195825791275, loss=0.3773936500349805, w0=0.0735942921530556, w1=-0.5738034701868857\n",
      "Gradient Descent(122/199): ||gradient||=0.01930338988021067, loss=0.3773502513239982, w0=0.07402822791850343, w1=-0.5748167485368086\n",
      "Gradient Descent(123/199): ||gradient||=0.019883352701387243, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(124/199): ||gradient||=0.03180432497976459, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(125/199): ||gradient||=0.03180432497976459, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(126/199): ||gradient||=0.0368369701461698, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(127/199): ||gradient||=0.0368369701461698, loss=0.3772087103489772, w0=0.07452601579779915, w1=-0.5780354108422809\n",
      "Gradient Descent(128/199): ||gradient||=0.018672376155633576, loss=0.3771703336844059, w0=0.07491495902096323, w1=-0.5789005133364988\n",
      "Gradient Descent(129/199): ||gradient||=0.01894887411582462, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(130/199): ||gradient||=0.027883181430691767, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(131/199): ||gradient||=0.027883181430691767, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(132/199): ||gradient||=0.029631455078381125, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(133/199): ||gradient||=0.029631455078381125, loss=0.37704490546502617, w0=0.07536964604927561, w1=-0.5816628169500092\n",
      "Gradient Descent(134/199): ||gradient||=0.01724997691647665, loss=0.3770105401718543, w0=0.07562881667941775, w1=-0.5824123704142081\n",
      "Gradient Descent(135/199): ||gradient||=0.017165877881501656, loss=0.37695177088913756, w0=0.07574484702850508, w1=-0.5837476546228534\n",
      "Gradient Descent(136/199): ||gradient||=0.01899421714878402, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(137/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(138/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(139/199): ||gradient||=0.054220090727918, loss=0.37682644524415465, w0=0.07640556694575028, w1=-0.5864102724253792\n",
      "Gradient Descent(140/199): ||gradient||=0.016650580776069446, loss=0.3767965645111003, w0=0.07664467148671401, w1=-0.5870356600147364\n",
      "Gradient Descent(141/199): ||gradient||=0.016501858917438078, loss=0.376745477937799, w0=0.07671846419935624, w1=-0.5881547717783094\n",
      "Gradient Descent(142/199): ||gradient||=0.01816804164140217, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(143/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(144/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(145/199): ||gradient||=0.04825610442092132, loss=0.3766356523318643, w0=0.07732128008506954, w1=-0.5903961372206211\n",
      "Gradient Descent(146/199): ||gradient||=0.01563931340254658, loss=0.3766094801974734, w0=0.07745336774852776, w1=-0.5909299030113423\n",
      "Gradient Descent(147/199): ||gradient||=0.01555626373968682, loss=0.37656297807301164, w0=0.07763617808606241, w1=-0.5918788948154293\n",
      "Gradient Descent(148/199): ||gradient||=0.015451985463614959, loss=0.3764819326011553, w0=0.07809347822581313, w1=-0.5935397672728017\n",
      "Gradient Descent(149/199): ||gradient||=0.016210974608243976, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(150/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(151/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(152/199): ||gradient||=0.0572020732177142, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(153/199): ||gradient||=0.04260833075213723, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(154/199): ||gradient||=0.04260833075213723, loss=0.37630531695231634, w0=0.0788118352881565, w1=-0.5970633924846425\n",
      "Gradient Descent(155/199): ||gradient||=0.014908211767569006, loss=0.3762840666377174, w0=0.0788347473226203, w1=-0.5974755644658697\n",
      "Gradient Descent(156/199): ||gradient||=0.014708048925549026, loss=0.3762467257517887, w0=0.07906897984336392, w1=-0.5982031234845887\n",
      "Gradient Descent(157/199): ||gradient||=0.014963885063396522, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(158/199): ||gradient||=0.021972572186543523, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(159/199): ||gradient||=0.021972572186543523, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(160/199): ||gradient||=0.03521861893866122, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(161/199): ||gradient||=0.03521861893866122, loss=0.3761362201008082, w0=0.07924611088708965, w1=-0.6005349176900081\n",
      "Gradient Descent(162/199): ||gradient||=0.02351622446106723, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(163/199): ||gradient||=0.03747859183540602, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(164/199): ||gradient||=0.03747859183540602, loss=0.3760848467338232, w0=0.07945208937262828, w1=-0.6015121960900223\n",
      "Gradient Descent(165/199): ||gradient||=0.023370968435216157, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(166/199): ||gradient||=0.03591853609344955, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(167/199): ||gradient||=0.03591853609344955, loss=0.37603359618573673, w0=0.07967763478331942, w1=-0.6024388052305133\n",
      "Gradient Descent(168/199): ||gradient||=0.021481809200450213, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(169/199): ||gradient||=0.030995904539739747, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(170/199): ||gradient||=0.030995904539739747, loss=0.3759837423493009, w0=0.07991361406706003, w1=-0.6033190135612887\n",
      "Gradient Descent(171/199): ||gradient||=0.018635502628599573, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(172/199): ||gradient||=0.0245163904254113, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(173/199): ||gradient||=0.0245163904254113, loss=0.3759366863297597, w0=0.08014500940912765, w1=-0.6041568478334536\n",
      "Gradient Descent(174/199): ||gradient||=0.01599958312904941, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(175/199): ||gradient||=0.018751655043872835, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(176/199): ||gradient||=0.018751655043872835, loss=0.3758928304184189, w0=0.0803586612622116, w1=-0.6049558909348881\n",
      "Gradient Descent(177/199): ||gradient||=0.014338863123069666, loss=0.3758671970659703, w0=0.08062719990283188, w1=-0.6054485791833413\n",
      "Gradient Descent(178/199): ||gradient||=0.015200543414685935, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(179/199): ||gradient||=0.027969392552206655, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(180/199): ||gradient||=0.027969392552206655, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(181/199): ||gradient||=0.036968997621708515, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(182/199): ||gradient||=0.036968997621708515, loss=0.375783531496386, w0=0.08070975465583485, w1=-0.6070644065150398\n",
      "Gradient Descent(183/199): ||gradient||=0.0167182053280009, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(184/199): ||gradient||=0.018986866136306145, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(185/199): ||gradient||=0.018986866136306145, loss=0.3757441578195452, w0=0.08094097589901549, w1=-0.6077511395045695\n",
      "Gradient Descent(186/199): ||gradient||=0.013572183029778981, loss=0.37572153696885513, w0=0.08114010120247711, w1=-0.6081779108393255\n",
      "Gradient Descent(187/199): ||gradient||=0.013800818079967348, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(188/199): ||gradient||=0.019086297838013175, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(189/199): ||gradient||=0.019086297838013175, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(190/199): ||gradient||=0.02127896202154825, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(191/199): ||gradient||=0.02127896202154825, loss=0.3756473456718056, w0=0.08130696324519089, w1=-0.6095769393921033\n",
      "Gradient Descent(192/199): ||gradient||=0.013232837805350147, loss=0.3756267043461724, w0=0.08148075967979389, w1=-0.6099657476358593\n",
      "Gradient Descent(193/199): ||gradient||=0.013302247084608905, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(194/199): ||gradient||=0.016545311030577412, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(195/199): ||gradient||=0.016545311030577412, loss=0.3755736084999899, w0=0.08179841617225363, w1=-0.6110356901740174\n",
      "Gradient Descent(196/199): ||gradient||=0.017100034533504855, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(197/199): ||gradient||=0.03414575672935005, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(198/199): ||gradient||=0.03414575672935005, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n",
      "Gradient Descent(199/199): ||gradient||=0.03491003301262125, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=3.819894318273993, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(1/99): |gradient|=3.313465869115332, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(2/99): |gradient|=2.7465500598415247, loss=0.5231169592050283, w0=0.006188461831573778, w1=-0.003969062074416842\n",
      "SGD(3/99): |gradient|=3.2750457421264016, loss=0.49026883819335143, w0=-0.0036366473237117534, w1=-0.008645873148768043\n",
      "SGD(4/99): |gradient|=3.648974628785057, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(5/99): |gradient|=3.4494660744980905, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(6/99): |gradient|=3.143628555043909, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(7/99): |gradient|=2.8915114659527927, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(8/99): |gradient|=2.897412774883511, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(9/99): |gradient|=2.1737890066966687, loss=0.4714443822232635, w0=-0.019904464199047903, w1=-0.02634329478613387\n",
      "SGD(10/99): |gradient|=5.937595790584886, loss=0.4709996675291747, w0=-0.019421647326106496, w1=-0.026244718166632628\n",
      "SGD(11/99): |gradient|=2.677548444145319, loss=0.47095357270473925, w0=-0.02081102663905905, w1=-0.026713517117417356\n",
      "SGD(12/99): |gradient|=12.171538978449524, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(13/99): |gradient|=6.554489780166215, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(14/99): |gradient|=3.1860265668993417, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(15/99): |gradient|=4.980416065195217, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(16/99): |gradient|=8.601456112840077, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(17/99): |gradient|=2.507560742564985, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(18/99): |gradient|=3.39722270862085, loss=0.46639024949469604, w0=-0.0204532765809344, w1=-0.027407190321897496\n",
      "SGD(19/99): |gradient|=5.7959604991936695, loss=0.46636056728990233, w0=-0.019964622091377646, w1=-0.02757084427383505\n",
      "SGD(20/99): |gradient|=2.6503861820488868, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(21/99): |gradient|=4.766341850026974, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(22/99): |gradient|=8.113459704551065, loss=0.4660904073374325, w0=-0.019901578391751246, w1=-0.027387751546646452\n",
      "SGD(23/99): |gradient|=2.1523951322923573, loss=0.4655962637317894, w0=-0.020503431962578023, w1=-0.02763510127265685\n",
      "SGD(24/99): |gradient|=2.125400495030948, loss=0.465050896308293, w0=-0.021228871282967347, w1=-0.02694446888606359\n",
      "SGD(25/99): |gradient|=2.90920195097991, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(26/99): |gradient|=3.3299678307457063, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(27/99): |gradient|=2.035550059511512, loss=0.46382531406925614, w0=-0.024134478424571018, w1=-0.028252426454319492\n",
      "SGD(28/99): |gradient|=2.3580415986033985, loss=0.463454498002539, w0=-0.025559249743730812, w1=-0.0282759613139578\n",
      "SGD(29/99): |gradient|=2.032813924807822, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(30/99): |gradient|=2.2382793772999583, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(31/99): |gradient|=1.8764337853530073, loss=0.4623414591560108, w0=-0.03057723228144025, w1=-0.030403073754884043\n",
      "SGD(32/99): |gradient|=1.8232787854563, loss=0.46205157824875615, w0=-0.03371482023092263, w1=-0.0319979397946591\n",
      "SGD(33/99): |gradient|=3.47991815484829, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(34/99): |gradient|=2.6051466504383227, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(35/99): |gradient|=3.122047619325702, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(36/99): |gradient|=2.2102969803941135, loss=0.46103141880159887, w0=-0.023194632035417127, w1=-0.036577706438919055\n",
      "SGD(37/99): |gradient|=1.874048874756718, loss=0.46038466659937205, w0=-0.02427213706531704, w1=-0.03684812713307184\n",
      "SGD(38/99): |gradient|=2.0605893824283457, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(39/99): |gradient|=4.445874973352969, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(40/99): |gradient|=2.9986231336102747, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(41/99): |gradient|=12.892590418104017, loss=0.4580599239196462, w0=-0.026000009060535678, w1=-0.03758569839571127\n",
      "SGD(42/99): |gradient|=4.933085208595288, loss=0.4580198866671206, w0=-0.024411383252656044, w1=-0.03756723828049817\n",
      "SGD(43/99): |gradient|=2.093934690011671, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(44/99): |gradient|=2.9960753128358437, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(45/99): |gradient|=2.438209540121207, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(46/99): |gradient|=4.548454818977723, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(47/99): |gradient|=2.492690399903696, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(48/99): |gradient|=5.610456340644089, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(49/99): |gradient|=3.318079123588936, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(50/99): |gradient|=4.566336903746895, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(51/99): |gradient|=2.6302184968784394, loss=0.45700336773487327, w0=-0.02627204818351755, w1=-0.0380443223126299\n",
      "SGD(52/99): |gradient|=4.585911442587165, loss=0.45698730432309054, w0=-0.026215919321215708, w1=-0.038060274350766095\n",
      "SGD(53/99): |gradient|=2.211072273144503, loss=0.45696654859836666, w0=-0.026284231954664526, w1=-0.03811421256649752\n",
      "SGD(54/99): |gradient|=1.9795518859859038, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(55/99): |gradient|=2.556518850535039, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(56/99): |gradient|=5.226010984055924, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(57/99): |gradient|=4.042771629866015, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(58/99): |gradient|=3.3067416727703223, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(59/99): |gradient|=1.8323943777272642, loss=0.45692388503402925, w0=-0.0263770162421353, w1=-0.03811226028038737\n",
      "SGD(60/99): |gradient|=11.470974679830231, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(61/99): |gradient|=3.0144147497867375, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(62/99): |gradient|=3.6818876157772396, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(63/99): |gradient|=2.163572003684967, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(64/99): |gradient|=4.289954026547787, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(65/99): |gradient|=2.714025703840808, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(66/99): |gradient|=6.1144593103535225, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(67/99): |gradient|=5.47835855595733, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(68/99): |gradient|=2.3926333231147896, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(69/99): |gradient|=3.578895865252755, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(70/99): |gradient|=1.9919780690927347, loss=0.45691478898804516, w0=-0.026372087220421152, w1=-0.03811124869164356\n",
      "SGD(71/99): |gradient|=2.987998220725619, loss=0.456914772142296, w0=-0.026372194256254262, w1=-0.03811126143458942\n",
      "SGD(72/99): |gradient|=2.6470399520361894, loss=0.456914747739093, w0=-0.026372328513162742, w1=-0.03811127085851918\n",
      "SGD(73/99): |gradient|=2.6022737766902844, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(74/99): |gradient|=4.187131906617635, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(75/99): |gradient|=2.015576101474963, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(76/99): |gradient|=2.860467748289693, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(77/99): |gradient|=4.06415115563897, loss=0.45691465332672976, w0=-0.02637251625322853, w1=-0.03811148912062281\n",
      "SGD(78/99): |gradient|=2.4510072413307586, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(79/99): |gradient|=3.826423961329138, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(80/99): |gradient|=2.74372652321718, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(81/99): |gradient|=5.885837597513265, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(82/99): |gradient|=4.653513935748076, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(83/99): |gradient|=2.989275111376364, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(84/99): |gradient|=1.938495142487275, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(85/99): |gradient|=4.135621499992417, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(86/99): |gradient|=8.998813146377053, loss=0.45691462517922826, w0=-0.02637258430471322, w1=-0.03811154343460073\n",
      "SGD(87/99): |gradient|=6.338983621184608, loss=0.4569146227568537, w0=-0.026372581103101932, w1=-0.03811154506135068\n",
      "SGD(88/99): |gradient|=24.4524925542933, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(89/99): |gradient|=3.656262524412902, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(90/99): |gradient|=2.87088407305221, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(91/99): |gradient|=7.309979979904143, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(92/99): |gradient|=3.2762100650218438, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(93/99): |gradient|=3.1031068732380334, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(94/99): |gradient|=3.610075038327429, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(95/99): |gradient|=3.3793881254517495, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(96/99): |gradient|=2.942350120659295, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(97/99): |gradient|=5.18818024609198, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(98/99): |gradient|=4.0703048291822475, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(99/99): |gradient|=2.903848600170023, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3461418418779361"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls = least_squares(y, tX)\n",
    "loss = compute_loss(y, tX, w_ls)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3705354861132588 0.371104805977637\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "w = ridge_regression(y_tr, x_tr, lambda_)\n",
    "loss_tr = compute_loss(y_tr, x_tr, w)\n",
    "loss_te = compute_loss(y_te, x_te, w)\n",
    "print(loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "tX_test_s, mean_tes, std_test = standardize(tX_test)\n",
    "y_pred = predict_labels(weights, tX_test_s)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.96637287, -1.59557484],\n",
       "       [-0.96443888,  2.30659937],\n",
       "       [-2.7925803 , -1.26619016],\n",
       "       ...,\n",
       "       [-2.78555006, -1.2651345 ],\n",
       "       [-0.97573941,  2.28919741],\n",
       "       [-2.80136474, -1.27570913]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
