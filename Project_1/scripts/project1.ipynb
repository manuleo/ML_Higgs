{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , ?\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DER_mass_MMC</th>\n",
       "      <th>DER_mass_transverse_met_lep</th>\n",
       "      <th>DER_mass_vis</th>\n",
       "      <th>DER_pt_h</th>\n",
       "      <th>DER_deltaeta_jet_jet</th>\n",
       "      <th>DER_mass_jet_jet</th>\n",
       "      <th>DER_prodeta_jet_jet</th>\n",
       "      <th>DER_deltar_tau_lep</th>\n",
       "      <th>DER_pt_tot</th>\n",
       "      <th>DER_sum_pt</th>\n",
       "      <th>...</th>\n",
       "      <th>PRI_met_phi</th>\n",
       "      <th>PRI_met_sumet</th>\n",
       "      <th>PRI_jet_num</th>\n",
       "      <th>PRI_jet_leading_pt</th>\n",
       "      <th>PRI_jet_leading_eta</th>\n",
       "      <th>PRI_jet_leading_phi</th>\n",
       "      <th>PRI_jet_subleading_pt</th>\n",
       "      <th>PRI_jet_subleading_eta</th>\n",
       "      <th>PRI_jet_subleading_phi</th>\n",
       "      <th>PRI_jet_all_pt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>138.470</td>\n",
       "      <td>51.655</td>\n",
       "      <td>97.827</td>\n",
       "      <td>27.980</td>\n",
       "      <td>0.91</td>\n",
       "      <td>124.711</td>\n",
       "      <td>2.666</td>\n",
       "      <td>3.064</td>\n",
       "      <td>41.928</td>\n",
       "      <td>197.760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>258.733</td>\n",
       "      <td>2</td>\n",
       "      <td>67.435</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.444</td>\n",
       "      <td>46.062</td>\n",
       "      <td>1.24</td>\n",
       "      <td>-2.475</td>\n",
       "      <td>113.497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>160.937</td>\n",
       "      <td>68.768</td>\n",
       "      <td>103.235</td>\n",
       "      <td>48.146</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.473</td>\n",
       "      <td>2.078</td>\n",
       "      <td>125.157</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.916</td>\n",
       "      <td>164.546</td>\n",
       "      <td>1</td>\n",
       "      <td>46.226</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.158</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>46.226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>162.172</td>\n",
       "      <td>125.953</td>\n",
       "      <td>35.635</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.148</td>\n",
       "      <td>9.336</td>\n",
       "      <td>197.814</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.186</td>\n",
       "      <td>260.414</td>\n",
       "      <td>1</td>\n",
       "      <td>44.251</td>\n",
       "      <td>2.053</td>\n",
       "      <td>-2.028</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>44.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>143.905</td>\n",
       "      <td>81.417</td>\n",
       "      <td>80.943</td>\n",
       "      <td>0.414</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.414</td>\n",
       "      <td>75.968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060</td>\n",
       "      <td>86.062</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>175.864</td>\n",
       "      <td>16.915</td>\n",
       "      <td>134.805</td>\n",
       "      <td>16.405</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>3.891</td>\n",
       "      <td>16.405</td>\n",
       "      <td>57.983</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>53.131</td>\n",
       "      <td>0</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>-999.00</td>\n",
       "      <td>-999.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DER_mass_MMC  DER_mass_transverse_met_lep  DER_mass_vis  DER_pt_h  \\\n",
       "0       138.470                       51.655        97.827    27.980   \n",
       "1       160.937                       68.768       103.235    48.146   \n",
       "2      -999.000                      162.172       125.953    35.635   \n",
       "3       143.905                       81.417        80.943     0.414   \n",
       "4       175.864                       16.915       134.805    16.405   \n",
       "\n",
       "   DER_deltaeta_jet_jet  DER_mass_jet_jet  DER_prodeta_jet_jet  \\\n",
       "0                  0.91           124.711                2.666   \n",
       "1               -999.00          -999.000             -999.000   \n",
       "2               -999.00          -999.000             -999.000   \n",
       "3               -999.00          -999.000             -999.000   \n",
       "4               -999.00          -999.000             -999.000   \n",
       "\n",
       "   DER_deltar_tau_lep  DER_pt_tot  DER_sum_pt  ...  PRI_met_phi  \\\n",
       "0               3.064      41.928     197.760  ...       -0.277   \n",
       "1               3.473       2.078     125.157  ...       -1.916   \n",
       "2               3.148       9.336     197.814  ...       -2.186   \n",
       "3               3.310       0.414      75.968  ...        0.060   \n",
       "4               3.891      16.405      57.983  ...       -0.871   \n",
       "\n",
       "   PRI_met_sumet  PRI_jet_num  PRI_jet_leading_pt  PRI_jet_leading_eta  \\\n",
       "0        258.733            2              67.435                2.150   \n",
       "1        164.546            1              46.226                0.725   \n",
       "2        260.414            1              44.251                2.053   \n",
       "3         86.062            0            -999.000             -999.000   \n",
       "4         53.131            0            -999.000             -999.000   \n",
       "\n",
       "   PRI_jet_leading_phi  PRI_jet_subleading_pt  PRI_jet_subleading_eta  \\\n",
       "0                0.444                 46.062                    1.24   \n",
       "1                1.158               -999.000                 -999.00   \n",
       "2               -2.028               -999.000                 -999.00   \n",
       "3             -999.000               -999.000                 -999.00   \n",
       "4             -999.000               -999.000                 -999.00   \n",
       "\n",
       "   PRI_jet_subleading_phi  PRI_jet_all_pt  \n",
       "0                  -2.475         113.497  \n",
       "1                -999.000          46.226  \n",
       "2                -999.000          44.251  \n",
       "3                -999.000           0.000  \n",
       "4                -999.000           0.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tX_pd = pd.read_csv('../data/train.csv')\n",
    "tX_pd.drop(labels=['Id', 'Prediction'], axis=1, inplace=True)\n",
    "tX_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation graph needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38114\n",
      "4 177457\n",
      "5 177457\n",
      "6 177457\n",
      "12 177457\n",
      "23 99913\n",
      "24 99913\n",
      "25 99913\n",
      "26 177457\n",
      "27 177457\n",
      "28 177457\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30):\n",
    "    s = tX_pd[tX_pd[i]==-999].index.size\n",
    "    if s > 900:\n",
    "        print(i, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funzione di Marco per cancellare tutto con numpy e mettere la media: da scrivere e usare al posto di pandas alla fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uno = np.where(tX==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere(s > 100000)\n",
    "# new = np.delete(tX,index[1:],axis=1)\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s>40000)&(s < 100000))\n",
    "# new = np.delete(new,index[1:],axis=1)\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s<40000)&(s>1))\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima\n",
    "\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere((s>40000)&(s < 100000))\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima\n",
    "\n",
    "# uno = np.where(new==-999,1,0)\n",
    "# s = np.sum(uno,axis=0)\n",
    "# index = np.argwhere(s > 100000)\n",
    "# prima = new[:,index]\n",
    "# index_ = np.argwhere(prima==-999)\n",
    "# prima_new = np.delete(prima, index_)\n",
    "# media = np.median(prima_new)\n",
    "# prima[prima==-999] = media\n",
    "# new[:,index] = prima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop PRI with a lot of null + 29 (29 is a Pri corr with 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drops = [5,6,12,24,25,26,27,28,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_pd.drop(drops, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace with median\n",
    "tX_pd.where(tX_pd!=-999, inplace=True)\n",
    "tX_pd.fillna(tX_pd.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX_pd.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX, mean_x, std_x = standardize(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree, mean=True):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    \n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "        \n",
    "        # form data with polynomial degree\n",
    "        x_te_poly = build_poly(x_te, degree)\n",
    "        x_tr_poly = build_poly(x_tr, degree)\n",
    "   \n",
    "        w = ridge_regression(y_tr, x_tr_poly, lambda_)\n",
    "    \n",
    "        # calculate the loss for train and test data\n",
    "        rmse_tr = compute_rmse(y_tr, x_tr_poly, w)\n",
    "        rmse_te = compute_rmse(y_te, x_te_poly, w)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "\n",
    "    if mean==True:\n",
    "        loss_tr = np.mean(losses_tr)\n",
    "        loss_te = np.mean(losses_te)\n",
    "    else:\n",
    "        loss_tr = losses_tr\n",
    "        loss_te = losses_te\n",
    "    return loss_tr, loss_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cross_validation_demo():\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    lambdas = np.logspace(-4, 1, 40)\n",
    "    for i in range (0, 3):\n",
    "        seed = i\n",
    "        # split data in k fold\n",
    "        y_sub, x_sub = get_subsample(y_tr, x_tr, 50000, seed)\n",
    "        k_indices = build_k_indices(y_sub, k_fold, seed)\n",
    "        # define lists to store the loss of training data and test data\n",
    "        rmse_tr = []\n",
    "        rmse_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr, loss_te = cross_validation(y_sub, x_sub, k_indices, k_fold, lambda_, degree)\n",
    "            rmse_tr.append(loss_tr)\n",
    "            rmse_te.append(loss_te)\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "\n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_degree(y, x, max_degree, k_fold, seed=1):\n",
    "    y_sub, x_sub = get_subsample(y, x, 50000, seed)\n",
    "    lambdas = np.logspace(-4, 1, 40)\n",
    "    k_indices = build_k_indices(y_sub, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    degree_star = 0\n",
    "    lambda_star = 0\n",
    "    for degree in range(1, max_degree+1):\n",
    "        rmse_te = []\n",
    "        for lambda_ in lambdas:\n",
    "            loss_tr, loss_te = cross_validation(y_sub, x_sub, k_indices, k_fold, lambda_, degree)\n",
    "            #print(degree, lambda_, loss_te)\n",
    "            if loss_te < loss_min:\n",
    "                loss_min = loss_te\n",
    "                degree_star = degree\n",
    "                lambda_star = lambda_\n",
    "    return degree_star, lambda_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0001 0.8460043829669812\n",
      "1 0.00013433993325989 0.8464146048572516\n",
      "1 0.00018047217668271703 0.8469522626515786\n",
      "1 0.00024244620170823283 0.84766358551704\n",
      "1 0.0003257020655659783 0.8485971103157804\n",
      "1 0.0004375479375074184 0.8498029914617542\n",
      "1 0.0005878016072274912 0.851333156852561\n",
      "1 0.0007896522868499724 0.8532409823584647\n",
      "1 0.0010608183551394483 0.855576997025409\n",
      "1 0.0014251026703029977 0.8583770822335092\n",
      "1 0.0019144819761699576 0.861643607412523\n",
      "1 0.0025719138090593446 0.8653265482780061\n",
      "1 0.003455107294592218 0.8693158734315984\n",
      "1 0.004641588833612777 0.8734534064957941\n",
      "1 0.006235507341273912 0.8775624838560654\n",
      "1 0.008376776400682916 0.8814840721813327\n",
      "1 0.011253355826007646 0.88510574820002\n",
      "1 0.015117750706156615 0.8883754979084321\n",
      "1 0.020309176209047347 0.8913002068285892\n",
      "1 0.027283333764867666 0.8939337202181175\n",
      "1 0.03665241237079626 0.8963600513956409\n",
      "1 0.049238826317067365 0.898675526988175\n",
      "1 0.06614740641230146 0.9009720970623798\n",
      "1 0.08886238162743398 0.9033240787584396\n",
      "1 0.11937766417144358 0.9057810297711103\n",
      "1 0.16037187437513292 0.9083677933483154\n",
      "1 0.21544346900318823 0.9110888774348488\n",
      "1 0.2894266124716749 0.9139318896544841\n",
      "1 0.38881551803080855 0.9168673022031557\n",
      "1 0.5223345074266837 0.9198476870973145\n",
      "1 0.7017038286703823 0.9228127527060364\n",
      "1 0.9426684551178844 0.9257034679075111\n",
      "1 1.2663801734674023 0.9284822876128518\n",
      "1 1.7012542798525856 0.9311527211103564\n",
      "1 2.2854638641349885 0.9337721272212169\n",
      "1 3.07029062975785 0.9364538732773136\n",
      "1 4.124626382901348 0.9393559249647581\n",
      "1 5.541020330009481 0.9426531587606646\n",
      "1 7.443803013251681 0.9464937410607972\n",
      "1 10.0 0.9509482371372063\n",
      "2 0.0001 0.867248704035785\n",
      "2 0.00013433993325989 0.861668740499127\n",
      "2 0.00018047217668271703 0.8579127312178767\n",
      "2 0.00024244620170823283 0.855537185387602\n",
      "2 0.0003257020655659783 0.8541704562149026\n",
      "2 0.0004375479375074184 0.853538917316379\n",
      "2 0.0005878016072274912 0.8534654317949116\n",
      "2 0.0007896522868499724 0.8538519590973909\n",
      "2 0.0010608183551394483 0.8546557427548026\n",
      "2 0.0014251026703029977 0.8558668713221166\n",
      "2 0.0019144819761699576 0.8574918470534406\n",
      "2 0.0025719138090593446 0.8595426587649759\n",
      "2 0.003455107294592218 0.8620271878843745\n",
      "2 0.004641588833612777 0.864937642212928\n",
      "2 0.006235507341273912 0.8682380407642831\n",
      "2 0.008376776400682916 0.8718554932261549\n",
      "2 0.011253355826007646 0.8756800272282914\n",
      "2 0.015117750706156615 0.8795744819717508\n",
      "2 0.020309176209047347 0.88339223657138\n",
      "2 0.027283333764867666 0.8869980916611645\n",
      "2 0.03665241237079626 0.8902867536292057\n",
      "2 0.049238826317067365 0.8931943184140951\n",
      "2 0.06614740641230146 0.8957009594029066\n",
      "2 0.08886238162743398 0.8978264116387288\n",
      "2 0.11937766417144358 0.8996219211934289\n",
      "2 0.16037187437513292 0.9011622563378157\n",
      "2 0.21544346900318823 0.9025396908191414\n",
      "2 0.2894266124716749 0.90385959543893\n",
      "2 0.38881551803080855 0.9052353013185821\n",
      "2 0.5223345074266837 0.906779081236726\n",
      "2 0.7017038286703823 0.9085873872003215\n",
      "2 0.9426684551178844 0.9107222390608684\n",
      "2 1.2663801734674023 0.9131953155669196\n",
      "2 1.7012542798525856 0.9159632485580148\n",
      "2 2.2854638641349885 0.9189389669529064\n",
      "2 3.07029062975785 0.9220161860247047\n",
      "2 4.124626382901348 0.925097973310661\n",
      "2 5.541020330009481 0.9281203525145955\n",
      "2 7.443803013251681 0.9310670014809208\n",
      "2 10.0 0.9339758118898918\n",
      "3 0.0001 1.5399532155859004\n",
      "3 0.00013433993325989 1.4678746812832815\n",
      "3 0.00018047217668271703 1.3945393418369483\n",
      "3 0.00024244620170823283 1.3214796684663668\n",
      "3 0.0003257020655659783 1.2502732276219717\n",
      "3 0.0004375479375074184 1.1827096856843782\n",
      "3 0.0005878016072274912 1.1206727985724005\n",
      "3 0.0007896522868499724 1.0658515456604896\n",
      "3 0.0010608183551394483 1.0194384046017766\n",
      "3 0.0014251026703029977 0.9819327739152989\n",
      "3 0.0019144819761699576 0.9531014323362375\n",
      "3 0.0025719138090593446 0.9320929274043418\n",
      "3 0.003455107294592218 0.9176578066161922\n",
      "3 0.004641588833612777 0.908394139339\n",
      "3 0.006235507341273912 0.9029417202961486\n",
      "3 0.008376776400682916 0.9000931599106279\n",
      "3 0.011253355826007646 0.89884069319933\n",
      "3 0.015117750706156615 0.8983973637216962\n",
      "3 0.020309176209047347 0.8982155153340734\n",
      "3 0.027283333764867666 0.8979964799599089\n",
      "3 0.03665241237079626 0.8976699294556967\n",
      "3 0.049238826317067365 0.8973309483141632\n",
      "3 0.06614740641230146 0.8971476069055311\n",
      "3 0.08886238162743398 0.8972714187828941\n",
      "3 0.11937766417144358 0.8977822712752452\n",
      "3 0.16037187437513292 0.8986786968961383\n",
      "3 0.21544346900318823 0.8999001795508645\n",
      "3 0.2894266124716749 0.9013584552994472\n",
      "3 0.38881551803080855 0.9029621954696859\n",
      "3 0.5223345074266837 0.9046322912441864\n",
      "3 0.7017038286703823 0.9063116224948362\n",
      "3 0.9426684551178844 0.9079719875956246\n",
      "3 1.2663801734674023 0.909617066335243\n",
      "3 1.7012542798525856 0.9112785769113774\n",
      "3 2.2854638641349885 0.9130041111246869\n",
      "3 3.07029062975785 0.9148384120566875\n",
      "3 4.124626382901348 0.9168037409797428\n",
      "3 5.541020330009481 0.9188874179091862\n",
      "3 7.443803013251681 0.9210430143632305\n",
      "3 10.0 0.9232056206236682\n",
      "4 0.0001 3.8169656576523785\n",
      "4 0.00013433993325989 3.882443892820834\n",
      "4 0.00018047217668271703 3.851313974773353\n",
      "4 0.00024244620170823283 3.748396934537974\n",
      "4 0.0003257020655659783 3.5939309109844677\n",
      "4 0.0004375479375074184 3.402436635158333\n",
      "4 0.0005878016072274912 3.1837187345796947\n",
      "4 0.0007896522868499724 2.945268665894341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-90335e4b86ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_degree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mk_fold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdegree_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_best_degree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_degree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m264\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best degree: {} with lambda = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree_star\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_star\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-df7c01ac3f49>\u001b[0m in \u001b[0;36mselect_best_degree\u001b[1;34m(y, x, max_degree, k_fold, seed)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mrmse_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_sub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_fold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss_te\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mloss_min\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-88901baad9a9>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree, mean)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# form data with polynomial degree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mx_te_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mx_tr_poly\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mridge_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tr_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\EPFL\\ML\\github_mine\\ML\\Project_1\\scripts\\my_helpers.py\u001b[0m in \u001b[0;36mbuild_poly\u001b[1;34m(x, degree)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mpsi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpsi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpsi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_degree = 10\n",
    "k_fold = 4\n",
    "degree_star, lambda_star = select_best_degree(y, tX, max_degree, k_fold, 264)\n",
    "print(\"Best degree: {} with lambda = {}\".format(degree_star, lambda_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        g = compute_gradient(y, tx, w)\n",
    "        new_w = w - gamma*g;\n",
    "        new_loss = compute_loss(y, tx, new_w)\n",
    "        # print TO DELETE IN FINAL VERSION\n",
    "        if new_loss <= loss:\n",
    "            loss, w = new_loss, new_w\n",
    "            gamma *=1.8 #accelerate algorithm learning rate\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3 #decelerate to avoid exponential growing\n",
    "            #print(\"Gradient Descent({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma)\n",
    "        print(\"Gradient Descent({bi}/{ti}): ||gradient||={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/199): ||gradient||=0.9198964473372747, loss=0.49991543478685996, w0=-3.1381702249274604e-05, w1=-1.870332018899931e-05\n",
      "Gradient Descent(1/199): ||gradient||=0.9186848945865361, loss=0.4997636980245322, w0=-8.78042153832626e-05, w1=-5.236507656267239e-05\n",
      "Gradient Descent(2/199): ||gradient||=0.9165086790795948, loss=0.4994921215723027, w0=-0.00018915591588298592, w1=-0.00011294257728487969\n",
      "Gradient Descent(3/199): ||gradient||=0.9126063083581455, loss=0.4990082616199879, w0=-0.000370914263529803, w1=-0.0002219378872869192\n",
      "Gradient Descent(4/199): ||gradient||=0.9056299309807633, loss=0.49815319084166065, w0=-0.0006959041130019329, w1=-0.00041798666987893115\n",
      "Gradient Descent(5/199): ||gradient||=0.8932269289430593, loss=0.49666407351067016, w0=-0.0012739014639035493, w1=-0.0007704142328603178\n",
      "Gradient Descent(6/199): ||gradient||=0.8713980371643467, loss=0.49413764114400255, w0=-0.002292032255140569, w1=-0.0014033062129404496\n",
      "Gradient Descent(7/199): ||gradient||=0.8336914854870378, loss=0.49004496000892644, w0=-0.004054621864207936, w1=-0.0025378020652307696\n",
      "Gradient Descent(8/199): ||gradient||=0.7708210576370715, loss=0.48392417182397535, w0=-0.007012143781032614, w1=-0.0045650758007922156\n",
      "Gradient Descent(9/199): ||gradient||=0.6730593787686956, loss=0.4758746897312079, w0=-0.011703585013512112, w1=-0.008168591631344086\n",
      "Gradient Descent(10/199): ||gradient||=0.5420469001358224, loss=0.4668314646613726, w0=-0.018440239398187373, w1=-0.014520131670101091\n",
      "Gradient Descent(11/199): ||gradient||=0.417585276699318, loss=0.45684219087207145, w0=-0.0266511349012321, w1=-0.02557881435937765\n",
      "Gradient Descent(12/199): ||gradient||=0.34951950406858123, loss=0.4441675056514142, w0=-0.03507313894297936, w1=-0.044530615642647674\n",
      "Gradient Descent(13/199): ||gradient||=0.2802581748299309, loss=0.4305010778316688, w0=-0.044834692707357696, w1=-0.07630731446605377\n",
      "Gradient Descent(14/199): ||gradient||=0.19933661937191804, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(15/199): ||gradient||=0.2126484850736893, loss=0.4194261493875751, w0=-0.0433225318650242, w1=-0.12709402240381473\n",
      "Gradient Descent(16/199): ||gradient||=0.2126484850736893, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(17/199): ||gradient||=0.41005196201736743, loss=0.4192164175189965, w0=-0.04417546498101249, w1=-0.15057107585432739\n",
      "Gradient Descent(18/199): ||gradient||=0.41005196201736743, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(19/199): ||gradient||=0.3562131324819593, loss=0.4161425994149832, w0=-0.03042881057717925, w1=-0.16207735779908952\n",
      "Gradient Descent(20/199): ||gradient||=0.3562131324819593, loss=0.4119259421855884, w0=-0.03309942831257582, w1=-0.16844869171449792\n",
      "Gradient Descent(21/199): ||gradient||=0.12649027507232175, loss=0.41025030347357094, w0=-0.029575140439970622, w1=-0.17946578929166102\n",
      "Gradient Descent(22/199): ||gradient||=0.12292495119423105, loss=0.40743339281067326, w0=-0.023442494523926122, w1=-0.19876354675726643\n",
      "Gradient Descent(23/199): ||gradient||=0.11680132506627634, loss=0.4029471327448956, w0=-0.01278468241212367, w1=-0.23182458833403902\n",
      "Gradient Descent(24/199): ||gradient||=0.10674668689952199, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(25/199): ||gradient||=0.11037853434564666, loss=0.3965790516938806, w0=0.0032309313983773826, w1=-0.2863021479535607\n",
      "Gradient Descent(26/199): ||gradient||=0.11037853434564666, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(27/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(28/199): ||gradient||=0.30949111243344546, loss=0.3964684701806759, w0=0.015448328414554098, w1=-0.31109478735450924\n",
      "Gradient Descent(29/199): ||gradient||=0.30949111243344546, loss=0.3934972452046127, w0=0.012645111284258674, w1=-0.3149399904670085\n",
      "Gradient Descent(30/199): ||gradient||=0.08518921631593725, loss=0.3928422773768506, w0=0.01390331604737596, w1=-0.3215594212339835\n",
      "Gradient Descent(31/199): ||gradient||=0.08143325683778938, loss=0.39176458577149614, w0=0.01769426852847459, w1=-0.333137608291744\n",
      "Gradient Descent(32/199): ||gradient||=0.08261374022271092, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(33/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(34/199): ||gradient||=0.15244231978895387, loss=0.3904851713208589, w0=0.020707710599576625, w1=-0.3532046313023332\n",
      "Gradient Descent(35/199): ||gradient||=0.15244231978895387, loss=0.38970194899647265, w0=0.023115513292314566, w1=-0.35613896221101615\n",
      "Gradient Descent(36/199): ||gradient||=0.07231686370955237, loss=0.38924702709110287, w0=0.024867215085094988, w1=-0.36144019878751626\n",
      "Gradient Descent(37/199): ||gradient||=0.0693800954973171, loss=0.3884863243303348, w0=0.026866841411884168, w1=-0.3708048228655269\n",
      "Gradient Descent(38/199): ||gradient||=0.06849694468173842, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(39/199): ||gradient||=0.09598461127388905, loss=0.38737654415708406, w0=0.032221443986200235, w1=-0.38691647993837636\n",
      "Gradient Descent(40/199): ||gradient||=0.09598461127388905, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(41/199): ||gradient||=0.1410714760751551, loss=0.38709755946614643, w0=0.031409639884593415, w1=-0.39510299988579783\n",
      "Gradient Descent(42/199): ||gradient||=0.1410714760751551, loss=0.3864215399793017, w0=0.0350727177951097, w1=-0.39921296841299686\n",
      "Gradient Descent(43/199): ||gradient||=0.08456831532148283, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(44/199): ||gradient||=0.11635687063747818, loss=0.38609283928743754, w0=0.034597824755885076, w1=-0.4066886332281262\n",
      "Gradient Descent(45/199): ||gradient||=0.11635687063747818, loss=0.3855801463907678, w0=0.037564023270261745, w1=-0.4104639083541116\n",
      "Gradient Descent(46/199): ||gradient||=0.07098725686300805, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(47/199): ||gradient||=0.08865280508918541, loss=0.38521117682227085, w0=0.03758284187875093, w1=-0.4172961953261667\n",
      "Gradient Descent(48/199): ||gradient||=0.08865280508918541, loss=0.38485406384997894, w0=0.0397988301803087, w1=-0.4207754388932331\n",
      "Gradient Descent(49/199): ||gradient||=0.05956788893440248, loss=0.38448192908571455, w0=0.0402881948329062, w1=-0.4270292492982512\n",
      "Gradient Descent(50/199): ||gradient||=0.06633820597340725, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(51/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(52/199): ||gradient||=0.15761065003057645, loss=0.38442089464413837, w0=0.04551447515592648, w1=-0.43773687255237875\n",
      "Gradient Descent(53/199): ||gradient||=0.15761065003057645, loss=0.38370536779400194, w0=0.044360770783240404, w1=-0.4394259860437441\n",
      "Gradient Descent(54/199): ||gradient||=0.06222099110230357, loss=0.38348800700174634, w0=0.044301326033474996, w1=-0.44236759427566763\n",
      "Gradient Descent(55/199): ||gradient||=0.04860668539425892, loss=0.383189073110083, w0=0.04591786180813736, w1=-0.4475058466448936\n",
      "Gradient Descent(56/199): ||gradient||=0.04885484002764845, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(57/199): ||gradient||=0.07218281893711131, loss=0.38276849340828356, w0=0.04693068305756577, w1=-0.45650588328763114\n",
      "Gradient Descent(58/199): ||gradient||=0.07218281893711131, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(59/199): ||gradient||=0.0865949367220621, loss=0.3825873718960974, w0=0.0498284052981392, w1=-0.46099331139984334\n",
      "Gradient Descent(60/199): ||gradient||=0.0865949367220621, loss=0.38229970311486866, w0=0.04905354484341635, w1=-0.4634154097612027\n",
      "Gradient Descent(61/199): ||gradient||=0.045554523575472565, loss=0.38208015281211827, w0=0.05059222337498455, w1=-0.46760109124772614\n",
      "Gradient Descent(62/199): ||gradient||=0.046707216525688364, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(63/199): ||gradient||=0.08067338628463173, loss=0.38183039129040586, w0=0.05093771047973362, w1=-0.47497826544308125\n",
      "Gradient Descent(64/199): ||gradient||=0.08067338628463173, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(65/199): ||gradient||=0.09181351733579902, loss=0.3816990151389566, w0=0.05392994899937147, w1=-0.47864293097104027\n",
      "Gradient Descent(66/199): ||gradient||=0.09181351733579902, loss=0.38140101837396745, w0=0.052993763875687355, w1=-0.48064650178588253\n",
      "Gradient Descent(67/199): ||gradient||=0.040172136024456635, loss=0.38123091391261776, w0=0.05420974414245746, w1=-0.48409726949739024\n",
      "Gradient Descent(68/199): ||gradient||=0.04011639835467928, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(69/199): ||gradient||=0.0587199843578713, loss=0.3809951674469164, w0=0.05467020818561967, w1=-0.4901830876387525\n",
      "Gradient Descent(70/199): ||gradient||=0.0587199843578713, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(71/199): ||gradient||=0.06065901120024391, loss=0.3808561671034156, w0=0.056712761214752244, w1=-0.4932358728465866\n",
      "Gradient Descent(72/199): ||gradient||=0.06065901120024391, loss=0.38070675484703786, w0=0.05629675863399411, w1=-0.4948880697081792\n",
      "Gradient Descent(73/199): ||gradient||=0.03437710169565648, loss=0.3805724543076369, w0=0.05709337174035709, w1=-0.4977650896919669\n",
      "Gradient Descent(74/199): ||gradient||=0.03371872029255384, loss=0.380347504802962, w0=0.057923161002275445, w1=-0.502821417967265\n",
      "Gradient Descent(75/199): ||gradient||=0.0353360741618754, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(76/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(77/199): ||gradient||=0.0868549972476703, loss=0.3801552624788844, w0=0.06117016381868622, w1=-0.5114152905216629\n",
      "Gradient Descent(78/199): ||gradient||=0.0868549972476703, loss=0.37990446055546606, w0=0.06025622386799663, w1=-0.512732449768847\n",
      "Gradient Descent(79/199): ||gradient||=0.03013797122742819, loss=0.3798059156030959, w0=0.06086479596353714, w1=-0.5149947769707354\n",
      "Gradient Descent(80/199): ||gradient||=0.029582646754048075, loss=0.3796377325021035, w0=0.06155759036056807, w1=-0.5189768598962371\n",
      "Gradient Descent(81/199): ||gradient||=0.02988603821652102, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(82/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(83/199): ||gradient||=0.05385331845836831, loss=0.3794139288338467, w0=0.06382969816757092, w1=-0.5257836738330686\n",
      "Gradient Descent(84/199): ||gradient||=0.05385331845836831, loss=0.37930748989144936, w0=0.06340685859282029, w1=-0.5268228016896819\n",
      "Gradient Descent(85/199): ||gradient||=0.026807087764073437, loss=0.379232343212017, w0=0.06379460699418472, w1=-0.5286305900589766\n",
      "Gradient Descent(86/199): ||gradient||=0.026429755894928714, loss=0.37910158102876307, w0=0.06449936504431049, w1=-0.5318081753946006\n",
      "Gradient Descent(87/199): ||gradient||=0.025779232451409895, loss=0.37887998603582906, w0=0.06565536813188565, w1=-0.5372914115868667\n",
      "Gradient Descent(88/199): ||gradient||=0.02479156357937694, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(89/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(90/199): ||gradient||=0.03258988562878005, loss=0.37853558176716484, w0=0.06800696343269726, w1=-0.5464117445901066\n",
      "Gradient Descent(91/199): ||gradient||=0.03258988562878005, loss=0.37847536538605114, w0=0.06774134339187216, w1=-0.5477141177050802\n",
      "Gradient Descent(92/199): ||gradient||=0.027738362406501706, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(93/199): ||gradient||=0.03951405284074884, loss=0.3784081137173264, w0=0.06891401614328861, w1=-0.5499543441431679\n",
      "Gradient Descent(94/199): ||gradient||=0.03951405284074884, loss=0.37834206727202646, w0=0.06842349019205861, w1=-0.5511561942990433\n",
      "Gradient Descent(95/199): ||gradient||=0.0304225158131295, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(96/199): ||gradient||=0.046421773810511664, loss=0.3782950369307274, w0=0.06974234133756098, w1=-0.5532027831040229\n",
      "Gradient Descent(97/199): ||gradient||=0.046421773810511664, loss=0.3782192101919261, w0=0.06906497913554238, w1=-0.5543145897328704\n",
      "Gradient Descent(98/199): ||gradient||=0.03256348800089378, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(99/199): ||gradient||=0.05053199788292059, loss=0.3781863625212313, w0=0.0704641466114305, w1=-0.5561901419642732\n",
      "Gradient Descent(100/199): ||gradient||=0.05053199788292059, loss=0.37810265907289226, w0=0.06968905597825942, w1=-0.5572191567845676\n",
      "Gradient Descent(101/199): ||gradient||=0.032866606098295975, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(102/199): ||gradient||=0.049904511233416445, loss=0.3780730239484249, w0=0.07106101099049744, w1=-0.5589450864985751\n",
      "Gradient Descent(103/199): ||gradient||=0.049904511233416445, loss=0.37799031022315804, w0=0.07030697705295337, w1=-0.5598962692587653\n",
      "Gradient Descent(104/199): ||gradient||=0.030864528540993398, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(105/199): ||gradient||=0.044474850521653266, loss=0.3779543503641231, w0=0.07153774920247563, w1=-0.5614919833996477\n",
      "Gradient Descent(106/199): ||gradient||=0.044474850521653266, loss=0.3778831177632533, w0=0.0709143275158598, w1=-0.5623694962267904\n",
      "Gradient Descent(107/199): ||gradient||=0.02725555397893965, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(108/199): ||gradient||=0.036174660635675905, loss=0.37783808449301604, w0=0.07192419161530243, w1=-0.5638513332719829\n",
      "Gradient Descent(109/199): ||gradient||=0.036174660635675905, loss=0.3777832329470302, w0=0.07149562027576611, w1=-0.5646599120664837\n",
      "Gradient Descent(110/199): ||gradient||=0.023491643257352396, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(111/199): ||gradient||=0.02798541531932825, loss=0.37773234827744734, w0=0.0722626339272173, w1=-0.5660406117723381\n",
      "Gradient Descent(112/199): ||gradient||=0.02798541531932825, loss=0.37769151450075156, w0=0.07203435972166758, w1=-0.5667861513379303\n",
      "Gradient Descent(113/199): ||gradient||=0.020796448379199806, loss=0.3776393572545536, w0=0.07258948801176464, w1=-0.568075243409437\n",
      "Gradient Descent(114/199): ||gradient||=0.02232622559275806, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(115/199): ||gradient||=0.044569960234299495, loss=0.377588333501605, w0=0.07236360170695846, w1=-0.5703726024359062\n",
      "Gradient Descent(116/199): ||gradient||=0.044569960234299495, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(117/199): ||gradient||=0.06049611094726945, loss=0.37758811354171706, w0=0.07398697827662784, w1=-0.571494113123698\n",
      "Gradient Descent(118/199): ||gradient||=0.06049611094726945, loss=0.3774734104704165, w0=0.07307154647503165, w1=-0.5721438415511358\n",
      "Gradient Descent(119/199): ||gradient||=0.02560307349675981, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(120/199): ||gradient||=0.030235195825791275, loss=0.37743437097577587, w0=0.07389537252526818, w1=-0.5732146504634791\n",
      "Gradient Descent(121/199): ||gradient||=0.030235195825791275, loss=0.3773936500349805, w0=0.0735942921530556, w1=-0.5738034701868857\n",
      "Gradient Descent(122/199): ||gradient||=0.01930338988021067, loss=0.3773502513239982, w0=0.07402822791850343, w1=-0.5748167485368086\n",
      "Gradient Descent(123/199): ||gradient||=0.019883352701387243, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(124/199): ||gradient||=0.03180432497976459, loss=0.3772907352516291, w0=0.07393681984074807, w1=-0.5766266491602585\n",
      "Gradient Descent(125/199): ||gradient||=0.03180432497976459, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(126/199): ||gradient||=0.0368369701461698, loss=0.3772601111394095, w0=0.07496055740277986, w1=-0.5775270322925545\n",
      "Gradient Descent(127/199): ||gradient||=0.0368369701461698, loss=0.3772087103489772, w0=0.07452601579779915, w1=-0.5780354108422809\n",
      "Gradient Descent(128/199): ||gradient||=0.018672376155633576, loss=0.3771703336844059, w0=0.07491495902096323, w1=-0.5789005133364988\n",
      "Gradient Descent(129/199): ||gradient||=0.01894887411582462, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(130/199): ||gradient||=0.027883181430691767, loss=0.37711429272072133, w0=0.07484020252240821, w1=-0.580450552171511\n",
      "Gradient Descent(131/199): ||gradient||=0.027883181430691767, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(132/199): ||gradient||=0.029631455078381125, loss=0.37708121348388723, w0=0.07566085878362429, w1=-0.5812272244984843\n",
      "Gradient Descent(133/199): ||gradient||=0.029631455078381125, loss=0.37704490546502617, w0=0.07536964604927561, w1=-0.5816628169500092\n",
      "Gradient Descent(134/199): ||gradient||=0.01724997691647665, loss=0.3770105401718543, w0=0.07562881667941775, w1=-0.5824123704142081\n",
      "Gradient Descent(135/199): ||gradient||=0.017165877881501656, loss=0.37695177088913756, w0=0.07574484702850508, w1=-0.5837476546228534\n",
      "Gradient Descent(136/199): ||gradient||=0.01899421714878402, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(137/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(138/199): ||gradient||=0.054220090727918, loss=0.37692148570562073, w0=0.07710169262791508, w1=-0.5860316206865467\n",
      "Gradient Descent(139/199): ||gradient||=0.054220090727918, loss=0.37682644524415465, w0=0.07640556694575028, w1=-0.5864102724253792\n",
      "Gradient Descent(140/199): ||gradient||=0.016650580776069446, loss=0.3767965645111003, w0=0.07664467148671401, w1=-0.5870356600147364\n",
      "Gradient Descent(141/199): ||gradient||=0.016501858917438078, loss=0.376745477937799, w0=0.07671846419935624, w1=-0.5881547717783094\n",
      "Gradient Descent(142/199): ||gradient||=0.01816804164140217, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(143/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(144/199): ||gradient||=0.04825610442092132, loss=0.37671160786478297, w0=0.07790088351724668, w1=-0.5900751936992078\n",
      "Gradient Descent(145/199): ||gradient||=0.04825610442092132, loss=0.3766356523318643, w0=0.07732128008506954, w1=-0.5903961372206211\n",
      "Gradient Descent(146/199): ||gradient||=0.01563931340254658, loss=0.3766094801974734, w0=0.07745336774852776, w1=-0.5909299030113423\n",
      "Gradient Descent(147/199): ||gradient||=0.01555626373968682, loss=0.37656297807301164, w0=0.07763617808606241, w1=-0.5918788948154293\n",
      "Gradient Descent(148/199): ||gradient||=0.015451985463614959, loss=0.3764819326011553, w0=0.07809347822581313, w1=-0.5935397672728017\n",
      "Gradient Descent(149/199): ||gradient||=0.016210974608243976, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(150/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(151/199): ||gradient||=0.0572020732177142, loss=0.37642885163632395, w0=0.0778620860907724, w1=-0.5964324994905928\n",
      "Gradient Descent(152/199): ||gradient||=0.0572020732177142, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(153/199): ||gradient||=0.04260833075213723, loss=0.3763639979162107, w0=0.07928178781519514, w1=-0.596816759315354\n",
      "Gradient Descent(154/199): ||gradient||=0.04260833075213723, loss=0.37630531695231634, w0=0.0788118352881565, w1=-0.5970633924846425\n",
      "Gradient Descent(155/199): ||gradient||=0.014908211767569006, loss=0.3762840666377174, w0=0.0788347473226203, w1=-0.5974755644658697\n",
      "Gradient Descent(156/199): ||gradient||=0.014708048925549026, loss=0.3762467257517887, w0=0.07906897984336392, w1=-0.5982031234845887\n",
      "Gradient Descent(157/199): ||gradient||=0.014963885063396522, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(158/199): ||gradient||=0.021972572186543523, loss=0.3761879108136801, w0=0.0790578901510292, w1=-0.5995025608408782\n",
      "Gradient Descent(159/199): ||gradient||=0.021972572186543523, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(160/199): ||gradient||=0.03521861893866122, loss=0.3761751577223521, w0=0.07988198597230842, w1=-0.6001532477635246\n",
      "Gradient Descent(161/199): ||gradient||=0.03521861893866122, loss=0.3761362201008082, w0=0.07924611088708965, w1=-0.6005349176900081\n",
      "Gradient Descent(162/199): ||gradient||=0.02351622446106723, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(163/199): ||gradient||=0.03747859183540602, loss=0.376127928882764, w0=0.08012627017357933, w1=-0.601147868777882\n",
      "Gradient Descent(164/199): ||gradient||=0.03747859183540602, loss=0.3760848467338232, w0=0.07945208937262828, w1=-0.6015121960900223\n",
      "Gradient Descent(165/199): ||gradient||=0.023370968435216157, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(166/199): ||gradient||=0.03591853609344955, loss=0.3760749662363606, w0=0.08030091303119544, w1=-0.6020933245359191\n",
      "Gradient Descent(167/199): ||gradient||=0.03591853609344955, loss=0.37603359618573673, w0=0.07967763478331942, w1=-0.6024388052305133\n",
      "Gradient Descent(168/199): ||gradient||=0.021481809200450213, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(169/199): ||gradient||=0.030995904539739747, loss=0.3760179930355585, w0=0.08041278478105703, w1=-0.6029935134188872\n",
      "Gradient Descent(170/199): ||gradient||=0.030995904539739747, loss=0.3759837423493009, w0=0.07991361406706003, w1=-0.6033190135612887\n",
      "Gradient Descent(171/199): ||gradient||=0.018635502628599573, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(172/199): ||gradient||=0.0245163904254113, loss=0.3759622996612751, w0=0.08048599604528822, w1=-0.603851288825571\n",
      "Gradient Descent(173/199): ||gradient||=0.0245163904254113, loss=0.3759366863297597, w0=0.08014500940912765, w1=-0.6041568478334536\n",
      "Gradient Descent(174/199): ||gradient||=0.01599958312904941, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(175/199): ||gradient||=0.018751655043872835, loss=0.3759118922941351, w0=0.08055016261254995, w1=-0.6046689176895607\n",
      "Gradient Descent(176/199): ||gradient||=0.018751655043872835, loss=0.3758928304184189, w0=0.0803586612622116, w1=-0.6049558909348881\n",
      "Gradient Descent(177/199): ||gradient||=0.014338863123069666, loss=0.3758671970659703, w0=0.08062719990283188, w1=-0.6054485791833413\n",
      "Gradient Descent(178/199): ||gradient||=0.015200543414685935, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(179/199): ||gradient||=0.027969392552206655, loss=0.37583654449523124, w0=0.08036489072717935, w1=-0.6063504997897442\n",
      "Gradient Descent(180/199): ||gradient||=0.027969392552206655, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(181/199): ||gradient||=0.036968997621708515, loss=0.37582886080109595, w0=0.08128370285356662, w1=-0.6067923065577789\n",
      "Gradient Descent(182/199): ||gradient||=0.036968997621708515, loss=0.375783531496386, w0=0.08070975465583485, w1=-0.6070644065150398\n",
      "Gradient Descent(183/199): ||gradient||=0.0167182053280009, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(184/199): ||gradient||=0.018986866136306145, loss=0.3757620984584493, w0=0.08113419016175932, w1=-0.607502584441539\n",
      "Gradient Descent(185/199): ||gradient||=0.018986866136306145, loss=0.3757441578195452, w0=0.08094097589901549, w1=-0.6077511395045695\n",
      "Gradient Descent(186/199): ||gradient||=0.013572183029778981, loss=0.37572153696885513, w0=0.08114010120247711, w1=-0.6081779108393255\n",
      "Gradient Descent(187/199): ||gradient||=0.013800818079967348, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(188/199): ||gradient||=0.019086297838013175, loss=0.3756860696494606, w0=0.08103313705429466, w1=-0.6089548792538191\n",
      "Gradient Descent(189/199): ||gradient||=0.019086297838013175, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(190/199): ||gradient||=0.02127896202154825, loss=0.3756670862808575, w0=0.08154565511964693, w1=-0.6093486850629586\n",
      "Gradient Descent(191/199): ||gradient||=0.02127896202154825, loss=0.3756473456718056, w0=0.08130696324519089, w1=-0.6095769393921033\n",
      "Gradient Descent(192/199): ||gradient||=0.013232837805350147, loss=0.3756267043461724, w0=0.08148075967979389, w1=-0.6099657476358593\n",
      "Gradient Descent(193/199): ||gradient||=0.013302247084608905, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(194/199): ||gradient||=0.016545311030577412, loss=0.3755926910253041, w0=0.08141711577151617, w1=-0.6106729210960264\n",
      "Gradient Descent(195/199): ||gradient||=0.016545311030577412, loss=0.3755736084999899, w0=0.08179841617225363, w1=-0.6110356901740174\n",
      "Gradient Descent(196/199): ||gradient||=0.017100034533504855, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(197/199): ||gradient||=0.03414575672935005, loss=0.37556434274899564, w0=0.0813233270414321, w1=-0.6117231829818586\n",
      "Gradient Descent(198/199): ||gradient||=0.03414575672935005, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n",
      "Gradient Descent(199/199): ||gradient||=0.03491003301262125, loss=0.37554725477411016, w0=0.08228859411954817, w1=-0.6120487848027445\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 200\n",
    "gamma = 0.0001\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start gradient descent.\n",
    "gradient_w, gradient_loss = gradient_descent(y, tX, w_initial, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\n",
    "    w = initial_w\n",
    "    loss = np.inf\n",
    "    for n_iter in range(max_iters):\n",
    "        for yn, xn in batch_iter(y, tx, batch_size):\n",
    "            g = compute_stoch_gradient(yn, xn, w)\n",
    "            new_w = w - gamma*g;\n",
    "            new_loss = compute_loss(y, tx, new_w)\n",
    "        if new_loss <= loss:\n",
    "            loss , w = new_loss , new_w\n",
    "            gamma *=1.8\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma aumenta\".format(\n",
    "             # bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        else:\n",
    "            gamma *=0.3\n",
    "            #print(\"SGD({bi}/{ti}): loss={l} ; gamma={g}; gamma diminuisce\".format(\n",
    "             #     bi=n_iter, ti=max_iters - 1, l=loss, g= gamma))\n",
    "        print(\"SGD({bi}/{ti}): |gradient|={grad}, loss={l}, w0={w0}, w1={w1}\".format(\n",
    "              bi=n_iter, ti=max_iters - 1, grad=np.linalg.norm(g), l=loss, w0=w[0], w1=w[1]))\n",
    "\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD(0/99): |gradient|=3.819894318273993, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(1/99): |gradient|=3.313465869115332, loss=0.5415004344865282, w0=0.011805540261408516, w1=-0.002686097454581817\n",
      "SGD(2/99): |gradient|=2.7465500598415247, loss=0.5231169592050283, w0=0.006188461831573778, w1=-0.003969062074416842\n",
      "SGD(3/99): |gradient|=3.2750457421264016, loss=0.49026883819335143, w0=-0.0036366473237117534, w1=-0.008645873148768043\n",
      "SGD(4/99): |gradient|=3.648974628785057, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(5/99): |gradient|=3.4494660744980905, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(6/99): |gradient|=3.143628555043909, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(7/99): |gradient|=2.8915114659527927, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(8/99): |gradient|=2.897412774883511, loss=0.4714873513207109, w0=-0.019703815429169942, w1=-0.026240410268246465\n",
      "SGD(9/99): |gradient|=2.1737890066966687, loss=0.4714443822232635, w0=-0.019904464199047903, w1=-0.02634329478613387\n",
      "SGD(10/99): |gradient|=5.937595790584886, loss=0.4709996675291747, w0=-0.019421647326106496, w1=-0.026244718166632628\n",
      "SGD(11/99): |gradient|=2.677548444145319, loss=0.47095357270473925, w0=-0.02081102663905905, w1=-0.026713517117417356\n",
      "SGD(12/99): |gradient|=12.171538978449524, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(13/99): |gradient|=6.554489780166215, loss=0.4669017373609154, w0=-0.02021290783452855, w1=-0.027149845066493348\n",
      "SGD(14/99): |gradient|=3.1860265668993417, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(15/99): |gradient|=4.980416065195217, loss=0.46683403689113295, w0=-0.020819615275740173, w1=-0.027362200812291265\n",
      "SGD(16/99): |gradient|=8.601456112840077, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(17/99): |gradient|=2.507560742564985, loss=0.46642613367989405, w0=-0.02032254885070125, w1=-0.027476523942035674\n",
      "SGD(18/99): |gradient|=3.39722270862085, loss=0.46639024949469604, w0=-0.0204532765809344, w1=-0.027407190321897496\n",
      "SGD(19/99): |gradient|=5.7959604991936695, loss=0.46636056728990233, w0=-0.019964622091377646, w1=-0.02757084427383505\n",
      "SGD(20/99): |gradient|=2.6503861820488868, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(21/99): |gradient|=4.766341850026974, loss=0.46615398548810694, w0=-0.02051819888045534, w1=-0.02718188475904513\n",
      "SGD(22/99): |gradient|=8.113459704551065, loss=0.4660904073374325, w0=-0.019901578391751246, w1=-0.027387751546646452\n",
      "SGD(23/99): |gradient|=2.1523951322923573, loss=0.4655962637317894, w0=-0.020503431962578023, w1=-0.02763510127265685\n",
      "SGD(24/99): |gradient|=2.125400495030948, loss=0.465050896308293, w0=-0.021228871282967347, w1=-0.02694446888606359\n",
      "SGD(25/99): |gradient|=2.90920195097991, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(26/99): |gradient|=3.3299678307457063, loss=0.46443857876838635, w0=-0.023087142508211, w1=-0.02780799635242779\n",
      "SGD(27/99): |gradient|=2.035550059511512, loss=0.46382531406925614, w0=-0.024134478424571018, w1=-0.028252426454319492\n",
      "SGD(28/99): |gradient|=2.3580415986033985, loss=0.463454498002539, w0=-0.025559249743730812, w1=-0.0282759613139578\n",
      "SGD(29/99): |gradient|=2.032813924807822, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(30/99): |gradient|=2.2382793772999583, loss=0.4625936487580744, w0=-0.028828125817051634, w1=-0.029481363367709734\n",
      "SGD(31/99): |gradient|=1.8764337853530073, loss=0.4623414591560108, w0=-0.03057723228144025, w1=-0.030403073754884043\n",
      "SGD(32/99): |gradient|=1.8232787854563, loss=0.46205157824875615, w0=-0.03371482023092263, w1=-0.0319979397946591\n",
      "SGD(33/99): |gradient|=3.47991815484829, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(34/99): |gradient|=2.6051466504383227, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(35/99): |gradient|=3.122047619325702, loss=0.4613252378794793, w0=-0.02254438255528187, w1=-0.03711256437853203\n",
      "SGD(36/99): |gradient|=2.2102969803941135, loss=0.46103141880159887, w0=-0.023194632035417127, w1=-0.036577706438919055\n",
      "SGD(37/99): |gradient|=1.874048874756718, loss=0.46038466659937205, w0=-0.02427213706531704, w1=-0.03684812713307184\n",
      "SGD(38/99): |gradient|=2.0605893824283457, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(39/99): |gradient|=4.445874973352969, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(40/99): |gradient|=2.9986231336102747, loss=0.45990233446963297, w0=-0.027233179187580878, w1=-0.03797723214509076\n",
      "SGD(41/99): |gradient|=12.892590418104017, loss=0.4580599239196462, w0=-0.026000009060535678, w1=-0.03758569839571127\n",
      "SGD(42/99): |gradient|=4.933085208595288, loss=0.4580198866671206, w0=-0.024411383252656044, w1=-0.03756723828049817\n",
      "SGD(43/99): |gradient|=2.093934690011671, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(44/99): |gradient|=2.9960753128358437, loss=0.457209388979194, w0=-0.025959960088391906, w1=-0.038170673039631155\n",
      "SGD(45/99): |gradient|=2.438209540121207, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(46/99): |gradient|=4.548454818977723, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(47/99): |gradient|=2.492690399903696, loss=0.4571245240613506, w0=-0.026458422816273697, w1=-0.037946214784046806\n",
      "SGD(48/99): |gradient|=5.610456340644089, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(49/99): |gradient|=3.318079123588936, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(50/99): |gradient|=4.566336903746895, loss=0.45700707456700934, w0=-0.026251402891296544, w1=-0.03803981706218313\n",
      "SGD(51/99): |gradient|=2.6302184968784394, loss=0.45700336773487327, w0=-0.02627204818351755, w1=-0.0380443223126299\n",
      "SGD(52/99): |gradient|=4.585911442587165, loss=0.45698730432309054, w0=-0.026215919321215708, w1=-0.038060274350766095\n",
      "SGD(53/99): |gradient|=2.211072273144503, loss=0.45696654859836666, w0=-0.026284231954664526, w1=-0.03811421256649752\n",
      "SGD(54/99): |gradient|=1.9795518859859038, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(55/99): |gradient|=2.556518850535039, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(56/99): |gradient|=5.226010984055924, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(57/99): |gradient|=4.042771629866015, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(58/99): |gradient|=3.3067416727703223, loss=0.4569246934038637, w0=-0.026376338993873042, w1=-0.03811185565460271\n",
      "SGD(59/99): |gradient|=1.8323943777272642, loss=0.45692388503402925, w0=-0.0263770162421353, w1=-0.03811226028038737\n",
      "SGD(60/99): |gradient|=11.470974679830231, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(61/99): |gradient|=3.0144147497867375, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(62/99): |gradient|=3.6818876157772396, loss=0.4569150311644476, w0=-0.02637107439232714, w1=-0.03811067943123103\n",
      "SGD(63/99): |gradient|=2.163572003684967, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(64/99): |gradient|=4.289954026547787, loss=0.4569148900111584, w0=-0.026371600580659144, w1=-0.038111111440099765\n",
      "SGD(65/99): |gradient|=2.714025703840808, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(66/99): |gradient|=6.1144593103535225, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(67/99): |gradient|=5.47835855595733, loss=0.45691481807419854, w0=-0.026372017508583073, w1=-0.038111186745683305\n",
      "SGD(68/99): |gradient|=2.3926333231147896, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(69/99): |gradient|=3.578895865252755, loss=0.4569147993632949, w0=-0.026372062029418, w1=-0.03811123014864691\n",
      "SGD(70/99): |gradient|=1.9919780690927347, loss=0.45691478898804516, w0=-0.026372087220421152, w1=-0.03811124869164356\n",
      "SGD(71/99): |gradient|=2.987998220725619, loss=0.456914772142296, w0=-0.026372194256254262, w1=-0.03811126143458942\n",
      "SGD(72/99): |gradient|=2.6470399520361894, loss=0.456914747739093, w0=-0.026372328513162742, w1=-0.03811127085851918\n",
      "SGD(73/99): |gradient|=2.6022737766902844, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(74/99): |gradient|=4.187131906617635, loss=0.45691470424649516, w0=-0.026372500639405515, w1=-0.0381114386159908\n",
      "SGD(75/99): |gradient|=2.015576101474963, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(76/99): |gradient|=2.860467748289693, loss=0.45691466217210414, w0=-0.026372580731252444, w1=-0.038111478596163824\n",
      "SGD(77/99): |gradient|=4.06415115563897, loss=0.45691465332672976, w0=-0.02637251625322853, w1=-0.03811148912062281\n",
      "SGD(78/99): |gradient|=2.4510072413307586, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(79/99): |gradient|=3.826423961329138, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(80/99): |gradient|=2.74372652321718, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(81/99): |gradient|=5.885837597513265, loss=0.4569146301993595, w0=-0.026372590750361666, w1=-0.038111538135083084\n",
      "SGD(82/99): |gradient|=4.653513935748076, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(83/99): |gradient|=2.989275111376364, loss=0.4569146283558581, w0=-0.026372584040386265, w1=-0.038111541406140904\n",
      "SGD(84/99): |gradient|=1.938495142487275, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(85/99): |gradient|=4.135621499992417, loss=0.45691462715299336, w0=-0.026372586115793333, w1=-0.03811154269589085\n",
      "SGD(86/99): |gradient|=8.998813146377053, loss=0.45691462517922826, w0=-0.02637258430471322, w1=-0.03811154343460073\n",
      "SGD(87/99): |gradient|=6.338983621184608, loss=0.4569146227568537, w0=-0.026372581103101932, w1=-0.03811154506135068\n",
      "SGD(88/99): |gradient|=24.4524925542933, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(89/99): |gradient|=3.656262524412902, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(90/99): |gradient|=2.87088407305221, loss=0.4569146111727842, w0=-0.02637251601434, w1=-0.038111548466985015\n",
      "SGD(91/99): |gradient|=7.309979979904143, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(92/99): |gradient|=3.2762100650218438, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(93/99): |gradient|=3.1031068732380334, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(94/99): |gradient|=3.610075038327429, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(95/99): |gradient|=3.3793881254517495, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(96/99): |gradient|=2.942350120659295, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(97/99): |gradient|=5.18818024609198, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(98/99): |gradient|=4.0703048291822475, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n",
      "SGD(99/99): |gradient|=2.903848600170023, loss=0.45691461075013645, w0=-0.026372514903032197, w1=-0.03811154805088536\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 100\n",
    "gamma = 0.01\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(tX.shape[1])\n",
    "\n",
    "# Start SGD.\n",
    "sgd_loss, sgd_w = stochastic_gradient_descent(\n",
    "    y, tX, w_initial, batch_size, max_iters, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    \"\"\"calculate the least squares solution.\"\"\"\n",
    "    D = tx.shape[1]\n",
    "    G = tx.T.dot(tx)\n",
    "    if(np.linalg.matrix_rank(G)==D):\n",
    "        w = np.linalg.inv(G).dot(tx.T).dot(y)\n",
    "    else:\n",
    "        w = np.linalg.lstsq(G,tx.T.dot(y), rcond=None) [0]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3461418418779361"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ls = least_squares(y, tX)\n",
    "loss = compute_loss(y, tX, w_ls)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(y, tx, lambda_):\n",
    "    \"\"\"implement ridge regression.\"\"\"\n",
    "    N = len(y)\n",
    "    G = tx.T.dot(tx)\n",
    "    i = np.linalg.inv(G + 2*N*lambda_*np.eye(G.shape[0]))\n",
    "    w_star = i.dot(tx.T).dot(y)\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3705354861132588 0.371104805977637\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.001\n",
    "w = ridge_regression(y_tr, x_tr, lambda_)\n",
    "loss_tr = compute_loss(y_tr, x_tr, w)\n",
    "loss_te = compute_loss(y_te, x_te, w)\n",
    "print(loss_tr, loss_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72462 0.5139609587348654\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = predict_labels(w, x_te)\n",
    "acc = accuracy_score(y_te, y_pred_test)\n",
    "f1 = f1_score(y_te, y_pred_test)\n",
    "print(acc, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "tX_test_s, mean_tes, std_test = standardize(tX_test)\n",
    "y_pred = predict_labels(w, tX_test_s)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
