{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , deal with outliers, adding log colm (?)\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for logistic\n",
    "y = y_for_logistic(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3          True\n",
      "5         False\n",
      "9         False\n",
      "13        False\n",
      "14        False\n",
      "          ...  \n",
      "199987    False\n",
      "199988    False\n",
      "199990    False\n",
      "199991    False\n",
      "199993    False\n",
      "Name: 0, Length: 40333, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(y_tr_tot, x_tr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        False\n",
      "6        False\n",
      "7        False\n",
      "9        False\n",
      "26       False\n",
      "         ...  \n",
      "49981    False\n",
      "49984    False\n",
      "49985    False\n",
      "49989    False\n",
      "49993    False\n",
      "Name: 0, Length: 10046, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(y_te_tot, x_te_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    ln = np.log(1 + np.exp(tx.dot(w)))\n",
    "    s = y.T.dot(tx).dot(w)\n",
    "    loss = np.sum(ln) - s\n",
    "    if (np.isinf(loss)):\n",
    "        loss = np.finfo(loss.dtype).max\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    w = w - gamma * grad\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x, gamma):\n",
    "    # init parameters\n",
    "    max_iter = 2000\n",
    "    threshold = 1e-8\n",
    "    #gamma = 0.00001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = 0.1 * np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        #if iter % 10 == 0:\n",
    "            #print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.8243033141192283\n",
      "Current iteration=10, loss=0.2198441377775378\n",
      "Current iteration=20, loss=0.19707586215677328\n",
      "Current iteration=30, loss=0.19084509480491382\n",
      "Current iteration=40, loss=0.18732902443561852\n",
      "Current iteration=50, loss=0.1847501020111033\n",
      "Current iteration=60, loss=0.18264869630211839\n",
      "Current iteration=70, loss=0.18085300082414002\n",
      "Current iteration=80, loss=0.18037106878743375\n",
      "Current iteration=90, loss=0.1810236014589492\n",
      "Current iteration=0, loss=6.815345924472263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=10, loss=inf\n",
      "Current iteration=20, loss=inf\n",
      "Current iteration=30, loss=inf\n",
      "Current iteration=40, loss=inf\n",
      "Current iteration=50, loss=inf\n",
      "Current iteration=60, loss=inf\n",
      "Current iteration=70, loss=inf\n",
      "Current iteration=80, loss=inf\n",
      "Current iteration=90, loss=inf\n",
      "Current iteration=0, loss=0.4389772048594174\n",
      "Current iteration=10, loss=0.3505732762398289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-013a19079a03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtX_tr_poly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_poly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_tr_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-40-08962b004ffa>\u001b[0m in \u001b[0;36mlogistic_regression_gradient_descent\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# log info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-66aa25d6c7f7>\u001b[0m in \u001b[0;36mlearning_by_gradient_descent\u001b[0;34m(y, tx, w, gamma)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a58f951d20ca>\u001b[0m in \u001b[0;36mcalculate_gradient\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"compute the gradient of loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    tX_tr_poly = build_poly(tX_tr[jet], degrees_star[jet])\n",
    "    loss, w = logistic_regression_gradient_descent(y_tr[jet], tX_tr_poly, lambdas_star[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5905\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "degrees = [3, 3, 3, 3, 3, 3, 3, 3]\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, degrees_star, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_sub_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_gradient_descent(yn, xn, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.19165192086134963\n",
      "Current iteration=100, loss=0.1326719226845673\n",
      "Current iteration=200, loss=0.019176618425624736\n",
      "Current iteration=300, loss=0.11726119739527065\n",
      "Current iteration=400, loss=0.027572469433210883\n",
      "Current iteration=500, loss=0.012710914846400589\n",
      "Current iteration=600, loss=0.04950661590728781\n",
      "Current iteration=700, loss=0.012959577380627724\n",
      "Current iteration=800, loss=0.02714906143323308\n",
      "Current iteration=900, loss=0.02049853672464766\n",
      "Current iteration=1000, loss=0.012506510848864096\n",
      "Current iteration=1100, loss=0.055355626239440694\n",
      "Current iteration=1200, loss=0.014321630840522813\n",
      "Current iteration=1300, loss=0.08756512562057436\n",
      "Current iteration=1400, loss=0.011688428836237297\n",
      "Current iteration=1500, loss=0.050162449528198726\n",
      "Current iteration=1600, loss=0.05514422166671924\n",
      "Current iteration=1700, loss=0.009402357212498156\n",
      "Current iteration=1800, loss=0.023124189575608568\n",
      "Current iteration=1900, loss=0.01605615649897521\n",
      "Current iteration=2000, loss=0.14417955844334077\n",
      "Current iteration=2100, loss=0.07224706116600038\n",
      "Current iteration=2200, loss=0.04380426125592814\n",
      "Current iteration=2300, loss=0.022751178305622725\n",
      "Current iteration=2400, loss=0.020983518093357845\n",
      "Current iteration=2500, loss=0.015492113264804298\n",
      "Current iteration=2600, loss=0.03982881175355022\n",
      "Current iteration=2700, loss=0.012283883551260357\n",
      "Current iteration=2800, loss=0.03505918418283782\n",
      "Current iteration=2900, loss=0.06840449647583187\n",
      "Current iteration=3000, loss=0.019560754817449445\n",
      "Current iteration=3100, loss=0.014314016919980084\n",
      "Current iteration=3200, loss=0.0180405403232721\n",
      "Current iteration=3300, loss=3.1246390750540565\n",
      "Current iteration=3400, loss=0.03597402227287628\n",
      "Current iteration=3500, loss=0.09074688815840819\n",
      "Current iteration=3600, loss=0.04803183042709471\n",
      "Current iteration=3700, loss=0.07028113149094894\n",
      "Current iteration=3800, loss=0.06801528889214267\n",
      "Current iteration=3900, loss=0.07704793812326054\n",
      "Current iteration=4000, loss=1.3458027246875526\n",
      "Current iteration=4100, loss=2.9653255103652354\n",
      "Current iteration=4200, loss=0.02889410880113761\n",
      "Current iteration=4300, loss=0.022636988275640903\n",
      "Current iteration=4400, loss=0.02254797242022823\n",
      "Current iteration=4500, loss=0.15644732004342385\n",
      "Current iteration=4600, loss=0.004018780257481367\n",
      "Current iteration=4700, loss=0.0029860039991921367\n",
      "Current iteration=4800, loss=0.16478205904407012\n",
      "Current iteration=4900, loss=0.035439573618969857\n",
      "Current iteration=5000, loss=0.041156276954350646\n",
      "Current iteration=5100, loss=0.041823141879593025\n",
      "Current iteration=5200, loss=0.007632788387512849\n",
      "Current iteration=5300, loss=0.02387861405817498\n",
      "Current iteration=5400, loss=0.006988087620942262\n",
      "Current iteration=5500, loss=0.022025620057518264\n",
      "Current iteration=5600, loss=0.029632596338883467\n",
      "Current iteration=5700, loss=0.012519549661575292\n",
      "Current iteration=5800, loss=0.019117714530348696\n",
      "Current iteration=5900, loss=0.02338767216454618\n",
      "Current iteration=6000, loss=0.03200464452922195\n",
      "Current iteration=6100, loss=0.022679767687065533\n",
      "Current iteration=6200, loss=0.011895386886769664\n",
      "Current iteration=6300, loss=0.010529500288671848\n",
      "Current iteration=6400, loss=0.00815389697361951\n",
      "Current iteration=6500, loss=0.015154066086565799\n",
      "Current iteration=6600, loss=0.024075086599395702\n",
      "Current iteration=6700, loss=0.026269192711048207\n",
      "Current iteration=6800, loss=0.07679972097285256\n",
      "Current iteration=6900, loss=0.020979705491653147\n",
      "Current iteration=7000, loss=0.010821208716608978\n",
      "Current iteration=7100, loss=0.005960280792343331\n",
      "Current iteration=7200, loss=0.01615262740925963\n",
      "Current iteration=7300, loss=0.009073715013762794\n",
      "Current iteration=7400, loss=0.07537098036863914\n",
      "Current iteration=7500, loss=0.010725790172485395\n",
      "Current iteration=7600, loss=0.010171018642229263\n",
      "Current iteration=7700, loss=0.13118558747399658\n",
      "Current iteration=7800, loss=0.06515285451849273\n",
      "Current iteration=7900, loss=0.024214473055921892\n",
      "Current iteration=8000, loss=0.09395637657440234\n",
      "Current iteration=8100, loss=0.04248739708834963\n",
      "Current iteration=8200, loss=0.009382643235380649\n",
      "Current iteration=8300, loss=0.0063148117805319984\n",
      "Current iteration=8400, loss=0.030069513606739574\n",
      "Current iteration=8500, loss=0.06502119667422096\n",
      "Current iteration=8600, loss=0.007366514592647612\n",
      "Current iteration=8700, loss=0.007936507489409316\n",
      "Current iteration=8800, loss=0.04377603840941422\n",
      "Current iteration=8900, loss=0.02305962993866526\n",
      "Current iteration=9000, loss=0.011414628753705447\n",
      "Current iteration=9100, loss=0.019053654245812124\n",
      "Current iteration=9200, loss=3.2109410774380422\n",
      "Current iteration=9300, loss=0.05351546663018332\n",
      "Current iteration=9400, loss=0.029689348015344418\n",
      "Current iteration=9500, loss=0.019476520686108265\n",
      "Current iteration=9600, loss=0.02432981127497183\n",
      "Current iteration=9700, loss=0.049201562848681935\n",
      "Current iteration=9800, loss=0.006457911670678839\n",
      "Current iteration=9900, loss=0.011365690469923579\n",
      "Current iteration=0, loss=1.7915439784736469\n",
      "Current iteration=100, loss=0.666065220571836\n",
      "Current iteration=200, loss=0.19885732141248508\n",
      "Current iteration=300, loss=0.08025643404797486\n",
      "Current iteration=400, loss=0.9259741823745935\n",
      "Current iteration=500, loss=0.6878810639432971\n",
      "Current iteration=600, loss=0.12363830520925799\n",
      "Current iteration=700, loss=0.847332307637794\n",
      "Current iteration=800, loss=0.09654833857302061\n",
      "Current iteration=900, loss=1.3074778895859096\n",
      "Current iteration=1000, loss=1.7735532647800707\n",
      "Current iteration=1100, loss=1.2089099718016438\n",
      "Current iteration=1200, loss=0.4494065707306252\n",
      "Current iteration=1300, loss=0.33688673162484695\n",
      "Current iteration=1400, loss=0.23684798437877808\n",
      "Current iteration=1500, loss=0.43272801460465915\n",
      "Current iteration=1600, loss=0.3381151806840492\n",
      "Current iteration=1700, loss=1.4468907858471303\n",
      "Current iteration=1800, loss=0.16665278717735224\n",
      "Current iteration=1900, loss=0.17293353884239054\n",
      "Current iteration=2000, loss=1.7494723499442544\n",
      "Current iteration=2100, loss=0.12050313786372821\n",
      "Current iteration=2200, loss=0.19936342920260094\n",
      "Current iteration=2300, loss=0.04681007149707401\n",
      "Current iteration=2400, loss=0.4348081259572245\n",
      "Current iteration=2500, loss=0.1926673093506848\n",
      "Current iteration=2600, loss=0.27939999514174707\n",
      "Current iteration=2700, loss=0.12017579123516565\n",
      "Current iteration=2800, loss=0.12763284429431557\n",
      "Current iteration=2900, loss=0.029419815002681152\n",
      "Current iteration=3000, loss=0.5926235347273555\n",
      "Current iteration=3100, loss=0.2189299010314902\n",
      "Current iteration=3200, loss=0.22540375435135226\n",
      "Current iteration=3300, loss=0.16356871022015002\n",
      "Current iteration=3400, loss=0.9141916649670938\n",
      "Current iteration=3500, loss=0.6809206812971497\n",
      "Current iteration=3600, loss=0.745068176238847\n",
      "Current iteration=3700, loss=1.188848747391257\n",
      "Current iteration=3800, loss=0.724091850922247\n",
      "Current iteration=3900, loss=0.06167589116167905\n",
      "Current iteration=4000, loss=1.3116053680675652\n",
      "Current iteration=4100, loss=0.45004013409721894\n",
      "Current iteration=4200, loss=0.3997224296467621\n",
      "Current iteration=4300, loss=1.0827193759053153\n",
      "Current iteration=4400, loss=1.1285035216313788\n",
      "Current iteration=4500, loss=0.6425678572685171\n",
      "Current iteration=4600, loss=2.447060790765093\n",
      "Current iteration=4700, loss=0.05684689614272204\n",
      "Current iteration=4800, loss=0.45185923870940237\n",
      "Current iteration=4900, loss=0.36117062199693417\n",
      "Current iteration=5000, loss=0.04792623753955051\n",
      "Current iteration=5100, loss=0.047727212432416725\n",
      "Current iteration=5200, loss=0.3922802676881192\n",
      "Current iteration=5300, loss=0.21885590511477493\n",
      "Current iteration=5400, loss=0.24582901389827905\n",
      "Current iteration=5500, loss=0.49342046214138696\n",
      "Current iteration=5600, loss=1.7239344945116244\n",
      "Current iteration=5700, loss=0.20968361686623038\n",
      "Current iteration=5800, loss=0.10899998988003867\n",
      "Current iteration=5900, loss=2.648458431390933\n",
      "Current iteration=6000, loss=0.0639685031481715\n",
      "Current iteration=6100, loss=0.39292694390914945\n",
      "Current iteration=6200, loss=0.15557693196815126\n",
      "Current iteration=6300, loss=0.7505644984594072\n",
      "Current iteration=6400, loss=0.699331325975629\n",
      "Current iteration=6500, loss=0.6265815940771654\n",
      "Current iteration=6600, loss=0.17449726075604977\n",
      "Current iteration=6700, loss=1.0850206381255254\n",
      "Current iteration=6800, loss=0.19200359010021195\n",
      "Current iteration=6900, loss=0.1697554043908739\n",
      "Current iteration=7000, loss=0.7354535839403858\n",
      "Current iteration=7100, loss=0.08298710108180896\n",
      "Current iteration=7200, loss=1.7458434173313189\n",
      "Current iteration=7300, loss=0.29280311846959606\n",
      "Current iteration=7400, loss=0.3168522375858194\n",
      "Current iteration=7500, loss=0.6572661278903794\n",
      "Current iteration=7600, loss=0.24433371940616513\n",
      "Current iteration=7700, loss=0.44851305679042786\n",
      "Current iteration=7800, loss=0.13380666418812812\n",
      "Current iteration=7900, loss=0.1326298811812316\n",
      "Current iteration=8000, loss=0.2910648079891469\n",
      "Current iteration=8100, loss=0.8188753715355062\n",
      "Current iteration=8200, loss=0.08680236063207654\n",
      "Current iteration=8300, loss=0.38784668771527847\n",
      "Current iteration=8400, loss=1.1234616019501111\n",
      "Current iteration=8500, loss=0.15889019691524378\n",
      "Current iteration=8600, loss=0.13801677091412984\n",
      "Current iteration=8700, loss=0.1721941654874901\n",
      "Current iteration=8800, loss=0.04054123078544114\n",
      "Current iteration=8900, loss=0.23677888791792265\n",
      "Current iteration=9000, loss=0.3348364665232453\n",
      "Current iteration=9100, loss=0.09110421066613059\n",
      "Current iteration=9200, loss=1.8319105649293088\n",
      "Current iteration=9300, loss=0.5171473133465875\n",
      "Current iteration=9400, loss=0.037551383224521194\n",
      "Current iteration=9500, loss=0.47530663923688876\n",
      "Current iteration=9600, loss=0.430227194082881\n",
      "Current iteration=9700, loss=0.2021397020750405\n",
      "Current iteration=9800, loss=0.9945915589349512\n",
      "Current iteration=9900, loss=0.614367558339794\n",
      "Current iteration=0, loss=0.026308737873004542\n",
      "Current iteration=100, loss=0.004817872768923253\n",
      "Current iteration=200, loss=0.06843250548425592\n",
      "Current iteration=300, loss=0.14995282515516858\n",
      "Current iteration=400, loss=0.01110805961258375\n",
      "Current iteration=500, loss=0.010975932813779581\n",
      "Current iteration=600, loss=0.3683386346266932\n",
      "Current iteration=700, loss=0.023117153978952268\n",
      "Current iteration=800, loss=0.017180208471907054\n",
      "Current iteration=900, loss=0.01072065010494394\n",
      "Current iteration=1000, loss=0.030698321705367192\n",
      "Current iteration=1100, loss=0.02410379747086119\n",
      "Current iteration=1200, loss=0.03934477010617011\n",
      "Current iteration=1300, loss=0.15034941191824958\n",
      "Current iteration=1400, loss=0.011947215551559094\n",
      "Current iteration=1500, loss=0.15596681743428994\n",
      "Current iteration=1600, loss=0.04620417675235919\n",
      "Current iteration=1700, loss=0.048895077972703066\n",
      "Current iteration=1800, loss=1.5608539329062003\n",
      "Current iteration=1900, loss=0.024950325240657496\n",
      "Current iteration=2000, loss=1.236553342803231\n",
      "Current iteration=2100, loss=0.0582400379095538\n",
      "Current iteration=2200, loss=0.04438122659259173\n",
      "Current iteration=2300, loss=0.4235160552359039\n",
      "Current iteration=2400, loss=0.21747435428048068\n",
      "Current iteration=2500, loss=0.014113154471846832\n",
      "Current iteration=2600, loss=0.052518029540971164\n",
      "Current iteration=2700, loss=0.10997175770964357\n",
      "Current iteration=2800, loss=0.1344452259390184\n",
      "Current iteration=2900, loss=0.14881986865203\n",
      "Current iteration=3000, loss=0.014401918435654045\n",
      "Current iteration=3100, loss=0.2395441869193159\n",
      "Current iteration=3200, loss=0.032894928343754855\n",
      "Current iteration=3300, loss=0.015483435501299509\n",
      "Current iteration=3400, loss=0.12868850708999494\n",
      "Current iteration=3500, loss=0.01565245745252534\n",
      "Current iteration=3600, loss=0.18148970682498386\n",
      "Current iteration=3700, loss=0.03765462942995546\n",
      "Current iteration=3800, loss=0.04169086706589933\n",
      "Current iteration=3900, loss=0.18945381481851403\n",
      "Current iteration=4000, loss=0.026942132004637565\n",
      "Current iteration=4100, loss=0.1025146054170699\n",
      "Current iteration=4200, loss=0.1561187066395351\n",
      "Current iteration=4300, loss=0.06493079051142087\n",
      "Current iteration=4400, loss=0.035702186062109245\n",
      "Current iteration=4500, loss=0.07276467503502168\n",
      "Current iteration=4600, loss=0.02069195112866432\n",
      "Current iteration=4700, loss=0.01808244288958868\n",
      "Current iteration=4800, loss=0.043199135320902186\n",
      "Current iteration=4900, loss=0.03534085934566489\n",
      "Current iteration=5000, loss=1.4733314324484543\n",
      "Current iteration=5100, loss=0.030163399164712258\n",
      "Current iteration=5200, loss=0.01698255364481758\n",
      "Current iteration=5300, loss=0.24553805330500622\n",
      "Current iteration=5400, loss=0.014903479422137692\n",
      "Current iteration=5500, loss=0.024777461983700316\n",
      "Current iteration=5600, loss=0.12651335684989665\n",
      "Current iteration=5700, loss=2.5380635389206194\n",
      "Current iteration=5800, loss=0.0269229152069405\n",
      "Current iteration=5900, loss=0.05500651797033491\n",
      "Current iteration=6000, loss=0.031863247901811345\n",
      "Current iteration=6100, loss=0.016738639669837713\n",
      "Current iteration=6200, loss=0.09535534882558408\n",
      "Current iteration=6300, loss=0.06666524214139338\n",
      "Current iteration=6400, loss=0.12463447593473602\n",
      "Current iteration=6500, loss=0.7120737185899941\n",
      "Current iteration=6600, loss=0.08308568898687312\n",
      "Current iteration=6700, loss=0.07908064310912122\n",
      "Current iteration=6800, loss=0.02895772729774949\n",
      "Current iteration=6900, loss=0.041216223709294925\n",
      "Current iteration=7000, loss=0.39767875482492043\n",
      "Current iteration=7100, loss=0.26279923119734605\n",
      "Current iteration=7200, loss=0.06256502979424336\n",
      "Current iteration=7300, loss=0.04030171313007891\n",
      "Current iteration=7400, loss=0.1801379530318557\n",
      "Current iteration=7500, loss=0.04253010330459825\n",
      "Current iteration=7600, loss=0.17423632568624622\n",
      "Current iteration=7700, loss=0.02435179874060579\n",
      "Current iteration=7800, loss=0.03934633821120701\n",
      "Current iteration=7900, loss=0.08531589476987306\n",
      "Current iteration=8000, loss=0.014870829221953685\n",
      "Current iteration=8100, loss=0.03627842010472018\n",
      "Current iteration=8200, loss=0.46074819226414376\n",
      "Current iteration=8300, loss=1.9333593059094127\n",
      "Current iteration=8400, loss=0.036613959902101215\n",
      "Current iteration=8500, loss=0.016288468999054014\n",
      "Current iteration=8600, loss=0.01701750224523366\n",
      "Current iteration=8700, loss=0.04685799006955895\n",
      "Current iteration=8800, loss=0.01994786562487623\n",
      "Current iteration=8900, loss=0.02842148216817333\n",
      "Current iteration=9000, loss=0.0056170156319303366\n",
      "Current iteration=9100, loss=0.36987730823005555\n",
      "Current iteration=9200, loss=0.06235573766001912\n",
      "Current iteration=9300, loss=0.016662764022294226\n",
      "Current iteration=9400, loss=0.0664525656643298\n",
      "Current iteration=9500, loss=0.30724160832955116\n",
      "Current iteration=9600, loss=0.028766676782174365\n",
      "Current iteration=9700, loss=0.032598495430784365\n",
      "Current iteration=9800, loss=0.03475171318983949\n",
      "Current iteration=9900, loss=0.09822657772708832\n",
      "Current iteration=0, loss=0.012870827073953437\n",
      "Current iteration=100, loss=0.2720940808749257\n",
      "Current iteration=200, loss=0.04770186876043079\n",
      "Current iteration=300, loss=1.6968096646000999\n",
      "Current iteration=400, loss=0.5795705813100273\n",
      "Current iteration=500, loss=0.9253475160528418\n",
      "Current iteration=600, loss=1.1628717024096318\n",
      "Current iteration=700, loss=0.16217037176033616\n",
      "Current iteration=800, loss=0.395752431305163\n",
      "Current iteration=900, loss=1.6468918948929452\n",
      "Current iteration=1000, loss=0.33344511859447656\n",
      "Current iteration=1100, loss=0.1505125881951784\n",
      "Current iteration=1200, loss=0.34752103099285353\n",
      "Current iteration=1300, loss=0.12350093542905216\n",
      "Current iteration=1400, loss=0.3412943298085159\n",
      "Current iteration=1500, loss=0.3266361689947559\n",
      "Current iteration=1600, loss=1.1600738258533352\n",
      "Current iteration=1700, loss=0.7008848183033414\n",
      "Current iteration=1800, loss=0.36880050443362605\n",
      "Current iteration=1900, loss=0.5522931467842338\n",
      "Current iteration=2000, loss=0.33590985242022287\n",
      "Current iteration=2100, loss=0.23612256000663273\n",
      "Current iteration=2200, loss=0.1443839401344826\n",
      "Current iteration=2300, loss=0.5608710549961984\n",
      "Current iteration=2400, loss=1.072277478256248\n",
      "Current iteration=2500, loss=0.30874725429198785\n",
      "Current iteration=2600, loss=0.4678163868883274\n",
      "Current iteration=2700, loss=1.2238568184765624\n",
      "Current iteration=2800, loss=0.19503040551850162\n",
      "Current iteration=2900, loss=0.5157488129157867\n",
      "Current iteration=3000, loss=0.8383800071233807\n",
      "Current iteration=3100, loss=0.07930929159587918\n",
      "Current iteration=3200, loss=0.15338358024973203\n",
      "Current iteration=3300, loss=0.36007000744193257\n",
      "Current iteration=3400, loss=0.2860628199799613\n",
      "Current iteration=3500, loss=0.08728687252937022\n",
      "Current iteration=3600, loss=0.3368631112556331\n",
      "Current iteration=3700, loss=0.321982753994404\n",
      "Current iteration=3800, loss=0.4686525441591952\n",
      "Current iteration=3900, loss=0.1289019606639124\n",
      "Current iteration=4000, loss=0.7535057412747294\n",
      "Current iteration=4100, loss=0.5157104312086633\n",
      "Current iteration=4200, loss=0.2645406762895466\n",
      "Current iteration=4300, loss=0.7628224443563868\n",
      "Current iteration=4400, loss=0.341732738047679\n",
      "Current iteration=4500, loss=0.7123481300762748\n",
      "Current iteration=4600, loss=0.16958519438957034\n",
      "Current iteration=4700, loss=0.5117040457088008\n",
      "Current iteration=4800, loss=1.473028280472546\n",
      "Current iteration=4900, loss=0.7230801572765836\n",
      "Current iteration=5000, loss=1.5625943066484433\n",
      "Current iteration=5100, loss=0.1534060567748797\n",
      "Current iteration=5200, loss=0.34494231375490847\n",
      "Current iteration=5300, loss=0.03445353900057073\n",
      "Current iteration=5400, loss=0.5686967369620692\n",
      "Current iteration=5500, loss=0.3968070257287301\n",
      "Current iteration=5600, loss=0.7037021788331005\n",
      "Current iteration=5700, loss=0.29143468564401376\n",
      "Current iteration=5800, loss=0.36383420836221764\n",
      "Current iteration=5900, loss=0.12305799749549297\n",
      "Current iteration=6000, loss=0.3651195157143957\n",
      "Current iteration=6100, loss=0.26123547095807154\n",
      "Current iteration=6200, loss=0.7665041689660974\n",
      "Current iteration=6300, loss=0.12546372007367976\n",
      "Current iteration=6400, loss=1.2408446693563295\n",
      "Current iteration=6500, loss=0.49909719024079424\n",
      "Current iteration=6600, loss=0.18584717671303497\n",
      "Current iteration=6700, loss=0.15579020589085066\n",
      "Current iteration=6800, loss=0.298533238506703\n",
      "Current iteration=6900, loss=1.031536994487575\n",
      "Current iteration=7000, loss=0.16921009499528958\n",
      "Current iteration=7100, loss=1.0767666180117028\n",
      "Current iteration=7200, loss=0.2363650705305708\n",
      "Current iteration=7300, loss=0.15058899430422423\n",
      "Current iteration=7400, loss=0.2117655964987363\n",
      "Current iteration=7500, loss=0.21574499058254548\n",
      "Current iteration=7600, loss=0.4054621934137152\n",
      "Current iteration=7700, loss=3.0024642497499356\n",
      "Current iteration=7800, loss=0.43662203917525283\n",
      "Current iteration=7900, loss=0.43146617567558226\n",
      "Current iteration=8000, loss=0.45146727399618697\n",
      "Current iteration=8100, loss=0.9146910155365738\n",
      "Current iteration=8200, loss=0.24459012787747017\n",
      "Current iteration=8300, loss=0.5103907457231939\n",
      "Current iteration=8400, loss=0.31892161629336874\n",
      "Current iteration=8500, loss=0.27741865967829976\n",
      "Current iteration=8600, loss=0.3012717297245549\n",
      "Current iteration=8700, loss=0.7377125800902578\n",
      "Current iteration=8800, loss=0.20350187862011831\n",
      "Current iteration=8900, loss=0.7264477717759167\n",
      "Current iteration=9000, loss=1.2266739892642098\n",
      "Current iteration=9100, loss=0.8076378366484672\n",
      "Current iteration=9200, loss=0.40039106397354246\n",
      "Current iteration=9300, loss=0.5445271859594861\n",
      "Current iteration=9400, loss=0.48187137108027084\n",
      "Current iteration=9500, loss=0.29399783520320777\n",
      "Current iteration=9600, loss=0.5353341964013852\n",
      "Current iteration=9700, loss=0.10964550790764421\n",
      "Current iteration=9800, loss=0.9057764961373761\n",
      "Current iteration=9900, loss=0.21712001583882848\n",
      "Current iteration=0, loss=0.008458448864838179\n",
      "Current iteration=100, loss=0.5384819725244732\n",
      "Current iteration=200, loss=1.3099515092590426\n",
      "Current iteration=300, loss=0.014966205315072756\n",
      "Current iteration=400, loss=0.8640694712576217\n",
      "Current iteration=500, loss=0.012193409725552221\n",
      "Current iteration=600, loss=0.04479772626365338\n",
      "Current iteration=700, loss=0.17153700271262093\n",
      "Current iteration=800, loss=0.019743086662762813\n",
      "Current iteration=900, loss=0.12635159897951662\n",
      "Current iteration=1000, loss=0.3177508906735098\n",
      "Current iteration=1100, loss=0.025630622572087758\n",
      "Current iteration=1200, loss=0.03164909203842226\n",
      "Current iteration=1300, loss=0.010295958870316672\n",
      "Current iteration=1400, loss=0.036087034575432025\n",
      "Current iteration=1500, loss=0.09727792010370699\n",
      "Current iteration=1600, loss=0.012039660052166988\n",
      "Current iteration=1700, loss=0.1327802656993747\n",
      "Current iteration=1800, loss=0.0685590622266238\n",
      "Current iteration=1900, loss=0.2100733166645509\n",
      "Current iteration=2000, loss=0.10269893426013722\n",
      "Current iteration=2100, loss=0.012160124141878633\n",
      "Current iteration=2200, loss=2.384026701465519\n",
      "Current iteration=2300, loss=0.004166836341341665\n",
      "Current iteration=2400, loss=0.04869090933762273\n",
      "Current iteration=2500, loss=0.28972651441018094\n",
      "Current iteration=2600, loss=0.0189344680251412\n",
      "Current iteration=2700, loss=0.023922608244323296\n",
      "Current iteration=2800, loss=0.004471792863583488\n",
      "Current iteration=2900, loss=0.42060357426435835\n",
      "Current iteration=3000, loss=0.023467254432178047\n",
      "Current iteration=3100, loss=0.06326451091394843\n",
      "Current iteration=3200, loss=0.0408737637673153\n",
      "Current iteration=3300, loss=0.011938421756037073\n",
      "Current iteration=3400, loss=0.23476960324993112\n",
      "Current iteration=3500, loss=0.024612917884813425\n",
      "Current iteration=3600, loss=0.015815444902717198\n",
      "Current iteration=3700, loss=0.01646541077973893\n",
      "Current iteration=3800, loss=0.07915328178472941\n",
      "Current iteration=3900, loss=2.5151537695458144\n",
      "Current iteration=4000, loss=1.3883817970062404\n",
      "Current iteration=4100, loss=1.9533358637605378\n",
      "Current iteration=4200, loss=0.3213296649915712\n",
      "Current iteration=4300, loss=0.04129093226365146\n",
      "Current iteration=4400, loss=0.06368651297992957\n",
      "Current iteration=4500, loss=0.08596093105440453\n",
      "Current iteration=4600, loss=0.3083800952009424\n",
      "Current iteration=4700, loss=0.22322797300776434\n",
      "Current iteration=4800, loss=0.04560945478935819\n",
      "Current iteration=4900, loss=0.06260552210628505\n",
      "Current iteration=5000, loss=0.07614661461909437\n",
      "Current iteration=5100, loss=0.0535659887093692\n",
      "Current iteration=5200, loss=0.13748732334289357\n",
      "Current iteration=5300, loss=0.019327988143198627\n",
      "Current iteration=5400, loss=0.029441454626753235\n",
      "Current iteration=5500, loss=0.07558680108724686\n",
      "Current iteration=5600, loss=0.03110249909311072\n",
      "Current iteration=5700, loss=2.0403925844889694\n",
      "Current iteration=5800, loss=0.2933443865053017\n",
      "Current iteration=5900, loss=0.052003096790159326\n",
      "Current iteration=6000, loss=0.04599240046135335\n",
      "Current iteration=6100, loss=0.33264915928486316\n",
      "Current iteration=6200, loss=0.09298629400985446\n",
      "Current iteration=6300, loss=0.05080177849332611\n",
      "Current iteration=6400, loss=0.057937238986051146\n",
      "Current iteration=6500, loss=0.18827235437132078\n",
      "Current iteration=6600, loss=0.018056308542944186\n",
      "Current iteration=6700, loss=0.04002394554850431\n",
      "Current iteration=6800, loss=1.0801413384631586\n",
      "Current iteration=6900, loss=0.427942307232531\n",
      "Current iteration=7000, loss=0.028760439232289957\n",
      "Current iteration=7100, loss=0.04096653298078208\n",
      "Current iteration=7200, loss=1.884554447457779\n",
      "Current iteration=7300, loss=0.020031811840107525\n",
      "Current iteration=7400, loss=0.048489519555890793\n",
      "Current iteration=7500, loss=0.027375743993705282\n",
      "Current iteration=7600, loss=1.0655226519814305\n",
      "Current iteration=7700, loss=0.027114499707551434\n",
      "Current iteration=7800, loss=2.152156266685607\n",
      "Current iteration=7900, loss=2.210830087510215\n",
      "Current iteration=8000, loss=0.02595751585818669\n",
      "Current iteration=8100, loss=0.029424028156318803\n",
      "Current iteration=8200, loss=0.0348439471570541\n",
      "Current iteration=8300, loss=0.10700723940839828\n",
      "Current iteration=8400, loss=0.09250562199370257\n",
      "Current iteration=8500, loss=0.5391189711368004\n",
      "Current iteration=8600, loss=0.039968737852090784\n",
      "Current iteration=8700, loss=0.03923086346865068\n",
      "Current iteration=8800, loss=0.05928801494744198\n",
      "Current iteration=8900, loss=0.012196501433220718\n",
      "Current iteration=9000, loss=0.03577375652640637\n",
      "Current iteration=9100, loss=0.05719420642384992\n",
      "Current iteration=9200, loss=0.13164388733562596\n",
      "Current iteration=9300, loss=0.028470087086103067\n",
      "Current iteration=9400, loss=0.013204954062002266\n",
      "Current iteration=9500, loss=0.014508206747049784\n",
      "Current iteration=9600, loss=0.029676984502480658\n",
      "Current iteration=9700, loss=0.09161316351897833\n",
      "Current iteration=9800, loss=0.03618807981278799\n",
      "Current iteration=9900, loss=0.0170552703878492\n",
      "Current iteration=0, loss=0.0004881407576213753\n",
      "Current iteration=100, loss=1.371564553801128\n",
      "Current iteration=200, loss=0.5279592135943842\n",
      "Current iteration=300, loss=0.41652979011257396\n",
      "Current iteration=400, loss=0.2577358131562266\n",
      "Current iteration=500, loss=0.1853424154887684\n",
      "Current iteration=600, loss=0.13195134315130264\n",
      "Current iteration=700, loss=0.24204273552315914\n",
      "Current iteration=800, loss=1.770180010164347\n",
      "Current iteration=900, loss=0.4877103572152341\n",
      "Current iteration=1000, loss=0.516552054585412\n",
      "Current iteration=1100, loss=0.193370244447445\n",
      "Current iteration=1200, loss=0.11418174343564402\n",
      "Current iteration=1300, loss=0.23621660429904695\n",
      "Current iteration=1400, loss=0.45380108171858763\n",
      "Current iteration=1500, loss=0.08822214215629254\n",
      "Current iteration=1600, loss=1.1866017260628983\n",
      "Current iteration=1700, loss=0.8410803000580448\n",
      "Current iteration=1800, loss=0.6599293985718513\n",
      "Current iteration=1900, loss=1.638618567570712\n",
      "Current iteration=2000, loss=0.895257854390008\n",
      "Current iteration=2100, loss=0.16706653267504087\n",
      "Current iteration=2200, loss=0.25557411267182495\n",
      "Current iteration=2300, loss=0.15874263257844157\n",
      "Current iteration=2400, loss=0.10124094056547905\n",
      "Current iteration=2500, loss=0.7558630975185391\n",
      "Current iteration=2600, loss=0.4331202607207189\n",
      "Current iteration=2700, loss=0.23911836456355462\n",
      "Current iteration=2800, loss=0.2681744181779267\n",
      "Current iteration=2900, loss=1.1259639284150293\n",
      "Current iteration=3000, loss=0.18054485353624994\n",
      "Current iteration=3100, loss=0.2123469213622453\n",
      "Current iteration=3200, loss=0.18430632591211804\n",
      "Current iteration=3300, loss=0.2815513873053268\n",
      "Current iteration=3400, loss=0.15045946018242007\n",
      "Current iteration=3500, loss=0.1095841655086649\n",
      "Current iteration=3600, loss=1.0382617703471486\n",
      "Current iteration=3700, loss=0.2650744233387081\n",
      "Current iteration=3800, loss=1.117274810831351\n",
      "Current iteration=3900, loss=0.37638947164087044\n",
      "Current iteration=4000, loss=0.0930551623046858\n",
      "Current iteration=4100, loss=0.15522930199825558\n",
      "Current iteration=4200, loss=0.5165961195766404\n",
      "Current iteration=4300, loss=0.3849532193676509\n",
      "Current iteration=4400, loss=0.21942182287839307\n",
      "Current iteration=4500, loss=0.3665682972154074\n",
      "Current iteration=4600, loss=0.31455660689791565\n",
      "Current iteration=4700, loss=0.31672699613322464\n",
      "Current iteration=4800, loss=0.4042916582195637\n",
      "Current iteration=4900, loss=0.6977396441993494\n",
      "Current iteration=5000, loss=1.3684653562673352\n",
      "Current iteration=5100, loss=0.09075396406281436\n",
      "Current iteration=5200, loss=0.9108325556710735\n",
      "Current iteration=5300, loss=1.039759615852357\n",
      "Current iteration=5400, loss=1.249355883071129\n",
      "Current iteration=5500, loss=2.037546665555464\n",
      "Current iteration=5600, loss=0.28087561418361306\n",
      "Current iteration=5700, loss=0.7348941416931373\n",
      "Current iteration=5800, loss=0.36060223342561415\n",
      "Current iteration=5900, loss=0.7141840440771349\n",
      "Current iteration=6000, loss=0.06581505456762615\n",
      "Current iteration=6100, loss=0.16602083069106932\n",
      "Current iteration=6200, loss=0.11638348673945101\n",
      "Current iteration=6300, loss=0.3285952290691686\n",
      "Current iteration=6400, loss=0.5008150730638743\n",
      "Current iteration=6500, loss=1.290961318263871\n",
      "Current iteration=6600, loss=0.06352516178698764\n",
      "Current iteration=6700, loss=0.050159723482956484\n",
      "Current iteration=6800, loss=0.17230082149058767\n",
      "Current iteration=6900, loss=0.31286450968899\n",
      "Current iteration=7000, loss=0.399207948925275\n",
      "Current iteration=7100, loss=1.0239093241362245\n",
      "Current iteration=7200, loss=0.47247236411928534\n",
      "Current iteration=7300, loss=0.23028766752162996\n",
      "Current iteration=7400, loss=0.643092711822182\n",
      "Current iteration=7500, loss=0.16064492771599648\n",
      "Current iteration=7600, loss=0.19682935048083194\n",
      "Current iteration=7700, loss=0.8811603499440828\n",
      "Current iteration=7800, loss=0.8386656563674622\n",
      "Current iteration=7900, loss=0.08372958579326051\n",
      "Current iteration=8000, loss=0.09330986427184262\n",
      "Current iteration=8100, loss=0.02603553198886699\n",
      "Current iteration=8200, loss=0.44308328977767075\n",
      "Current iteration=8300, loss=0.5969526037785623\n",
      "Current iteration=8400, loss=0.4726009817886051\n",
      "Current iteration=8500, loss=0.5362352183427082\n",
      "Current iteration=8600, loss=0.212080773160505\n",
      "Current iteration=8700, loss=0.6197900144601105\n",
      "Current iteration=8800, loss=0.9038544990355862\n",
      "Current iteration=8900, loss=0.5016567761480388\n",
      "Current iteration=9000, loss=0.0785685882981082\n",
      "Current iteration=9100, loss=0.14247201880990756\n",
      "Current iteration=9200, loss=0.09617103644354863\n",
      "Current iteration=9300, loss=0.1464655272472092\n",
      "Current iteration=9400, loss=0.1071786034536526\n",
      "Current iteration=9500, loss=0.27644103141710263\n",
      "Current iteration=9600, loss=0.8253199935227569\n",
      "Current iteration=9700, loss=0.6436548605655334\n",
      "Current iteration=9800, loss=0.19035718376491215\n",
      "Current iteration=9900, loss=0.9454836866777903\n",
      "Current iteration=0, loss=0.6203477875914247\n",
      "Current iteration=100, loss=0.00021493525058179102\n",
      "Current iteration=200, loss=0.0011768421221815112\n",
      "Current iteration=300, loss=0.2467091421431282\n",
      "Current iteration=400, loss=0.010321953616682306\n",
      "Current iteration=500, loss=0.009218947744108858\n",
      "Current iteration=600, loss=0.010905568832445225\n",
      "Current iteration=700, loss=0.06487102406967761\n",
      "Current iteration=800, loss=0.0028239346540629515\n",
      "Current iteration=900, loss=0.02648016422527905\n",
      "Current iteration=1000, loss=0.005528389220063339\n",
      "Current iteration=1100, loss=0.11285051113445962\n",
      "Current iteration=1200, loss=0.009214634551681516\n",
      "Current iteration=1300, loss=0.0011506126683205626\n",
      "Current iteration=1400, loss=0.021958821071040677\n",
      "Current iteration=1500, loss=0.04651369578268978\n",
      "Current iteration=1600, loss=0.044541391372623605\n",
      "Current iteration=1700, loss=0.05245234180990152\n",
      "Current iteration=1800, loss=0.14275787040365345\n",
      "Current iteration=1900, loss=0.06449948595826864\n",
      "Current iteration=2000, loss=0.027658985962091653\n",
      "Current iteration=2100, loss=0.022375705327871358\n",
      "Current iteration=2200, loss=0.009472794065386654\n",
      "Current iteration=2300, loss=0.02543134979216074\n",
      "Current iteration=2400, loss=0.017315787517878403\n",
      "Current iteration=2500, loss=0.014922776544786276\n",
      "Current iteration=2600, loss=0.02945404256987116\n",
      "Current iteration=2700, loss=0.04079132373666029\n",
      "Current iteration=2800, loss=0.03356301054391827\n",
      "Current iteration=2900, loss=0.036197791370419494\n",
      "Current iteration=3000, loss=0.07278716165329382\n",
      "Current iteration=3100, loss=0.02559026728510361\n",
      "Current iteration=3200, loss=0.036645970129579486\n",
      "Current iteration=3300, loss=0.025824347285792852\n",
      "Current iteration=3400, loss=0.04830102560654722\n",
      "Current iteration=3500, loss=0.02474136280402101\n",
      "Current iteration=3600, loss=0.014739365138612407\n",
      "Current iteration=3700, loss=0.018892529698360597\n",
      "Current iteration=3800, loss=0.024522090519984708\n",
      "Current iteration=3900, loss=0.035380937925513674\n",
      "Current iteration=4000, loss=0.02468614645980597\n",
      "Current iteration=4100, loss=0.0014902108307677951\n",
      "Current iteration=4200, loss=0.05856798638246417\n",
      "Current iteration=4300, loss=0.0173911620913137\n",
      "Current iteration=4400, loss=0.013424890099398908\n",
      "Current iteration=4500, loss=0.06964887809553168\n",
      "Current iteration=4600, loss=0.008368400916644122\n",
      "Current iteration=4700, loss=0.03379085212707959\n",
      "Current iteration=4800, loss=0.033299715679772905\n",
      "Current iteration=4900, loss=0.05237963311738669\n",
      "Current iteration=5000, loss=0.03295027141451228\n",
      "Current iteration=5100, loss=0.04534827910523419\n",
      "Current iteration=5200, loss=0.09347766110126818\n",
      "Current iteration=5300, loss=2.8184853781431385\n",
      "Current iteration=5400, loss=0.040937234130328945\n",
      "Current iteration=5500, loss=0.01418310515807097\n",
      "Current iteration=5600, loss=0.005638806942196703\n",
      "Current iteration=5700, loss=0.033055092537059905\n",
      "Current iteration=5800, loss=0.022463523747592126\n",
      "Current iteration=5900, loss=0.03959063968020341\n",
      "Current iteration=6000, loss=0.049680213276115555\n",
      "Current iteration=6100, loss=0.023674501964901964\n",
      "Current iteration=6200, loss=0.4835391277094161\n",
      "Current iteration=6300, loss=0.0150565235749449\n",
      "Current iteration=6400, loss=0.012344870759937074\n",
      "Current iteration=6500, loss=0.02074793859463232\n",
      "Current iteration=6600, loss=0.06237432878597498\n",
      "Current iteration=6700, loss=2.8808680437913456\n",
      "Current iteration=6800, loss=0.005802169051356913\n",
      "Current iteration=6900, loss=0.07789659727467324\n",
      "Current iteration=7000, loss=0.008463576236319326\n",
      "Current iteration=7100, loss=0.07201825356559317\n",
      "Current iteration=7200, loss=0.08087944321067772\n",
      "Current iteration=7300, loss=0.022336942374370467\n",
      "Current iteration=7400, loss=0.026607523679546132\n",
      "Current iteration=7500, loss=0.00622653135912388\n",
      "Current iteration=7600, loss=0.015142924352203553\n",
      "Current iteration=7700, loss=0.013794670594341184\n",
      "Current iteration=7800, loss=0.008767471093182695\n",
      "Current iteration=7900, loss=0.017266470680057846\n",
      "Current iteration=8000, loss=0.13083066004635746\n",
      "Current iteration=8100, loss=0.01439595832458285\n",
      "Current iteration=8200, loss=0.013844143889267059\n",
      "Current iteration=8300, loss=0.0342891622376676\n",
      "Current iteration=8400, loss=0.027381383477306096\n",
      "Current iteration=8500, loss=0.02035495348917956\n",
      "Current iteration=8600, loss=0.03690069800634872\n",
      "Current iteration=8700, loss=0.08363906203033933\n",
      "Current iteration=8800, loss=0.0322980811124154\n",
      "Current iteration=8900, loss=0.0075162728948702535\n",
      "Current iteration=9000, loss=0.05782439174322647\n",
      "Current iteration=9100, loss=0.04743504510746338\n",
      "Current iteration=9200, loss=0.0013949406274072745\n",
      "Current iteration=9300, loss=0.052791559599227524\n",
      "Current iteration=9400, loss=0.018159447526803117\n",
      "Current iteration=9500, loss=0.10357245912866837\n",
      "Current iteration=9600, loss=0.1264043736410259\n",
      "Current iteration=9700, loss=0.022932533278862445\n",
      "Current iteration=9800, loss=2.266985212877949\n",
      "Current iteration=9900, loss=0.24807444106627602\n",
      "Current iteration=0, loss=6.481986568189434\n",
      "Current iteration=100, loss=0.0027651553076654825\n",
      "Current iteration=200, loss=0.01417289220197067\n",
      "Current iteration=300, loss=0.8439341686107924\n",
      "Current iteration=400, loss=0.2305902713359737\n",
      "Current iteration=500, loss=0.18233366882232507\n",
      "Current iteration=600, loss=0.0787038738121401\n",
      "Current iteration=700, loss=0.0933955393081356\n",
      "Current iteration=800, loss=0.1470127828350003\n",
      "Current iteration=900, loss=0.11846394424293398\n",
      "Current iteration=1000, loss=0.35314118396453\n",
      "Current iteration=1100, loss=0.08318501303618965\n",
      "Current iteration=1200, loss=0.01233266102642435\n",
      "Current iteration=1300, loss=0.19995028650605695\n",
      "Current iteration=1400, loss=0.012052301641528101\n",
      "Current iteration=1500, loss=0.218380587481978\n",
      "Current iteration=1600, loss=0.3880062962919302\n",
      "Current iteration=1700, loss=0.10946516337340961\n",
      "Current iteration=1800, loss=0.2603914140206119\n",
      "Current iteration=1900, loss=0.7870321751622642\n",
      "Current iteration=2000, loss=0.2228387970783494\n",
      "Current iteration=2100, loss=0.6330107688610819\n",
      "Current iteration=2200, loss=0.73570506835324\n",
      "Current iteration=2300, loss=0.8966185244838233\n",
      "Current iteration=2400, loss=0.7160238400304003\n",
      "Current iteration=2500, loss=1.0020371620487316\n",
      "Current iteration=2600, loss=0.586542878840805\n",
      "Current iteration=2700, loss=0.35681267436267583\n",
      "Current iteration=2800, loss=1.3218136895383903\n",
      "Current iteration=2900, loss=0.37900419677383224\n",
      "Current iteration=3000, loss=0.05215831987057353\n",
      "Current iteration=3100, loss=0.0640256806008041\n",
      "Current iteration=3200, loss=0.3573067504113035\n",
      "Current iteration=3300, loss=0.11558750489714673\n",
      "Current iteration=3400, loss=0.6726616673304107\n",
      "Current iteration=3500, loss=0.5547916837252626\n",
      "Current iteration=3600, loss=0.016441433281761952\n",
      "Current iteration=3700, loss=0.9607238550942194\n",
      "Current iteration=3800, loss=0.3895263391697704\n",
      "Current iteration=3900, loss=0.616325868868807\n",
      "Current iteration=4000, loss=0.037154737903940305\n",
      "Current iteration=4100, loss=0.10642428121947572\n",
      "Current iteration=4200, loss=0.15010848783189687\n",
      "Current iteration=4300, loss=0.11719391059280777\n",
      "Current iteration=4400, loss=0.04336864009668652\n",
      "Current iteration=4500, loss=0.21286710555208527\n",
      "Current iteration=4600, loss=0.4454734824335479\n",
      "Current iteration=4700, loss=0.5994950260569265\n",
      "Current iteration=4800, loss=0.1460413062094956\n",
      "Current iteration=4900, loss=0.13739961282284177\n",
      "Current iteration=5000, loss=0.542323535160133\n",
      "Current iteration=5100, loss=0.7946559780842167\n",
      "Current iteration=5200, loss=0.03149562650838211\n",
      "Current iteration=5300, loss=1.2748896643202838\n",
      "Current iteration=5400, loss=1.0248308106392416\n",
      "Current iteration=5500, loss=1.1318645431016514\n",
      "Current iteration=5600, loss=0.6629457235328052\n",
      "Current iteration=5700, loss=0.3942344608210685\n",
      "Current iteration=5800, loss=0.8553798289122065\n",
      "Current iteration=5900, loss=0.1774154650980473\n",
      "Current iteration=6000, loss=0.7143890048540553\n",
      "Current iteration=6100, loss=0.2929745389686765\n",
      "Current iteration=6200, loss=0.22138168043118922\n",
      "Current iteration=6300, loss=0.49983467941792886\n",
      "Current iteration=6400, loss=1.505958308282335\n",
      "Current iteration=6500, loss=0.5414852121111301\n",
      "Current iteration=6600, loss=0.1884880926556508\n",
      "Current iteration=6700, loss=0.6023636487447287\n",
      "Current iteration=6800, loss=0.11474791546644925\n",
      "Current iteration=6900, loss=0.9504154626925279\n",
      "Current iteration=7000, loss=0.13224261144043892\n",
      "Current iteration=7100, loss=0.19737414148081092\n",
      "Current iteration=7200, loss=0.3147553286255227\n",
      "Current iteration=7300, loss=0.1820876556504078\n",
      "Current iteration=7400, loss=0.1827740932070517\n",
      "Current iteration=7500, loss=0.13391667966024856\n",
      "Current iteration=7600, loss=0.23543960478898956\n",
      "Current iteration=7700, loss=0.5847379980485674\n",
      "Current iteration=7800, loss=0.15748558510203536\n",
      "Current iteration=7900, loss=0.17872497966686968\n",
      "Current iteration=8000, loss=0.5720904588308178\n",
      "Current iteration=8100, loss=0.3556452211200248\n",
      "Current iteration=8200, loss=0.1895368064516898\n",
      "Current iteration=8300, loss=0.14146701370091827\n",
      "Current iteration=8400, loss=0.41587563621040347\n",
      "Current iteration=8500, loss=0.061025739827742745\n",
      "Current iteration=8600, loss=0.12706015895855396\n",
      "Current iteration=8700, loss=1.6751740356095484\n",
      "Current iteration=8800, loss=0.33817220670811043\n",
      "Current iteration=8900, loss=0.5539284442093853\n",
      "Current iteration=9000, loss=1.6448626095578212\n",
      "Current iteration=9100, loss=1.1235797003571246\n",
      "Current iteration=9200, loss=0.014508078619963954\n",
      "Current iteration=9300, loss=0.25523540586299814\n",
      "Current iteration=9400, loss=0.17130924332081693\n",
      "Current iteration=9500, loss=0.34729740498142064\n",
      "Current iteration=9600, loss=0.38947560948486265\n",
      "Current iteration=9700, loss=0.013250638107745075\n",
      "Current iteration=9800, loss=0.1581529279768504\n",
      "Current iteration=9900, loss=0.12929341952443835\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    tX_tr_poly = build_poly(tX_tr[jet], 1)\n",
    "    loss, w = logistic_regression_sub_gradient_descent(y_tr[jet], tX_tr_poly)\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5252,119) and (17,) not aligned: 119 (dim 1) != 17 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-281a2baefd09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtX_te\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtX_te_poly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_te_poly\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexes_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te_tot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ML\\ML\\Project_1\\scripts\\my_helpers.py\u001b[0m in \u001b[0;36mbuild_predictions\u001b[1;34m(tX, indexes, w, degrees, logistic)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0my_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0my_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels_logistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\ML\\ML\\Project_1\\scripts\\proj1_helpers.py\u001b[0m in \u001b[0;36mpredict_labels_logistic\u001b[1;34m(weights, data)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_labels_logistic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;34m\"\"\"Generates class predictions given weights, and a test data matrix\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5252,119) and (17,) not aligned: 119 (dim 1) != 17 (dim 0)"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "tX_te_poly = list()\n",
    "for t in tX_te:\n",
    "    tX_te_poly.append(build_poly(t, 7))\n",
    "y_pred = build_predictions(tX_te_poly, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    S = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        sigma = sigmoid(tx[i,:].T.dot(w))\n",
    "        S[i,i] = sigma*(1-sigma)\n",
    "    return tx.T.dot(S).dot(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    return calculate_loss(y, tx, w), calculate_gradient(y, tx, w), calculate_hessian(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # mettiamo il gamma???????\n",
    "    gamma = 0.001\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w = w - gamma * np.linalg.inv(hessian).dot(gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method_stochastic(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.random.randn(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 10):\n",
    "            loss, w = learning_by_newton_method(yn, xn, w)\n",
    "            #print (xn.shape)\n",
    "            #print(calculate_hessian(yn, xn, w).shape)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=12.390456798923466\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1c5decea44bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlist_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression_newton_method_stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-4c96f4b139a5>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method_stochastic\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# get loss and update w.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0myn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;31m#print (xn.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#print(calculate_hessian(yn, xn, w).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-34d0d7dd8dc9>\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[0;34m(y, tx, w)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_newton_method_stochastic(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    loss = calculate_loss(y, tx, w) + (lambda_ / 2) * np.linalg.norm(w)**2\n",
    "    gradient = calculate_gradient(y, tx, w) + lambda_ * w\n",
    "    \n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression_newton_method(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ / 2 * np.linalg.norm(w)\n",
    "    gradient = calculate_gradient(y, tx, w) + lambda_ * w\n",
    "    hessian = calculate_hessian(y, tx, w) + N * np.identity(D)\n",
    "    return loss, gradient, hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    #loss, gradient, hessian = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x, gamma, lambda_):\n",
    "    # init parameters\n",
    "    max_iter = 2500\n",
    "    gamma = 0.00001\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        #if iter % 1000 == 0:\n",
    "         #   print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=14466.674805466619\n",
      "Current iteration=1000, loss=4072.1175177931636\n",
      "Current iteration=2000, loss=3797.3365914956603\n",
      "Current iteration=3000, loss=3698.6415491698453\n",
      "Current iteration=4000, loss=3656.664583534501\n",
      "Current iteration=5000, loss=3636.328388515326\n",
      "Current iteration=6000, loss=3625.176408578701\n",
      "Current iteration=7000, loss=3618.215276214807\n",
      "Current iteration=8000, loss=3613.2743696359357\n",
      "Current iteration=9000, loss=3609.3555650016547\n",
      "Current iteration=0, loss=40931.72730642589\n",
      "Current iteration=1000, loss=30168.2204338154\n",
      "Current iteration=2000, loss=29727.17483776414\n",
      "Current iteration=3000, loss=29521.12283087714\n",
      "Current iteration=4000, loss=29369.565185911986\n",
      "Current iteration=5000, loss=29247.52917751685\n",
      "Current iteration=6000, loss=29145.808426618063\n",
      "Current iteration=7000, loss=29058.96640328991\n",
      "Current iteration=8000, loss=28983.381937750593\n",
      "Current iteration=9000, loss=28916.541636075282\n",
      "Current iteration=0, loss=4160.962524901352\n",
      "Current iteration=1000, loss=1725.630300075346\n",
      "Current iteration=2000, loss=1653.6575756129382\n",
      "Current iteration=3000, loss=1615.607987764153\n",
      "Current iteration=4000, loss=1592.7915383111101\n",
      "Current iteration=5000, loss=1577.9688408467\n",
      "Current iteration=6000, loss=1567.5790802181475\n",
      "Current iteration=7000, loss=1559.7743970456295\n",
      "Current iteration=8000, loss=1553.5599884476187\n",
      "Current iteration=9000, loss=1548.379538475589\n",
      "Current iteration=0, loss=38803.76546210686\n",
      "Current iteration=1000, loss=32376.068448508893\n",
      "Current iteration=2000, loss=32203.560617231367\n",
      "Current iteration=3000, loss=32129.07761041942\n",
      "Current iteration=4000, loss=32085.396312377703\n",
      "Current iteration=5000, loss=32055.650434053376\n",
      "Current iteration=6000, loss=32033.62283484901\n",
      "Current iteration=7000, loss=32016.36217294422\n",
      "Current iteration=8000, loss=32002.24242815425\n",
      "Current iteration=9000, loss=31990.29215147526\n",
      "Current iteration=0, loss=1655.2354671771493\n",
      "Current iteration=1000, loss=1012.9582768188113\n",
      "Current iteration=2000, loss=1001.6529846165639\n",
      "Current iteration=3000, loss=991.9306092021292\n",
      "Current iteration=4000, loss=983.1926909295072\n",
      "Current iteration=5000, loss=975.1186734281127\n",
      "Current iteration=6000, loss=967.5361806200733\n",
      "Current iteration=7000, loss=960.3507200536568\n",
      "Current iteration=8000, loss=953.5083030843466\n",
      "Current iteration=9000, loss=946.9758603902688\n",
      "Current iteration=0, loss=26301.469766347123\n",
      "Current iteration=1000, loss=33873.318231673344\n",
      "Current iteration=2000, loss=32230.107844769514\n",
      "Current iteration=3000, loss=31013.91121617226\n",
      "Current iteration=4000, loss=30101.165147402524\n",
      "Current iteration=5000, loss=29415.60279829868\n",
      "Current iteration=6000, loss=28899.110440765682\n",
      "Current iteration=7000, loss=28505.97726391117\n",
      "Current iteration=8000, loss=28201.362424788513\n",
      "Current iteration=9000, loss=27959.591562754526\n",
      "Current iteration=0, loss=829.6971751302544\n",
      "Current iteration=1000, loss=322.20254839164966\n",
      "Current iteration=2000, loss=319.0842105426148\n",
      "Current iteration=3000, loss=316.3889624368629\n",
      "Current iteration=4000, loss=314.06092816000404\n",
      "Current iteration=5000, loss=312.0509326680827\n",
      "Current iteration=6000, loss=310.31585741090163\n",
      "Current iteration=7000, loss=308.8180121543427\n",
      "Current iteration=8000, loss=307.5245475682802\n",
      "Current iteration=9000, loss=306.4069091538475\n",
      "Current iteration=0, loss=11479.903604433814\n",
      "Current iteration=1000, loss=9242.73354478644\n",
      "Current iteration=2000, loss=9205.638304159216\n",
      "Current iteration=3000, loss=9182.281970483378\n",
      "Current iteration=4000, loss=9164.258681178133\n",
      "Current iteration=5000, loss=9149.712851197579\n",
      "Current iteration=6000, loss=9137.665389219781\n",
      "Current iteration=7000, loss=9127.481575034406\n",
      "Current iteration=8000, loss=9118.716675942831\n",
      "Current iteration=9000, loss=9111.047950295628\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_penalized_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, gamma, acc = False, ls = False):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    losses_tr = []\n",
    "    losses_te = []\n",
    "    accuracies = []\n",
    "    ws = []\n",
    "    \n",
    "    for k_group in range(k):\n",
    "        index_te = k_indices[k_group]\n",
    "        index_tr = np.setdiff1d(np.arange(len(y)), index_te)\n",
    "        #print(index_te, index_tr)\n",
    "        x_te = x[index_te]\n",
    "        x_tr = x[index_tr]\n",
    "        y_te = y[index_te]\n",
    "        y_tr = y[index_tr]\n",
    "        \n",
    "        # form data with polynomial degree\n",
    "        x_te_poly = x_te\n",
    "        x_tr_poly = x_tr\n",
    "        \n",
    "        if (ls==False):\n",
    "            _, w = logistic_regression_penalized_gradient_descent(y_tr, x_tr_poly, gamma, lambda_)\n",
    "        else:\n",
    "            w = least_squares_lstsq_ver(y_tr, x_tr_poly)\n",
    "        \n",
    "        # calculate the loss for train and test data\n",
    "        rmse_tr = calculate_loss(y_tr, x_tr_poly, w)\n",
    "        rmse_te = calculate_loss(y_te, x_te_poly, w)\n",
    "        #print(lambda_, rmse_te)\n",
    "        losses_tr.append(rmse_tr)\n",
    "        losses_te.append(rmse_te)\n",
    "        \n",
    "        y_pred = predict_labels(w, x_te_poly)\n",
    "        accuracies.append(accuracy(y_te, y_pred))\n",
    "        \n",
    "        ws.append(w)\n",
    "        \n",
    "    if acc==False:\n",
    "        loss_tr = np.median(losses_tr)\n",
    "        loss_te = np.median(losses_te)\n",
    "        return loss_tr, loss_te, np.mean(ws, axis=0)\n",
    "    else:\n",
    "        return np.mean(accuracies), np.mean(ws, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_hypers(y, x, k_fold, seed=1):\n",
    "    #y_sub, x_sub = get_subsample(y, x, 100000, seed)\n",
    "    lambdas = np.logspace(-8, 0, 8)\n",
    "    gammas = np.logspace(-6, 0, 5)\n",
    "    #lambdas=[1e-06]\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    loss_min = np.inf\n",
    "    gamma_star = 0\n",
    "    lambda_star = 0\n",
    "    w_star = 0\n",
    "    for gamma in gammas:\n",
    "        for lambda_ in lambdas:\n",
    "            #print(degree)\n",
    "            loss_tr, loss_te, w = cross_validation(y, x, k_indices, k_fold, lambda_, gamma)\n",
    "            #print(loss_te)\n",
    "            if loss_te < loss_min:\n",
    "                loss_min = loss_te\n",
    "                print(\"New loss: {}, gamma: {}, lambda: {}\".format(loss_te, gamma, lambda_))\n",
    "                gamma_star = gamma\n",
    "                lambda_star = lambda_\n",
    "                w_star = w\n",
    "    return gamma_star, lambda_star, loss_min, w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet 0: \n",
      "New loss: 1179.8024473808698, gamma: 1e-06, lambda: 1e-08\n",
      "New loss: 910.3755491534118, gamma: 3.1622776601683795e-05, lambda: 1e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\core\\_methods.py:75: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jet 0: best loss: 910.3755491534118, degree: 1, lambda: 1e-08\n",
      "jet 1: \n",
      "New loss: 8175.298042598759, gamma: 1e-06, lambda: 1e-08\n",
      "jet 1: best loss: 8175.298042598759, degree: 1, lambda: 1e-08\n",
      "jet 2: \n",
      "New loss: 468.3120611902291, gamma: 1e-06, lambda: 1e-08\n",
      "New loss: 404.87080616488157, gamma: 3.1622776601683795e-05, lambda: 1e-08\n",
      "jet 2: best loss: 404.87080616488157, degree: 1, lambda: 1e-08\n",
      "jet 3: \n",
      "New loss: 8267.720289705223, gamma: 1e-06, lambda: 1e-08\n",
      "jet 3: best loss: 8267.720289705223, degree: 1, lambda: 1e-08\n",
      "jet 4: \n",
      "New loss: 253.01828315110697, gamma: 1e-06, lambda: 1e-08\n",
      "New loss: 240.01456169090176, gamma: 3.1622776601683795e-05, lambda: 1e-08\n",
      "jet 4: best loss: 240.01456169090176, degree: 1, lambda: 1e-08\n",
      "jet 5: \n",
      "New loss: 6438.087913350116, gamma: 1e-06, lambda: 1e-08\n",
      "jet 5: best loss: 6438.087913350116, degree: 1, lambda: 1e-08\n",
      "jet 6: \n",
      "New loss: 79.17604593049067, gamma: 1e-06, lambda: 1e-08\n",
      "New loss: 75.435635555198, gamma: 3.1622776601683795e-05, lambda: 1e-08\n",
      "New loss: 70.24615354077577, gamma: 0.001, lambda: 1e-08\n",
      "jet 6: best loss: 70.24615354077577, degree: 1, lambda: 1e-08\n",
      "jet 7: \n",
      "New loss: 2381.149674944877, gamma: 1e-06, lambda: 1e-08\n",
      "New loss: 2291.0542210783888, gamma: 3.1622776601683795e-05, lambda: 1e-08\n",
      "jet 7: best loss: 2291.0542210783888, degree: 1, lambda: 1e-08\n"
     ]
    }
   ],
   "source": [
    "k_fold = 4\n",
    "gammas_star=[]\n",
    "lambdas_star=[]\n",
    "w_star = []\n",
    "for jet in range(0, 8):\n",
    "    print(\"jet {}: \".format(jet))\n",
    "    gamma_star, lambda_star, loss, w = select_best_hypers(y_tr[jet], tX_tr[jet], k_fold, 1)\n",
    "    gammas_star.append(gamma_star)\n",
    "    lambdas_star.append(lambda_star)\n",
    "    w_star.append(w)\n",
    "    #print(\"jet {}: Best accuracy {}, degree: {},  lambda: {}\".format(jet, acc, degree_star, lambda_star))\n",
    "    print(\"jet {}: best loss: {}, degree: {}, lambda: {}\".format(jet, loss, degree_star, lambda_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7112\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, w_star, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_stochastic_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.1\n",
    "    lambda_ = 0\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_penalized_gradient(yn, xn, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % (max_iter/10) == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.2155590653912363\n",
      "Current iteration=10, loss=0.1508956355151484\n",
      "Current iteration=20, loss=0.0020168964005311254\n",
      "Current iteration=30, loss=2.0536565901292745\n",
      "Current iteration=40, loss=0.009753072049158141\n",
      "Current iteration=50, loss=0.18196398243413742\n",
      "Current iteration=60, loss=0.0038205935751314053\n",
      "Current iteration=70, loss=0.08913086273519967\n",
      "Current iteration=80, loss=0.014503272238130561\n",
      "Current iteration=90, loss=0.018292738459861436\n",
      "Current iteration=0, loss=0.07062056551842473\n",
      "Current iteration=10, loss=0.7965282597665011\n",
      "Current iteration=20, loss=0.7069885815787936\n",
      "Current iteration=30, loss=3.3652117903857186\n",
      "Current iteration=40, loss=0.05276396680956142\n",
      "Current iteration=50, loss=1.1741340392983488\n",
      "Current iteration=60, loss=0.77488158018766\n",
      "Current iteration=70, loss=0.12485615212489057\n",
      "Current iteration=80, loss=0.23986765298619164\n",
      "Current iteration=90, loss=1.2311763015687613\n",
      "Current iteration=0, loss=0.0029167687216279095\n",
      "Current iteration=10, loss=0.021367818904807295\n",
      "Current iteration=20, loss=0.0022494814015029878\n",
      "Current iteration=30, loss=2.434644739319207\n",
      "Current iteration=40, loss=0.16672101624668104\n",
      "Current iteration=50, loss=0.005853835075353798\n",
      "Current iteration=60, loss=0.002749117425444652\n",
      "Current iteration=70, loss=0.01629874211883129\n",
      "Current iteration=80, loss=0.02125738914944768\n",
      "Current iteration=90, loss=0.052018358523026026\n",
      "Current iteration=0, loss=0.010188103066775908\n",
      "Current iteration=10, loss=0.09457305404697514\n",
      "Current iteration=20, loss=1.063952070677507\n",
      "Current iteration=30, loss=0.07628783470567589\n",
      "Current iteration=40, loss=0.17076797240450436\n",
      "Current iteration=50, loss=0.6516560775956867\n",
      "Current iteration=60, loss=0.29506425740766756\n",
      "Current iteration=70, loss=5.682058570462945\n",
      "Current iteration=80, loss=0.05583766193104012\n",
      "Current iteration=90, loss=0.7859035555059567\n",
      "Current iteration=0, loss=0.13487064082953465\n",
      "Current iteration=10, loss=0.7344191883173444\n",
      "Current iteration=20, loss=0.05752590481452944\n",
      "Current iteration=30, loss=0.3102277188524323\n",
      "Current iteration=40, loss=0.3107262984870857\n",
      "Current iteration=50, loss=0.0009706481565988736\n",
      "Current iteration=60, loss=0.011305212676654035\n",
      "Current iteration=70, loss=0.043991016852195225\n",
      "Current iteration=80, loss=0.059185529759595915\n",
      "Current iteration=90, loss=0.002561413820366523\n",
      "Current iteration=0, loss=0.01986858618163969\n",
      "Current iteration=10, loss=0.45886209842804515\n",
      "Current iteration=20, loss=0.5435520444942193\n",
      "Current iteration=30, loss=0.002874157511303821\n",
      "Current iteration=40, loss=5.055377941822594\n",
      "Current iteration=50, loss=0.2762251395221105\n",
      "Current iteration=60, loss=0.0552015729277211\n",
      "Current iteration=70, loss=1.8806696552714863\n",
      "Current iteration=80, loss=0.8584742317246123\n",
      "Current iteration=90, loss=0.08345474863041291\n",
      "Current iteration=0, loss=0.0007656058098928085\n",
      "Current iteration=10, loss=0.06594580542427854\n",
      "Current iteration=20, loss=0.0013599989595891818\n",
      "Current iteration=30, loss=0.00024703116683264426\n",
      "Current iteration=40, loss=0.011314838442327078\n",
      "Current iteration=50, loss=0.005342973082128088\n",
      "Current iteration=60, loss=0.007235139173774501\n",
      "Current iteration=70, loss=1.7346423525934176\n",
      "Current iteration=80, loss=0.002403401626948914\n",
      "Current iteration=90, loss=0.00037601234421835006\n",
      "Current iteration=0, loss=0.0007040750090558231\n",
      "Current iteration=10, loss=0.5451478299902335\n",
      "Current iteration=20, loss=0.10638859504523486\n",
      "Current iteration=30, loss=0.006710839313700654\n",
      "Current iteration=40, loss=0.33062631491524436\n",
      "Current iteration=50, loss=2.730155166791711\n",
      "Current iteration=60, loss=0.16019062857125924\n",
      "Current iteration=70, loss=0.8497380768119966\n",
      "Current iteration=80, loss=0.21064532780962397\n",
      "Current iteration=90, loss=1.4924527062949693\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_penalized_gradient_descent(y_tr[jet], tX_tr[jet], lambdas_star[jet], gammas_star[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.64656\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(tX_te[i][np.isnan(tX_te[i])].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_newton_method(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    loss, gradient, hessian = penalized_logistic_regression_newton_method(y, tx, w, lambda_)\n",
    "    w = w - gamma * np.linalg.inv(hessian).dot(gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_stochastic_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 1000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.01\n",
    "    lambda_ = 0.00001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 10):\n",
    "            loss, w = learning_by_penalized_newton_method(yn, xn, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=61.163505199107995\n",
      "Current iteration=100, loss=4.495607085124813\n",
      "Current iteration=200, loss=6.008608529547024\n",
      "Current iteration=300, loss=4.340153518666623\n",
      "Current iteration=400, loss=4.4714533021184195\n",
      "Current iteration=500, loss=3.2827953213383276\n",
      "Current iteration=600, loss=4.082524019965291\n",
      "Current iteration=700, loss=3.4382800842043415\n",
      "Current iteration=800, loss=0.2766099375315662\n",
      "Current iteration=900, loss=2.697715091604993\n",
      "Current iteration=0, loss=26.663539751757227\n",
      "Current iteration=100, loss=7.9030892799918\n",
      "Current iteration=200, loss=5.204082968295155\n",
      "Current iteration=300, loss=9.722157288835515\n",
      "Current iteration=400, loss=7.466749452980252\n",
      "Current iteration=500, loss=14.622667541512756\n",
      "Current iteration=600, loss=4.063807413810574\n",
      "Current iteration=700, loss=8.542093994134419\n",
      "Current iteration=800, loss=3.1285467370552444\n",
      "Current iteration=900, loss=7.330349870474667\n",
      "Current iteration=0, loss=27.957074090843985\n",
      "Current iteration=100, loss=21.33274630874513\n",
      "Current iteration=200, loss=3.7431162736099006\n",
      "Current iteration=300, loss=0.6386635821380139\n",
      "Current iteration=400, loss=2.4781593220297475\n",
      "Current iteration=500, loss=2.2126047386853243\n",
      "Current iteration=600, loss=9.343717890382297\n",
      "Current iteration=700, loss=1.3927563298696977\n",
      "Current iteration=800, loss=12.370392403096321\n",
      "Current iteration=900, loss=16.55204703279618\n",
      "Current iteration=0, loss=17.52968216620749\n",
      "Current iteration=100, loss=16.882677471375754\n",
      "Current iteration=200, loss=15.690863236725315\n",
      "Current iteration=300, loss=12.031838670271437\n",
      "Current iteration=400, loss=8.484633786228175\n",
      "Current iteration=500, loss=5.991421367190608\n",
      "Current iteration=600, loss=12.340214253851089\n",
      "Current iteration=700, loss=13.78951901732402\n",
      "Current iteration=800, loss=17.438008040161964\n",
      "Current iteration=900, loss=7.215494021082569\n",
      "Current iteration=0, loss=1.4544907794602557\n",
      "Current iteration=100, loss=4.216409483076406\n",
      "Current iteration=200, loss=11.966941580349035\n",
      "Current iteration=300, loss=6.958891210870859\n",
      "Current iteration=400, loss=6.924556025248855\n",
      "Current iteration=500, loss=9.462512126434795\n",
      "Current iteration=600, loss=20.919162685100037\n",
      "Current iteration=700, loss=5.7332572285001975\n",
      "Current iteration=800, loss=3.7890035917435045\n",
      "Current iteration=900, loss=7.745335345419678\n",
      "Current iteration=0, loss=23.67165864532671\n",
      "Current iteration=100, loss=4.567368072537844\n",
      "Current iteration=200, loss=5.719491913302933\n",
      "Current iteration=300, loss=10.849148184316828\n",
      "Current iteration=400, loss=12.520807137067894\n",
      "Current iteration=500, loss=17.1001008990745\n",
      "Current iteration=600, loss=15.553199467337974\n",
      "Current iteration=700, loss=12.099530616462655\n",
      "Current iteration=800, loss=15.59798713594841\n",
      "Current iteration=900, loss=7.012063544191768\n",
      "Current iteration=0, loss=10.109521807992138\n",
      "Current iteration=100, loss=1.6324239166920842\n",
      "Current iteration=200, loss=3.749000319820116\n",
      "Current iteration=300, loss=0.5179947213379986\n",
      "Current iteration=400, loss=2.6374931438271627\n",
      "Current iteration=500, loss=0.23020599730964314\n",
      "Current iteration=600, loss=4.296913507126206\n",
      "Current iteration=700, loss=1.2916338882035852\n",
      "Current iteration=800, loss=6.877921078219946\n",
      "Current iteration=900, loss=5.878032733715277\n",
      "Current iteration=0, loss=22.51818784795066\n",
      "Current iteration=100, loss=22.447186059908322\n",
      "Current iteration=200, loss=18.2986177122384\n",
      "Current iteration=300, loss=18.482883711317605\n",
      "Current iteration=400, loss=13.957465402490003\n",
      "Current iteration=500, loss=6.404274644461693\n",
      "Current iteration=600, loss=17.797165533072636\n",
      "Current iteration=700, loss=19.653500160869253\n",
      "Current iteration=800, loss=4.756992587383142\n",
      "Current iteration=900, loss=12.777436956856747\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_penalized_stochastic_newton_method(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.66538\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3],[2,3,4]])\n",
    "degree = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(1, degree+1).repeat(x.shape[1])\n",
    "psi = np.tile(x, degree)\n",
    "psi = np.power(psi, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2, 3, 3, 3])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  1,  4,  9,  1,  8, 27],\n",
       "       [ 2,  3,  4,  4,  9, 16,  8, 27, 64]], dtype=int32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w,logistic=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7516\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing([], tX_test, test=True) #same function as train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, list_w, logistic=True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
