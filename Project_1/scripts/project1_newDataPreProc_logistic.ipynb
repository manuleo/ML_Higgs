{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , deal with outliers, adding log colm (?)\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for logistic\n",
    "y = y_for_logistic(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(y_tr_tot, x_tr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(y_te_tot, x_te_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c000f93808>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAH1CAYAAADI0JsIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de1xUdf7H8fdw8wZkFiPeNm1tzV+KGpZiRnYRvKAGmRe8ZipumWW/tcw0ykjTreyitepmaZppqZitoam7roqV2pZdzHUrKsUAtQIUlMv5/eHP2RAbc+R8R2Zez8djHnnOzOHzOSDju+/5zvc4LMuyBAAAACMCvN0AAACAPyF8AQAAGET4AgAAMIjwBQAAYBDhCwAAwCDCFwAAgEGELwAAAIOCzvaCr776SuvWrdMPP/yggIAAOZ1OXX/99WrdurWJ/gAAAHyK25GvJUuW6P7775cktW7dWldddZUkacqUKVqwYIH93QEAAPgYh7sV7uPj45Wenq5atWpV2F9UVKTExERlZGTY3iAAAIAvcTvyFRQUpNLS0kr7i4uLFRwcbFtTAAAAvsrtnK8xY8bo1ltvVUxMjCIiIuRwOJSbm6v3339f48ePN9UjAACAz3B72VGScnJytH37duXm5qq8vFyRkZGKiYlR/fr1TfUIAADgM84avgAAAFB1WOcLAADAIMIXAACAQYQvAAAAgwhfAAAABhG+AAAADCJ8AQAAGET4AgAAMIjwBQAAYBDhCwAAwCDCFwAAgEGELwAAAIMIXwAAAAYRvgAAAAwifAEAABhE+AIAADCI8AUAAGAQ4QsAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAMInwBAAAYRPgCAAAwKMjdk9nZ2W4PbtiwYZU2AwAA4OsclmVZv/Zkr169lJWVJafTqdNf5nA4tHHjRtsbBAAA8CVuw1dhYaGSk5OVmpqq6Ohok30BAAD4JLdzvkJDQ5WWlqb09HRT/QAAAPg0tyNfAAAAqFp82hEAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAMInwBAAAYRPgCAAAwiPAFAABgEOELAADAoCBvN2CnYx/uMlKn9rXc9xIAAPw2jHwBAAAYRPgCAAAwiPAFAABgEOELAADAIMIXAACAQYQvAAAAgwhfAAAABhG+AAAADCJ8AQAAGET4AgAAMOis4WvDhg167bXX9N1331XYv2zZMtuaAgAA8FVuw9dTTz2lxYsXKysrSwMHDtTq1atdz73xxhu2NwcAAOBr3N5Ye/PmzVq1apWCgoI0ZMgQjRgxQiEhIerevbssyzLVIwAAgM9wG74sy5LD4ZAkNW3aVHPnztUdd9yhevXqufYDAADgt3N72bFbt24aMmSIdu/eLUm64oor9Nxzz+m+++6rNAcMAAAAZ+d25Gvs2LGKjo5WnTp1XPuio6O1cuVKLViwwPbmAAAAfI3D8uHJW8c+3GWkTu1ro43UAQAA1R/rfAEAABhE+AIAADCI8AUAAGAQ4QsAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAM8ulFVgEAAC40bm8vVN0V79lrpE7Nli1Usv+AkVrBjRsZqQMAAOzBZUcAAACDCF8AAAAGEb4AAAAMInwBAAAYRPgCAAAwiPAFAABgEOELAADAIMIXAACAQYQvAAAAgwhfAAAABp01fGVlZSknJ0eS9OabbyotLU1r1661vTEAAAA7FBYWKiEhQfv376/03J49e5SUlKT4+Hg9/PDDKi0tlSRlZ2dr0KBB6tatm/74xz/q6NGjHtd3G75effVV3XnnnRowYIAeeugh/e1vf1OzZs20YsUKzZkzx+OiAAAA3vDJJ59o4MCBysrKOuPzEyZM0COPPKJ169bJsiwtX75ckvTYY48pOTlZGRkZatWqlV588UWPe3AbvlasWKG1a9dq8eLFysjI0Ny5czVo0CC99NJLWrduncdFAQAAqkp+fr72799f6ZGfn1/ptcuXL1dqaqqcTmel5w4cOKDi4mK1bdtWkpSUlKSMjAyVlJRox44dio+Pr7DfU0HuniwvL1dISIgaNWqkESNGqEaNGq7nysrKPC4KAADwS/s6x3t8bEb/BM2ePbvS/rFjx+qee+6psO+JJ5741a+Tm5uriIgI13ZERIRycnL0448/KjQ0VEFBQRX2e8pt+IqLi9PgwYO1aNEiV/NffvmlJk+erO7du3tcFAAAoKoMGzZMiYmJlfaHh4ef09cpLy+Xw+FwbVuWJYfD4frvL52+fS7chq97771XO3bsUGBgoGtfSEiI7rnnHt1www0eFwUAAKgq4eHh5xy0ziQyMlJ5eXmu7UOHDsnpdKpevXoqKChQWVmZAgMDlZeXd8bLlr/VWT/teM0111TYvvzyywleAACgajkCPH9UkUaNGqlGjRratWuXJGn16tWKjY1VcHCw2rdv71rtIT09XbGxsR7XYZ0vAADgfQ6H54/zNGrUKH366aeSpKeeekrTp09Xt27ddOzYMQ0dOlSSlJqaquXLl6tHjx7auXOn7rvvPs9P1bIs67y7vkAV79lrpE7Nli1Usv+AkVrBjRsZqQMAgEn7Ynt4fOwV/6xe64+6nfMFAABggiPg/EewqgvCFwAA8L4qnLt1ofOfMwUAALgAMPIFAAC8rwomzlcXjHwBAAAYxMgXAADwPibc+4aaLVsYq8USEAAAeO58btdT3fh0+AIAANVEgP/MhPLp8FVQUGCkTlhYmL4dPNpIrcsWz1PJgYNGagU3amCkDgAA/sR/YiYAAMAFwKdHvgAAQDXBnC8AAACDCF8AAADmOPxowr3/nCkAAMAFgJEvAADgfX408kX4AgAA3udHc778J2YCAABcABj5AgAAXudPtxdi5AsAAMCgcwpfTz75pF19AAAAfxbg8PxRzfzqZceHHnqo0r5Nmzbp559/liRNnz7dvq4AAIB/cfjPxbhfDV9169ZVenq6xowZo/DwcEnS+++/r2uvvdZYcwAAwE9UwxEsT/1qzHzwwQf1zDPPaO3atWrYsKESExN10UUXKTExUYmJiSZ7BAAA8BluP+0YExOjli1bKjU1Vf/4xz9UVlZmqi8AAACfdNYLrHXr1tVzzz2nyy+/XBERESZ6AgAAfsbhcHj8qG5+8zpft99+u26//XY7ewEAAP6KCfcAAAAGMeEeAAAAdmDkCwAAeJ0jwH/GgwhfAADA+6rhxHlP+U/MBAAAuAAw8gUAALyPkS8AAADYgZEvAADgfUy49w1hYWHGal22eJ6xWsGNGhirBQCACdVxpXpP+XT4Kt6z10idmi1bqGT/ASO1ghs30r7O8UZqXbF1nSSp5MBB22sRKAHAz7HIKgAAAOxA+AIAADDIpy87AgCAaoIbawMAABjEhHsAAABzHH404Z7wBQAA/MaaNWv00ksvqbS0VMOGDdOgQYNcz+3Zs0cTJ050bR85ckQXXXSR3nnnHa1atUpPP/20LrnkEklSly5dNH78eI96IHwBAADvM3DZMScnR7NmzdLKlSsVEhKiAQMGqEOHDmrevLkkqWXLllq9erUkqaioSLfffrseffRRSdJnn32miRMnKiEh4bz7IHwBAADvO48V7vPz85Wfn19pf3h4uMLDw13bmZmZ6tixo+rWrStJio+PV0ZGhsaOHVvp2Llz5+qaa65R+/btJUmffvqpsrKyNHfuXLVo0UJTpkzRRRdd5FG//vPRAgAA4JMWLlyom2++udJj4cKFFV6Xm5uriIgI17bT6VROTk6lr1dQUKDly5dXCGURERG666679Pbbb6tBgwaaOnWqx/0y8gUAAKq1YcOGKTExsdL+X456SVJ5eXmF2xhZlnXG2xq9/fbbuuWWW1zzuyRpzpw5rj+PHDlSXbt29bhft+Fr9+7dioqKkiRt375dmzdvVlBQkLp27ao2bdp4XBQAAOCXHOdx2fH0y4u/JjIyUjt37nRt5+Xlyel0Vnrdhg0blJKS4touKCjQihUrNHz4cEknQ1tgYKDH/bo909TUVEnSkiVLNG3aNEVGRurSSy/VI488osWLF3tcFAAAoAKHw/PHb9SpUydt375dR44cUVFRkdavX6/Y2NgKr7EsS59//rnatWvn2le7dm399a9/1SeffCJJWrx4sX0jX6csX75cixYt0sUXXyxJ6tu3r/r27avBgwd7XBgAAMDFwKcd69evr/Hjx2vo0KEqKSlR3759FRUVpVGjRmncuHFq3bq1jhw5ouDgYNWoUcN1XGBgoJ599lk9+uijKi4uVtOmTTVz5kyP+3AbvkpLS1VeXq66desqJCTEtT8kJEQB5zE8CAAA4A29evVSr169KuybP3++68+XXHKJtm3bVum49u3ba9WqVVXSg9sEVbduXXXp0kXffPONHn/8cUkn534NGDBA3bp1q5IGAAAAFBDg+aOacTvy9dprr0mSvv76a9f6GSEhIRo3bpy6dOlie3MAAAC+5jfN+br88stdf46OjratGQAA4J/OtOSDr2KdLwAA4H1+FL6q34VSAACAaoyRLwAA4H0B/jPyRfgCAADe5/Cfi3GELwAA4HUOPxr58p+YCQAAcAEgfAEAABjEZUcAAOB91XClek/5dPiq2bKFsVrBjRsZq3XF1nXGaklScKMGRusBAPyQH63z5dPhK++5vxipE3HvGOVMe8ZIrfqT7lfJgYNGap0KXUff32l7rTod20uS8XMDAFwY/GmFe/8Z4wMAALgAEL4AAAAM8unLjgAAoJpgwj0AAIBBfjTni/AFAAC8z4/Cl/+M8QEAAFwAGPkCAABe52DOFwAAgEFcdgQAAIAdGPkCAADeF8DIl8uWLVuUn58vSUpPT9fUqVO1YsUK2xsDAADwRW7D1xNPPKG5c+fq+PHjevbZZ/X222+refPmeu+995SWlmaqRwAA4OscDs8f1Yzby46ZmZl6++23FRgYqM2bN2vZsmUKCQlR//79lZCQYKpHAADg4/zp045uz7RmzZo6fPiwJCkyMlLHjh2TJBUVFSkoiOliAACgijgCPH9UM24T1N13362+ffuqZ8+eaty4sYYMGaKYmBht3bpVI0eONNUjAACAz3Abvm666SZdccUV2rBhg7799lu1bdtWderU0ZNPPqmoqChTPQIAAPiMs147bNKkie644w4TvQAAAH/lR0tNMHELAAB4naMafmrRU4QvAADgfdVw4ryn/OdMAQAALgCMfAEAAO9jzhcAAIBBfjTni8uOAAAABjHyBQAAvM7hR5cdGfkCAAAwyGFZluXtJgAAgH/Le/Ylj4+NuO+PVdiJ/bjsCAAAvM+PJtz7dPjKfeoFI3Wcf7pHJdk/GKkV3DBSBQUFRmqFhYVJkoo+/tT2WrXatpYko+dWmpNnpFZQ/QgjdQAA1QNzvgAAgPcFODx/nIM1a9aoR48eiouL05IlSyo9P3v2bN14443q06eP+vTp43rNnj17lJSUpPj4eD388MMqLS31+FR9euQLAADglJycHM2aNUsrV65USEiIBgwYoA4dOqh58+au13z22Wd65pln1K5duwrHTpgwQWlpaWrbtq0mTZqk5cuXKzk52aM+GPkCAABe5wgI8PjxW2VmZqpjx46qW7euateurfj4eGVkZFR4zWeffaa5c+eqV69emjp1qo4fP64DBw6ouLhYbdu2lSQlJSVVOu5cMPIFAAC87zxurJ2fn6/8/PxK+8PDwxUeHu7azs3NVUTEf+fhOp1O7d6927V99OhRtWzZUhMmTNBll12miRMn6sUXX1SXLl0qHBcREaGcnByP+yV8AQCAam3hwoWaPXt2pf1jx47VPffc49ouLy+X4xefqrQsq8J2nTp1NH/+fNf2iBEjNGnSJMXGxro97lwRvgAAgPedxwr3wwYPU2JiYqX9vxz1kqTIyEjt3LnTtZ2Xlyen0+nazs7OVmZmpvr27SvpZMgKCgpSZGSk8vL++wn5Q4cOVTjuXDHnCwAAeJ3D4fD4ER4ersaNG1d6nB6+OnXqpO3bt+vIkSMqKirS+vXrFRsb63q+Zs2a+vOf/6zvv/9elmVpyZIl6tq1qxo1aqQaNWpo165dkqTVq1dXOO5cMfIFAAC8z8Aiq/Xr19f48eM1dOhQlZSUqG/fvoqKitKoUaM0btw4tW7dWlOnTtUf//hHlZSU6Oqrr9Ydd9whSXrqqac0efJkFRYW6qqrrtLQoUM97sOnby/EIqvnh0VWqwaLrALA2R2e96rHx14yeniV9WGC28uOaWlp+vnnn031AgAA4PPchq/09HT169dP69evN9UPAADwRwEBnj+qGbcdN27cWHPmzNGiRYt0++23a+3atSouLjbVGwAA8BcOh+ePasbthHuHw6HmzZtr8eLFyszM1LJly/TEE0+oadOmioyM1NNPP22qTwAA4MPOZ92s6sZt+PrlXPxOnTqpU6dOKikp0d69e/X999/b3hwAAICvcRu+Bg0aVGlfcHCwWrVqpVatWtnWFAAA8DPVcO6Wp9ye6e23326qDwAAAL/AIqsAAMD7mPMFAABgEJcdAQAAYAdGvgAAgNc5ArjsCAAAYA5zvgAAAAxy+M9MKP85UwAAgAsA4QsAAMAgh/XLewgBAAB4wU/LVnp8bN3+SVXYif2Y8wUAALyPCfe+4fje/xipU6NFcx0+Wmyk1iV1aurIa8uM1Ko3pL8k6avcH22v9XvnxZJk9NwKCgqM1AoLC9Pxr74xUqvG75sZqQMAVY4J9wAAALCDT498AQCAasKPFlll5AsAAMAgRr4AAIDXOZhwDwAAYBCXHQEAAGAHRr4AAID3BfjPeBDhCwAAeJ8frfNF+AIAAF7nTxPu/SdmAgAAXADOOvK1fft21axZU+3atdOCBQv04YcfqlWrVho9erRCQkJM9AgAAOAz3IavmTNnaufOnSotLVXjxo3lcDg0cOBAbdq0SVOnTlVaWpqpPgEAgC/zo6Um3IavLVu2aPXq1Tpx4oS6dOmiLVu2KDg4WLGxserTp4+pHgEAgK/zozlfbsOXZVkqKCjQsWPHVFRUpMLCQl188cUqLi5WSUmJqR4BAICv49OOJ40aNUpxcXGyLEsTJkzQiBEjFBMTo+3bt+u2224z1SMAAIDPcBu++vTpo/j4eJWVlalOnTq65pprtHXrVv3pT3/SddddZ6pHAADg4xzM+fqvmjVruv7cokULtWjRwtaGAAAAfBmLrAIAAO/zown3/jO7DQAA4ALAyBcAAPA+bqwNAABgjj/d25HwBQAAvM+PRr7850wBAAAuAIQvAAAAgwhfAADA+xwOzx/nYM2aNerRo4fi4uK0ZMmSSs9v2LBBffr0Ue/evXXXXXfp559/liStWrVKnTt3Vp8+fdSnTx/NmjXL81O1LMvy+GgAAIAqULgl0+NjQ6/v9Jtel5OTo4EDB2rlypUKCQnRgAED9Mwzz6h58+YneygsVLdu3bRixQrVr19fzz33nAoKCjR58mQ9/vjjateunRISEjzu8xSfnnB/4tvvjdQJuayJjixcaqRWvWEDVZqTZ6RWUP0ISdKJ7/bbXivkd40lyei5fTt4tJFaly2epwX/+NBIrRFdrpUk5RYcs72WM6y27TUA+A/HedxYOz8/X/n5+ZX2h4eHKzw83LWdmZmpjh07qm7dupKk+Ph4ZWRkaOzYsZKkkpISpaamqn79+pJO3tlnzZo1kqRPP/1UWVlZmjt3rlq0aKEpU6booosu8qhfLjsCAIBqbeHChbr55psrPRYuXFjhdbm5uYqIiHBtO51O5eTkuLYvvvhide3aVZJUXFysefPm6ZZbbpEkRURE6K677tLbb7+tBg0aaOrUqR7369MjXwAAoJo4j3W+hg0bpsTExEr7fznqJUnl5eUV1hOzLOuM64sVFBTo7rvv1pVXXun6unPmzHE9P3LkSFdI8wThCwAAeF+A5+Hr9MuLvyYyMlI7d+50befl5cnpdFZ4TW5uru6880517NhRkyZNknQyjK1YsULDhw+XdDK0BQYGetwvlx0BAIBf6NSpk7Zv364jR46oqKhI69evV2xsrOv5srIyjRkzRt27d9fDDz/sGhWrXbu2/vrXv+qTTz6RJC1evJiRLwAAUM2dx4T736p+/foaP368hg4dqpKSEvXt21dRUVEaNWqUxo0bpx9++EFffPGFysrKtG7dOklSq1at9MQTT+jZZ5/Vo48+quLiYjVt2lQzZ870uA/CFwAA8Bu9evVSr169KuybP3++JKl169b68ssvz3hc+/bttWrVqirpgfAFAAC8znEec76qG8IXAADwvvP4tGN1Q/gCAADe50fhi087AgAAGHTWka8NGzZow4YNysvLU3BwsH73u9+pe/fuateunYn+AAAAfIrbka+5c+dqxYoVioqKksPhUNu2bVW/fn1NmjRJy5cvN9UjAADwcY6AAI8f1Y3bka+1a9cqPT1dDodDt912m0aNGqVFixapX79+rgcAAMB5q4YhylNuz/T48eMqKiqSdPIGkz/99JOkkyu9BvjRNwkAANjM4fD8Uc24HflKSkrSwIED1blzZ23dulVJSUnKzs7WXXfdpYSEBFM9AgAA+Ay34Wv06NFq3bq1vvjiC02cOFExMTE6evSoZsyYoRYtWpjqEQAA+DoWWf2vmJgYxcTEuLbr1KlD8AIAAFXKYeDejhcK/zlTAACACwAr3AMAAO+rhhPnPcXIFwAAgEGMfAEAAO9jwj0AAIBBfnTZkfAFAAC8jk87AgAAwBaELwAAAIMclmVZ3m4CAAD4t+P7vvL42BpX/L4KO7GfT8/5KigoMFInLCzMZ2tJZr6PJmudquertSTf/ZkB8GEB/nMxzqfDFwAAqB4cfvRpR/+JmQAAABcARr4AAID3cdkRAADAIC47AgAAwA6MfAEAAO9j5AsAAAB2YOQLAAB4nSPAf0a+CF8AAMD7uLE2AAAA7MDIFwAA8D4m3AMAAMAOZx352rJlizIyMvTDDz8oICBATqdTsbGxio+PN9EfAADwB0y4P+m5557T7t271bt3bzmdTlmWpby8PL311lv6+OOP9eCDD5rqEwAA+DCHH024dxu+1q5dq3fffVcBp91vKSEhQQkJCYQvAACAc+Q2ZtaoUUM//PBDpf3Z2dkKCQmxrSkAAOBnAhyeP6oZtyNfEydO1KBBg9S0aVNFRETI4XAoNzdXWVlZmj59uqkeAQCAjyuqWcPjY8OqsA8T3IavTp06KSMjQ7t371Zubq7Ky8sVGRmpNm3aMPIFAADgAbfhKzs7W5LUqFEjNWrUyLX/0KFDkqSGDRva2BoAAIDvcRu+UlJSlJWV5fqk4y85HA5t3LjR1uYAAAB8jdvwtXTpUiUnJys1NVXR0dGmegIAALDFmjVr9NJLL6m0tFTDhg3ToEGDKjy/Z88ePfzwwzp69Kjat2+vxx57TEFBQcrOztaECRN0+PBhNWvWTE899ZTq1KnjUQ9uP+0YGhqqtLQ0paene/TFAQAALhQ5OTmaNWuWXn/9daWnp2vZsmX6z3/+U+E1EyZM0COPPKJ169bJsiwtX75ckvTYY48pOTlZGRkZatWqlV588UWP+zjrimZRUVF6/PHHPS4AAABwIcjMzFTHjh1Vt25d1a5dW/Hx8crIyHA9f+DAARUXF6tt27aSpKSkJGVkZKikpEQ7duxw3d3n1H5PcWNtAABQreXn5ys/P7/S/vDwcIWHh7u2c3NzFRER4dp2Op3avXv3rz4fERGhnJwc/fjjjwoNDVVQUFCF/Z4ifAEAgGpt4cKFmj17dqX9Y8eO1T333OPaLi8vl8Px30VZLcuqsP1rz5/+OkmVts+FT4evsDBzy675ai3T9ahV/eqZPjcAON2wYcOUmJhYaf8vR70kKTIyUjt37nRt5+Xlyel0Vng+Ly/PtX3o0CE5nU7Vq1dPBQUFKisrU2BgYKXjzpX/3MUSAAD4pPDwcDVu3LjS4/Tw1alTJ23fvl1HjhxRUVGR1q9fr9jYWNfzjRo1Uo0aNbRr1y5J0urVqxUbG6vg4GC1b99ea9eulSSlp6dXOO5cOazTF/DyIQUFBUbqhIWF+Wwtycz30WStU/V8tZbEz6wqagEw63x+v8/ld3bNmjWaO3euSkpK1LdvX40aNUqjRo3SuHHj1Lp1a3355ZeaPHmyCgsLddVVV2n69OkKCQnRgQMHNHHiRB0+fFgNGjTQM888o4suusijfglfVYB/yKtXrVP1fLWWxM+sKmoBMMtU+LoQcNkRAADAIMIXAACAQT79aUcAAFA9lAQGe7sFYxj5AgAAMIjwBQAAYBCXHQEAgNf57toLlRG+AACA15X7UfoifAEAAK/z4WVHK2HOFwAAgEGMfAEAAK9j5AsAAAC2YOQLAAB4HRPu/9+OHTvcHnzNNddUaTMAAMA/+VH2ch++5syZo48//lhRUVGVrsU6HA4tWrTI1uYAAAB8jdvwNX/+fA0dOlTDhg3TzTffbKonAADgZ5hw//+Cg4M1bdo0/etf/zLVDwAA8EPlsjx+VDdnnXDfrFkz/elPfzLRCwAA8FP+NPLlNnxlZ2e7Pbhhw4ZV2gwAAICvcxu+UlJSlJWVJafTecYJ9xs3brS1OQAAAF/jNnwtXbpUycnJSk1NVXR0tKmeAACAn/Gndb7cTrgPDQ1VWlqa0tPTTfUDAAD8UHm55fGjujnrhPuoqChFRUWZ6AUAAPgpPxr44t6OAAAAJhG+AAAADHJY/rSwBgAAuCD9J+eIx8c2r1+vCjux31nnfFVn348aZ6ROk/nP69CLLxupdeldd6rkwEEjtYIbNZAkFbz3d9trhXW9UZKMntt3I8YaqfW7BbP1w+MzjdSKnPKAJKmgoMD2WmFhYZKkwn9stb2WJIV26WzkvKT/npvJ7yPg76rjSvWe4rIjAACAQT498gUAAKoHf5oFRfgCAABeR/gCAAAwqBquleox5nwBAAAYRPgCAAAwiMuOAADA65jzBQAAYBDhCwAAwKByPwpfzPkCAAAwiJEvAADgdYx8AQAAwBZuw1dpaakWLlyoJ598Ujt37qzw3AsvvGBrYwAAwH9YluXxo7pxG74eeeQR7dmzR06nUw888ID+8pe/uJ7btGmT7c0BAAD/UG5ZHj+qG/YFKXQAABhgSURBVLdzvj777DO9/fbbkqRbb71Vw4cPV82aNTV8+PBqmTQBAAC8zW34sixLx44dU+3atVWvXj3Nnz9fAwcOVL169eRwOEz1CAAAfJw/jem4DV+DBw9WYmKiHn30UcXExKh+/fqaP3++Ro4cqcOHD5vqEQAA+DhvX1HLzs7WhAkTdPjwYTVr1kxPPfWU6tSpU+E1ubm5euihh3To0CEFBATogQceUExMjEpKStShQwc1adLE9dqVK1cqMDDwjLXchq/+/furQ4cOCgkJce37/e9/rzVr1uitt946n3MEAABw8fbcrccee0zJycnq2bOn5syZoxdffFETJkyo8JqZM2fqpptu0qBBg/T1119ryJAh+uc//6m9e/eqXbt2evnll39TLbcT7rOzs13BKzs72/XIz89XXFych6cHAABQdfLz87V///5Kj/z8/N90fElJiXbs2KH4+HhJUlJSkjIyMiq9rmvXrkpISJAkXXbZZTp+/LiOHTumTz/9VEeOHFFSUpL69eunDz/80G09tyNfKSkpysrKktPprDQc6HA4tHHjxt90UgAAAHZZuHChZs+eXWn/2LFjdc8995z1+B9//FGhoaEKCjoZiyIiIpSTk1PpdafCmSS9/PLLatmypcLCwuRwOHTzzTcrJSVF+/bt06hRo7RmzRrVq1fvjPXchq+lS5cqOTlZqampio6OPmvzAAAAnjifOV/Dhg1TYmJipf3h4eGV9r377ruaPn16hX2XXXZZpQ8Suvtg4auvvqply5Zp8eLFkqQBAwa4nvuf//kfRUVF6aOPPtItt9xyxuPdhq/Q0FClpaXpzTffJHwBAADbnM+Ur/Dw8DMGrTPp3r27unfvXmHfqQnzZWVlCgwMVF5enpxO5xmPnzlzpjZv3qwlS5YoMjJSkpSenq6rr75av/vd7/7/XCwFBwf/ag9nvb1QVFSUHn/88d90QgAAAJ7w5iKrwcHBat++vdauXSvpZJiKjY2t9LpXX31VH3zwgZYuXeoKXpK0d+9eLViwQJL09ddfa8+ePW4HrbixNgAA8HupqamaOHGiXnrpJTVo0EDPPPOMpJNTsHJzczVu3DjNmTNHoaGhGjJkiOu4efPm6e6779akSZOUkJAgh8OhGTNmKDQ09FdrEb4AAIDXeXudr0aNGum1116rtH/gwIGuP+/YseNXj3/++ed/cy2H5e2zBQAAfm/r3iyPj+3commV9WGCT498FRQUGKkTFhbms7UkM99Hk7VO1fPVWpLv/swOz3vVSK1LRg+X5LvfR+BC5O1FVk3y6fAFAACqB38KX2f9tCMAAACqDiNfAADA6/xpCjrhCwAAeB3hCwAAwKBy/8lezPkCAAAwifAFAABgEJcdAQCA1zHnCwAAwCDCFwAAgEHl8p/wxZwvAAAAgxj5AgAAXsdlx1/IzMxUWFiYWrZsqRdeeEF79+5VdHS0RowYocDAQBM9AgAA+Ay34evPf/6zPvroIxUWFsrpdOqSSy7RwIEDlZGRoWnTpmnKlCmm+gQAAD7MnxZZdRu+Nm/erDVr1uinn35S165d9eGHHyogIECxsbG69dZbTfUIAAB8XLkfpa+zTrg/ceKELr74Yj344IMKCDj58qNHj6q0tNT25gAAAHyN2/CVnJys3r17q6ysTLfffrsk6aOPPlLv3r01bNgwIw0CAADfZ1mWx4/qxu1lx+TkZMXGxlaYWN+wYUPNnTtXV1xxhe3NAQAA/1AdQ5Sn3Iav7OxsBQQEKDs7u8L+OnXqKDs7Ww0bNrS1OQAAAF/jNnylpKQoKytLTqezUiJ1OBzauHGjrc0BAAD4Grfha+nSpUpOTlZqaqqio6NN9QQAAPwMtxf6f6GhoUpLS1N6erqpfgAAgB9iwv0vREVFKSoqykQvAADAT1XDDOUxbqwNAABgEDfWBgAAXlfuR0NfDqs6XiwFAAA+Zfn7n3h8bL+ObaqwE/tx2REAAMAgn77sWFBQYKROWFiYz9aSzHwfTdY6Vc9Xa0n8zKqilsT3sSpqAb+VP12IY+QLAADAIJ8e+QIAANWDP024J3wBAACvI3wBAAAYxJwvAAAA2ILwBQAAYBCXHQEAgNeV+89VR8IXAADwPn+a80X4AgAAXudP4Ys5XwAAAAYx8gUAALyOdb7cuP/++/XMM8/Y0QsAAPBTI7pc6+0WjHEbvoYMGSKHw1Fh32effaahQ4dKkhYtWmRfZwAAAD7IbfiKj4/X/Pnzde+996px48ayLEtTpkzR2LFjTfUHAADgU9xOuB88eLBefvllrVixQtnZ2erQoYPq1Kmja6+9Vtde6z/DgwAAAFXlrJ92bN68uV555RV9+eWXGjdunE6cOGGiLwAAAJ/0m5aaCAkJ0cSJE9W/f3+1adPG7p4AAAB8lts5X9nZ2RW2mzVrprFjx7r2N2zY0L7OAAAAfJDb8JWSkqKsrCw5nU7XyrMOh0OWZcnhcGjjxo1GmgQAAPAVbsPX0qVLlZycrNTUVEVHR5vqCQAAwGe5nfMVGhqqtLQ0paenm+oHAADAp511hfuoqChFRUWZ6AUAAMDncWNtAAAAgwhfAAAABjksy49uIw4AAOBljHwBAAAYdNYJ99VZ9oQpRuo0/PPjOvF1lpFaIZc3VcH6TUZqhcXdJEn6IjvP9lr/0zBCkoye2+F5rxqpdcno4SooKDBSKywsTJJU/OW/ba9V88o/SJKOLFhsey1JqjdisPHvo4l6rlrv/d32WpIU1vVGo99H0z8zoDpg5AsAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAMInwBAAAYRPgCAAAwiPAFAABgEOELAADAIMIXAACAQYQvAAAAgwhfAAAABrm9sfaGDRt0yy23SJLefPNN/fOf/1RQUJC6du2qHj16GGkQAADAl7gd+ZozZ44k6YUXXtA777yjPn36qEePHlq5cqVmzZplpEEAAABf4nbk65T33ntPb775pmrUqCFJ6tKlixISEjR+/HhbmwMAAPA1bke+jh07pkOHDikyMlKFhYWu/cXFxQoK+k25DQAAAL/gNnxdffXVuuOOO/TRRx/p0UcflSStX79evXv31uDBg030BwAA4FPcDl9Nnz5d0smRrry8PElS06ZN9Ze//EUtWrSwvzsAAAAf4zZ8ZWdnu/4cGBio7OxshYaGup5r2LChvd0BAAD4GLfhKyUlRVlZWXI6nbIsq8JzDodDGzdutLU5AAAAX+M2fC1dulTJyclKTU1VdHS0qZ4AAAB8ltsJ96GhoUpLS1N6erqpfgAAAHzaWdeLiIqKUlRUlIleAAAAfB73dgQAADCI8AUAAGCQwzr9Y4wAAACwjU/fI6igoMBInbCwMJ+tJZn5Ppqsdaqer9aS+JlVRS1J2vDZf2yvdUur5pJ89/v45ge7jdS6vcPJuckm/+4DnuKyIwAAgEGELwAAAIMIXwAAAAYRvgAAAAwifAEAABhE+AIAADCI8AUAAGAQ4QsAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAMchu+SktL9cYbb+jw4cM6ceKEZs+erZSUFD3//PM6fvy4qR4BAAB8htvw9eCDD2rHjh0KCAjQjBkzdODAASUnJ+vHH3/UpEmTTPUIAADgM4LcPfnvf/9ba9askSTt2rVLq1atksPh0A033KAePXoYaRAAAMCXuB35ql27tvbt2ydJuvzyy3Xw4EFJUk5OjkJCQuzvDgAAwMe4HfmaOHGi7rjjDl199dWqVauW+vXrpzZt2ujzzz/XY489ZqpHAAAAn+E2fLVr104ZGRnKzMzUt99+q2bNmunSSy/VlClTFBkZaapHAAAAn+E2fGVnZ0uSWrVqpVatWrn2l5eXKzs7Ww0bNrS3OwAAAB/jNnylpKQoKytLTqdTlmVJkhwOhyzLksPh0MaNG400CQAA4Cvchq+lS5cqOTlZqampio6ONtUTAACAz3L7acfQ0FClpaUpPT3dVD8AAAA+ze3IlyRFRUUpKirKRC8AAAA+j3s7AgAAGET4AgAAMMhhnfoYIwAAAGx31jlf1VnBe383Uies640q3JJppFbo9Z10eO4rRmpdknKHJKk0J8/2WkH1IyTJ6LkdnveqmVqjhyu34JiRWs6w2pKkok8/t71WrdZXSZKOLFxqey1JqjdsoAoKCozUCgsLkyQj9U7Vys8ws3RPeLebjX4fTf/Mjrz6uu216g1PlmTm74f033OD7+CyIwAAgEGELwAAAIMIXwAAAAYRvgAAAAwifAEAABhE+AIAADCI8AUAAGAQ4QsAAMAgwhcAAIBBhC8AAACDCF8AAAAGEb4AAAAMcliWZf3ak2PGjNHDDz+sJk2amOwJAADAZ7kd+frkk0905513asGCBSopKTHVEwAAgM9yG77q16+v119/XV9++aXi4uI0b948HThwwFRvAAAAPsftZcfExEStWrVKkpSVlaXly5dr48aNOn78uCIjI/XGG28YaxQAAMAXuA1ft956q9LT0yvt//HHH/X9998rKirK1uYAAAB8jdvwtXnzZt1www0m+wEAAPBpbsNXdna224MbNmxY5Q0BAAD4Mrfhq1evXsrKypLT6dTpL3M4HNq4caPtDQIAAPgSt+GrsLBQycnJSk1NVXR0tMm+AAAAfJLbpSZCQ0OVlpZ2xkn3AAAAOHdnvb1QVFSUHn/8cRO9VLJmzRr16NFDcXFxWrJkie31CgsLlZCQoP3799taZ/bs2erZs6d69uypmTNn2lpLkp577jn16NFDPXv21CuvvGJ7PUmaMWOGJk6caGuNIUOGqGfPnurTp4/69OmjTz75xLZamzZtUlJSkrp37660tDTb6kjSm2++6TqnPn36KDo6WlOnTrWt3urVq11/H2fMmGFbHUmaN2+e4uPj1atXL7300ku21Dj99zgzM1O9evVSXFycZs2aZWstSSopKdGwYcP0wQcfVGmtM9VbtmyZEhIS1KtXLz300EM6ceKEbbVef/119ezZUz169NCMGTMqTUWpylqnLF68WEOGDKmyOr9W76GHHlJcXJzrd+69996zrda//vUv9evXTz179tT9999v289s8+bNFd5HOnbsqJSUlCqrhfNkXaB++OEH68Ybb7R+/PFH6+jRo1avXr2sffv22Vbv448/thISEqyrrrrK+v77722rs23bNqt///7W8ePHrRMnTlhDhw611q9fb1u9Dz74wBowYIBVUlJiFRUVWTfeeKP11Vdf2VbPsiwrMzPT6tChg/Xggw/aVqO8vNzq3LmzVVJSYluNU7777jurc+fO1sGDB60TJ05YAwcOtP7xj3/YXteyLOvf//631bVrV+vw4cO2fP1jx45Z11xzjXX48GGrpKTE6tu3r7Vt2zZbam3bts1KSEiwCgoKrNLSUislJcVat25dldY4/fe4qKjIuuGGG6zvvvvOKikpsUaMGFFlP7szvWd89dVXVv/+/a3WrVtb77//fpXU+bV6X3/9tdW1a1eroKDAKi8vtx544AHrlVdesaXWd999Z3Xt2tU6evSoVVpaavXv39/asmWLLbVO2bdvn3X99ddbgwcPrpI67uolJCRYOTk5VVrnTLUKCgqs6667ztqzZ49lWZY1fvx4a8mSJbbU+qXc3Fzr5ptvtr755psqqYXzd8HeWDszM1MdO3ZU3bp1Vbt2bcXHxysjI8O2esuXL1dqaqqcTqdtNSQpIiJCEydOVEhIiIKDg/X73//+rJ8qPR/XXnutFi1apKCgIB0+fFhlZWWqXbu2bfV++uknzZo1S2PGjLGthiR9/fXXkqQRI0aod+/eWrx4sW213nvvPfXo0UORkZEKDg7WrFmz1KZNG9vq/dKjjz6q8ePHq169erZ8/bKyMpWXl6uoqEilpaUqLS1VjRo1bKn1xRdfqHPnzgoNDVVgYKCuv/56bdiwoUprnP57vHv3bl122WVq0qSJgoKC1KtXryp7HznTe8Zbb72lkSNH2vL34/R6ISEhSk1NVWhoqBwOh/7whz9U2XvJ6bWaNGmiv/3tb6pdu7by8/NVWFio8PBwW2pJ0okTJ/TII49o3LhxVVLDXb2ioiJlZ2dr0qRJ6tWrl55//nmVl5fbUmvbtm1q27atrrzySknS5MmT1bVrV1tq/dLMmTM1YMAANW3atEpq4fwFebuBX5Obm6uIiAjXttPp1O7du22r98QTT9j2tX/piiuucP05KytL7777rpYuXWprzeDgYD3//PNasGCBunXrpvr169tW65FHHtH48eN18OBB22pIUn5+vmJiYjRlyhSVlJRo6NChatasma677roqr/Xtt98qODhYY8aM0cGDB9WlSxfdd999VV7ndJmZmSouLlb37t1tqxEaGqp7771X3bt3V61atXTNNdfo6quvtqXWVVddpWnTpiklJUW1atXSpk2bqvTSlVT59/hM7yM5OTm21JKkBx54QJK0cOHCKqnhrl6jRo3UqFEjSdKRI0e0ZMkSTZ8+3ZZa0sn3keXLl2vGjBmKiopyBQg7aj399NO67bbb1Lhx4yqp4a7eoUOH1LFjR6WmpiosLEwpKSl666231K9fvyqv9e2336p27doaP368vv76a1199dVVNj3j1/4Ny8rK0ocffmjs3zj8NhfsyFd5ebkcDodr27KsCtvV3b59+zRixAg98MADRv5vZNy4cdq+fbsOHjyo5cuX21LjzTffVIMGDRQTE2PL1/+ldu3aaebMmQoLC1O9evXUt29fbd682ZZaZWVl2r59u6ZNm6Zly5Zp9+7drttu2emNN97QHXfcYWuNL7/8UitWrNDf//53bdmyRQEBAXr55ZdtqRUTE6OkpCQNGTJEI0eOVHR0tIKDg22pdYqvv49IUk5OjoYNG6bbbrtNHTp0sLVWv3799MEHH+jSSy/V7Nmzbamxbds2HTx4ULfddpstX/90TZo00Zw5c+R0OlWrVi0NGTLE1veSrVu36v7779fKlStVVFSkefPm2VLrlGXLlik5OVkhISG21sG5uWDDV2RkpPLy8lzbeXl5tl8SNGXXrl0aPny4/vd//1eJiYm21vrqq6+0Z88eSVKtWrUUFxenvXv32lJr7dq12rZtm/r06aPnn39emzZt0rRp02yptXPnTm3fvt21bVmWgoLsGci99NJLFRMTo3r16qlmzZq65ZZbbB2FlU5edtmxY4duuukmW+ts3bpVMTExuuSSSxQSEqKkpCR9+OGHttQqLCxUXFyc1qxZo9dee00hISFq0qSJLbVO8eX3Eenk7/eAAQOUmJiou+++27Y6Bw8e1K5duyRJQUFB6tmzp23vI++884727dunPn36aPLkyfrss89sHWneu3ev1q1b59q2+72kTZs2atKkiQIDA9W9e3fb30s2btyoHj162FoD5+6CDV+dOnXS9u3bdeTIERUVFWn9+vWKjY31dlvn7eDBg7r77rv11FNPqWfPnrbX279/vyZPnqwTJ07oxIkT2rhxo21rtr3yyit65513tHr1ao0bN0433XSTJk2aZEutgoICzZw5U8ePH1dhYaFWrVpVZXMnTnfjjTdq69atys/PV1lZmbZs2aKrrrrKllqn7N27V02bNrV1fp4kXXnllcrMzNSxY8dkWZY2bdqk1q1b21Jr//79uuuuu1RaWqqCggK99dZbtl5SlaQ2bdrom2++0bfffquysjK98847PvE+Ip0Ms3feeafuvfdejRgxwtZaBQUFmjBhgvLz82VZltatW2fb+8j06dP17rvvavXq1UpLS1OrVq307LPP2lJLOhm2pk2bpp9//lklJSVatmyZbe8lnTt31ueff+6alvH3v//d1veSI0eOqLi42Pb/ycG5u2DnfNWvX1/jx4/X0KFDVVJSor59+/rEjbxffvllHT9+XE8++aRr34ABAzRw4EBb6t1www3avXu3br31VgUGBiouLs5I6LPbjTfeqE8++US33nqrysvLlZycrHbt2tlSq02bNho5cqSSk5NVUlKi6667zvZLIt9//70iIyNtrSGd/Mfgiy++UFJSkoKDg9W6dWuNHj3allpXXnml4uLi1Lt3b5WVlWn48OG2L95co0YNPfnkk7rnnnt0/Phx3XDDDerWrZutNU156623dOjQIb3yyiuuJWRuuukm3XvvvVVe6w9/+INGjx6tAQMGKDAwUO3bt7f9krgpV155pUaPHq2BAweqtLRUcXFxSkhIsKVWgwYNNHXqVI0ZM0bHjx9Xy5Yt9eCDD9pSSzr5Pzwm3kdw7tyucA8AAICqdcFedgQAAPBFhC8AAACDCF8AAAAGEb4AAAAMInwBAAAYRPgCAAAwiPAFAABgEOELAADAoP8DeKO7f58l5/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 792x792 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "d = pd.DataFrame(tX_tr[0])\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 11))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.222818</td>\n",
       "      <td>0.153118</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.153118</td>\n",
       "      <td>0.526825</td>\n",
       "      <td>0.554497</td>\n",
       "      <td>0.139487</td>\n",
       "      <td>0.097695</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>0.767174</td>\n",
       "      <td>0.009557</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.683952</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>0.297913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.222818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>0.816215</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>0.591818</td>\n",
       "      <td>0.066611</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>0.444042</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>-0.009865</td>\n",
       "      <td>0.503706</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.112627</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.264899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153118</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079633</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.010717</td>\n",
       "      <td>0.076707</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.488733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013577</td>\n",
       "      <td>0.816215</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>0.235120</td>\n",
       "      <td>0.190329</td>\n",
       "      <td>-0.129888</td>\n",
       "      <td>0.088310</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>-0.013987</td>\n",
       "      <td>0.294493</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>-0.172573</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.051648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.153118</td>\n",
       "      <td>0.080822</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068972</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079633</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.010717</td>\n",
       "      <td>0.076707</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.488733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.526825</td>\n",
       "      <td>0.591818</td>\n",
       "      <td>0.079633</td>\n",
       "      <td>0.235120</td>\n",
       "      <td>0.079633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054352</td>\n",
       "      <td>-0.023773</td>\n",
       "      <td>0.814027</td>\n",
       "      <td>-0.016802</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>0.782848</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.475407</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.477054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.554497</td>\n",
       "      <td>0.066611</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.190329</td>\n",
       "      <td>0.028400</td>\n",
       "      <td>0.054352</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125626</td>\n",
       "      <td>-0.481483</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>-0.014858</td>\n",
       "      <td>0.605737</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>-0.014036</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>-0.060124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139487</td>\n",
       "      <td>-0.052930</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>-0.129888</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>-0.023773</td>\n",
       "      <td>0.125626</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.069603</td>\n",
       "      <td>-0.008716</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.035220</td>\n",
       "      <td>-0.003812</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.146494</td>\n",
       "      <td>-0.013417</td>\n",
       "      <td>0.177136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.097695</td>\n",
       "      <td>0.444042</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>0.088310</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>0.814027</td>\n",
       "      <td>-0.481483</td>\n",
       "      <td>-0.069603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.275861</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>0.453835</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.442722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.011750</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>0.018118</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.016802</td>\n",
       "      <td>-0.007290</td>\n",
       "      <td>-0.008716</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>-0.015613</td>\n",
       "      <td>0.481195</td>\n",
       "      <td>-0.006965</td>\n",
       "      <td>-0.008973</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.016894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.009865</td>\n",
       "      <td>-0.010717</td>\n",
       "      <td>-0.013987</td>\n",
       "      <td>-0.010717</td>\n",
       "      <td>-0.017608</td>\n",
       "      <td>-0.014858</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021377</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>-0.029406</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>-0.317964</td>\n",
       "      <td>-0.009931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.767174</td>\n",
       "      <td>0.503706</td>\n",
       "      <td>0.076707</td>\n",
       "      <td>0.294493</td>\n",
       "      <td>0.076707</td>\n",
       "      <td>0.782848</td>\n",
       "      <td>0.605737</td>\n",
       "      <td>0.035220</td>\n",
       "      <td>0.275861</td>\n",
       "      <td>-0.015613</td>\n",
       "      <td>-0.021377</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>-0.005040</td>\n",
       "      <td>0.300568</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.315199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009557</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>-0.003812</td>\n",
       "      <td>-0.006110</td>\n",
       "      <td>0.481195</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.003898</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>0.003439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>-0.001631</td>\n",
       "      <td>-0.008005</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>-0.014036</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.007499</td>\n",
       "      <td>-0.006965</td>\n",
       "      <td>-0.029406</td>\n",
       "      <td>-0.005040</td>\n",
       "      <td>-0.003898</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>-0.409755</td>\n",
       "      <td>0.003176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.683952</td>\n",
       "      <td>0.112627</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>-0.172573</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>0.475407</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>0.146494</td>\n",
       "      <td>0.453835</td>\n",
       "      <td>-0.008973</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.300568</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.010220</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011333</td>\n",
       "      <td>0.445081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012558</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>0.001739</td>\n",
       "      <td>-0.013417</td>\n",
       "      <td>0.005364</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.317964</td>\n",
       "      <td>-0.000959</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>-0.409755</td>\n",
       "      <td>-0.011333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.297913</td>\n",
       "      <td>0.264899</td>\n",
       "      <td>0.488733</td>\n",
       "      <td>0.051648</td>\n",
       "      <td>0.488733</td>\n",
       "      <td>0.477054</td>\n",
       "      <td>-0.060124</td>\n",
       "      <td>0.177136</td>\n",
       "      <td>0.442722</td>\n",
       "      <td>-0.016894</td>\n",
       "      <td>-0.009931</td>\n",
       "      <td>0.315199</td>\n",
       "      <td>0.003439</td>\n",
       "      <td>0.003176</td>\n",
       "      <td>0.445081</td>\n",
       "      <td>0.005510</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  NaN       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1  NaN  1.000000  0.222818  0.153118  0.013577  0.153118  0.526825  0.554497   \n",
       "2  NaN  0.222818  1.000000  0.080822  0.816215  0.080822  0.591818  0.066611   \n",
       "3  NaN  0.153118  0.080822  1.000000  0.068972  1.000000  0.079633  0.028400   \n",
       "4  NaN  0.013577  0.816215  0.068972  1.000000  0.068972  0.235120  0.190329   \n",
       "5  NaN  0.153118  0.080822  1.000000  0.068972  1.000000  0.079633  0.028400   \n",
       "6  NaN  0.526825  0.591818  0.079633  0.235120  0.079633  1.000000  0.054352   \n",
       "7  NaN  0.554497  0.066611  0.028400  0.190329  0.028400  0.054352  1.000000   \n",
       "8  NaN  0.139487 -0.052930  0.427578 -0.129888  0.427578 -0.023773  0.125626   \n",
       "9  NaN  0.097695  0.444042  0.051413  0.088310  0.051413  0.814027 -0.481483   \n",
       "10 NaN -0.011750  0.008455 -0.004841  0.018118 -0.004841 -0.016802 -0.007290   \n",
       "11 NaN -0.009587 -0.009865 -0.010717 -0.013987 -0.010717 -0.017608 -0.014858   \n",
       "12 NaN  0.767174  0.503706  0.076707  0.294493  0.076707  0.782848  0.605737   \n",
       "13 NaN  0.009557  0.003396  0.014837  0.004991  0.014837 -0.002624  0.004836   \n",
       "14 NaN  0.000045  0.002153 -0.008005 -0.001631 -0.008005  0.001809 -0.014036   \n",
       "15 NaN  0.683952  0.112627  0.305847 -0.172573  0.305847  0.475407 -0.022419   \n",
       "16 NaN -0.012558  0.002814  0.005470  0.006680  0.005470  0.002892  0.001739   \n",
       "17 NaN  0.297913  0.264899  0.488733  0.051648  0.488733  0.477054 -0.060124   \n",
       "\n",
       "          8         9         10        11        12        13        14  \\\n",
       "0        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "1   0.139487  0.097695 -0.011750 -0.009587  0.767174  0.009557  0.000045   \n",
       "2  -0.052930  0.444042  0.008455 -0.009865  0.503706  0.003396  0.002153   \n",
       "3   0.427578  0.051413 -0.004841 -0.010717  0.076707  0.014837 -0.008005   \n",
       "4  -0.129888  0.088310  0.018118 -0.013987  0.294493  0.004991 -0.001631   \n",
       "5   0.427578  0.051413 -0.004841 -0.010717  0.076707  0.014837 -0.008005   \n",
       "6  -0.023773  0.814027 -0.016802 -0.017608  0.782848 -0.002624  0.001809   \n",
       "7   0.125626 -0.481483 -0.007290 -0.014858  0.605737  0.004836 -0.014036   \n",
       "8   1.000000 -0.069603 -0.008716  0.004768  0.035220 -0.003812  0.001460   \n",
       "9  -0.069603  1.000000 -0.011382 -0.007245  0.275861 -0.006110  0.007499   \n",
       "10 -0.008716 -0.011382  1.000000  0.012633 -0.015613  0.481195 -0.006965   \n",
       "11  0.004768 -0.007245  0.012633  1.000000 -0.021377  0.004622 -0.029406   \n",
       "12  0.035220  0.275861 -0.015613 -0.021377  1.000000  0.002202 -0.005040   \n",
       "13 -0.003812 -0.006110  0.481195  0.004622  0.002202  1.000000 -0.003898   \n",
       "14  0.001460  0.007499 -0.006965 -0.029406 -0.005040 -0.003898  1.000000   \n",
       "15  0.146494  0.453835 -0.008973  0.001295  0.300568  0.007707  0.010220   \n",
       "16 -0.013417  0.005364  0.001119 -0.317964 -0.000959  0.011532 -0.409755   \n",
       "17  0.177136  0.442722 -0.016894 -0.009931  0.315199  0.003439  0.003176   \n",
       "\n",
       "          15        16        17  \n",
       "0        NaN       NaN       NaN  \n",
       "1   0.683952 -0.012558  0.297913  \n",
       "2   0.112627  0.002814  0.264899  \n",
       "3   0.305847  0.005470  0.488733  \n",
       "4  -0.172573  0.006680  0.051648  \n",
       "5   0.305847  0.005470  0.488733  \n",
       "6   0.475407  0.002892  0.477054  \n",
       "7  -0.022419  0.001739 -0.060124  \n",
       "8   0.146494 -0.013417  0.177136  \n",
       "9   0.453835  0.005364  0.442722  \n",
       "10 -0.008973  0.001119 -0.016894  \n",
       "11  0.001295 -0.317964 -0.009931  \n",
       "12  0.300568 -0.000959  0.315199  \n",
       "13  0.007707  0.011532  0.003439  \n",
       "14  0.010220 -0.409755  0.003176  \n",
       "15  1.000000 -0.011333  0.445081  \n",
       "16 -0.011333  1.000000  0.005510  \n",
       "17  0.445081  0.005510  1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    first = np.sum(np.log(1+np.exp(tx.dot(w))))\n",
    "    second = y.T.dot(tx.dot(w))\n",
    "    return first - second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    new_w = w - gamma * grad\n",
    "    new_loss = calculate_loss(y, tx, new_w)\n",
    "    if new_loss <= loss:\n",
    "        loss , w = new_loss, new_w\n",
    "        gamma *=2\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.00001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=9713.564627367217\n",
      "Current iteration=1000, loss=3598.244479355567\n",
      "Current iteration=2000, loss=3588.0652021514547\n",
      "Current iteration=3000, loss=3585.067063205915\n",
      "Current iteration=4000, loss=3583.1956217387524\n",
      "Current iteration=5000, loss=3581.688429287375\n",
      "Current iteration=6000, loss=3580.3920323164257\n",
      "Current iteration=7000, loss=3579.2580002973227\n",
      "Current iteration=8000, loss=3578.2602771104357\n",
      "Current iteration=9000, loss=3577.3794429350373\n",
      "Current iteration=0, loss=68304.08975300708\n",
      "Current iteration=1000, loss=29060.060617504856\n",
      "Current iteration=2000, loss=28949.45433270966\n",
      "Current iteration=3000, loss=28922.961199817397\n",
      "Current iteration=4000, loss=28906.169958351576\n",
      "Current iteration=5000, loss=28891.74603589508\n",
      "Current iteration=6000, loss=28878.641286660728\n",
      "Current iteration=7000, loss=28866.521418362696\n",
      "Current iteration=8000, loss=28855.178621915184\n",
      "Current iteration=9000, loss=28844.457470306326\n",
      "Current iteration=0, loss=4053.5660788639666\n",
      "Current iteration=1000, loss=1533.431859503074\n",
      "Current iteration=2000, loss=1501.876623478683\n",
      "Current iteration=3000, loss=1493.2696492749078\n",
      "Current iteration=4000, loss=1489.7744633953635\n",
      "Current iteration=5000, loss=1487.870248666898\n",
      "Current iteration=6000, loss=1486.6093165526252\n",
      "Current iteration=7000, loss=1485.6867163710217\n",
      "Current iteration=8000, loss=1484.981488258655\n",
      "Current iteration=9000, loss=1484.4322002652646\n",
      "Current iteration=0, loss=59676.855599822506\n",
      "Current iteration=0, loss=1711.6710457863912\n",
      "Current iteration=1000, loss=742.0883943040546\n",
      "Current iteration=2000, loss=704.6303599417963\n",
      "Current iteration=3000, loss=685.0567361464664\n",
      "Current iteration=4000, loss=672.9680370499495\n",
      "Current iteration=5000, loss=664.8376492355364\n",
      "Current iteration=6000, loss=659.1980271851564\n",
      "Current iteration=7000, loss=655.2121107275028\n",
      "Current iteration=8000, loss=652.3325294085364\n",
      "Current iteration=9000, loss=650.1992010698459\n",
      "Current iteration=0, loss=60210.38756463796\n",
      "Current iteration=1000, loss=21110.250395422976\n",
      "Current iteration=2000, loss=21076.167111878167\n",
      "Current iteration=3000, loss=21065.175124666595\n",
      "Current iteration=4000, loss=21059.641162999353\n",
      "Current iteration=5000, loss=21056.09155433216\n",
      "Current iteration=6000, loss=21053.366046118674\n",
      "Current iteration=7000, loss=21051.015179829483\n",
      "Current iteration=8000, loss=21048.848762919974\n",
      "Current iteration=9000, loss=21046.780530881093\n",
      "Current iteration=0, loss=672.4064103720339\n",
      "Current iteration=1000, loss=290.758645961482\n",
      "Current iteration=2000, loss=282.27460544884957\n",
      "Current iteration=3000, loss=277.8907560265848\n",
      "Current iteration=4000, loss=274.66839072637634\n",
      "Current iteration=5000, loss=272.0689961364204\n",
      "Current iteration=6000, loss=269.8939565871066\n",
      "Current iteration=7000, loss=268.0448298907208\n",
      "Current iteration=8000, loss=266.45773835371676\n",
      "Current iteration=9000, loss=265.084662998981\n",
      "Current iteration=0, loss=15243.552032783738\n",
      "Current iteration=1000, loss=9430.424140499308\n",
      "Current iteration=2000, loss=9394.284926385546\n",
      "Current iteration=3000, loss=9381.337181715968\n",
      "Current iteration=4000, loss=9373.852927759875\n",
      "Current iteration=5000, loss=9369.103502846201\n",
      "Current iteration=6000, loss=9365.98151217543\n",
      "Current iteration=7000, loss=9363.857706920575\n",
      "Current iteration=8000, loss=9362.350075382246\n",
      "Current iteration=9000, loss=9361.224255159696\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69218\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_sub_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 3000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_gradient_descent(yn, xn, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.13529661680985303\n",
      "Current iteration=100, loss=0.006619917537084934\n",
      "Current iteration=200, loss=0.006474145952022971\n",
      "Current iteration=300, loss=0.04584552160523338\n",
      "Current iteration=400, loss=0.02196314570109435\n",
      "Current iteration=500, loss=0.0010193212563617598\n",
      "Current iteration=600, loss=4.5593942843424884e-08\n",
      "Current iteration=700, loss=0.006477341750429219\n",
      "Current iteration=800, loss=0.03187686532075339\n",
      "Current iteration=900, loss=0.0009545005459552203\n",
      "Current iteration=1000, loss=0.03497321334828652\n",
      "Current iteration=1100, loss=0.00670726426427068\n",
      "Current iteration=1200, loss=0.0036648902550479997\n",
      "Current iteration=1300, loss=0.002549692532486056\n",
      "Current iteration=1400, loss=0.13098449083227326\n",
      "Current iteration=1500, loss=0.001784941310865318\n",
      "Current iteration=1600, loss=0.027322230317328042\n",
      "Current iteration=1700, loss=0.0024520583874172775\n",
      "Current iteration=1800, loss=0.1972891090069856\n",
      "Current iteration=1900, loss=0.008183409894541912\n",
      "Current iteration=2000, loss=0.08912346310741993\n",
      "Current iteration=2100, loss=0.01776600948372882\n",
      "Current iteration=2200, loss=0.007130470796913223\n",
      "Current iteration=2300, loss=0.011308634966877338\n",
      "Current iteration=2400, loss=0.002938586953851202\n",
      "Current iteration=2500, loss=0.024348964086685115\n",
      "Current iteration=2600, loss=0.014951964122438882\n",
      "Current iteration=2700, loss=0.05332827978324441\n",
      "Current iteration=2800, loss=0.01748057125907841\n",
      "Current iteration=2900, loss=0.016785346535944278\n",
      "Current iteration=0, loss=0.05019441778325939\n",
      "Current iteration=100, loss=0.03477293736870012\n",
      "Current iteration=200, loss=0.13646496650267406\n",
      "Current iteration=300, loss=0.10395808551074807\n",
      "Current iteration=400, loss=0.09549625014125951\n",
      "Current iteration=500, loss=0.5295685571595005\n",
      "Current iteration=600, loss=0.4386033838638915\n",
      "Current iteration=700, loss=0.7289631170087331\n",
      "Current iteration=800, loss=1.1198934942764958\n",
      "Current iteration=900, loss=0.4239835371862703\n",
      "Current iteration=1000, loss=0.01670229886361955\n",
      "Current iteration=1100, loss=0.14079322128480953\n",
      "Current iteration=1200, loss=1.0306683063102104\n",
      "Current iteration=1300, loss=1.1158842369982216\n",
      "Current iteration=1400, loss=0.07300190955870062\n",
      "Current iteration=1500, loss=0.22897596807268905\n",
      "Current iteration=1600, loss=0.7633999186701627\n",
      "Current iteration=1700, loss=0.520625051766742\n",
      "Current iteration=1800, loss=0.2750553675205888\n",
      "Current iteration=1900, loss=0.2949663080583693\n",
      "Current iteration=2000, loss=0.08986404585726838\n",
      "Current iteration=2100, loss=0.09752990230312121\n",
      "Current iteration=2200, loss=0.7032391149348621\n",
      "Current iteration=2300, loss=0.17617873273636378\n",
      "Current iteration=2400, loss=0.0002216866720168019\n",
      "Current iteration=2500, loss=0.9208106060654747\n",
      "Current iteration=2600, loss=0.6010936758440921\n",
      "Current iteration=2700, loss=0.07722427233159035\n",
      "Current iteration=2800, loss=0.34224679585637136\n",
      "Current iteration=2900, loss=0.026581194643121985\n",
      "Current iteration=0, loss=7.2612418026258005\n",
      "Current iteration=100, loss=0.0017756761302759616\n",
      "Current iteration=200, loss=0.033159560405060025\n",
      "Current iteration=300, loss=0.2549801103215111\n",
      "Current iteration=400, loss=0.01263557922717138\n",
      "Current iteration=500, loss=1.4993498058503283\n",
      "Current iteration=600, loss=0.02070428958457445\n",
      "Current iteration=700, loss=0.0041359613550073955\n",
      "Current iteration=800, loss=0.07652762207914376\n",
      "Current iteration=900, loss=0.07471008317130709\n",
      "Current iteration=1000, loss=0.019954407056879266\n",
      "Current iteration=1100, loss=0.03331685405720732\n",
      "Current iteration=1200, loss=0.0662975978491371\n",
      "Current iteration=1300, loss=0.01615946462422826\n",
      "Current iteration=1400, loss=0.024314672217061455\n",
      "Current iteration=1500, loss=0.0795388246546616\n",
      "Current iteration=1600, loss=0.0005956206659885137\n",
      "Current iteration=1700, loss=1.7852605755209168\n",
      "Current iteration=1800, loss=0.016519145617110407\n",
      "Current iteration=1900, loss=0.008985222843481796\n",
      "Current iteration=2000, loss=0.005979076929624782\n",
      "Current iteration=2100, loss=0.03501525044541258\n",
      "Current iteration=2200, loss=0.06043759604841721\n",
      "Current iteration=2300, loss=0.008155495210901049\n",
      "Current iteration=2400, loss=0.01680905986033263\n",
      "Current iteration=2500, loss=0.077791053929951\n",
      "Current iteration=2600, loss=0.013779862070782486\n",
      "Current iteration=2700, loss=0.03969310790906923\n",
      "Current iteration=2800, loss=0.0007434733560302406\n",
      "Current iteration=2900, loss=0.006534126907423533\n",
      "Current iteration=0, loss=0.31304282712256076\n",
      "Current iteration=100, loss=0.7100565425490849\n",
      "Current iteration=200, loss=0.1894139613243473\n",
      "Current iteration=300, loss=0.7385685817868431\n",
      "Current iteration=400, loss=0.02056745413968675\n",
      "Current iteration=500, loss=0.6399710350596542\n",
      "Current iteration=600, loss=1.274722121103611\n",
      "Current iteration=700, loss=0.18809690082432445\n",
      "Current iteration=800, loss=0.2265192754381271\n",
      "Current iteration=900, loss=0.7083000827603259\n",
      "Current iteration=1000, loss=0.6285899622094042\n",
      "Current iteration=1100, loss=0.18476322711963003\n",
      "Current iteration=1200, loss=0.05420753423823165\n",
      "Current iteration=1300, loss=0.1543564236025805\n",
      "Current iteration=1400, loss=1.4515714589651565\n",
      "Current iteration=1500, loss=0.07254372739218251\n",
      "Current iteration=1600, loss=0.08893304007493219\n",
      "Current iteration=1700, loss=0.21369077819881932\n",
      "Current iteration=1800, loss=0.1747051782301432\n",
      "Current iteration=1900, loss=0.16833509885436682\n",
      "Current iteration=2000, loss=0.8096032843886657\n",
      "Current iteration=2100, loss=0.6033890239626414\n",
      "Current iteration=2200, loss=0.227653152963187\n",
      "Current iteration=2300, loss=0.036281315453175395\n",
      "Current iteration=2400, loss=0.3783543359525108\n",
      "Current iteration=2500, loss=0.09004801699765413\n",
      "Current iteration=2600, loss=0.2723487494089294\n",
      "Current iteration=2700, loss=0.07437202071129564\n",
      "Current iteration=2800, loss=0.09989519435918304\n",
      "Current iteration=2900, loss=0.869752750568884\n",
      "Current iteration=0, loss=0.04747052095912352\n",
      "Current iteration=100, loss=0.1290284974324485\n",
      "Current iteration=200, loss=0.011292252675489351\n",
      "Current iteration=300, loss=0.01419804318274152\n",
      "Current iteration=400, loss=0.035939257881105746\n",
      "Current iteration=500, loss=0.022469059167431474\n",
      "Current iteration=600, loss=0.009095771012479466\n",
      "Current iteration=700, loss=0.09964133028354819\n",
      "Current iteration=800, loss=1.8571648009235366\n",
      "Current iteration=900, loss=0.1025875974415339\n",
      "Current iteration=1000, loss=0.028405779010155976\n",
      "Current iteration=1100, loss=0.008445467749300967\n",
      "Current iteration=1200, loss=0.24480339755494285\n",
      "Current iteration=1300, loss=0.04211769613361532\n",
      "Current iteration=1400, loss=0.00045515323589338493\n",
      "Current iteration=1500, loss=0.015589415250958801\n",
      "Current iteration=1600, loss=0.2830229553930821\n",
      "Current iteration=1700, loss=2.2843441279243124\n",
      "Current iteration=1800, loss=0.10162750374527994\n",
      "Current iteration=1900, loss=0.08547535624996527\n",
      "Current iteration=2000, loss=0.06200693843462002\n",
      "Current iteration=2100, loss=0.13705334767824118\n",
      "Current iteration=2200, loss=2.4733400877889857\n",
      "Current iteration=2300, loss=0.002959490132393074\n",
      "Current iteration=2400, loss=0.17933149529470915\n",
      "Current iteration=2500, loss=0.0008222502916229057\n",
      "Current iteration=2600, loss=0.0006060330827420754\n",
      "Current iteration=2700, loss=0.027542537389259745\n",
      "Current iteration=2800, loss=0.07390715965703931\n",
      "Current iteration=2900, loss=0.0027648351781273638\n",
      "Current iteration=0, loss=0.585586081053784\n",
      "Current iteration=100, loss=2.273530848045819\n",
      "Current iteration=200, loss=0.8381914198558148\n",
      "Current iteration=300, loss=0.3528298699614418\n",
      "Current iteration=400, loss=0.3860510219975084\n",
      "Current iteration=500, loss=0.21020676137868355\n",
      "Current iteration=600, loss=0.1447976103266544\n",
      "Current iteration=700, loss=0.3041873863701671\n",
      "Current iteration=800, loss=0.77488052104436\n",
      "Current iteration=900, loss=0.2551421118879589\n",
      "Current iteration=1000, loss=0.12748626092900064\n",
      "Current iteration=1100, loss=0.27945106812820386\n",
      "Current iteration=1200, loss=0.7330864708747485\n",
      "Current iteration=1300, loss=0.7887292328528996\n",
      "Current iteration=1400, loss=0.00943399071824036\n",
      "Current iteration=1500, loss=0.35218452116221155\n",
      "Current iteration=1600, loss=0.023980431265927304\n",
      "Current iteration=1700, loss=0.37777410086429736\n",
      "Current iteration=1800, loss=0.6042115910020207\n",
      "Current iteration=1900, loss=0.4184831951161798\n",
      "Current iteration=2000, loss=0.8097435335451356\n",
      "Current iteration=2100, loss=0.009120009757737968\n",
      "Current iteration=2200, loss=0.1538330663755052\n",
      "Current iteration=2300, loss=0.5076209163816437\n",
      "Current iteration=2400, loss=0.24710271875872603\n",
      "Current iteration=2500, loss=0.07525080766283399\n",
      "Current iteration=2600, loss=1.4292373624187362\n",
      "Current iteration=2700, loss=0.3854278241934965\n",
      "Current iteration=2800, loss=0.8167243430296238\n",
      "Current iteration=2900, loss=0.9445020703579424\n",
      "Current iteration=0, loss=0.009777282845723437\n",
      "Current iteration=100, loss=0.015546488081518975\n",
      "Current iteration=200, loss=0.001847785138261447\n",
      "Current iteration=300, loss=0.00021490289833522283\n",
      "Current iteration=400, loss=0.0005092382845622301\n",
      "Current iteration=500, loss=0.001591787194154684\n",
      "Current iteration=600, loss=0.5090718104559646\n",
      "Current iteration=700, loss=0.006203953207534367\n",
      "Current iteration=800, loss=0.17008431474600544\n",
      "Current iteration=900, loss=0.00336560818169643\n",
      "Current iteration=1000, loss=0.035516831900465055\n",
      "Current iteration=1100, loss=0.1904049264678693\n",
      "Current iteration=1200, loss=0.16116954509528725\n",
      "Current iteration=1300, loss=0.04482103129448288\n",
      "Current iteration=1400, loss=0.026153293192836706\n",
      "Current iteration=1500, loss=0.019572216509841412\n",
      "Current iteration=1600, loss=0.03914166706114664\n",
      "Current iteration=1700, loss=0.06775538726096968\n",
      "Current iteration=1800, loss=0.010106395094688476\n",
      "Current iteration=1900, loss=0.002247356540477799\n",
      "Current iteration=2000, loss=7.2941081383606106e-06\n",
      "Current iteration=2100, loss=0.03049036472002321\n",
      "Current iteration=2200, loss=0.01432532613696613\n",
      "Current iteration=2300, loss=2.133214530783632\n",
      "Current iteration=2400, loss=0.09077050586677311\n",
      "Current iteration=2500, loss=1.1124508795057442\n",
      "Current iteration=2600, loss=0.06649325529646327\n",
      "Current iteration=2700, loss=0.7451480256999542\n",
      "Current iteration=2800, loss=0.07182599973256974\n",
      "Current iteration=2900, loss=0.005302032208565951\n",
      "Current iteration=0, loss=0.14043966893292303\n",
      "Current iteration=100, loss=0.0035152765238270693\n",
      "Current iteration=200, loss=0.08392101242928818\n",
      "Current iteration=300, loss=0.08310654667872307\n",
      "Current iteration=400, loss=0.1274413001551754\n",
      "Current iteration=500, loss=0.893933022225802\n",
      "Current iteration=600, loss=0.20194099961394807\n",
      "Current iteration=700, loss=0.006328189077647671\n",
      "Current iteration=800, loss=0.7893378285347099\n",
      "Current iteration=900, loss=0.009849078052315422\n",
      "Current iteration=1000, loss=0.05164555060264273\n",
      "Current iteration=1100, loss=0.1936052385291216\n",
      "Current iteration=1200, loss=7.917824868533604e-05\n",
      "Current iteration=1300, loss=0.022814237460021758\n",
      "Current iteration=1400, loss=0.44383561807983796\n",
      "Current iteration=1500, loss=0.023803402329742932\n",
      "Current iteration=1600, loss=0.38187146355969503\n",
      "Current iteration=1700, loss=0.06582119696358536\n",
      "Current iteration=1800, loss=0.2629744919600021\n",
      "Current iteration=1900, loss=0.7847709477820106\n",
      "Current iteration=2000, loss=0.02903867213895106\n",
      "Current iteration=2100, loss=0.06679832402806798\n",
      "Current iteration=2200, loss=0.5239929774799432\n",
      "Current iteration=2300, loss=0.09678800614178282\n",
      "Current iteration=2400, loss=0.16923623430663687\n",
      "Current iteration=2500, loss=0.2858354120437832\n",
      "Current iteration=2600, loss=0.05980532384923058\n",
      "Current iteration=2700, loss=1.2342915477119423\n",
      "Current iteration=2800, loss=0.4364734294110856\n",
      "Current iteration=2900, loss=0.19200044972611685\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_sub_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72424\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    S = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        sigma = sigmoid(tx[i,:].T.dot(w))\n",
    "        S[i,i] = sigma*(1-sigma)\n",
    "    return tx.T.dot(S).dot(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    return calculate_loss(y, tx, w), calculate_gradient(y, tx, w), calculate_hessian(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # mettiamo il gamma???????\n",
    "    gamma = 0.001\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w = w - gamma * np.linalg.inv(hessian).dot(gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.random.randn(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_newton_method(yn, xn, w)\n",
    "            #print (xn.shape)\n",
    "            #print(calculate_hessian(yn, xn, w).shape)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=22.732827798849165\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-63af591f0cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-fac930ddf20f>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0myn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m#print (xn.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#print(calculate_hessian(yn, xn, w).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-34d0d7dd8dc9>\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_newton_method(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ / 2 * np.linalg.norm(w)\n",
    "    gradient = calculate_gradient(y, tx, w) + lambda_ * w\n",
    "    #hessian = calculate_hessian(y, tx, w) + N * np.identity(D)\n",
    "    #return loss, gradient, hessian\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    #loss, gradient, hessian = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.00001\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=14466.674805466619\n",
      "Current iteration=1000, loss=3596.1595805480833\n",
      "Current iteration=2000, loss=3590.400230578218\n",
      "Current iteration=3000, loss=3588.004053035492\n",
      "Current iteration=4000, loss=3586.0540170462436\n",
      "Current iteration=5000, loss=3584.374003250526\n",
      "Current iteration=6000, loss=3582.911314632176\n",
      "Current iteration=7000, loss=3581.6332421478874\n",
      "Current iteration=8000, loss=3580.51377168331\n",
      "Current iteration=9000, loss=3579.5308840230578\n",
      "Current iteration=0, loss=40931.72730642589\n",
      "Current iteration=1000, loss=28898.054993920876\n",
      "Current iteration=2000, loss=28841.118146807963\n",
      "Current iteration=3000, loss=28827.039627332924\n",
      "Current iteration=4000, loss=28817.220521561856\n",
      "Current iteration=5000, loss=28808.083030179223\n",
      "Current iteration=6000, loss=28799.214736122674\n",
      "Current iteration=7000, loss=28790.539796435227\n",
      "Current iteration=8000, loss=28782.025698634723\n",
      "Current iteration=9000, loss=28773.650047570416\n",
      "Current iteration=0, loss=4160.962524901352\n",
      "Current iteration=1000, loss=1511.6737722597936\n",
      "Current iteration=2000, loss=1491.1868340527346\n",
      "Current iteration=3000, loss=1487.3243107333308\n",
      "Current iteration=4000, loss=1486.3508290861948\n",
      "Current iteration=5000, loss=1486.0354668546888\n",
      "Current iteration=6000, loss=1485.8926006290224\n",
      "Current iteration=7000, loss=1485.8022767328205\n",
      "Current iteration=8000, loss=1485.7319582047928\n",
      "Current iteration=9000, loss=1485.6716264493186\n",
      "Current iteration=0, loss=38803.76546210686\n",
      "Current iteration=1000, loss=35022.90413015993\n",
      "Current iteration=2000, loss=34857.33337950706\n",
      "Current iteration=3000, loss=34812.6373759742\n",
      "Current iteration=4000, loss=34790.417750192566\n",
      "Current iteration=5000, loss=34773.45875234425\n",
      "Current iteration=6000, loss=34757.91833604011\n",
      "Current iteration=7000, loss=34742.895704833085\n",
      "Current iteration=8000, loss=34728.17475366688\n",
      "Current iteration=9000, loss=34713.70029588803\n",
      "Current iteration=0, loss=1655.2354671771493\n",
      "Current iteration=1000, loss=712.8840889564807\n",
      "Current iteration=2000, loss=691.4961595812878\n",
      "Current iteration=3000, loss=679.1012417860933\n",
      "Current iteration=4000, loss=670.3470294861003\n",
      "Current iteration=5000, loss=663.8604253672398\n",
      "Current iteration=6000, loss=658.9602761330088\n",
      "Current iteration=7000, loss=655.2114777240499\n",
      "Current iteration=8000, loss=652.3124988177409\n",
      "Current iteration=9000, loss=650.0486360297003\n",
      "Current iteration=0, loss=26301.469766347123\n",
      "Current iteration=1000, loss=21121.343871338744\n",
      "Current iteration=2000, loss=21065.78311266629\n",
      "Current iteration=3000, loss=21047.133594055522\n",
      "Current iteration=4000, loss=21038.56957812285\n",
      "Current iteration=5000, loss=21033.722483385463\n",
      "Current iteration=6000, loss=21030.436126215176\n",
      "Current iteration=7000, loss=21027.865265215925\n",
      "Current iteration=8000, loss=21025.6453648062\n",
      "Current iteration=9000, loss=21023.607829722194\n",
      "Current iteration=0, loss=829.6971751302544\n",
      "Current iteration=1000, loss=277.2700708918913\n",
      "Current iteration=2000, loss=269.740894241617\n",
      "Current iteration=3000, loss=266.71704575992817\n",
      "Current iteration=4000, loss=264.8317522694682\n",
      "Current iteration=5000, loss=263.3332694038534\n",
      "Current iteration=6000, loss=262.0402974880975\n",
      "Current iteration=7000, loss=260.89462047479196\n",
      "Current iteration=8000, loss=259.86908175731486\n",
      "Current iteration=9000, loss=258.94676513248896\n",
      "Current iteration=0, loss=11479.903604433814\n",
      "Current iteration=1000, loss=9376.92681671738\n",
      "Current iteration=2000, loss=9360.022309419623\n",
      "Current iteration=3000, loss=9356.250200030712\n",
      "Current iteration=4000, loss=9354.424773527062\n",
      "Current iteration=5000, loss=9353.24456470238\n",
      "Current iteration=6000, loss=9352.387893679053\n",
      "Current iteration=7000, loss=9351.71698632975\n",
      "Current iteration=8000, loss=9351.157534124077\n",
      "Current iteration=9000, loss=9350.66644261706\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-bc158582d0f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_penalized_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_penalized_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w,logistic=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7516\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing([], tX_test, test=True) #same function as train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, list_w, logistic=True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "hw1_ADA",
   "language": "python",
   "name": "hw1_ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
