{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from costs import *\n",
    "from lab_helpers import *\n",
    "from plots_lab4 import *\n",
    "from my_helpers import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do Graphs\n",
    "- PCA Decomposition + Classification\n",
    "- Correlation\n",
    "- Changing loss for different algorithms\n",
    "- ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do tasks\n",
    "- Manuel: implement K-fold to choose degree, change split data , deal with outliers, adding log colm (?)\n",
    "- Gabbo: ?\n",
    "- Marco: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../data/train.csv'\n",
    "y_or, tX_or, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_or\n",
    "tX = tX_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for logistic\n",
    "y = y_for_logistic(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_tot, x_te_tot, y_tr_tot, y_te_tot = split_data(y,tX,0.8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manuel\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "y_tr, tX_tr, indexes_tr, means_tr, std_tr = preprocessing(y_tr_tot, x_tr_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te, tX_te, indexes_te, means_te, std_te = preprocessing(y_te_tot, x_te_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    first = np.sum(np.log(1+np.exp(tx.dot(w))))\n",
    "    second = y.T.dot(tx.dot(w))\n",
    "    return first - second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\"\"\"\n",
    "    return tx.T.dot(sigmoid(tx.dot(w))-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression.\n",
    "    Return the loss and the updated w.\n",
    "    \"\"\"\n",
    "    loss = calculate_loss(y, tx, w)\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    new_w = w - gamma * grad\n",
    "    new_loss = calculate_loss(y, tx, new_w)\n",
    "    if new_loss <= loss:\n",
    "        loss , w = new_loss, new_w\n",
    "        gamma *=2\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.00001\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=9713.564627367217\n",
      "Current iteration=1000, loss=3598.244479355567\n",
      "Current iteration=2000, loss=3588.0652021514547\n",
      "Current iteration=3000, loss=3585.067063205915\n",
      "Current iteration=4000, loss=3583.1956217387524\n",
      "Current iteration=5000, loss=3581.688429287375\n",
      "Current iteration=6000, loss=3580.3920323164257\n",
      "Current iteration=7000, loss=3579.2580002973227\n",
      "Current iteration=8000, loss=3578.2602771104357\n",
      "Current iteration=9000, loss=3577.3794429350373\n",
      "Current iteration=0, loss=68304.08975300708\n",
      "Current iteration=1000, loss=29060.060617504856\n",
      "Current iteration=2000, loss=28949.45433270966\n",
      "Current iteration=3000, loss=28922.961199817397\n",
      "Current iteration=4000, loss=28906.169958351576\n",
      "Current iteration=5000, loss=28891.74603589508\n",
      "Current iteration=6000, loss=28878.641286660728\n",
      "Current iteration=7000, loss=28866.521418362696\n",
      "Current iteration=8000, loss=28855.178621915184\n",
      "Current iteration=9000, loss=28844.457470306326\n",
      "Current iteration=0, loss=4053.5660788639666\n",
      "Current iteration=1000, loss=1533.431859503074\n",
      "Current iteration=2000, loss=1501.876623478683\n",
      "Current iteration=3000, loss=1493.2696492749078\n",
      "Current iteration=4000, loss=1489.7744633953635\n",
      "Current iteration=5000, loss=1487.870248666898\n",
      "Current iteration=6000, loss=1486.6093165526252\n",
      "Current iteration=7000, loss=1485.6867163710217\n",
      "Current iteration=8000, loss=1484.981488258655\n",
      "Current iteration=9000, loss=1484.4322002652646\n",
      "Current iteration=0, loss=59676.855599822506\n",
      "Current iteration=0, loss=1711.6710457863912\n",
      "Current iteration=1000, loss=742.0883943040546\n",
      "Current iteration=2000, loss=704.6303599417963\n",
      "Current iteration=3000, loss=685.0567361464664\n",
      "Current iteration=4000, loss=672.9680370499495\n",
      "Current iteration=5000, loss=664.8376492355364\n",
      "Current iteration=6000, loss=659.1980271851564\n",
      "Current iteration=7000, loss=655.2121107275028\n",
      "Current iteration=8000, loss=652.3325294085364\n",
      "Current iteration=9000, loss=650.1992010698459\n",
      "Current iteration=0, loss=60210.38756463796\n",
      "Current iteration=1000, loss=21110.250395422976\n",
      "Current iteration=2000, loss=21076.167111878167\n",
      "Current iteration=3000, loss=21065.175124666595\n",
      "Current iteration=4000, loss=21059.641162999353\n",
      "Current iteration=5000, loss=21056.09155433216\n",
      "Current iteration=6000, loss=21053.366046118674\n",
      "Current iteration=7000, loss=21051.015179829483\n",
      "Current iteration=8000, loss=21048.848762919974\n",
      "Current iteration=9000, loss=21046.780530881093\n",
      "Current iteration=0, loss=672.4064103720339\n",
      "Current iteration=1000, loss=290.758645961482\n",
      "Current iteration=2000, loss=282.27460544884957\n",
      "Current iteration=3000, loss=277.8907560265848\n",
      "Current iteration=4000, loss=274.66839072637634\n",
      "Current iteration=5000, loss=272.0689961364204\n",
      "Current iteration=6000, loss=269.8939565871066\n",
      "Current iteration=7000, loss=268.0448298907208\n",
      "Current iteration=8000, loss=266.45773835371676\n",
      "Current iteration=9000, loss=265.084662998981\n",
      "Current iteration=0, loss=15243.552032783738\n",
      "Current iteration=1000, loss=9430.424140499308\n",
      "Current iteration=2000, loss=9394.284926385546\n",
      "Current iteration=3000, loss=9381.337181715968\n",
      "Current iteration=4000, loss=9373.852927759875\n",
      "Current iteration=5000, loss=9369.103502846201\n",
      "Current iteration=6000, loss=9365.98151217543\n",
      "Current iteration=7000, loss=9363.857706920575\n",
      "Current iteration=8000, loss=9362.350075382246\n",
      "Current iteration=9000, loss=9361.224255159696\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69218\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_sub_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 3000\n",
    "    threshold = 1e-8\n",
    "    gamma = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.ones((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_gradient_descent(yn, xn, w, gamma)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    #print(\"loss={l}\".format(l=calculate_loss(y, tx, w)))\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.13529661680985303\n",
      "Current iteration=100, loss=0.006619917537084934\n",
      "Current iteration=200, loss=0.006474145952022971\n",
      "Current iteration=300, loss=0.04584552160523338\n",
      "Current iteration=400, loss=0.02196314570109435\n",
      "Current iteration=500, loss=0.0010193212563617598\n",
      "Current iteration=600, loss=4.5593942843424884e-08\n",
      "Current iteration=700, loss=0.006477341750429219\n",
      "Current iteration=800, loss=0.03187686532075339\n",
      "Current iteration=900, loss=0.0009545005459552203\n",
      "Current iteration=1000, loss=0.03497321334828652\n",
      "Current iteration=1100, loss=0.00670726426427068\n",
      "Current iteration=1200, loss=0.0036648902550479997\n",
      "Current iteration=1300, loss=0.002549692532486056\n",
      "Current iteration=1400, loss=0.13098449083227326\n",
      "Current iteration=1500, loss=0.001784941310865318\n",
      "Current iteration=1600, loss=0.027322230317328042\n",
      "Current iteration=1700, loss=0.0024520583874172775\n",
      "Current iteration=1800, loss=0.1972891090069856\n",
      "Current iteration=1900, loss=0.008183409894541912\n",
      "Current iteration=2000, loss=0.08912346310741993\n",
      "Current iteration=2100, loss=0.01776600948372882\n",
      "Current iteration=2200, loss=0.007130470796913223\n",
      "Current iteration=2300, loss=0.011308634966877338\n",
      "Current iteration=2400, loss=0.002938586953851202\n",
      "Current iteration=2500, loss=0.024348964086685115\n",
      "Current iteration=2600, loss=0.014951964122438882\n",
      "Current iteration=2700, loss=0.05332827978324441\n",
      "Current iteration=2800, loss=0.01748057125907841\n",
      "Current iteration=2900, loss=0.016785346535944278\n",
      "Current iteration=0, loss=0.05019441778325939\n",
      "Current iteration=100, loss=0.03477293736870012\n",
      "Current iteration=200, loss=0.13646496650267406\n",
      "Current iteration=300, loss=0.10395808551074807\n",
      "Current iteration=400, loss=0.09549625014125951\n",
      "Current iteration=500, loss=0.5295685571595005\n",
      "Current iteration=600, loss=0.4386033838638915\n",
      "Current iteration=700, loss=0.7289631170087331\n",
      "Current iteration=800, loss=1.1198934942764958\n",
      "Current iteration=900, loss=0.4239835371862703\n",
      "Current iteration=1000, loss=0.01670229886361955\n",
      "Current iteration=1100, loss=0.14079322128480953\n",
      "Current iteration=1200, loss=1.0306683063102104\n",
      "Current iteration=1300, loss=1.1158842369982216\n",
      "Current iteration=1400, loss=0.07300190955870062\n",
      "Current iteration=1500, loss=0.22897596807268905\n",
      "Current iteration=1600, loss=0.7633999186701627\n",
      "Current iteration=1700, loss=0.520625051766742\n",
      "Current iteration=1800, loss=0.2750553675205888\n",
      "Current iteration=1900, loss=0.2949663080583693\n",
      "Current iteration=2000, loss=0.08986404585726838\n",
      "Current iteration=2100, loss=0.09752990230312121\n",
      "Current iteration=2200, loss=0.7032391149348621\n",
      "Current iteration=2300, loss=0.17617873273636378\n",
      "Current iteration=2400, loss=0.0002216866720168019\n",
      "Current iteration=2500, loss=0.9208106060654747\n",
      "Current iteration=2600, loss=0.6010936758440921\n",
      "Current iteration=2700, loss=0.07722427233159035\n",
      "Current iteration=2800, loss=0.34224679585637136\n",
      "Current iteration=2900, loss=0.026581194643121985\n",
      "Current iteration=0, loss=7.2612418026258005\n",
      "Current iteration=100, loss=0.0017756761302759616\n",
      "Current iteration=200, loss=0.033159560405060025\n",
      "Current iteration=300, loss=0.2549801103215111\n",
      "Current iteration=400, loss=0.01263557922717138\n",
      "Current iteration=500, loss=1.4993498058503283\n",
      "Current iteration=600, loss=0.02070428958457445\n",
      "Current iteration=700, loss=0.0041359613550073955\n",
      "Current iteration=800, loss=0.07652762207914376\n",
      "Current iteration=900, loss=0.07471008317130709\n",
      "Current iteration=1000, loss=0.019954407056879266\n",
      "Current iteration=1100, loss=0.03331685405720732\n",
      "Current iteration=1200, loss=0.0662975978491371\n",
      "Current iteration=1300, loss=0.01615946462422826\n",
      "Current iteration=1400, loss=0.024314672217061455\n",
      "Current iteration=1500, loss=0.0795388246546616\n",
      "Current iteration=1600, loss=0.0005956206659885137\n",
      "Current iteration=1700, loss=1.7852605755209168\n",
      "Current iteration=1800, loss=0.016519145617110407\n",
      "Current iteration=1900, loss=0.008985222843481796\n",
      "Current iteration=2000, loss=0.005979076929624782\n",
      "Current iteration=2100, loss=0.03501525044541258\n",
      "Current iteration=2200, loss=0.06043759604841721\n",
      "Current iteration=2300, loss=0.008155495210901049\n",
      "Current iteration=2400, loss=0.01680905986033263\n",
      "Current iteration=2500, loss=0.077791053929951\n",
      "Current iteration=2600, loss=0.013779862070782486\n",
      "Current iteration=2700, loss=0.03969310790906923\n",
      "Current iteration=2800, loss=0.0007434733560302406\n",
      "Current iteration=2900, loss=0.006534126907423533\n",
      "Current iteration=0, loss=0.31304282712256076\n",
      "Current iteration=100, loss=0.7100565425490849\n",
      "Current iteration=200, loss=0.1894139613243473\n",
      "Current iteration=300, loss=0.7385685817868431\n",
      "Current iteration=400, loss=0.02056745413968675\n",
      "Current iteration=500, loss=0.6399710350596542\n",
      "Current iteration=600, loss=1.274722121103611\n",
      "Current iteration=700, loss=0.18809690082432445\n",
      "Current iteration=800, loss=0.2265192754381271\n",
      "Current iteration=900, loss=0.7083000827603259\n",
      "Current iteration=1000, loss=0.6285899622094042\n",
      "Current iteration=1100, loss=0.18476322711963003\n",
      "Current iteration=1200, loss=0.05420753423823165\n",
      "Current iteration=1300, loss=0.1543564236025805\n",
      "Current iteration=1400, loss=1.4515714589651565\n",
      "Current iteration=1500, loss=0.07254372739218251\n",
      "Current iteration=1600, loss=0.08893304007493219\n",
      "Current iteration=1700, loss=0.21369077819881932\n",
      "Current iteration=1800, loss=0.1747051782301432\n",
      "Current iteration=1900, loss=0.16833509885436682\n",
      "Current iteration=2000, loss=0.8096032843886657\n",
      "Current iteration=2100, loss=0.6033890239626414\n",
      "Current iteration=2200, loss=0.227653152963187\n",
      "Current iteration=2300, loss=0.036281315453175395\n",
      "Current iteration=2400, loss=0.3783543359525108\n",
      "Current iteration=2500, loss=0.09004801699765413\n",
      "Current iteration=2600, loss=0.2723487494089294\n",
      "Current iteration=2700, loss=0.07437202071129564\n",
      "Current iteration=2800, loss=0.09989519435918304\n",
      "Current iteration=2900, loss=0.869752750568884\n",
      "Current iteration=0, loss=0.04747052095912352\n",
      "Current iteration=100, loss=0.1290284974324485\n",
      "Current iteration=200, loss=0.011292252675489351\n",
      "Current iteration=300, loss=0.01419804318274152\n",
      "Current iteration=400, loss=0.035939257881105746\n",
      "Current iteration=500, loss=0.022469059167431474\n",
      "Current iteration=600, loss=0.009095771012479466\n",
      "Current iteration=700, loss=0.09964133028354819\n",
      "Current iteration=800, loss=1.8571648009235366\n",
      "Current iteration=900, loss=0.1025875974415339\n",
      "Current iteration=1000, loss=0.028405779010155976\n",
      "Current iteration=1100, loss=0.008445467749300967\n",
      "Current iteration=1200, loss=0.24480339755494285\n",
      "Current iteration=1300, loss=0.04211769613361532\n",
      "Current iteration=1400, loss=0.00045515323589338493\n",
      "Current iteration=1500, loss=0.015589415250958801\n",
      "Current iteration=1600, loss=0.2830229553930821\n",
      "Current iteration=1700, loss=2.2843441279243124\n",
      "Current iteration=1800, loss=0.10162750374527994\n",
      "Current iteration=1900, loss=0.08547535624996527\n",
      "Current iteration=2000, loss=0.06200693843462002\n",
      "Current iteration=2100, loss=0.13705334767824118\n",
      "Current iteration=2200, loss=2.4733400877889857\n",
      "Current iteration=2300, loss=0.002959490132393074\n",
      "Current iteration=2400, loss=0.17933149529470915\n",
      "Current iteration=2500, loss=0.0008222502916229057\n",
      "Current iteration=2600, loss=0.0006060330827420754\n",
      "Current iteration=2700, loss=0.027542537389259745\n",
      "Current iteration=2800, loss=0.07390715965703931\n",
      "Current iteration=2900, loss=0.0027648351781273638\n",
      "Current iteration=0, loss=0.585586081053784\n",
      "Current iteration=100, loss=2.273530848045819\n",
      "Current iteration=200, loss=0.8381914198558148\n",
      "Current iteration=300, loss=0.3528298699614418\n",
      "Current iteration=400, loss=0.3860510219975084\n",
      "Current iteration=500, loss=0.21020676137868355\n",
      "Current iteration=600, loss=0.1447976103266544\n",
      "Current iteration=700, loss=0.3041873863701671\n",
      "Current iteration=800, loss=0.77488052104436\n",
      "Current iteration=900, loss=0.2551421118879589\n",
      "Current iteration=1000, loss=0.12748626092900064\n",
      "Current iteration=1100, loss=0.27945106812820386\n",
      "Current iteration=1200, loss=0.7330864708747485\n",
      "Current iteration=1300, loss=0.7887292328528996\n",
      "Current iteration=1400, loss=0.00943399071824036\n",
      "Current iteration=1500, loss=0.35218452116221155\n",
      "Current iteration=1600, loss=0.023980431265927304\n",
      "Current iteration=1700, loss=0.37777410086429736\n",
      "Current iteration=1800, loss=0.6042115910020207\n",
      "Current iteration=1900, loss=0.4184831951161798\n",
      "Current iteration=2000, loss=0.8097435335451356\n",
      "Current iteration=2100, loss=0.009120009757737968\n",
      "Current iteration=2200, loss=0.1538330663755052\n",
      "Current iteration=2300, loss=0.5076209163816437\n",
      "Current iteration=2400, loss=0.24710271875872603\n",
      "Current iteration=2500, loss=0.07525080766283399\n",
      "Current iteration=2600, loss=1.4292373624187362\n",
      "Current iteration=2700, loss=0.3854278241934965\n",
      "Current iteration=2800, loss=0.8167243430296238\n",
      "Current iteration=2900, loss=0.9445020703579424\n",
      "Current iteration=0, loss=0.009777282845723437\n",
      "Current iteration=100, loss=0.015546488081518975\n",
      "Current iteration=200, loss=0.001847785138261447\n",
      "Current iteration=300, loss=0.00021490289833522283\n",
      "Current iteration=400, loss=0.0005092382845622301\n",
      "Current iteration=500, loss=0.001591787194154684\n",
      "Current iteration=600, loss=0.5090718104559646\n",
      "Current iteration=700, loss=0.006203953207534367\n",
      "Current iteration=800, loss=0.17008431474600544\n",
      "Current iteration=900, loss=0.00336560818169643\n",
      "Current iteration=1000, loss=0.035516831900465055\n",
      "Current iteration=1100, loss=0.1904049264678693\n",
      "Current iteration=1200, loss=0.16116954509528725\n",
      "Current iteration=1300, loss=0.04482103129448288\n",
      "Current iteration=1400, loss=0.026153293192836706\n",
      "Current iteration=1500, loss=0.019572216509841412\n",
      "Current iteration=1600, loss=0.03914166706114664\n",
      "Current iteration=1700, loss=0.06775538726096968\n",
      "Current iteration=1800, loss=0.010106395094688476\n",
      "Current iteration=1900, loss=0.002247356540477799\n",
      "Current iteration=2000, loss=7.2941081383606106e-06\n",
      "Current iteration=2100, loss=0.03049036472002321\n",
      "Current iteration=2200, loss=0.01432532613696613\n",
      "Current iteration=2300, loss=2.133214530783632\n",
      "Current iteration=2400, loss=0.09077050586677311\n",
      "Current iteration=2500, loss=1.1124508795057442\n",
      "Current iteration=2600, loss=0.06649325529646327\n",
      "Current iteration=2700, loss=0.7451480256999542\n",
      "Current iteration=2800, loss=0.07182599973256974\n",
      "Current iteration=2900, loss=0.005302032208565951\n",
      "Current iteration=0, loss=0.14043966893292303\n",
      "Current iteration=100, loss=0.0035152765238270693\n",
      "Current iteration=200, loss=0.08392101242928818\n",
      "Current iteration=300, loss=0.08310654667872307\n",
      "Current iteration=400, loss=0.1274413001551754\n",
      "Current iteration=500, loss=0.893933022225802\n",
      "Current iteration=600, loss=0.20194099961394807\n",
      "Current iteration=700, loss=0.006328189077647671\n",
      "Current iteration=800, loss=0.7893378285347099\n",
      "Current iteration=900, loss=0.009849078052315422\n",
      "Current iteration=1000, loss=0.05164555060264273\n",
      "Current iteration=1100, loss=0.1936052385291216\n",
      "Current iteration=1200, loss=7.917824868533604e-05\n",
      "Current iteration=1300, loss=0.022814237460021758\n",
      "Current iteration=1400, loss=0.44383561807983796\n",
      "Current iteration=1500, loss=0.023803402329742932\n",
      "Current iteration=1600, loss=0.38187146355969503\n",
      "Current iteration=1700, loss=0.06582119696358536\n",
      "Current iteration=1800, loss=0.2629744919600021\n",
      "Current iteration=1900, loss=0.7847709477820106\n",
      "Current iteration=2000, loss=0.02903867213895106\n",
      "Current iteration=2100, loss=0.06679832402806798\n",
      "Current iteration=2200, loss=0.5239929774799432\n",
      "Current iteration=2300, loss=0.09678800614178282\n",
      "Current iteration=2400, loss=0.16923623430663687\n",
      "Current iteration=2500, loss=0.2858354120437832\n",
      "Current iteration=2600, loss=0.05980532384923058\n",
      "Current iteration=2700, loss=1.2342915477119423\n",
      "Current iteration=2800, loss=0.4364734294110856\n",
      "Current iteration=2900, loss=0.19200044972611685\n"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_sub_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72424\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the hessian of the loss function.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    S = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        sigma = sigmoid(tx[i,:].T.dot(w))\n",
    "        S[i,i] = sigma*(1-sigma)\n",
    "    return tx.T.dot(S).dot(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    return calculate_loss(y, tx, w), calculate_gradient(y, tx, w), calculate_hessian(y, tx, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_newton_method(y, tx, w):\n",
    "    \"\"\"\n",
    "    Do one step on Newton's method.\n",
    "    return the loss and updated w.\n",
    "    \"\"\"\n",
    "    # mettiamo il gamma???????\n",
    "    gamma = 0.001\n",
    "    loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    w = w - gamma * np.linalg.inv(hessian).dot(gradient)\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_newton_method(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 100\n",
    "    threshold = 1e-8\n",
    "    lambda_ = 0.1\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.random.randn(tx.shape[1])\n",
    "    \n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        for yn, xn in batch_iter(y, tx, 1):\n",
    "            loss, w = learning_by_newton_method(yn, xn, w)\n",
    "            #print (xn.shape)\n",
    "            #print(calculate_hessian(yn, xn, w).shape)\n",
    "        # log info\n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=22.732827798849165\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-63af591f0cde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-fac930ddf20f>\u001b[0m in \u001b[0;36mlogistic_regression_newton_method\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# get loss and update w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0myn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearning_by_newton_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m             \u001b[1;31m#print (xn.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#print(calculate_hessian(yn, xn, w).shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-34d0d7dd8dc9>\u001b[0m in \u001b[0;36mlearning_by_newton_method\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhessian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_newton_method(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss, gradient, and hessian.\"\"\"\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    loss = calculate_loss(y, tx, w) + lambda_ / 2 * np.linalg.norm(w)\n",
    "    gradient = calculate_gradient(y, tx, w) + lambda_ * w\n",
    "    #hessian = calculate_hessian(y, tx, w) + N * np.identity(D)\n",
    "    #return loss, gradient, hessian\n",
    "    return loss, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "    \"\"\"\n",
    "    #loss, gradient, hessian = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma * gradient\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_penalized_gradient_descent(y, x):\n",
    "    # init parameters\n",
    "    max_iter = 10000\n",
    "    gamma = 0.00001\n",
    "    lambda_ = 0.1\n",
    "    threshold = 1e-8\n",
    "    losses = []\n",
    "\n",
    "    # build tx\n",
    "    #tx = np.c_[np.ones((y.shape[0],)), x]\n",
    "    tx = x\n",
    "    w = np.zeros((tx.shape[1],))\n",
    "\n",
    "    # start the logistic regression\n",
    "    for iter in range(max_iter):\n",
    "        # get loss and update w.\n",
    "        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "        # log info\n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, loss={l}\".format(i=iter, l=loss))\n",
    "        # converge criterion\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:\n",
    "            break\n",
    "    return loss, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=14466.674805466619\n",
      "Current iteration=1000, loss=3596.1595805480833\n",
      "Current iteration=2000, loss=3590.400230578218\n",
      "Current iteration=3000, loss=3588.004053035492\n",
      "Current iteration=4000, loss=3586.0540170462436\n",
      "Current iteration=5000, loss=3584.374003250526\n",
      "Current iteration=6000, loss=3582.911314632176\n",
      "Current iteration=7000, loss=3581.6332421478874\n",
      "Current iteration=8000, loss=3580.51377168331\n",
      "Current iteration=9000, loss=3579.5308840230578\n",
      "Current iteration=0, loss=40931.72730642589\n",
      "Current iteration=1000, loss=28898.054993920876\n",
      "Current iteration=2000, loss=28841.118146807963\n",
      "Current iteration=3000, loss=28827.039627332924\n",
      "Current iteration=4000, loss=28817.220521561856\n",
      "Current iteration=5000, loss=28808.083030179223\n",
      "Current iteration=6000, loss=28799.214736122674\n",
      "Current iteration=7000, loss=28790.539796435227\n",
      "Current iteration=8000, loss=28782.025698634723\n",
      "Current iteration=9000, loss=28773.650047570416\n",
      "Current iteration=0, loss=4160.962524901352\n",
      "Current iteration=1000, loss=1511.6737722597936\n",
      "Current iteration=2000, loss=1491.1868340527346\n",
      "Current iteration=3000, loss=1487.3243107333308\n",
      "Current iteration=4000, loss=1486.3508290861948\n",
      "Current iteration=5000, loss=1486.0354668546888\n",
      "Current iteration=6000, loss=1485.8926006290224\n",
      "Current iteration=7000, loss=1485.8022767328205\n",
      "Current iteration=8000, loss=1485.7319582047928\n",
      "Current iteration=9000, loss=1485.6716264493186\n",
      "Current iteration=0, loss=38803.76546210686\n",
      "Current iteration=1000, loss=35022.90413015993\n",
      "Current iteration=2000, loss=34857.33337950706\n",
      "Current iteration=3000, loss=34812.6373759742\n",
      "Current iteration=4000, loss=34790.417750192566\n",
      "Current iteration=5000, loss=34773.45875234425\n",
      "Current iteration=6000, loss=34757.91833604011\n",
      "Current iteration=7000, loss=34742.895704833085\n",
      "Current iteration=8000, loss=34728.17475366688\n",
      "Current iteration=9000, loss=34713.70029588803\n",
      "Current iteration=0, loss=1655.2354671771493\n",
      "Current iteration=1000, loss=712.8840889564807\n",
      "Current iteration=2000, loss=691.4961595812878\n",
      "Current iteration=3000, loss=679.1012417860933\n",
      "Current iteration=4000, loss=670.3470294861003\n",
      "Current iteration=5000, loss=663.8604253672398\n",
      "Current iteration=6000, loss=658.9602761330088\n",
      "Current iteration=7000, loss=655.2114777240499\n",
      "Current iteration=8000, loss=652.3124988177409\n",
      "Current iteration=9000, loss=650.0486360297003\n",
      "Current iteration=0, loss=26301.469766347123\n",
      "Current iteration=1000, loss=21121.343871338744\n",
      "Current iteration=2000, loss=21065.78311266629\n",
      "Current iteration=3000, loss=21047.133594055522\n",
      "Current iteration=4000, loss=21038.56957812285\n",
      "Current iteration=5000, loss=21033.722483385463\n",
      "Current iteration=6000, loss=21030.436126215176\n",
      "Current iteration=7000, loss=21027.865265215925\n",
      "Current iteration=8000, loss=21025.6453648062\n",
      "Current iteration=9000, loss=21023.607829722194\n",
      "Current iteration=0, loss=829.6971751302544\n",
      "Current iteration=1000, loss=277.2700708918913\n",
      "Current iteration=2000, loss=269.740894241617\n",
      "Current iteration=3000, loss=266.71704575992817\n",
      "Current iteration=4000, loss=264.8317522694682\n",
      "Current iteration=5000, loss=263.3332694038534\n",
      "Current iteration=6000, loss=262.0402974880975\n",
      "Current iteration=7000, loss=260.89462047479196\n",
      "Current iteration=8000, loss=259.86908175731486\n",
      "Current iteration=9000, loss=258.94676513248896\n",
      "Current iteration=0, loss=11479.903604433814\n",
      "Current iteration=1000, loss=9376.92681671738\n",
      "Current iteration=2000, loss=9360.022309419623\n",
      "Current iteration=3000, loss=9356.250200030712\n",
      "Current iteration=4000, loss=9354.424773527062\n",
      "Current iteration=5000, loss=9353.24456470238\n",
      "Current iteration=6000, loss=9352.387893679053\n",
      "Current iteration=7000, loss=9351.71698632975\n",
      "Current iteration=8000, loss=9351.157534124077\n",
      "Current iteration=9000, loss=9350.66644261706\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-bc158582d0f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlist_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mjet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_penalized_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjet\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mlist_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlist_w\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "list_loss = []\n",
    "list_w = []\n",
    "for jet in range(0,8):\n",
    "    loss, w = logistic_regression_penalized_gradient_descent(y_tr[jet], tX_tr[jet])\n",
    "    list_loss.append(loss)\n",
    "    list_w.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7504\n"
     ]
    }
   ],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w, logistic=True) \n",
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Accuracy and F1 Score on our classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred = build_predictions(tX_te, indexes_te, list_w,logistic=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7516\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(y_te_tot, y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_test_new, indexes_test_new, means_test, stds_test = preprocessing([], tX_test, test=True) #same function as train,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell for logistic\n",
    "y_pred_test = build_predictions(tX_test_new, indexes_test_new, list_w, logistic=True)\n",
    "OUTPUT_PATH = 'submission.csv'\n",
    "create_csv_submission(ids_test, y_pred_test, OUTPUT_PATH, logistic=True) #logistic flags convert [0, 1] to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tX_test_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-794235a07782>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprincipalComponents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX_test_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprincipalComponents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tX_test_s' is not defined"
     ]
    }
   ],
   "source": [
    "#we will need something like this\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(tX_test_s)\n",
    "plt.scatter(principalComponents[:,0], principalComponents[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
